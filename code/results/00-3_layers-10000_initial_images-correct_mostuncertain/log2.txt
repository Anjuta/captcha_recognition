I1026 00:27:46.686631 29273 convert_imageset.cpp:70] Shuffling data
I1026 00:27:47.431223 29273 convert_imageset.cpp:73] A total of 60000 images.
I1026 00:27:47.431303 29273 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1026 00:27:49.468374 29273 convert_imageset.cpp:177] Processed 1000 files.
E1026 00:27:51.117151 29273 convert_imageset.cpp:177] Processed 2000 files.
E1026 00:27:52.882869 29273 convert_imageset.cpp:177] Processed 3000 files.
E1026 00:27:54.558425 29273 convert_imageset.cpp:177] Processed 4000 files.
E1026 00:27:56.257643 29273 convert_imageset.cpp:177] Processed 5000 files.
E1026 00:27:57.920478 29273 convert_imageset.cpp:177] Processed 6000 files.
E1026 00:27:59.690300 29273 convert_imageset.cpp:177] Processed 7000 files.
E1026 00:28:01.599767 29273 convert_imageset.cpp:177] Processed 8000 files.
E1026 00:28:03.452625 29273 convert_imageset.cpp:177] Processed 9000 files.
E1026 00:28:04.918802 29273 convert_imageset.cpp:177] Processed 10000 files.
E1026 00:28:06.648075 29273 convert_imageset.cpp:177] Processed 11000 files.
E1026 00:28:08.270108 29273 convert_imageset.cpp:177] Processed 12000 files.
E1026 00:28:09.810299 29273 convert_imageset.cpp:177] Processed 13000 files.
E1026 00:28:11.491045 29273 convert_imageset.cpp:177] Processed 14000 files.
E1026 00:28:13.348608 29273 convert_imageset.cpp:177] Processed 15000 files.
E1026 00:28:14.916065 29273 convert_imageset.cpp:177] Processed 16000 files.
E1026 00:28:16.563205 29273 convert_imageset.cpp:177] Processed 17000 files.
E1026 00:28:18.307179 29273 convert_imageset.cpp:177] Processed 18000 files.
E1026 00:28:19.809980 29273 convert_imageset.cpp:177] Processed 19000 files.
E1026 00:28:21.444243 29273 convert_imageset.cpp:177] Processed 20000 files.
E1026 00:28:23.064792 29273 convert_imageset.cpp:177] Processed 21000 files.
E1026 00:28:24.854749 29273 convert_imageset.cpp:177] Processed 22000 files.
E1026 00:28:26.687305 29273 convert_imageset.cpp:177] Processed 23000 files.
E1026 00:28:28.268857 29273 convert_imageset.cpp:177] Processed 24000 files.
E1026 00:28:29.994531 29273 convert_imageset.cpp:177] Processed 25000 files.
E1026 00:28:31.582901 29273 convert_imageset.cpp:177] Processed 26000 files.
E1026 00:28:33.249986 29273 convert_imageset.cpp:177] Processed 27000 files.
E1026 00:28:34.801942 29273 convert_imageset.cpp:177] Processed 28000 files.
E1026 00:28:36.378715 29273 convert_imageset.cpp:177] Processed 29000 files.
E1026 00:28:37.968885 29273 convert_imageset.cpp:177] Processed 30000 files.
E1026 00:28:39.646699 29273 convert_imageset.cpp:177] Processed 31000 files.
E1026 00:28:41.349068 29273 convert_imageset.cpp:177] Processed 32000 files.
E1026 00:28:43.008729 29273 convert_imageset.cpp:177] Processed 33000 files.
E1026 00:28:44.672158 29273 convert_imageset.cpp:177] Processed 34000 files.
E1026 00:28:46.370400 29273 convert_imageset.cpp:177] Processed 35000 files.
E1026 00:28:47.912673 29273 convert_imageset.cpp:177] Processed 36000 files.
E1026 00:28:49.359062 29273 convert_imageset.cpp:177] Processed 37000 files.
E1026 00:28:50.906774 29273 convert_imageset.cpp:177] Processed 38000 files.
E1026 00:28:52.407052 29273 convert_imageset.cpp:177] Processed 39000 files.
E1026 00:28:54.263890 29273 convert_imageset.cpp:177] Processed 40000 files.
E1026 00:28:56.006319 29273 convert_imageset.cpp:177] Processed 41000 files.
E1026 00:28:57.580325 29273 convert_imageset.cpp:177] Processed 42000 files.
E1026 00:28:59.231668 29273 convert_imageset.cpp:177] Processed 43000 files.
E1026 00:29:00.842561 29273 convert_imageset.cpp:177] Processed 44000 files.
E1026 00:29:02.383447 29273 convert_imageset.cpp:177] Processed 45000 files.
E1026 00:29:03.945062 29273 convert_imageset.cpp:177] Processed 46000 files.
E1026 00:29:05.509274 29273 convert_imageset.cpp:177] Processed 47000 files.
E1026 00:29:07.037143 29273 convert_imageset.cpp:177] Processed 48000 files.
E1026 00:29:08.659718 29273 convert_imageset.cpp:177] Processed 49000 files.
E1026 00:29:10.206253 29273 convert_imageset.cpp:177] Processed 50000 files.
E1026 00:29:11.673136 29273 convert_imageset.cpp:177] Processed 51000 files.
E1026 00:29:13.144567 29273 convert_imageset.cpp:177] Processed 52000 files.
E1026 00:29:14.701674 29273 convert_imageset.cpp:177] Processed 53000 files.
E1026 00:29:16.183516 29273 convert_imageset.cpp:177] Processed 54000 files.
E1026 00:29:17.710968 29273 convert_imageset.cpp:177] Processed 55000 files.
E1026 00:29:19.152153 29273 convert_imageset.cpp:177] Processed 56000 files.
E1026 00:29:20.627809 29273 convert_imageset.cpp:177] Processed 57000 files.
E1026 00:29:22.253274 29273 convert_imageset.cpp:177] Processed 58000 files.
E1026 00:29:23.892163 29273 convert_imageset.cpp:177] Processed 59000 files.
E1026 00:29:25.506674 29273 convert_imageset.cpp:177] Processed 60000 files.
I1026 00:29:25.765606 29874 caffe.cpp:99] Use GPU with device ID 0
I1026 00:29:26.211874 29874 caffe.cpp:107] Starting Optimization
I1026 00:29:26.211997 29874 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 50000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1026 00:29:26.212023 29874 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1026 00:29:26.214270 29874 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1026 00:29:26.214485 29874 net.cpp:67] Creating Layer mnist
I1026 00:29:26.214512 29874 net.cpp:356] mnist -> data
I1026 00:29:26.214547 29874 net.cpp:356] mnist -> label
I1026 00:29:26.214579 29874 net.cpp:96] Setting up mnist
I1026 00:29:26.221263 29874 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1026 00:29:26.253363 29874 data_layer.cpp:128] output data size: 64,1,50,180
I1026 00:29:26.254345 29874 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1026 00:29:26.254377 29874 net.cpp:103] Top shape: 64 1 1 1 (64)
I1026 00:29:26.254411 29874 net.cpp:67] Creating Layer conv1
I1026 00:29:26.254426 29874 net.cpp:394] conv1 <- data
I1026 00:29:26.254462 29874 net.cpp:356] conv1 -> conv1
I1026 00:29:26.254490 29874 net.cpp:96] Setting up conv1
I1026 00:29:26.255444 29874 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1026 00:29:26.255513 29874 net.cpp:67] Creating Layer pool1
I1026 00:29:26.255530 29874 net.cpp:394] pool1 <- conv1
I1026 00:29:26.255548 29874 net.cpp:356] pool1 -> pool1
I1026 00:29:26.255568 29874 net.cpp:96] Setting up pool1
I1026 00:29:26.255600 29874 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 00:29:26.255625 29874 net.cpp:67] Creating Layer relu1
I1026 00:29:26.255638 29874 net.cpp:394] relu1 <- pool1
I1026 00:29:26.255655 29874 net.cpp:345] relu1 -> pool1 (in-place)
I1026 00:29:26.255671 29874 net.cpp:96] Setting up relu1
I1026 00:29:26.255684 29874 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 00:29:26.255703 29874 net.cpp:67] Creating Layer drop1
I1026 00:29:26.255717 29874 net.cpp:394] drop1 <- pool1
I1026 00:29:26.255736 29874 net.cpp:345] drop1 -> pool1 (in-place)
I1026 00:29:26.255754 29874 net.cpp:96] Setting up drop1
I1026 00:29:26.255769 29874 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 00:29:26.255789 29874 net.cpp:67] Creating Layer conv2
I1026 00:29:26.255800 29874 net.cpp:394] conv2 <- pool1
I1026 00:29:26.255821 29874 net.cpp:356] conv2 -> conv2
I1026 00:29:26.255844 29874 net.cpp:96] Setting up conv2
I1026 00:29:26.257066 29874 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1026 00:29:26.257094 29874 net.cpp:67] Creating Layer pool2
I1026 00:29:26.257103 29874 net.cpp:394] pool2 <- conv2
I1026 00:29:26.257114 29874 net.cpp:356] pool2 -> pool2
I1026 00:29:26.257127 29874 net.cpp:96] Setting up pool2
I1026 00:29:26.257135 29874 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 00:29:26.257145 29874 net.cpp:67] Creating Layer relu2
I1026 00:29:26.257153 29874 net.cpp:394] relu2 <- pool2
I1026 00:29:26.257165 29874 net.cpp:345] relu2 -> pool2 (in-place)
I1026 00:29:26.257176 29874 net.cpp:96] Setting up relu2
I1026 00:29:26.257184 29874 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 00:29:26.257195 29874 net.cpp:67] Creating Layer drop2
I1026 00:29:26.257203 29874 net.cpp:394] drop2 <- pool2
I1026 00:29:26.257213 29874 net.cpp:345] drop2 -> pool2 (in-place)
I1026 00:29:26.257223 29874 net.cpp:96] Setting up drop2
I1026 00:29:26.257232 29874 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 00:29:26.257246 29874 net.cpp:67] Creating Layer conv3
I1026 00:29:26.257254 29874 net.cpp:394] conv3 <- pool2
I1026 00:29:26.257266 29874 net.cpp:356] conv3 -> conv3
I1026 00:29:26.257277 29874 net.cpp:96] Setting up conv3
I1026 00:29:26.259758 29874 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1026 00:29:26.259793 29874 net.cpp:67] Creating Layer pool3
I1026 00:29:26.259801 29874 net.cpp:394] pool3 <- conv3
I1026 00:29:26.259815 29874 net.cpp:356] pool3 -> pool3
I1026 00:29:26.259829 29874 net.cpp:96] Setting up pool3
I1026 00:29:26.259845 29874 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 00:29:26.259857 29874 net.cpp:67] Creating Layer relu3
I1026 00:29:26.259865 29874 net.cpp:394] relu3 <- pool3
I1026 00:29:26.259874 29874 net.cpp:345] relu3 -> pool3 (in-place)
I1026 00:29:26.259884 29874 net.cpp:96] Setting up relu3
I1026 00:29:26.259892 29874 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 00:29:26.259906 29874 net.cpp:67] Creating Layer drop3
I1026 00:29:26.259913 29874 net.cpp:394] drop3 <- pool3
I1026 00:29:26.259923 29874 net.cpp:345] drop3 -> pool3 (in-place)
I1026 00:29:26.259934 29874 net.cpp:96] Setting up drop3
I1026 00:29:26.259943 29874 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 00:29:26.259954 29874 net.cpp:67] Creating Layer ip1
I1026 00:29:26.259961 29874 net.cpp:394] ip1 <- pool3
I1026 00:29:26.259976 29874 net.cpp:356] ip1 -> ip1
I1026 00:29:26.260025 29874 net.cpp:96] Setting up ip1
I1026 00:29:26.725177 29874 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 00:29:26.725239 29874 net.cpp:67] Creating Layer relu4
I1026 00:29:26.725246 29874 net.cpp:394] relu4 <- ip1
I1026 00:29:26.725256 29874 net.cpp:345] relu4 -> ip1 (in-place)
I1026 00:29:26.725266 29874 net.cpp:96] Setting up relu4
I1026 00:29:26.725271 29874 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 00:29:26.725278 29874 net.cpp:67] Creating Layer drop4
I1026 00:29:26.725282 29874 net.cpp:394] drop4 <- ip1
I1026 00:29:26.725288 29874 net.cpp:345] drop4 -> ip1 (in-place)
I1026 00:29:26.725294 29874 net.cpp:96] Setting up drop4
I1026 00:29:26.725301 29874 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 00:29:26.725316 29874 net.cpp:67] Creating Layer ip2
I1026 00:29:26.725320 29874 net.cpp:394] ip2 <- ip1
I1026 00:29:26.725327 29874 net.cpp:356] ip2 -> ip2
I1026 00:29:26.725337 29874 net.cpp:96] Setting up ip2
I1026 00:29:26.733420 29874 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1026 00:29:26.733489 29874 net.cpp:67] Creating Layer loss
I1026 00:29:26.733497 29874 net.cpp:394] loss <- ip2
I1026 00:29:26.733505 29874 net.cpp:394] loss <- label
I1026 00:29:26.733512 29874 net.cpp:356] loss -> loss
I1026 00:29:26.733522 29874 net.cpp:96] Setting up loss
I1026 00:29:26.733536 29874 net.cpp:103] Top shape: 1 1 1 1 (1)
I1026 00:29:26.733541 29874 net.cpp:109]     with loss weight 1
I1026 00:29:26.733584 29874 net.cpp:170] loss needs backward computation.
I1026 00:29:26.733589 29874 net.cpp:170] ip2 needs backward computation.
I1026 00:29:26.733593 29874 net.cpp:170] drop4 needs backward computation.
I1026 00:29:26.733598 29874 net.cpp:170] relu4 needs backward computation.
I1026 00:29:26.733603 29874 net.cpp:170] ip1 needs backward computation.
I1026 00:29:26.733607 29874 net.cpp:170] drop3 needs backward computation.
I1026 00:29:26.733611 29874 net.cpp:170] relu3 needs backward computation.
I1026 00:29:26.733615 29874 net.cpp:170] pool3 needs backward computation.
I1026 00:29:26.733620 29874 net.cpp:170] conv3 needs backward computation.
I1026 00:29:26.733625 29874 net.cpp:170] drop2 needs backward computation.
I1026 00:29:26.733629 29874 net.cpp:170] relu2 needs backward computation.
I1026 00:29:26.733634 29874 net.cpp:170] pool2 needs backward computation.
I1026 00:29:26.733639 29874 net.cpp:170] conv2 needs backward computation.
I1026 00:29:26.733644 29874 net.cpp:170] drop1 needs backward computation.
I1026 00:29:26.733647 29874 net.cpp:170] relu1 needs backward computation.
I1026 00:29:26.733652 29874 net.cpp:170] pool1 needs backward computation.
I1026 00:29:26.733656 29874 net.cpp:170] conv1 needs backward computation.
I1026 00:29:26.733661 29874 net.cpp:172] mnist does not need backward computation.
I1026 00:29:26.733666 29874 net.cpp:208] This network produces output loss
I1026 00:29:26.733676 29874 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 00:29:26.733685 29874 net.cpp:219] Network initialization done.
I1026 00:29:26.733688 29874 net.cpp:220] Memory required for data: 119788292
I1026 00:29:26.733748 29874 solver.cpp:41] Solver scaffolding done.
I1026 00:29:26.733755 29874 solver.cpp:160] Solving Captcha
I1026 00:29:27.388330 29874 solver.cpp:191] Iteration 0, loss = 5.93463
I1026 00:29:27.388389 29874 solver.cpp:206]     Train net output #0: loss = 5.93463 (* 1 = 5.93463 loss)
I1026 00:29:27.388406 29874 solver.cpp:403] Iteration 0, lr = 0.01
I1026 00:37:53.428499 29874 solver.cpp:191] Iteration 1000, loss = 5.00359
I1026 00:37:53.429134 29874 solver.cpp:206]     Train net output #0: loss = 5.00359 (* 1 = 5.00359 loss)
I1026 00:37:53.429168 29874 solver.cpp:403] Iteration 1000, lr = 0.00931012
I1026 00:46:19.244590 29874 solver.cpp:191] Iteration 2000, loss = 4.95355
I1026 00:46:19.245439 29874 solver.cpp:206]     Train net output #0: loss = 4.95355 (* 1 = 4.95355 loss)
I1026 00:46:19.245471 29874 solver.cpp:403] Iteration 2000, lr = 0.00872196
I1026 00:54:47.483010 29874 solver.cpp:191] Iteration 3000, loss = 4.69576
I1026 00:54:47.483855 29874 solver.cpp:206]     Train net output #0: loss = 4.69576 (* 1 = 4.69576 loss)
I1026 00:54:47.483892 29874 solver.cpp:403] Iteration 3000, lr = 0.00821377
I1026 01:03:16.023356 29874 solver.cpp:191] Iteration 4000, loss = 4.46405
I1026 01:03:16.024096 29874 solver.cpp:206]     Train net output #0: loss = 4.46405 (* 1 = 4.46405 loss)
I1026 01:03:16.024128 29874 solver.cpp:403] Iteration 4000, lr = 0.0077697
I1026 01:11:44.857728 29874 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_5000.caffemodel
I1026 01:11:49.293901 29874 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_5000.solverstate
I1026 01:11:53.014431 29874 solver.cpp:191] Iteration 5000, loss = 4.3596
I1026 01:11:53.014925 29874 solver.cpp:206]     Train net output #0: loss = 4.3596 (* 1 = 4.3596 loss)
I1026 01:11:53.014961 29874 solver.cpp:403] Iteration 5000, lr = 0.00737788
I1026 01:20:19.726407 29874 solver.cpp:191] Iteration 6000, loss = 4.17737
I1026 01:20:19.727136 29874 solver.cpp:206]     Train net output #0: loss = 4.17737 (* 1 = 4.17737 loss)
I1026 01:20:19.727170 29874 solver.cpp:403] Iteration 6000, lr = 0.00702927
I1026 01:28:46.092334 29874 solver.cpp:191] Iteration 7000, loss = 4.18965
I1026 01:28:46.093029 29874 solver.cpp:206]     Train net output #0: loss = 4.18965 (* 1 = 4.18965 loss)
I1026 01:28:46.093063 29874 solver.cpp:403] Iteration 7000, lr = 0.00671681
I1026 01:37:11.941165 29874 solver.cpp:191] Iteration 8000, loss = 4.12245
I1026 01:37:11.941709 29874 solver.cpp:206]     Train net output #0: loss = 4.12245 (* 1 = 4.12245 loss)
I1026 01:37:11.941742 29874 solver.cpp:403] Iteration 8000, lr = 0.00643496
I1026 01:45:39.331249 29874 solver.cpp:191] Iteration 9000, loss = 4.00859
I1026 01:45:39.331979 29874 solver.cpp:206]     Train net output #0: loss = 4.00859 (* 1 = 4.00859 loss)
I1026 01:45:39.332012 29874 solver.cpp:403] Iteration 9000, lr = 0.00617924
I1026 01:54:06.646306 29874 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_10000.caffemodel
I1026 01:54:11.171386 29874 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_10000.solverstate
I1026 01:54:15.151006 29874 solver.cpp:191] Iteration 10000, loss = 4.08831
I1026 01:54:15.151512 29874 solver.cpp:206]     Train net output #0: loss = 4.08831 (* 1 = 4.08831 loss)
I1026 01:54:15.151551 29874 solver.cpp:403] Iteration 10000, lr = 0.00594604
I1026 02:02:43.283941 29874 solver.cpp:191] Iteration 11000, loss = 3.85122
I1026 02:02:43.284562 29874 solver.cpp:206]     Train net output #0: loss = 3.85122 (* 1 = 3.85122 loss)
I1026 02:02:43.284595 29874 solver.cpp:403] Iteration 11000, lr = 0.00573239
I1026 02:11:10.945663 29874 solver.cpp:191] Iteration 12000, loss = 3.6395
I1026 02:11:10.946250 29874 solver.cpp:206]     Train net output #0: loss = 3.6395 (* 1 = 3.6395 loss)
I1026 02:11:10.946282 29874 solver.cpp:403] Iteration 12000, lr = 0.00553583
I1026 02:19:37.267364 29874 solver.cpp:191] Iteration 13000, loss = 3.89928
I1026 02:19:37.267915 29874 solver.cpp:206]     Train net output #0: loss = 3.89928 (* 1 = 3.89928 loss)
I1026 02:19:37.267954 29874 solver.cpp:403] Iteration 13000, lr = 0.00535432
I1026 02:28:05.575444 29874 solver.cpp:191] Iteration 14000, loss = 3.82775
I1026 02:28:05.576083 29874 solver.cpp:206]     Train net output #0: loss = 3.82775 (* 1 = 3.82775 loss)
I1026 02:28:05.576117 29874 solver.cpp:403] Iteration 14000, lr = 0.00518611
I1026 02:36:33.682212 29874 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_15000.caffemodel
I1026 02:36:38.118710 29874 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_15000.solverstate
I1026 02:36:42.027804 29874 solver.cpp:191] Iteration 15000, loss = 3.85341
I1026 02:36:42.028355 29874 solver.cpp:206]     Train net output #0: loss = 3.85341 (* 1 = 3.85341 loss)
I1026 02:36:42.028394 29874 solver.cpp:403] Iteration 15000, lr = 0.00502973
I1026 02:45:10.566962 29874 solver.cpp:191] Iteration 16000, loss = 3.64617
I1026 02:45:10.567507 29874 solver.cpp:206]     Train net output #0: loss = 3.64617 (* 1 = 3.64617 loss)
I1026 02:45:10.567544 29874 solver.cpp:403] Iteration 16000, lr = 0.00488394
I1026 02:53:36.639268 29874 solver.cpp:191] Iteration 17000, loss = 3.87458
I1026 02:53:36.640501 29874 solver.cpp:206]     Train net output #0: loss = 3.87458 (* 1 = 3.87458 loss)
I1026 02:53:36.640534 29874 solver.cpp:403] Iteration 17000, lr = 0.00474763
I1026 03:02:02.956593 29874 solver.cpp:191] Iteration 18000, loss = 3.42091
I1026 03:02:02.957211 29874 solver.cpp:206]     Train net output #0: loss = 3.42091 (* 1 = 3.42091 loss)
I1026 03:02:02.957247 29874 solver.cpp:403] Iteration 18000, lr = 0.00461989
I1026 03:10:28.455039 29874 solver.cpp:191] Iteration 19000, loss = 3.49654
I1026 03:10:28.455860 29874 solver.cpp:206]     Train net output #0: loss = 3.49654 (* 1 = 3.49654 loss)
I1026 03:10:28.455893 29874 solver.cpp:403] Iteration 19000, lr = 0.00449989
I1026 03:18:55.687078 29874 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_20000.caffemodel
I1026 03:19:00.484256 29874 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_20000.solverstate
I1026 03:19:04.458447 29874 solver.cpp:191] Iteration 20000, loss = 3.45032
I1026 03:19:04.459162 29874 solver.cpp:206]     Train net output #0: loss = 3.45032 (* 1 = 3.45032 loss)
I1026 03:19:04.459188 29874 solver.cpp:403] Iteration 20000, lr = 0.00438691
I1026 03:27:31.966192 29874 solver.cpp:191] Iteration 21000, loss = 3.51408
I1026 03:27:31.966958 29874 solver.cpp:206]     Train net output #0: loss = 3.51408 (* 1 = 3.51408 loss)
I1026 03:27:31.966990 29874 solver.cpp:403] Iteration 21000, lr = 0.00428034
I1026 03:35:58.999964 29874 solver.cpp:191] Iteration 22000, loss = 3.25561
I1026 03:35:59.000540 29874 solver.cpp:206]     Train net output #0: loss = 3.25561 (* 1 = 3.25561 loss)
I1026 03:35:59.000573 29874 solver.cpp:403] Iteration 22000, lr = 0.00417963
I1026 03:44:25.955423 29874 solver.cpp:191] Iteration 23000, loss = 3.30496
I1026 03:44:25.956229 29874 solver.cpp:206]     Train net output #0: loss = 3.30496 (* 1 = 3.30496 loss)
I1026 03:44:25.956262 29874 solver.cpp:403] Iteration 23000, lr = 0.00408427
I1026 03:52:51.654372 29874 solver.cpp:191] Iteration 24000, loss = 3.38606
I1026 03:52:51.654983 29874 solver.cpp:206]     Train net output #0: loss = 3.38606 (* 1 = 3.38606 loss)
I1026 03:52:51.655015 29874 solver.cpp:403] Iteration 24000, lr = 0.00399384
I1026 04:01:18.131289 29874 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_25000.caffemodel
I1026 04:01:22.485749 29874 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_25000.solverstate
I1026 04:01:26.396734 29874 solver.cpp:191] Iteration 25000, loss = 3.41182
I1026 04:01:26.397408 29874 solver.cpp:206]     Train net output #0: loss = 3.41182 (* 1 = 3.41182 loss)
I1026 04:01:26.397439 29874 solver.cpp:403] Iteration 25000, lr = 0.00390795
I1026 04:09:28.304358 29874 solver.cpp:191] Iteration 26000, loss = 2.9809
I1026 04:09:28.305233 29874 solver.cpp:206]     Train net output #0: loss = 2.9809 (* 1 = 2.9809 loss)
I1026 04:09:28.305266 29874 solver.cpp:403] Iteration 26000, lr = 0.00382625
I1026 04:17:53.820402 29874 solver.cpp:191] Iteration 27000, loss = 3.40589
I1026 04:17:53.820998 29874 solver.cpp:206]     Train net output #0: loss = 3.40589 (* 1 = 3.40589 loss)
I1026 04:17:53.821018 29874 solver.cpp:403] Iteration 27000, lr = 0.00374842
I1026 04:26:19.326598 29874 solver.cpp:191] Iteration 28000, loss = 3.31826
I1026 04:26:19.327474 29874 solver.cpp:206]     Train net output #0: loss = 3.31826 (* 1 = 3.31826 loss)
I1026 04:26:19.327507 29874 solver.cpp:403] Iteration 28000, lr = 0.0036742
I1026 04:34:46.060809 29874 solver.cpp:191] Iteration 29000, loss = 3.0735
I1026 04:34:46.061630 29874 solver.cpp:206]     Train net output #0: loss = 3.0735 (* 1 = 3.0735 loss)
I1026 04:34:46.061663 29874 solver.cpp:403] Iteration 29000, lr = 0.00360331
I1026 04:43:13.619920 29874 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_30000.caffemodel
I1026 04:43:18.553597 29874 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_30000.solverstate
I1026 04:43:22.285029 29874 solver.cpp:191] Iteration 30000, loss = 3.26911
I1026 04:43:22.285594 29874 solver.cpp:206]     Train net output #0: loss = 3.26911 (* 1 = 3.26911 loss)
I1026 04:43:22.285634 29874 solver.cpp:403] Iteration 30000, lr = 0.00353553
I1026 04:51:48.960636 29874 solver.cpp:191] Iteration 31000, loss = 3.27461
I1026 04:51:48.961488 29874 solver.cpp:206]     Train net output #0: loss = 3.27461 (* 1 = 3.27461 loss)
I1026 04:51:48.961519 29874 solver.cpp:403] Iteration 31000, lr = 0.00347066
I1026 05:00:15.427819 29874 solver.cpp:191] Iteration 32000, loss = 3.16581
I1026 05:00:15.428474 29874 solver.cpp:206]     Train net output #0: loss = 3.16581 (* 1 = 3.16581 loss)
I1026 05:00:15.428516 29874 solver.cpp:403] Iteration 32000, lr = 0.0034085
I1026 05:08:42.649215 29874 solver.cpp:191] Iteration 33000, loss = 2.86137
I1026 05:08:42.649812 29874 solver.cpp:206]     Train net output #0: loss = 2.86137 (* 1 = 2.86137 loss)
I1026 05:08:42.649844 29874 solver.cpp:403] Iteration 33000, lr = 0.00334887
I1026 05:17:08.850260 29874 solver.cpp:191] Iteration 34000, loss = 2.91089
I1026 05:17:08.850847 29874 solver.cpp:206]     Train net output #0: loss = 2.91089 (* 1 = 2.91089 loss)
I1026 05:17:08.850880 29874 solver.cpp:403] Iteration 34000, lr = 0.00329163
I1026 05:25:35.065413 29874 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_35000.caffemodel
I1026 05:25:39.327335 29874 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_35000.solverstate
I1026 05:25:43.512301 29874 solver.cpp:191] Iteration 35000, loss = 3.22653
I1026 05:25:43.512863 29874 solver.cpp:206]     Train net output #0: loss = 3.22653 (* 1 = 3.22653 loss)
I1026 05:25:43.512902 29874 solver.cpp:403] Iteration 35000, lr = 0.00323661
I1026 05:34:09.047896 29874 solver.cpp:191] Iteration 36000, loss = 3.03863
I1026 05:34:09.048498 29874 solver.cpp:206]     Train net output #0: loss = 3.03863 (* 1 = 3.03863 loss)
I1026 05:34:09.048535 29874 solver.cpp:403] Iteration 36000, lr = 0.0031837
I1026 05:42:36.264492 29874 solver.cpp:191] Iteration 37000, loss = 2.92327
I1026 05:42:36.265158 29874 solver.cpp:206]     Train net output #0: loss = 2.92327 (* 1 = 2.92327 loss)
I1026 05:42:36.265194 29874 solver.cpp:403] Iteration 37000, lr = 0.00313276
I1026 05:51:02.543390 29874 solver.cpp:191] Iteration 38000, loss = 3.03302
I1026 05:51:02.544214 29874 solver.cpp:206]     Train net output #0: loss = 3.03302 (* 1 = 3.03302 loss)
I1026 05:51:02.544250 29874 solver.cpp:403] Iteration 38000, lr = 0.00308368
I1026 05:59:28.414713 29874 solver.cpp:191] Iteration 39000, loss = 2.9167
I1026 05:59:28.415356 29874 solver.cpp:206]     Train net output #0: loss = 2.9167 (* 1 = 2.9167 loss)
I1026 05:59:28.415388 29874 solver.cpp:403] Iteration 39000, lr = 0.00303636
I1026 06:07:55.026716 29874 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_40000.caffemodel
I1026 06:07:59.361624 29874 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_40000.solverstate
I1026 06:08:02.870307 29874 solver.cpp:191] Iteration 40000, loss = 3.05405
I1026 06:08:02.870841 29874 solver.cpp:206]     Train net output #0: loss = 3.05405 (* 1 = 3.05405 loss)
I1026 06:08:02.870873 29874 solver.cpp:403] Iteration 40000, lr = 0.0029907
I1026 06:16:28.787361 29874 solver.cpp:191] Iteration 41000, loss = 3.0071
I1026 06:16:28.788342 29874 solver.cpp:206]     Train net output #0: loss = 3.0071 (* 1 = 3.0071 loss)
I1026 06:16:28.788377 29874 solver.cpp:403] Iteration 41000, lr = 0.00294661
I1026 06:24:53.740798 29874 solver.cpp:191] Iteration 42000, loss = 2.83267
I1026 06:24:53.741407 29874 solver.cpp:206]     Train net output #0: loss = 2.83267 (* 1 = 2.83267 loss)
I1026 06:24:53.741441 29874 solver.cpp:403] Iteration 42000, lr = 0.00290401
I1026 06:33:20.062202 29874 solver.cpp:191] Iteration 43000, loss = 2.8809
I1026 06:33:20.063009 29874 solver.cpp:206]     Train net output #0: loss = 2.8809 (* 1 = 2.8809 loss)
I1026 06:33:20.063041 29874 solver.cpp:403] Iteration 43000, lr = 0.00286281
I1026 06:41:46.114466 29874 solver.cpp:191] Iteration 44000, loss = 2.99778
I1026 06:41:46.115226 29874 solver.cpp:206]     Train net output #0: loss = 2.99778 (* 1 = 2.99778 loss)
I1026 06:41:46.115264 29874 solver.cpp:403] Iteration 44000, lr = 0.00282296
I1026 06:50:11.652344 29874 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_45000.caffemodel
I1026 06:50:16.162767 29874 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_45000.solverstate
I1026 06:50:20.088744 29874 solver.cpp:191] Iteration 45000, loss = 3.0848
I1026 06:50:20.089288 29874 solver.cpp:206]     Train net output #0: loss = 3.0848 (* 1 = 3.0848 loss)
I1026 06:50:20.089321 29874 solver.cpp:403] Iteration 45000, lr = 0.00278438
I1026 06:58:47.281091 29874 solver.cpp:191] Iteration 46000, loss = 2.7973
I1026 06:58:47.281745 29874 solver.cpp:206]     Train net output #0: loss = 2.7973 (* 1 = 2.7973 loss)
I1026 06:58:47.281779 29874 solver.cpp:403] Iteration 46000, lr = 0.002747
I1026 07:07:14.095726 29874 solver.cpp:191] Iteration 47000, loss = 2.75074
I1026 07:07:14.096501 29874 solver.cpp:206]     Train net output #0: loss = 2.75074 (* 1 = 2.75074 loss)
I1026 07:07:14.096534 29874 solver.cpp:403] Iteration 47000, lr = 0.00271078
I1026 07:15:38.623525 29874 solver.cpp:191] Iteration 48000, loss = 2.77088
I1026 07:15:38.624253 29874 solver.cpp:206]     Train net output #0: loss = 2.77088 (* 1 = 2.77088 loss)
I1026 07:15:38.624291 29874 solver.cpp:403] Iteration 48000, lr = 0.00267565
I1026 07:24:04.736399 29874 solver.cpp:191] Iteration 49000, loss = 2.55768
I1026 07:24:04.737784 29874 solver.cpp:206]     Train net output #0: loss = 2.55768 (* 1 = 2.55768 loss)
I1026 07:24:04.737818 29874 solver.cpp:403] Iteration 49000, lr = 0.00264156
I1026 07:32:31.549073 29874 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_50000.caffemodel
I1026 07:32:35.861225 29874 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_50000.solverstate
I1026 07:32:39.236917 29874 solver.cpp:228] Iteration 50000, loss = 2.79108
I1026 07:32:39.237422 29874 solver.cpp:233] Optimization Done.
I1026 07:32:39.237445 29874 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1026 07:53:02.740896 28033 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 07:53:02.740999 28033 net.cpp:358] Input 0 -> data
I1026 07:53:02.741024 28033 net.cpp:67] Creating Layer conv1
I1026 07:53:02.741029 28033 net.cpp:394] conv1 <- data
I1026 07:53:02.741035 28033 net.cpp:356] conv1 -> conv1
I1026 07:53:02.741045 28033 net.cpp:96] Setting up conv1
I1026 07:53:02.741358 28033 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 07:53:02.741376 28033 net.cpp:67] Creating Layer pool1
I1026 07:53:02.741381 28033 net.cpp:394] pool1 <- conv1
I1026 07:53:02.741387 28033 net.cpp:356] pool1 -> pool1
I1026 07:53:02.741394 28033 net.cpp:96] Setting up pool1
I1026 07:53:02.741408 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:53:02.741416 28033 net.cpp:67] Creating Layer relu1
I1026 07:53:02.741420 28033 net.cpp:394] relu1 <- pool1
I1026 07:53:02.741427 28033 net.cpp:345] relu1 -> pool1 (in-place)
I1026 07:53:02.741433 28033 net.cpp:96] Setting up relu1
I1026 07:53:02.741438 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:53:02.741444 28033 net.cpp:67] Creating Layer drop1
I1026 07:53:02.741447 28033 net.cpp:394] drop1 <- pool1
I1026 07:53:02.741453 28033 net.cpp:345] drop1 -> pool1 (in-place)
I1026 07:53:02.741459 28033 net.cpp:96] Setting up drop1
I1026 07:53:02.741463 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:53:02.741471 28033 net.cpp:67] Creating Layer conv2
I1026 07:53:02.741477 28033 net.cpp:394] conv2 <- pool1
I1026 07:53:02.741483 28033 net.cpp:356] conv2 -> conv2
I1026 07:53:02.741489 28033 net.cpp:96] Setting up conv2
I1026 07:53:02.742045 28033 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 07:53:02.742060 28033 net.cpp:67] Creating Layer pool2
I1026 07:53:02.742064 28033 net.cpp:394] pool2 <- conv2
I1026 07:53:02.742072 28033 net.cpp:356] pool2 -> pool2
I1026 07:53:02.742079 28033 net.cpp:96] Setting up pool2
I1026 07:53:02.742085 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:53:02.742090 28033 net.cpp:67] Creating Layer relu2
I1026 07:53:02.742094 28033 net.cpp:394] relu2 <- pool2
I1026 07:53:02.742101 28033 net.cpp:345] relu2 -> pool2 (in-place)
I1026 07:53:02.742107 28033 net.cpp:96] Setting up relu2
I1026 07:53:02.742111 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:53:02.742116 28033 net.cpp:67] Creating Layer drop2
I1026 07:53:02.742120 28033 net.cpp:394] drop2 <- pool2
I1026 07:53:02.742125 28033 net.cpp:345] drop2 -> pool2 (in-place)
I1026 07:53:02.742130 28033 net.cpp:96] Setting up drop2
I1026 07:53:02.742136 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:53:02.742143 28033 net.cpp:67] Creating Layer conv3
I1026 07:53:02.742147 28033 net.cpp:394] conv3 <- pool2
I1026 07:53:02.742153 28033 net.cpp:356] conv3 -> conv3
I1026 07:53:02.742161 28033 net.cpp:96] Setting up conv3
I1026 07:53:02.743605 28033 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 07:53:02.743621 28033 net.cpp:67] Creating Layer pool3
I1026 07:53:02.743625 28033 net.cpp:394] pool3 <- conv3
I1026 07:53:02.743633 28033 net.cpp:356] pool3 -> pool3
I1026 07:53:02.743639 28033 net.cpp:96] Setting up pool3
I1026 07:53:02.743645 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:53:02.743650 28033 net.cpp:67] Creating Layer relu3
I1026 07:53:02.743654 28033 net.cpp:394] relu3 <- pool3
I1026 07:53:02.743659 28033 net.cpp:345] relu3 -> pool3 (in-place)
I1026 07:53:02.743664 28033 net.cpp:96] Setting up relu3
I1026 07:53:02.743669 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:53:02.743674 28033 net.cpp:67] Creating Layer drop3
I1026 07:53:02.743677 28033 net.cpp:394] drop3 <- pool3
I1026 07:53:02.743685 28033 net.cpp:345] drop3 -> pool3 (in-place)
I1026 07:53:02.743690 28033 net.cpp:96] Setting up drop3
I1026 07:53:02.743695 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:53:02.743701 28033 net.cpp:67] Creating Layer ip1
I1026 07:53:02.743705 28033 net.cpp:394] ip1 <- pool3
I1026 07:53:02.743712 28033 net.cpp:356] ip1 -> ip1
I1026 07:53:02.743719 28033 net.cpp:96] Setting up ip1
I1026 07:53:03.199214 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:53:03.199272 28033 net.cpp:67] Creating Layer relu4
I1026 07:53:03.199280 28033 net.cpp:394] relu4 <- ip1
I1026 07:53:03.199288 28033 net.cpp:345] relu4 -> ip1 (in-place)
I1026 07:53:03.199297 28033 net.cpp:96] Setting up relu4
I1026 07:53:03.199302 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:53:03.199312 28033 net.cpp:67] Creating Layer drop4
I1026 07:53:03.199316 28033 net.cpp:394] drop4 <- ip1
I1026 07:53:03.199322 28033 net.cpp:345] drop4 -> ip1 (in-place)
I1026 07:53:03.199328 28033 net.cpp:96] Setting up drop4
I1026 07:53:03.199333 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:53:03.199342 28033 net.cpp:67] Creating Layer ip2
I1026 07:53:03.199345 28033 net.cpp:394] ip2 <- ip1
I1026 07:53:03.199353 28033 net.cpp:356] ip2 -> ip2
I1026 07:53:03.199364 28033 net.cpp:96] Setting up ip2
I1026 07:53:03.208513 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:53:03.208575 28033 net.cpp:67] Creating Layer prob
I1026 07:53:03.208582 28033 net.cpp:394] prob <- ip2
I1026 07:53:03.208591 28033 net.cpp:356] prob -> prob
I1026 07:53:03.208601 28033 net.cpp:96] Setting up prob
I1026 07:53:03.208611 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:53:03.208616 28033 net.cpp:172] prob does not need backward computation.
I1026 07:53:03.208619 28033 net.cpp:172] ip2 does not need backward computation.
I1026 07:53:03.208622 28033 net.cpp:172] drop4 does not need backward computation.
I1026 07:53:03.208633 28033 net.cpp:172] relu4 does not need backward computation.
I1026 07:53:03.208637 28033 net.cpp:172] ip1 does not need backward computation.
I1026 07:53:03.208641 28033 net.cpp:172] drop3 does not need backward computation.
I1026 07:53:03.208644 28033 net.cpp:172] relu3 does not need backward computation.
I1026 07:53:03.208647 28033 net.cpp:172] pool3 does not need backward computation.
I1026 07:53:03.208652 28033 net.cpp:172] conv3 does not need backward computation.
I1026 07:53:03.208654 28033 net.cpp:172] drop2 does not need backward computation.
I1026 07:53:03.208658 28033 net.cpp:172] relu2 does not need backward computation.
I1026 07:53:03.208662 28033 net.cpp:172] pool2 does not need backward computation.
I1026 07:53:03.208665 28033 net.cpp:172] conv2 does not need backward computation.
I1026 07:53:03.208668 28033 net.cpp:172] drop1 does not need backward computation.
I1026 07:53:03.208673 28033 net.cpp:172] relu1 does not need backward computation.
I1026 07:53:03.208675 28033 net.cpp:172] pool1 does not need backward computation.
I1026 07:53:03.208678 28033 net.cpp:172] conv1 does not need backward computation.
I1026 07:53:03.208683 28033 net.cpp:208] This network produces output prob
I1026 07:53:03.208694 28033 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 07:53:03.208703 28033 net.cpp:219] Network initialization done.
I1026 07:53:03.208706 28033 net.cpp:220] Memory required for data: 1837200
I1026 07:54:08.099407 28033 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 07:54:08.099912 28033 net.cpp:358] Input 0 -> data
I1026 07:54:08.099967 28033 net.cpp:67] Creating Layer conv1
I1026 07:54:08.099983 28033 net.cpp:394] conv1 <- data
I1026 07:54:08.100003 28033 net.cpp:356] conv1 -> conv1
I1026 07:54:08.100026 28033 net.cpp:96] Setting up conv1
I1026 07:54:08.100092 28033 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 07:54:08.100128 28033 net.cpp:67] Creating Layer pool1
I1026 07:54:08.100142 28033 net.cpp:394] pool1 <- conv1
I1026 07:54:08.100157 28033 net.cpp:356] pool1 -> pool1
I1026 07:54:08.100177 28033 net.cpp:96] Setting up pool1
I1026 07:54:08.100194 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:54:08.100213 28033 net.cpp:67] Creating Layer relu1
I1026 07:54:08.100224 28033 net.cpp:394] relu1 <- pool1
I1026 07:54:08.100239 28033 net.cpp:345] relu1 -> pool1 (in-place)
I1026 07:54:08.100255 28033 net.cpp:96] Setting up relu1
I1026 07:54:08.100267 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:54:08.100281 28033 net.cpp:67] Creating Layer drop1
I1026 07:54:08.100293 28033 net.cpp:394] drop1 <- pool1
I1026 07:54:08.100307 28033 net.cpp:345] drop1 -> pool1 (in-place)
I1026 07:54:08.100323 28033 net.cpp:96] Setting up drop1
I1026 07:54:08.100337 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:54:08.100355 28033 net.cpp:67] Creating Layer conv2
I1026 07:54:08.100366 28033 net.cpp:394] conv2 <- pool1
I1026 07:54:08.100383 28033 net.cpp:356] conv2 -> conv2
I1026 07:54:08.100402 28033 net.cpp:96] Setting up conv2
I1026 07:54:08.101824 28033 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 07:54:08.101866 28033 net.cpp:67] Creating Layer pool2
I1026 07:54:08.101879 28033 net.cpp:394] pool2 <- conv2
I1026 07:54:08.101896 28033 net.cpp:356] pool2 -> pool2
I1026 07:54:08.101915 28033 net.cpp:96] Setting up pool2
I1026 07:54:08.101932 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:54:08.101948 28033 net.cpp:67] Creating Layer relu2
I1026 07:54:08.101958 28033 net.cpp:394] relu2 <- pool2
I1026 07:54:08.101972 28033 net.cpp:345] relu2 -> pool2 (in-place)
I1026 07:54:08.101989 28033 net.cpp:96] Setting up relu2
I1026 07:54:08.101999 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:54:08.102015 28033 net.cpp:67] Creating Layer drop2
I1026 07:54:08.102026 28033 net.cpp:394] drop2 <- pool2
I1026 07:54:08.102041 28033 net.cpp:345] drop2 -> pool2 (in-place)
I1026 07:54:08.102056 28033 net.cpp:96] Setting up drop2
I1026 07:54:08.102069 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:54:08.102088 28033 net.cpp:67] Creating Layer conv3
I1026 07:54:08.102100 28033 net.cpp:394] conv3 <- pool2
I1026 07:54:08.102118 28033 net.cpp:356] conv3 -> conv3
I1026 07:54:08.102136 28033 net.cpp:96] Setting up conv3
I1026 07:54:08.105813 28033 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 07:54:08.105854 28033 net.cpp:67] Creating Layer pool3
I1026 07:54:08.105868 28033 net.cpp:394] pool3 <- conv3
I1026 07:54:08.105885 28033 net.cpp:356] pool3 -> pool3
I1026 07:54:08.105904 28033 net.cpp:96] Setting up pool3
I1026 07:54:08.105919 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:54:08.105936 28033 net.cpp:67] Creating Layer relu3
I1026 07:54:08.105947 28033 net.cpp:394] relu3 <- pool3
I1026 07:54:08.105960 28033 net.cpp:345] relu3 -> pool3 (in-place)
I1026 07:54:08.105976 28033 net.cpp:96] Setting up relu3
I1026 07:54:08.105988 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:54:08.106003 28033 net.cpp:67] Creating Layer drop3
I1026 07:54:08.106014 28033 net.cpp:394] drop3 <- pool3
I1026 07:54:08.106029 28033 net.cpp:345] drop3 -> pool3 (in-place)
I1026 07:54:08.106045 28033 net.cpp:96] Setting up drop3
I1026 07:54:08.106058 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:54:08.106076 28033 net.cpp:67] Creating Layer ip1
I1026 07:54:08.106086 28033 net.cpp:394] ip1 <- pool3
I1026 07:54:08.106103 28033 net.cpp:356] ip1 -> ip1
I1026 07:54:08.106130 28033 net.cpp:96] Setting up ip1
I1026 07:54:08.509495 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:54:08.509557 28033 net.cpp:67] Creating Layer relu4
I1026 07:54:08.509565 28033 net.cpp:394] relu4 <- ip1
I1026 07:54:08.509575 28033 net.cpp:345] relu4 -> ip1 (in-place)
I1026 07:54:08.509585 28033 net.cpp:96] Setting up relu4
I1026 07:54:08.509591 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:54:08.509599 28033 net.cpp:67] Creating Layer drop4
I1026 07:54:08.509603 28033 net.cpp:394] drop4 <- ip1
I1026 07:54:08.509610 28033 net.cpp:345] drop4 -> ip1 (in-place)
I1026 07:54:08.509616 28033 net.cpp:96] Setting up drop4
I1026 07:54:08.509623 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:54:08.509634 28033 net.cpp:67] Creating Layer ip2
I1026 07:54:08.509637 28033 net.cpp:394] ip2 <- ip1
I1026 07:54:08.509644 28033 net.cpp:356] ip2 -> ip2
I1026 07:54:08.509659 28033 net.cpp:96] Setting up ip2
I1026 07:54:08.517292 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:54:08.517359 28033 net.cpp:67] Creating Layer prob
I1026 07:54:08.517366 28033 net.cpp:394] prob <- ip2
I1026 07:54:08.517375 28033 net.cpp:356] prob -> prob
I1026 07:54:08.517387 28033 net.cpp:96] Setting up prob
I1026 07:54:08.517395 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:54:08.517400 28033 net.cpp:172] prob does not need backward computation.
I1026 07:54:08.517405 28033 net.cpp:172] ip2 does not need backward computation.
I1026 07:54:08.517410 28033 net.cpp:172] drop4 does not need backward computation.
I1026 07:54:08.517413 28033 net.cpp:172] relu4 does not need backward computation.
I1026 07:54:08.517417 28033 net.cpp:172] ip1 does not need backward computation.
I1026 07:54:08.517421 28033 net.cpp:172] drop3 does not need backward computation.
I1026 07:54:08.517426 28033 net.cpp:172] relu3 does not need backward computation.
I1026 07:54:08.517429 28033 net.cpp:172] pool3 does not need backward computation.
I1026 07:54:08.517432 28033 net.cpp:172] conv3 does not need backward computation.
I1026 07:54:08.517436 28033 net.cpp:172] drop2 does not need backward computation.
I1026 07:54:08.517441 28033 net.cpp:172] relu2 does not need backward computation.
I1026 07:54:08.517444 28033 net.cpp:172] pool2 does not need backward computation.
I1026 07:54:08.517448 28033 net.cpp:172] conv2 does not need backward computation.
I1026 07:54:08.517452 28033 net.cpp:172] drop1 does not need backward computation.
I1026 07:54:08.517455 28033 net.cpp:172] relu1 does not need backward computation.
I1026 07:54:08.517459 28033 net.cpp:172] pool1 does not need backward computation.
I1026 07:54:08.517463 28033 net.cpp:172] conv1 does not need backward computation.
I1026 07:54:08.517467 28033 net.cpp:208] This network produces output prob
I1026 07:54:08.517482 28033 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 07:54:08.517493 28033 net.cpp:219] Network initialization done.
I1026 07:54:08.517496 28033 net.cpp:220] Memory required for data: 1837200
I1026 07:55:07.121896 28033 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 07:55:07.122628 28033 net.cpp:358] Input 0 -> data
I1026 07:55:07.122692 28033 net.cpp:67] Creating Layer conv1
I1026 07:55:07.122709 28033 net.cpp:394] conv1 <- data
I1026 07:55:07.122728 28033 net.cpp:356] conv1 -> conv1
I1026 07:55:07.122752 28033 net.cpp:96] Setting up conv1
I1026 07:55:07.122818 28033 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 07:55:07.122853 28033 net.cpp:67] Creating Layer pool1
I1026 07:55:07.122867 28033 net.cpp:394] pool1 <- conv1
I1026 07:55:07.122884 28033 net.cpp:356] pool1 -> pool1
I1026 07:55:07.122903 28033 net.cpp:96] Setting up pool1
I1026 07:55:07.122920 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:55:07.122938 28033 net.cpp:67] Creating Layer relu1
I1026 07:55:07.122951 28033 net.cpp:394] relu1 <- pool1
I1026 07:55:07.122964 28033 net.cpp:345] relu1 -> pool1 (in-place)
I1026 07:55:07.122980 28033 net.cpp:96] Setting up relu1
I1026 07:55:07.122993 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:55:07.123008 28033 net.cpp:67] Creating Layer drop1
I1026 07:55:07.123020 28033 net.cpp:394] drop1 <- pool1
I1026 07:55:07.123035 28033 net.cpp:345] drop1 -> pool1 (in-place)
I1026 07:55:07.123051 28033 net.cpp:96] Setting up drop1
I1026 07:55:07.123065 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:55:07.123083 28033 net.cpp:67] Creating Layer conv2
I1026 07:55:07.123095 28033 net.cpp:394] conv2 <- pool1
I1026 07:55:07.123113 28033 net.cpp:356] conv2 -> conv2
I1026 07:55:07.123132 28033 net.cpp:96] Setting up conv2
I1026 07:55:07.124549 28033 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 07:55:07.124588 28033 net.cpp:67] Creating Layer pool2
I1026 07:55:07.124601 28033 net.cpp:394] pool2 <- conv2
I1026 07:55:07.124619 28033 net.cpp:356] pool2 -> pool2
I1026 07:55:07.124639 28033 net.cpp:96] Setting up pool2
I1026 07:55:07.124655 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:55:07.124670 28033 net.cpp:67] Creating Layer relu2
I1026 07:55:07.124681 28033 net.cpp:394] relu2 <- pool2
I1026 07:55:07.124696 28033 net.cpp:345] relu2 -> pool2 (in-place)
I1026 07:55:07.124711 28033 net.cpp:96] Setting up relu2
I1026 07:55:07.124723 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:55:07.124738 28033 net.cpp:67] Creating Layer drop2
I1026 07:55:07.124750 28033 net.cpp:394] drop2 <- pool2
I1026 07:55:07.124773 28033 net.cpp:345] drop2 -> pool2 (in-place)
I1026 07:55:07.124790 28033 net.cpp:96] Setting up drop2
I1026 07:55:07.124802 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:55:07.124822 28033 net.cpp:67] Creating Layer conv3
I1026 07:55:07.124835 28033 net.cpp:394] conv3 <- pool2
I1026 07:55:07.124851 28033 net.cpp:356] conv3 -> conv3
I1026 07:55:07.124871 28033 net.cpp:96] Setting up conv3
I1026 07:55:07.128501 28033 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 07:55:07.128538 28033 net.cpp:67] Creating Layer pool3
I1026 07:55:07.128551 28033 net.cpp:394] pool3 <- conv3
I1026 07:55:07.128567 28033 net.cpp:356] pool3 -> pool3
I1026 07:55:07.128585 28033 net.cpp:96] Setting up pool3
I1026 07:55:07.128600 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:55:07.128615 28033 net.cpp:67] Creating Layer relu3
I1026 07:55:07.128626 28033 net.cpp:394] relu3 <- pool3
I1026 07:55:07.128641 28033 net.cpp:345] relu3 -> pool3 (in-place)
I1026 07:55:07.128656 28033 net.cpp:96] Setting up relu3
I1026 07:55:07.128669 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:55:07.128684 28033 net.cpp:67] Creating Layer drop3
I1026 07:55:07.128695 28033 net.cpp:394] drop3 <- pool3
I1026 07:55:07.128710 28033 net.cpp:345] drop3 -> pool3 (in-place)
I1026 07:55:07.128726 28033 net.cpp:96] Setting up drop3
I1026 07:55:07.128738 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:55:07.128756 28033 net.cpp:67] Creating Layer ip1
I1026 07:55:07.128767 28033 net.cpp:394] ip1 <- pool3
I1026 07:55:07.128783 28033 net.cpp:356] ip1 -> ip1
I1026 07:55:07.128803 28033 net.cpp:96] Setting up ip1
I1026 07:55:07.485079 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:55:07.485146 28033 net.cpp:67] Creating Layer relu4
I1026 07:55:07.485153 28033 net.cpp:394] relu4 <- ip1
I1026 07:55:07.485164 28033 net.cpp:345] relu4 -> ip1 (in-place)
I1026 07:55:07.485175 28033 net.cpp:96] Setting up relu4
I1026 07:55:07.485180 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:55:07.485189 28033 net.cpp:67] Creating Layer drop4
I1026 07:55:07.485193 28033 net.cpp:394] drop4 <- ip1
I1026 07:55:07.485200 28033 net.cpp:345] drop4 -> ip1 (in-place)
I1026 07:55:07.485208 28033 net.cpp:96] Setting up drop4
I1026 07:55:07.485213 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:55:07.485224 28033 net.cpp:67] Creating Layer ip2
I1026 07:55:07.485229 28033 net.cpp:394] ip2 <- ip1
I1026 07:55:07.485236 28033 net.cpp:356] ip2 -> ip2
I1026 07:55:07.485249 28033 net.cpp:96] Setting up ip2
I1026 07:55:07.492801 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:55:07.492866 28033 net.cpp:67] Creating Layer prob
I1026 07:55:07.492873 28033 net.cpp:394] prob <- ip2
I1026 07:55:07.492883 28033 net.cpp:356] prob -> prob
I1026 07:55:07.492894 28033 net.cpp:96] Setting up prob
I1026 07:55:07.492903 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:55:07.492908 28033 net.cpp:172] prob does not need backward computation.
I1026 07:55:07.492913 28033 net.cpp:172] ip2 does not need backward computation.
I1026 07:55:07.492916 28033 net.cpp:172] drop4 does not need backward computation.
I1026 07:55:07.492920 28033 net.cpp:172] relu4 does not need backward computation.
I1026 07:55:07.492924 28033 net.cpp:172] ip1 does not need backward computation.
I1026 07:55:07.492928 28033 net.cpp:172] drop3 does not need backward computation.
I1026 07:55:07.492933 28033 net.cpp:172] relu3 does not need backward computation.
I1026 07:55:07.492936 28033 net.cpp:172] pool3 does not need backward computation.
I1026 07:55:07.492940 28033 net.cpp:172] conv3 does not need backward computation.
I1026 07:55:07.492944 28033 net.cpp:172] drop2 does not need backward computation.
I1026 07:55:07.492949 28033 net.cpp:172] relu2 does not need backward computation.
I1026 07:55:07.492952 28033 net.cpp:172] pool2 does not need backward computation.
I1026 07:55:07.492956 28033 net.cpp:172] conv2 does not need backward computation.
I1026 07:55:07.492960 28033 net.cpp:172] drop1 does not need backward computation.
I1026 07:55:07.492964 28033 net.cpp:172] relu1 does not need backward computation.
I1026 07:55:07.492985 28033 net.cpp:172] pool1 does not need backward computation.
I1026 07:55:07.492988 28033 net.cpp:172] conv1 does not need backward computation.
I1026 07:55:07.492992 28033 net.cpp:208] This network produces output prob
I1026 07:55:07.493007 28033 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 07:55:07.493016 28033 net.cpp:219] Network initialization done.
I1026 07:55:07.493021 28033 net.cpp:220] Memory required for data: 1837200
I1026 07:56:06.725103 28033 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 07:56:06.725636 28033 net.cpp:358] Input 0 -> data
I1026 07:56:06.725685 28033 net.cpp:67] Creating Layer conv1
I1026 07:56:06.725698 28033 net.cpp:394] conv1 <- data
I1026 07:56:06.725713 28033 net.cpp:356] conv1 -> conv1
I1026 07:56:06.725733 28033 net.cpp:96] Setting up conv1
I1026 07:56:06.725787 28033 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 07:56:06.725817 28033 net.cpp:67] Creating Layer pool1
I1026 07:56:06.725828 28033 net.cpp:394] pool1 <- conv1
I1026 07:56:06.725841 28033 net.cpp:356] pool1 -> pool1
I1026 07:56:06.725857 28033 net.cpp:96] Setting up pool1
I1026 07:56:06.725872 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:56:06.725886 28033 net.cpp:67] Creating Layer relu1
I1026 07:56:06.725895 28033 net.cpp:394] relu1 <- pool1
I1026 07:56:06.725920 28033 net.cpp:345] relu1 -> pool1 (in-place)
I1026 07:56:06.725934 28033 net.cpp:96] Setting up relu1
I1026 07:56:06.725945 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:56:06.725957 28033 net.cpp:67] Creating Layer drop1
I1026 07:56:06.725966 28033 net.cpp:394] drop1 <- pool1
I1026 07:56:06.725980 28033 net.cpp:345] drop1 -> pool1 (in-place)
I1026 07:56:06.725992 28033 net.cpp:96] Setting up drop1
I1026 07:56:06.726003 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:56:06.726018 28033 net.cpp:67] Creating Layer conv2
I1026 07:56:06.726028 28033 net.cpp:394] conv2 <- pool1
I1026 07:56:06.726042 28033 net.cpp:356] conv2 -> conv2
I1026 07:56:06.726057 28033 net.cpp:96] Setting up conv2
I1026 07:56:06.727192 28033 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 07:56:06.727221 28033 net.cpp:67] Creating Layer pool2
I1026 07:56:06.727232 28033 net.cpp:394] pool2 <- conv2
I1026 07:56:06.727246 28033 net.cpp:356] pool2 -> pool2
I1026 07:56:06.727260 28033 net.cpp:96] Setting up pool2
I1026 07:56:06.727273 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:56:06.727285 28033 net.cpp:67] Creating Layer relu2
I1026 07:56:06.727295 28033 net.cpp:394] relu2 <- pool2
I1026 07:56:06.727306 28033 net.cpp:345] relu2 -> pool2 (in-place)
I1026 07:56:06.727319 28033 net.cpp:96] Setting up relu2
I1026 07:56:06.727329 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:56:06.727340 28033 net.cpp:67] Creating Layer drop2
I1026 07:56:06.727350 28033 net.cpp:394] drop2 <- pool2
I1026 07:56:06.727362 28033 net.cpp:345] drop2 -> pool2 (in-place)
I1026 07:56:06.727382 28033 net.cpp:96] Setting up drop2
I1026 07:56:06.727396 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:56:06.727416 28033 net.cpp:67] Creating Layer conv3
I1026 07:56:06.727427 28033 net.cpp:394] conv3 <- pool2
I1026 07:56:06.727443 28033 net.cpp:356] conv3 -> conv3
I1026 07:56:06.727462 28033 net.cpp:96] Setting up conv3
I1026 07:56:06.731143 28033 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 07:56:06.731184 28033 net.cpp:67] Creating Layer pool3
I1026 07:56:06.731199 28033 net.cpp:394] pool3 <- conv3
I1026 07:56:06.731215 28033 net.cpp:356] pool3 -> pool3
I1026 07:56:06.731233 28033 net.cpp:96] Setting up pool3
I1026 07:56:06.731248 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:56:06.731262 28033 net.cpp:67] Creating Layer relu3
I1026 07:56:06.731274 28033 net.cpp:394] relu3 <- pool3
I1026 07:56:06.731288 28033 net.cpp:345] relu3 -> pool3 (in-place)
I1026 07:56:06.731304 28033 net.cpp:96] Setting up relu3
I1026 07:56:06.731317 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:56:06.731331 28033 net.cpp:67] Creating Layer drop3
I1026 07:56:06.731343 28033 net.cpp:394] drop3 <- pool3
I1026 07:56:06.731356 28033 net.cpp:345] drop3 -> pool3 (in-place)
I1026 07:56:06.731372 28033 net.cpp:96] Setting up drop3
I1026 07:56:06.731384 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:56:06.731401 28033 net.cpp:67] Creating Layer ip1
I1026 07:56:06.731412 28033 net.cpp:394] ip1 <- pool3
I1026 07:56:06.731430 28033 net.cpp:356] ip1 -> ip1
I1026 07:56:06.731448 28033 net.cpp:96] Setting up ip1
I1026 07:56:07.136370 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:56:07.136456 28033 net.cpp:67] Creating Layer relu4
I1026 07:56:07.136466 28033 net.cpp:394] relu4 <- ip1
I1026 07:56:07.136477 28033 net.cpp:345] relu4 -> ip1 (in-place)
I1026 07:56:07.136487 28033 net.cpp:96] Setting up relu4
I1026 07:56:07.136493 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:56:07.136502 28033 net.cpp:67] Creating Layer drop4
I1026 07:56:07.136507 28033 net.cpp:394] drop4 <- ip1
I1026 07:56:07.136513 28033 net.cpp:345] drop4 -> ip1 (in-place)
I1026 07:56:07.136521 28033 net.cpp:96] Setting up drop4
I1026 07:56:07.136528 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:56:07.136538 28033 net.cpp:67] Creating Layer ip2
I1026 07:56:07.136543 28033 net.cpp:394] ip2 <- ip1
I1026 07:56:07.136550 28033 net.cpp:356] ip2 -> ip2
I1026 07:56:07.136564 28033 net.cpp:96] Setting up ip2
I1026 07:56:07.144398 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:56:07.144490 28033 net.cpp:67] Creating Layer prob
I1026 07:56:07.144500 28033 net.cpp:394] prob <- ip2
I1026 07:56:07.144510 28033 net.cpp:356] prob -> prob
I1026 07:56:07.144522 28033 net.cpp:96] Setting up prob
I1026 07:56:07.144531 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:56:07.144536 28033 net.cpp:172] prob does not need backward computation.
I1026 07:56:07.144539 28033 net.cpp:172] ip2 does not need backward computation.
I1026 07:56:07.144543 28033 net.cpp:172] drop4 does not need backward computation.
I1026 07:56:07.144547 28033 net.cpp:172] relu4 does not need backward computation.
I1026 07:56:07.144551 28033 net.cpp:172] ip1 does not need backward computation.
I1026 07:56:07.144556 28033 net.cpp:172] drop3 does not need backward computation.
I1026 07:56:07.144559 28033 net.cpp:172] relu3 does not need backward computation.
I1026 07:56:07.144564 28033 net.cpp:172] pool3 does not need backward computation.
I1026 07:56:07.144568 28033 net.cpp:172] conv3 does not need backward computation.
I1026 07:56:07.144572 28033 net.cpp:172] drop2 does not need backward computation.
I1026 07:56:07.144577 28033 net.cpp:172] relu2 does not need backward computation.
I1026 07:56:07.144580 28033 net.cpp:172] pool2 does not need backward computation.
I1026 07:56:07.144584 28033 net.cpp:172] conv2 does not need backward computation.
I1026 07:56:07.144588 28033 net.cpp:172] drop1 does not need backward computation.
I1026 07:56:07.144593 28033 net.cpp:172] relu1 does not need backward computation.
I1026 07:56:07.144597 28033 net.cpp:172] pool1 does not need backward computation.
I1026 07:56:07.144600 28033 net.cpp:172] conv1 does not need backward computation.
I1026 07:56:07.144604 28033 net.cpp:208] This network produces output prob
I1026 07:56:07.144620 28033 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 07:56:07.144629 28033 net.cpp:219] Network initialization done.
I1026 07:56:07.144634 28033 net.cpp:220] Memory required for data: 1837200
I1026 07:57:05.530438 28033 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 07:57:05.530967 28033 net.cpp:358] Input 0 -> data
I1026 07:57:05.530997 28033 net.cpp:67] Creating Layer conv1
I1026 07:57:05.531003 28033 net.cpp:394] conv1 <- data
I1026 07:57:05.531011 28033 net.cpp:356] conv1 -> conv1
I1026 07:57:05.531021 28033 net.cpp:96] Setting up conv1
I1026 07:57:05.531052 28033 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 07:57:05.531069 28033 net.cpp:67] Creating Layer pool1
I1026 07:57:05.531072 28033 net.cpp:394] pool1 <- conv1
I1026 07:57:05.531078 28033 net.cpp:356] pool1 -> pool1
I1026 07:57:05.531086 28033 net.cpp:96] Setting up pool1
I1026 07:57:05.531093 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:57:05.531100 28033 net.cpp:67] Creating Layer relu1
I1026 07:57:05.531105 28033 net.cpp:394] relu1 <- pool1
I1026 07:57:05.531110 28033 net.cpp:345] relu1 -> pool1 (in-place)
I1026 07:57:05.531116 28033 net.cpp:96] Setting up relu1
I1026 07:57:05.531119 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:57:05.531126 28033 net.cpp:67] Creating Layer drop1
I1026 07:57:05.531129 28033 net.cpp:394] drop1 <- pool1
I1026 07:57:05.531134 28033 net.cpp:345] drop1 -> pool1 (in-place)
I1026 07:57:05.531141 28033 net.cpp:96] Setting up drop1
I1026 07:57:05.531146 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:57:05.531152 28033 net.cpp:67] Creating Layer conv2
I1026 07:57:05.531157 28033 net.cpp:394] conv2 <- pool1
I1026 07:57:05.531162 28033 net.cpp:356] conv2 -> conv2
I1026 07:57:05.531169 28033 net.cpp:96] Setting up conv2
I1026 07:57:05.531673 28033 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 07:57:05.531687 28033 net.cpp:67] Creating Layer pool2
I1026 07:57:05.531692 28033 net.cpp:394] pool2 <- conv2
I1026 07:57:05.531699 28033 net.cpp:356] pool2 -> pool2
I1026 07:57:05.531707 28033 net.cpp:96] Setting up pool2
I1026 07:57:05.531713 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:57:05.531718 28033 net.cpp:67] Creating Layer relu2
I1026 07:57:05.531721 28033 net.cpp:394] relu2 <- pool2
I1026 07:57:05.531728 28033 net.cpp:345] relu2 -> pool2 (in-place)
I1026 07:57:05.531733 28033 net.cpp:96] Setting up relu2
I1026 07:57:05.531738 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:57:05.531743 28033 net.cpp:67] Creating Layer drop2
I1026 07:57:05.531746 28033 net.cpp:394] drop2 <- pool2
I1026 07:57:05.531751 28033 net.cpp:345] drop2 -> pool2 (in-place)
I1026 07:57:05.531756 28033 net.cpp:96] Setting up drop2
I1026 07:57:05.531761 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:57:05.531767 28033 net.cpp:67] Creating Layer conv3
I1026 07:57:05.531772 28033 net.cpp:394] conv3 <- pool2
I1026 07:57:05.531779 28033 net.cpp:356] conv3 -> conv3
I1026 07:57:05.531785 28033 net.cpp:96] Setting up conv3
I1026 07:57:05.533442 28033 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 07:57:05.533464 28033 net.cpp:67] Creating Layer pool3
I1026 07:57:05.533471 28033 net.cpp:394] pool3 <- conv3
I1026 07:57:05.533479 28033 net.cpp:356] pool3 -> pool3
I1026 07:57:05.533489 28033 net.cpp:96] Setting up pool3
I1026 07:57:05.533498 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:57:05.533505 28033 net.cpp:67] Creating Layer relu3
I1026 07:57:05.533510 28033 net.cpp:394] relu3 <- pool3
I1026 07:57:05.533519 28033 net.cpp:345] relu3 -> pool3 (in-place)
I1026 07:57:05.533526 28033 net.cpp:96] Setting up relu3
I1026 07:57:05.533537 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:57:05.533545 28033 net.cpp:67] Creating Layer drop3
I1026 07:57:05.533551 28033 net.cpp:394] drop3 <- pool3
I1026 07:57:05.533558 28033 net.cpp:345] drop3 -> pool3 (in-place)
I1026 07:57:05.533566 28033 net.cpp:96] Setting up drop3
I1026 07:57:05.533573 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:57:05.533582 28033 net.cpp:67] Creating Layer ip1
I1026 07:57:05.533587 28033 net.cpp:394] ip1 <- pool3
I1026 07:57:05.533596 28033 net.cpp:356] ip1 -> ip1
I1026 07:57:05.533607 28033 net.cpp:96] Setting up ip1
I1026 07:57:05.889264 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:57:05.889328 28033 net.cpp:67] Creating Layer relu4
I1026 07:57:05.889336 28033 net.cpp:394] relu4 <- ip1
I1026 07:57:05.889348 28033 net.cpp:345] relu4 -> ip1 (in-place)
I1026 07:57:05.889359 28033 net.cpp:96] Setting up relu4
I1026 07:57:05.889364 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:57:05.889374 28033 net.cpp:67] Creating Layer drop4
I1026 07:57:05.889377 28033 net.cpp:394] drop4 <- ip1
I1026 07:57:05.889385 28033 net.cpp:345] drop4 -> ip1 (in-place)
I1026 07:57:05.889392 28033 net.cpp:96] Setting up drop4
I1026 07:57:05.889399 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:57:05.889410 28033 net.cpp:67] Creating Layer ip2
I1026 07:57:05.889413 28033 net.cpp:394] ip2 <- ip1
I1026 07:57:05.889421 28033 net.cpp:356] ip2 -> ip2
I1026 07:57:05.889436 28033 net.cpp:96] Setting up ip2
I1026 07:57:05.897323 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:57:05.897385 28033 net.cpp:67] Creating Layer prob
I1026 07:57:05.897394 28033 net.cpp:394] prob <- ip2
I1026 07:57:05.897403 28033 net.cpp:356] prob -> prob
I1026 07:57:05.897414 28033 net.cpp:96] Setting up prob
I1026 07:57:05.897423 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:57:05.897428 28033 net.cpp:172] prob does not need backward computation.
I1026 07:57:05.897433 28033 net.cpp:172] ip2 does not need backward computation.
I1026 07:57:05.897436 28033 net.cpp:172] drop4 does not need backward computation.
I1026 07:57:05.897440 28033 net.cpp:172] relu4 does not need backward computation.
I1026 07:57:05.897444 28033 net.cpp:172] ip1 does not need backward computation.
I1026 07:57:05.897449 28033 net.cpp:172] drop3 does not need backward computation.
I1026 07:57:05.897452 28033 net.cpp:172] relu3 does not need backward computation.
I1026 07:57:05.897456 28033 net.cpp:172] pool3 does not need backward computation.
I1026 07:57:05.897460 28033 net.cpp:172] conv3 does not need backward computation.
I1026 07:57:05.897464 28033 net.cpp:172] drop2 does not need backward computation.
I1026 07:57:05.897467 28033 net.cpp:172] relu2 does not need backward computation.
I1026 07:57:05.897471 28033 net.cpp:172] pool2 does not need backward computation.
I1026 07:57:05.897475 28033 net.cpp:172] conv2 does not need backward computation.
I1026 07:57:05.897480 28033 net.cpp:172] drop1 does not need backward computation.
I1026 07:57:05.897483 28033 net.cpp:172] relu1 does not need backward computation.
I1026 07:57:05.897487 28033 net.cpp:172] pool1 does not need backward computation.
I1026 07:57:05.897491 28033 net.cpp:172] conv1 does not need backward computation.
I1026 07:57:05.897495 28033 net.cpp:208] This network produces output prob
I1026 07:57:05.897512 28033 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 07:57:05.897521 28033 net.cpp:219] Network initialization done.
I1026 07:57:05.897526 28033 net.cpp:220] Memory required for data: 1837200
I1026 07:58:04.926786 28033 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 07:58:04.927359 28033 net.cpp:358] Input 0 -> data
I1026 07:58:04.927412 28033 net.cpp:67] Creating Layer conv1
I1026 07:58:04.927428 28033 net.cpp:394] conv1 <- data
I1026 07:58:04.927446 28033 net.cpp:356] conv1 -> conv1
I1026 07:58:04.927470 28033 net.cpp:96] Setting up conv1
I1026 07:58:04.927534 28033 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 07:58:04.927569 28033 net.cpp:67] Creating Layer pool1
I1026 07:58:04.927582 28033 net.cpp:394] pool1 <- conv1
I1026 07:58:04.927597 28033 net.cpp:356] pool1 -> pool1
I1026 07:58:04.927618 28033 net.cpp:96] Setting up pool1
I1026 07:58:04.927634 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:58:04.927652 28033 net.cpp:67] Creating Layer relu1
I1026 07:58:04.927664 28033 net.cpp:394] relu1 <- pool1
I1026 07:58:04.927680 28033 net.cpp:345] relu1 -> pool1 (in-place)
I1026 07:58:04.927695 28033 net.cpp:96] Setting up relu1
I1026 07:58:04.927707 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:58:04.927722 28033 net.cpp:67] Creating Layer drop1
I1026 07:58:04.927733 28033 net.cpp:394] drop1 <- pool1
I1026 07:58:04.927747 28033 net.cpp:345] drop1 -> pool1 (in-place)
I1026 07:58:04.927763 28033 net.cpp:96] Setting up drop1
I1026 07:58:04.927777 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:58:04.927794 28033 net.cpp:67] Creating Layer conv2
I1026 07:58:04.927806 28033 net.cpp:394] conv2 <- pool1
I1026 07:58:04.927822 28033 net.cpp:356] conv2 -> conv2
I1026 07:58:04.927841 28033 net.cpp:96] Setting up conv2
I1026 07:58:04.929263 28033 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 07:58:04.929303 28033 net.cpp:67] Creating Layer pool2
I1026 07:58:04.929316 28033 net.cpp:394] pool2 <- conv2
I1026 07:58:04.929332 28033 net.cpp:356] pool2 -> pool2
I1026 07:58:04.929352 28033 net.cpp:96] Setting up pool2
I1026 07:58:04.929376 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:58:04.929393 28033 net.cpp:67] Creating Layer relu2
I1026 07:58:04.929404 28033 net.cpp:394] relu2 <- pool2
I1026 07:58:04.929419 28033 net.cpp:345] relu2 -> pool2 (in-place)
I1026 07:58:04.929435 28033 net.cpp:96] Setting up relu2
I1026 07:58:04.929446 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:58:04.929461 28033 net.cpp:67] Creating Layer drop2
I1026 07:58:04.929471 28033 net.cpp:394] drop2 <- pool2
I1026 07:58:04.929486 28033 net.cpp:345] drop2 -> pool2 (in-place)
I1026 07:58:04.929502 28033 net.cpp:96] Setting up drop2
I1026 07:58:04.929513 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:58:04.929533 28033 net.cpp:67] Creating Layer conv3
I1026 07:58:04.929544 28033 net.cpp:394] conv3 <- pool2
I1026 07:58:04.929560 28033 net.cpp:356] conv3 -> conv3
I1026 07:58:04.929579 28033 net.cpp:96] Setting up conv3
I1026 07:58:04.933135 28033 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 07:58:04.933169 28033 net.cpp:67] Creating Layer pool3
I1026 07:58:04.933181 28033 net.cpp:394] pool3 <- conv3
I1026 07:58:04.933192 28033 net.cpp:356] pool3 -> pool3
I1026 07:58:04.933207 28033 net.cpp:96] Setting up pool3
I1026 07:58:04.933218 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:58:04.933230 28033 net.cpp:67] Creating Layer relu3
I1026 07:58:04.933238 28033 net.cpp:394] relu3 <- pool3
I1026 07:58:04.933249 28033 net.cpp:345] relu3 -> pool3 (in-place)
I1026 07:58:04.933261 28033 net.cpp:96] Setting up relu3
I1026 07:58:04.933270 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:58:04.933281 28033 net.cpp:67] Creating Layer drop3
I1026 07:58:04.933290 28033 net.cpp:394] drop3 <- pool3
I1026 07:58:04.933301 28033 net.cpp:345] drop3 -> pool3 (in-place)
I1026 07:58:04.933313 28033 net.cpp:96] Setting up drop3
I1026 07:58:04.933322 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:58:04.933336 28033 net.cpp:67] Creating Layer ip1
I1026 07:58:04.933344 28033 net.cpp:394] ip1 <- pool3
I1026 07:58:04.933357 28033 net.cpp:356] ip1 -> ip1
I1026 07:58:04.933372 28033 net.cpp:96] Setting up ip1
I1026 07:58:05.354267 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:58:05.354331 28033 net.cpp:67] Creating Layer relu4
I1026 07:58:05.354337 28033 net.cpp:394] relu4 <- ip1
I1026 07:58:05.354348 28033 net.cpp:345] relu4 -> ip1 (in-place)
I1026 07:58:05.354358 28033 net.cpp:96] Setting up relu4
I1026 07:58:05.354364 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:58:05.354372 28033 net.cpp:67] Creating Layer drop4
I1026 07:58:05.354377 28033 net.cpp:394] drop4 <- ip1
I1026 07:58:05.354383 28033 net.cpp:345] drop4 -> ip1 (in-place)
I1026 07:58:05.354390 28033 net.cpp:96] Setting up drop4
I1026 07:58:05.354396 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:58:05.354406 28033 net.cpp:67] Creating Layer ip2
I1026 07:58:05.354411 28033 net.cpp:394] ip2 <- ip1
I1026 07:58:05.354418 28033 net.cpp:356] ip2 -> ip2
I1026 07:58:05.354432 28033 net.cpp:96] Setting up ip2
I1026 07:58:05.362059 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:58:05.362124 28033 net.cpp:67] Creating Layer prob
I1026 07:58:05.362133 28033 net.cpp:394] prob <- ip2
I1026 07:58:05.362143 28033 net.cpp:356] prob -> prob
I1026 07:58:05.362154 28033 net.cpp:96] Setting up prob
I1026 07:58:05.362164 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:58:05.362169 28033 net.cpp:172] prob does not need backward computation.
I1026 07:58:05.362172 28033 net.cpp:172] ip2 does not need backward computation.
I1026 07:58:05.362176 28033 net.cpp:172] drop4 does not need backward computation.
I1026 07:58:05.362180 28033 net.cpp:172] relu4 does not need backward computation.
I1026 07:58:05.362185 28033 net.cpp:172] ip1 does not need backward computation.
I1026 07:58:05.362188 28033 net.cpp:172] drop3 does not need backward computation.
I1026 07:58:05.362192 28033 net.cpp:172] relu3 does not need backward computation.
I1026 07:58:05.362196 28033 net.cpp:172] pool3 does not need backward computation.
I1026 07:58:05.362200 28033 net.cpp:172] conv3 does not need backward computation.
I1026 07:58:05.362215 28033 net.cpp:172] drop2 does not need backward computation.
I1026 07:58:05.362220 28033 net.cpp:172] relu2 does not need backward computation.
I1026 07:58:05.362223 28033 net.cpp:172] pool2 does not need backward computation.
I1026 07:58:05.362231 28033 net.cpp:172] conv2 does not need backward computation.
I1026 07:58:05.362236 28033 net.cpp:172] drop1 does not need backward computation.
I1026 07:58:05.362239 28033 net.cpp:172] relu1 does not need backward computation.
I1026 07:58:05.362242 28033 net.cpp:172] pool1 does not need backward computation.
I1026 07:58:05.362246 28033 net.cpp:172] conv1 does not need backward computation.
I1026 07:58:05.362251 28033 net.cpp:208] This network produces output prob
I1026 07:58:05.362265 28033 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 07:58:05.362274 28033 net.cpp:219] Network initialization done.
I1026 07:58:05.362278 28033 net.cpp:220] Memory required for data: 1837200
I1026 07:59:02.959122 28033 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 07:59:02.959656 28033 net.cpp:358] Input 0 -> data
I1026 07:59:02.959710 28033 net.cpp:67] Creating Layer conv1
I1026 07:59:02.959727 28033 net.cpp:394] conv1 <- data
I1026 07:59:02.959744 28033 net.cpp:356] conv1 -> conv1
I1026 07:59:02.959769 28033 net.cpp:96] Setting up conv1
I1026 07:59:02.959846 28033 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 07:59:02.959882 28033 net.cpp:67] Creating Layer pool1
I1026 07:59:02.959895 28033 net.cpp:394] pool1 <- conv1
I1026 07:59:02.959913 28033 net.cpp:356] pool1 -> pool1
I1026 07:59:02.959933 28033 net.cpp:96] Setting up pool1
I1026 07:59:02.959949 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:59:02.959967 28033 net.cpp:67] Creating Layer relu1
I1026 07:59:02.959980 28033 net.cpp:394] relu1 <- pool1
I1026 07:59:02.959993 28033 net.cpp:345] relu1 -> pool1 (in-place)
I1026 07:59:02.960011 28033 net.cpp:96] Setting up relu1
I1026 07:59:02.960023 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:59:02.960038 28033 net.cpp:67] Creating Layer drop1
I1026 07:59:02.960050 28033 net.cpp:394] drop1 <- pool1
I1026 07:59:02.960065 28033 net.cpp:345] drop1 -> pool1 (in-place)
I1026 07:59:02.960083 28033 net.cpp:96] Setting up drop1
I1026 07:59:02.960096 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 07:59:02.960114 28033 net.cpp:67] Creating Layer conv2
I1026 07:59:02.960126 28033 net.cpp:394] conv2 <- pool1
I1026 07:59:02.960142 28033 net.cpp:356] conv2 -> conv2
I1026 07:59:02.960162 28033 net.cpp:96] Setting up conv2
I1026 07:59:02.960923 28033 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 07:59:02.960942 28033 net.cpp:67] Creating Layer pool2
I1026 07:59:02.960947 28033 net.cpp:394] pool2 <- conv2
I1026 07:59:02.960953 28033 net.cpp:356] pool2 -> pool2
I1026 07:59:02.960963 28033 net.cpp:96] Setting up pool2
I1026 07:59:02.960968 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:59:02.960973 28033 net.cpp:67] Creating Layer relu2
I1026 07:59:02.960978 28033 net.cpp:394] relu2 <- pool2
I1026 07:59:02.960983 28033 net.cpp:345] relu2 -> pool2 (in-place)
I1026 07:59:02.960988 28033 net.cpp:96] Setting up relu2
I1026 07:59:02.960994 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:59:02.960999 28033 net.cpp:67] Creating Layer drop2
I1026 07:59:02.961002 28033 net.cpp:394] drop2 <- pool2
I1026 07:59:02.961007 28033 net.cpp:345] drop2 -> pool2 (in-place)
I1026 07:59:02.961014 28033 net.cpp:96] Setting up drop2
I1026 07:59:02.961019 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 07:59:02.961026 28033 net.cpp:67] Creating Layer conv3
I1026 07:59:02.961030 28033 net.cpp:394] conv3 <- pool2
I1026 07:59:02.961036 28033 net.cpp:356] conv3 -> conv3
I1026 07:59:02.961043 28033 net.cpp:96] Setting up conv3
I1026 07:59:02.962364 28033 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 07:59:02.962379 28033 net.cpp:67] Creating Layer pool3
I1026 07:59:02.962383 28033 net.cpp:394] pool3 <- conv3
I1026 07:59:02.962389 28033 net.cpp:356] pool3 -> pool3
I1026 07:59:02.962396 28033 net.cpp:96] Setting up pool3
I1026 07:59:02.962401 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:59:02.962407 28033 net.cpp:67] Creating Layer relu3
I1026 07:59:02.962411 28033 net.cpp:394] relu3 <- pool3
I1026 07:59:02.962416 28033 net.cpp:345] relu3 -> pool3 (in-place)
I1026 07:59:02.962422 28033 net.cpp:96] Setting up relu3
I1026 07:59:02.962426 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:59:02.962432 28033 net.cpp:67] Creating Layer drop3
I1026 07:59:02.962436 28033 net.cpp:394] drop3 <- pool3
I1026 07:59:02.962441 28033 net.cpp:345] drop3 -> pool3 (in-place)
I1026 07:59:02.962447 28033 net.cpp:96] Setting up drop3
I1026 07:59:02.962452 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 07:59:02.962458 28033 net.cpp:67] Creating Layer ip1
I1026 07:59:02.962462 28033 net.cpp:394] ip1 <- pool3
I1026 07:59:02.962469 28033 net.cpp:356] ip1 -> ip1
I1026 07:59:02.962476 28033 net.cpp:96] Setting up ip1
I1026 07:59:03.309831 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:59:03.309897 28033 net.cpp:67] Creating Layer relu4
I1026 07:59:03.309905 28033 net.cpp:394] relu4 <- ip1
I1026 07:59:03.309916 28033 net.cpp:345] relu4 -> ip1 (in-place)
I1026 07:59:03.309926 28033 net.cpp:96] Setting up relu4
I1026 07:59:03.309931 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:59:03.309939 28033 net.cpp:67] Creating Layer drop4
I1026 07:59:03.309955 28033 net.cpp:394] drop4 <- ip1
I1026 07:59:03.309962 28033 net.cpp:345] drop4 -> ip1 (in-place)
I1026 07:59:03.309970 28033 net.cpp:96] Setting up drop4
I1026 07:59:03.309976 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 07:59:03.309986 28033 net.cpp:67] Creating Layer ip2
I1026 07:59:03.309990 28033 net.cpp:394] ip2 <- ip1
I1026 07:59:03.309999 28033 net.cpp:356] ip2 -> ip2
I1026 07:59:03.310014 28033 net.cpp:96] Setting up ip2
I1026 07:59:03.317631 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:59:03.317692 28033 net.cpp:67] Creating Layer prob
I1026 07:59:03.317698 28033 net.cpp:394] prob <- ip2
I1026 07:59:03.317708 28033 net.cpp:356] prob -> prob
I1026 07:59:03.317719 28033 net.cpp:96] Setting up prob
I1026 07:59:03.317728 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 07:59:03.317734 28033 net.cpp:172] prob does not need backward computation.
I1026 07:59:03.317737 28033 net.cpp:172] ip2 does not need backward computation.
I1026 07:59:03.317741 28033 net.cpp:172] drop4 does not need backward computation.
I1026 07:59:03.317745 28033 net.cpp:172] relu4 does not need backward computation.
I1026 07:59:03.317749 28033 net.cpp:172] ip1 does not need backward computation.
I1026 07:59:03.317754 28033 net.cpp:172] drop3 does not need backward computation.
I1026 07:59:03.317757 28033 net.cpp:172] relu3 does not need backward computation.
I1026 07:59:03.317760 28033 net.cpp:172] pool3 does not need backward computation.
I1026 07:59:03.317764 28033 net.cpp:172] conv3 does not need backward computation.
I1026 07:59:03.317769 28033 net.cpp:172] drop2 does not need backward computation.
I1026 07:59:03.317772 28033 net.cpp:172] relu2 does not need backward computation.
I1026 07:59:03.317776 28033 net.cpp:172] pool2 does not need backward computation.
I1026 07:59:03.317780 28033 net.cpp:172] conv2 does not need backward computation.
I1026 07:59:03.317785 28033 net.cpp:172] drop1 does not need backward computation.
I1026 07:59:03.317788 28033 net.cpp:172] relu1 does not need backward computation.
I1026 07:59:03.317792 28033 net.cpp:172] pool1 does not need backward computation.
I1026 07:59:03.317796 28033 net.cpp:172] conv1 does not need backward computation.
I1026 07:59:03.317800 28033 net.cpp:208] This network produces output prob
I1026 07:59:03.317814 28033 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 07:59:03.317824 28033 net.cpp:219] Network initialization done.
I1026 07:59:03.317828 28033 net.cpp:220] Memory required for data: 1837200
I1026 08:00:01.821044 28033 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 08:00:01.821635 28033 net.cpp:358] Input 0 -> data
I1026 08:00:01.821691 28033 net.cpp:67] Creating Layer conv1
I1026 08:00:01.821707 28033 net.cpp:394] conv1 <- data
I1026 08:00:01.821725 28033 net.cpp:356] conv1 -> conv1
I1026 08:00:01.821748 28033 net.cpp:96] Setting up conv1
I1026 08:00:01.821810 28033 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 08:00:01.821846 28033 net.cpp:67] Creating Layer pool1
I1026 08:00:01.821858 28033 net.cpp:394] pool1 <- conv1
I1026 08:00:01.821874 28033 net.cpp:356] pool1 -> pool1
I1026 08:00:01.821893 28033 net.cpp:96] Setting up pool1
I1026 08:00:01.821912 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 08:00:01.821928 28033 net.cpp:67] Creating Layer relu1
I1026 08:00:01.821940 28033 net.cpp:394] relu1 <- pool1
I1026 08:00:01.821955 28033 net.cpp:345] relu1 -> pool1 (in-place)
I1026 08:00:01.821971 28033 net.cpp:96] Setting up relu1
I1026 08:00:01.821984 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 08:00:01.821998 28033 net.cpp:67] Creating Layer drop1
I1026 08:00:01.822010 28033 net.cpp:394] drop1 <- pool1
I1026 08:00:01.822026 28033 net.cpp:345] drop1 -> pool1 (in-place)
I1026 08:00:01.822041 28033 net.cpp:96] Setting up drop1
I1026 08:00:01.822054 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 08:00:01.822072 28033 net.cpp:67] Creating Layer conv2
I1026 08:00:01.822084 28033 net.cpp:394] conv2 <- pool1
I1026 08:00:01.822100 28033 net.cpp:356] conv2 -> conv2
I1026 08:00:01.822119 28033 net.cpp:96] Setting up conv2
I1026 08:00:01.823500 28033 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 08:00:01.823534 28033 net.cpp:67] Creating Layer pool2
I1026 08:00:01.823547 28033 net.cpp:394] pool2 <- conv2
I1026 08:00:01.823564 28033 net.cpp:356] pool2 -> pool2
I1026 08:00:01.823583 28033 net.cpp:96] Setting up pool2
I1026 08:00:01.823599 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 08:00:01.823614 28033 net.cpp:67] Creating Layer relu2
I1026 08:00:01.823626 28033 net.cpp:394] relu2 <- pool2
I1026 08:00:01.823642 28033 net.cpp:345] relu2 -> pool2 (in-place)
I1026 08:00:01.823657 28033 net.cpp:96] Setting up relu2
I1026 08:00:01.823669 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 08:00:01.823683 28033 net.cpp:67] Creating Layer drop2
I1026 08:00:01.823694 28033 net.cpp:394] drop2 <- pool2
I1026 08:00:01.823709 28033 net.cpp:345] drop2 -> pool2 (in-place)
I1026 08:00:01.823725 28033 net.cpp:96] Setting up drop2
I1026 08:00:01.823737 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 08:00:01.823755 28033 net.cpp:67] Creating Layer conv3
I1026 08:00:01.823767 28033 net.cpp:394] conv3 <- pool2
I1026 08:00:01.823783 28033 net.cpp:356] conv3 -> conv3
I1026 08:00:01.823802 28033 net.cpp:96] Setting up conv3
I1026 08:00:01.827497 28033 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 08:00:01.827538 28033 net.cpp:67] Creating Layer pool3
I1026 08:00:01.827559 28033 net.cpp:394] pool3 <- conv3
I1026 08:00:01.827576 28033 net.cpp:356] pool3 -> pool3
I1026 08:00:01.827595 28033 net.cpp:96] Setting up pool3
I1026 08:00:01.827610 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 08:00:01.827625 28033 net.cpp:67] Creating Layer relu3
I1026 08:00:01.827637 28033 net.cpp:394] relu3 <- pool3
I1026 08:00:01.827651 28033 net.cpp:345] relu3 -> pool3 (in-place)
I1026 08:00:01.827667 28033 net.cpp:96] Setting up relu3
I1026 08:00:01.827678 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 08:00:01.827693 28033 net.cpp:67] Creating Layer drop3
I1026 08:00:01.827704 28033 net.cpp:394] drop3 <- pool3
I1026 08:00:01.827719 28033 net.cpp:345] drop3 -> pool3 (in-place)
I1026 08:00:01.827735 28033 net.cpp:96] Setting up drop3
I1026 08:00:01.827747 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 08:00:01.827764 28033 net.cpp:67] Creating Layer ip1
I1026 08:00:01.827775 28033 net.cpp:394] ip1 <- pool3
I1026 08:00:01.827792 28033 net.cpp:356] ip1 -> ip1
I1026 08:00:01.827811 28033 net.cpp:96] Setting up ip1
I1026 08:00:02.258960 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 08:00:02.259021 28033 net.cpp:67] Creating Layer relu4
I1026 08:00:02.259029 28033 net.cpp:394] relu4 <- ip1
I1026 08:00:02.259040 28033 net.cpp:345] relu4 -> ip1 (in-place)
I1026 08:00:02.259052 28033 net.cpp:96] Setting up relu4
I1026 08:00:02.259057 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 08:00:02.259065 28033 net.cpp:67] Creating Layer drop4
I1026 08:00:02.259070 28033 net.cpp:394] drop4 <- ip1
I1026 08:00:02.259078 28033 net.cpp:345] drop4 -> ip1 (in-place)
I1026 08:00:02.259084 28033 net.cpp:96] Setting up drop4
I1026 08:00:02.259089 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 08:00:02.259100 28033 net.cpp:67] Creating Layer ip2
I1026 08:00:02.259104 28033 net.cpp:394] ip2 <- ip1
I1026 08:00:02.259112 28033 net.cpp:356] ip2 -> ip2
I1026 08:00:02.259125 28033 net.cpp:96] Setting up ip2
I1026 08:00:02.266690 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 08:00:02.266748 28033 net.cpp:67] Creating Layer prob
I1026 08:00:02.266757 28033 net.cpp:394] prob <- ip2
I1026 08:00:02.266765 28033 net.cpp:356] prob -> prob
I1026 08:00:02.266777 28033 net.cpp:96] Setting up prob
I1026 08:00:02.266785 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 08:00:02.266790 28033 net.cpp:172] prob does not need backward computation.
I1026 08:00:02.266794 28033 net.cpp:172] ip2 does not need backward computation.
I1026 08:00:02.266798 28033 net.cpp:172] drop4 does not need backward computation.
I1026 08:00:02.266803 28033 net.cpp:172] relu4 does not need backward computation.
I1026 08:00:02.266806 28033 net.cpp:172] ip1 does not need backward computation.
I1026 08:00:02.266810 28033 net.cpp:172] drop3 does not need backward computation.
I1026 08:00:02.266813 28033 net.cpp:172] relu3 does not need backward computation.
I1026 08:00:02.266818 28033 net.cpp:172] pool3 does not need backward computation.
I1026 08:00:02.266821 28033 net.cpp:172] conv3 does not need backward computation.
I1026 08:00:02.266825 28033 net.cpp:172] drop2 does not need backward computation.
I1026 08:00:02.266829 28033 net.cpp:172] relu2 does not need backward computation.
I1026 08:00:02.266832 28033 net.cpp:172] pool2 does not need backward computation.
I1026 08:00:02.266836 28033 net.cpp:172] conv2 does not need backward computation.
I1026 08:00:02.266840 28033 net.cpp:172] drop1 does not need backward computation.
I1026 08:00:02.266844 28033 net.cpp:172] relu1 does not need backward computation.
I1026 08:00:02.266847 28033 net.cpp:172] pool1 does not need backward computation.
I1026 08:00:02.266851 28033 net.cpp:172] conv1 does not need backward computation.
I1026 08:00:02.266855 28033 net.cpp:208] This network produces output prob
I1026 08:00:02.266870 28033 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 08:00:02.266880 28033 net.cpp:219] Network initialization done.
I1026 08:00:02.266883 28033 net.cpp:220] Memory required for data: 1837200
I1026 08:01:00.642014 28033 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 08:01:00.642495 28033 net.cpp:358] Input 0 -> data
I1026 08:01:00.642524 28033 net.cpp:67] Creating Layer conv1
I1026 08:01:00.642530 28033 net.cpp:394] conv1 <- data
I1026 08:01:00.642539 28033 net.cpp:356] conv1 -> conv1
I1026 08:01:00.642549 28033 net.cpp:96] Setting up conv1
I1026 08:01:00.642578 28033 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 08:01:00.642593 28033 net.cpp:67] Creating Layer pool1
I1026 08:01:00.642598 28033 net.cpp:394] pool1 <- conv1
I1026 08:01:00.642604 28033 net.cpp:356] pool1 -> pool1
I1026 08:01:00.642611 28033 net.cpp:96] Setting up pool1
I1026 08:01:00.642618 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 08:01:00.642626 28033 net.cpp:67] Creating Layer relu1
I1026 08:01:00.642629 28033 net.cpp:394] relu1 <- pool1
I1026 08:01:00.642634 28033 net.cpp:345] relu1 -> pool1 (in-place)
I1026 08:01:00.642640 28033 net.cpp:96] Setting up relu1
I1026 08:01:00.642645 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 08:01:00.642650 28033 net.cpp:67] Creating Layer drop1
I1026 08:01:00.642654 28033 net.cpp:394] drop1 <- pool1
I1026 08:01:00.642660 28033 net.cpp:345] drop1 -> pool1 (in-place)
I1026 08:01:00.642665 28033 net.cpp:96] Setting up drop1
I1026 08:01:00.642670 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 08:01:00.642685 28033 net.cpp:67] Creating Layer conv2
I1026 08:01:00.642690 28033 net.cpp:394] conv2 <- pool1
I1026 08:01:00.642696 28033 net.cpp:356] conv2 -> conv2
I1026 08:01:00.642704 28033 net.cpp:96] Setting up conv2
I1026 08:01:00.643204 28033 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 08:01:00.643218 28033 net.cpp:67] Creating Layer pool2
I1026 08:01:00.643224 28033 net.cpp:394] pool2 <- conv2
I1026 08:01:00.643229 28033 net.cpp:356] pool2 -> pool2
I1026 08:01:00.643236 28033 net.cpp:96] Setting up pool2
I1026 08:01:00.643242 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 08:01:00.643249 28033 net.cpp:67] Creating Layer relu2
I1026 08:01:00.643251 28033 net.cpp:394] relu2 <- pool2
I1026 08:01:00.643257 28033 net.cpp:345] relu2 -> pool2 (in-place)
I1026 08:01:00.643262 28033 net.cpp:96] Setting up relu2
I1026 08:01:00.643266 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 08:01:00.643271 28033 net.cpp:67] Creating Layer drop2
I1026 08:01:00.643275 28033 net.cpp:394] drop2 <- pool2
I1026 08:01:00.643280 28033 net.cpp:345] drop2 -> pool2 (in-place)
I1026 08:01:00.643286 28033 net.cpp:96] Setting up drop2
I1026 08:01:00.643290 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 08:01:00.643297 28033 net.cpp:67] Creating Layer conv3
I1026 08:01:00.643302 28033 net.cpp:394] conv3 <- pool2
I1026 08:01:00.643308 28033 net.cpp:356] conv3 -> conv3
I1026 08:01:00.643316 28033 net.cpp:96] Setting up conv3
I1026 08:01:00.644657 28033 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 08:01:00.644675 28033 net.cpp:67] Creating Layer pool3
I1026 08:01:00.644680 28033 net.cpp:394] pool3 <- conv3
I1026 08:01:00.644685 28033 net.cpp:356] pool3 -> pool3
I1026 08:01:00.644692 28033 net.cpp:96] Setting up pool3
I1026 08:01:00.644697 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 08:01:00.644703 28033 net.cpp:67] Creating Layer relu3
I1026 08:01:00.644707 28033 net.cpp:394] relu3 <- pool3
I1026 08:01:00.644712 28033 net.cpp:345] relu3 -> pool3 (in-place)
I1026 08:01:00.644718 28033 net.cpp:96] Setting up relu3
I1026 08:01:00.644722 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 08:01:00.644727 28033 net.cpp:67] Creating Layer drop3
I1026 08:01:00.644731 28033 net.cpp:394] drop3 <- pool3
I1026 08:01:00.644737 28033 net.cpp:345] drop3 -> pool3 (in-place)
I1026 08:01:00.644742 28033 net.cpp:96] Setting up drop3
I1026 08:01:00.644747 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 08:01:00.644753 28033 net.cpp:67] Creating Layer ip1
I1026 08:01:00.644757 28033 net.cpp:394] ip1 <- pool3
I1026 08:01:00.644764 28033 net.cpp:356] ip1 -> ip1
I1026 08:01:00.644772 28033 net.cpp:96] Setting up ip1
I1026 08:01:00.993160 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 08:01:00.993221 28033 net.cpp:67] Creating Layer relu4
I1026 08:01:00.993229 28033 net.cpp:394] relu4 <- ip1
I1026 08:01:00.993239 28033 net.cpp:345] relu4 -> ip1 (in-place)
I1026 08:01:00.993249 28033 net.cpp:96] Setting up relu4
I1026 08:01:00.993254 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 08:01:00.993263 28033 net.cpp:67] Creating Layer drop4
I1026 08:01:00.993268 28033 net.cpp:394] drop4 <- ip1
I1026 08:01:00.993274 28033 net.cpp:345] drop4 -> ip1 (in-place)
I1026 08:01:00.993281 28033 net.cpp:96] Setting up drop4
I1026 08:01:00.993288 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 08:01:00.993296 28033 net.cpp:67] Creating Layer ip2
I1026 08:01:00.993301 28033 net.cpp:394] ip2 <- ip1
I1026 08:01:00.993309 28033 net.cpp:356] ip2 -> ip2
I1026 08:01:00.993322 28033 net.cpp:96] Setting up ip2
I1026 08:01:01.000895 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 08:01:01.000952 28033 net.cpp:67] Creating Layer prob
I1026 08:01:01.000959 28033 net.cpp:394] prob <- ip2
I1026 08:01:01.000969 28033 net.cpp:356] prob -> prob
I1026 08:01:01.000980 28033 net.cpp:96] Setting up prob
I1026 08:01:01.000988 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 08:01:01.000993 28033 net.cpp:172] prob does not need backward computation.
I1026 08:01:01.000998 28033 net.cpp:172] ip2 does not need backward computation.
I1026 08:01:01.001011 28033 net.cpp:172] drop4 does not need backward computation.
I1026 08:01:01.001016 28033 net.cpp:172] relu4 does not need backward computation.
I1026 08:01:01.001020 28033 net.cpp:172] ip1 does not need backward computation.
I1026 08:01:01.001024 28033 net.cpp:172] drop3 does not need backward computation.
I1026 08:01:01.001029 28033 net.cpp:172] relu3 does not need backward computation.
I1026 08:01:01.001032 28033 net.cpp:172] pool3 does not need backward computation.
I1026 08:01:01.001035 28033 net.cpp:172] conv3 does not need backward computation.
I1026 08:01:01.001039 28033 net.cpp:172] drop2 does not need backward computation.
I1026 08:01:01.001044 28033 net.cpp:172] relu2 does not need backward computation.
I1026 08:01:01.001047 28033 net.cpp:172] pool2 does not need backward computation.
I1026 08:01:01.001051 28033 net.cpp:172] conv2 does not need backward computation.
I1026 08:01:01.001055 28033 net.cpp:172] drop1 does not need backward computation.
I1026 08:01:01.001058 28033 net.cpp:172] relu1 does not need backward computation.
I1026 08:01:01.001062 28033 net.cpp:172] pool1 does not need backward computation.
I1026 08:01:01.001066 28033 net.cpp:172] conv1 does not need backward computation.
I1026 08:01:01.001070 28033 net.cpp:208] This network produces output prob
I1026 08:01:01.001085 28033 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 08:01:01.001094 28033 net.cpp:219] Network initialization done.
I1026 08:01:01.001098 28033 net.cpp:220] Memory required for data: 1837200
I1026 08:01:59.128957 28033 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 08:01:59.129525 28033 net.cpp:358] Input 0 -> data
I1026 08:01:59.129573 28033 net.cpp:67] Creating Layer conv1
I1026 08:01:59.129586 28033 net.cpp:394] conv1 <- data
I1026 08:01:59.129601 28033 net.cpp:356] conv1 -> conv1
I1026 08:01:59.129622 28033 net.cpp:96] Setting up conv1
I1026 08:01:59.129676 28033 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 08:01:59.129706 28033 net.cpp:67] Creating Layer pool1
I1026 08:01:59.129717 28033 net.cpp:394] pool1 <- conv1
I1026 08:01:59.129730 28033 net.cpp:356] pool1 -> pool1
I1026 08:01:59.129746 28033 net.cpp:96] Setting up pool1
I1026 08:01:59.129760 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 08:01:59.129776 28033 net.cpp:67] Creating Layer relu1
I1026 08:01:59.129784 28033 net.cpp:394] relu1 <- pool1
I1026 08:01:59.129796 28033 net.cpp:345] relu1 -> pool1 (in-place)
I1026 08:01:59.129809 28033 net.cpp:96] Setting up relu1
I1026 08:01:59.129819 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 08:01:59.129832 28033 net.cpp:67] Creating Layer drop1
I1026 08:01:59.129842 28033 net.cpp:394] drop1 <- pool1
I1026 08:01:59.129853 28033 net.cpp:345] drop1 -> pool1 (in-place)
I1026 08:01:59.129868 28033 net.cpp:96] Setting up drop1
I1026 08:01:59.129878 28033 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 08:01:59.129894 28033 net.cpp:67] Creating Layer conv2
I1026 08:01:59.129904 28033 net.cpp:394] conv2 <- pool1
I1026 08:01:59.129917 28033 net.cpp:356] conv2 -> conv2
I1026 08:01:59.129932 28033 net.cpp:96] Setting up conv2
I1026 08:01:59.131063 28033 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 08:01:59.131091 28033 net.cpp:67] Creating Layer pool2
I1026 08:01:59.131103 28033 net.cpp:394] pool2 <- conv2
I1026 08:01:59.131116 28033 net.cpp:356] pool2 -> pool2
I1026 08:01:59.131132 28033 net.cpp:96] Setting up pool2
I1026 08:01:59.131145 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 08:01:59.131157 28033 net.cpp:67] Creating Layer relu2
I1026 08:01:59.131166 28033 net.cpp:394] relu2 <- pool2
I1026 08:01:59.131178 28033 net.cpp:345] relu2 -> pool2 (in-place)
I1026 08:01:59.131191 28033 net.cpp:96] Setting up relu2
I1026 08:01:59.131201 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 08:01:59.131212 28033 net.cpp:67] Creating Layer drop2
I1026 08:01:59.131222 28033 net.cpp:394] drop2 <- pool2
I1026 08:01:59.131242 28033 net.cpp:345] drop2 -> pool2 (in-place)
I1026 08:01:59.131258 28033 net.cpp:96] Setting up drop2
I1026 08:01:59.131271 28033 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 08:01:59.131290 28033 net.cpp:67] Creating Layer conv3
I1026 08:01:59.131302 28033 net.cpp:394] conv3 <- pool2
I1026 08:01:59.131319 28033 net.cpp:356] conv3 -> conv3
I1026 08:01:59.131337 28033 net.cpp:96] Setting up conv3
I1026 08:01:59.135020 28033 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 08:01:59.135059 28033 net.cpp:67] Creating Layer pool3
I1026 08:01:59.135073 28033 net.cpp:394] pool3 <- conv3
I1026 08:01:59.135089 28033 net.cpp:356] pool3 -> pool3
I1026 08:01:59.135108 28033 net.cpp:96] Setting up pool3
I1026 08:01:59.135123 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 08:01:59.135138 28033 net.cpp:67] Creating Layer relu3
I1026 08:01:59.135150 28033 net.cpp:394] relu3 <- pool3
I1026 08:01:59.135165 28033 net.cpp:345] relu3 -> pool3 (in-place)
I1026 08:01:59.135180 28033 net.cpp:96] Setting up relu3
I1026 08:01:59.135192 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 08:01:59.135207 28033 net.cpp:67] Creating Layer drop3
I1026 08:01:59.135218 28033 net.cpp:394] drop3 <- pool3
I1026 08:01:59.135231 28033 net.cpp:345] drop3 -> pool3 (in-place)
I1026 08:01:59.135236 28033 net.cpp:96] Setting up drop3
I1026 08:01:59.135241 28033 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 08:01:59.135248 28033 net.cpp:67] Creating Layer ip1
I1026 08:01:59.135252 28033 net.cpp:394] ip1 <- pool3
I1026 08:01:59.135262 28033 net.cpp:356] ip1 -> ip1
I1026 08:01:59.135270 28033 net.cpp:96] Setting up ip1
I1026 08:01:59.559453 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 08:01:59.559516 28033 net.cpp:67] Creating Layer relu4
I1026 08:01:59.559525 28033 net.cpp:394] relu4 <- ip1
I1026 08:01:59.559535 28033 net.cpp:345] relu4 -> ip1 (in-place)
I1026 08:01:59.559545 28033 net.cpp:96] Setting up relu4
I1026 08:01:59.559550 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 08:01:59.559558 28033 net.cpp:67] Creating Layer drop4
I1026 08:01:59.559562 28033 net.cpp:394] drop4 <- ip1
I1026 08:01:59.559571 28033 net.cpp:345] drop4 -> ip1 (in-place)
I1026 08:01:59.559577 28033 net.cpp:96] Setting up drop4
I1026 08:01:59.559583 28033 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 08:01:59.559593 28033 net.cpp:67] Creating Layer ip2
I1026 08:01:59.559597 28033 net.cpp:394] ip2 <- ip1
I1026 08:01:59.559605 28033 net.cpp:356] ip2 -> ip2
I1026 08:01:59.559619 28033 net.cpp:96] Setting up ip2
I1026 08:01:59.567167 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 08:01:59.567225 28033 net.cpp:67] Creating Layer prob
I1026 08:01:59.567234 28033 net.cpp:394] prob <- ip2
I1026 08:01:59.567244 28033 net.cpp:356] prob -> prob
I1026 08:01:59.567253 28033 net.cpp:96] Setting up prob
I1026 08:01:59.567262 28033 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 08:01:59.567266 28033 net.cpp:172] prob does not need backward computation.
I1026 08:01:59.567271 28033 net.cpp:172] ip2 does not need backward computation.
I1026 08:01:59.567275 28033 net.cpp:172] drop4 does not need backward computation.
I1026 08:01:59.567278 28033 net.cpp:172] relu4 does not need backward computation.
I1026 08:01:59.567282 28033 net.cpp:172] ip1 does not need backward computation.
I1026 08:01:59.567286 28033 net.cpp:172] drop3 does not need backward computation.
I1026 08:01:59.567291 28033 net.cpp:172] relu3 does not need backward computation.
I1026 08:01:59.567294 28033 net.cpp:172] pool3 does not need backward computation.
I1026 08:01:59.567298 28033 net.cpp:172] conv3 does not need backward computation.
I1026 08:01:59.567301 28033 net.cpp:172] drop2 does not need backward computation.
I1026 08:01:59.567306 28033 net.cpp:172] relu2 does not need backward computation.
I1026 08:01:59.567309 28033 net.cpp:172] pool2 does not need backward computation.
I1026 08:01:59.567313 28033 net.cpp:172] conv2 does not need backward computation.
I1026 08:01:59.567317 28033 net.cpp:172] drop1 does not need backward computation.
I1026 08:01:59.567322 28033 net.cpp:172] relu1 does not need backward computation.
I1026 08:01:59.567324 28033 net.cpp:172] pool1 does not need backward computation.
I1026 08:01:59.567328 28033 net.cpp:172] conv1 does not need backward computation.
I1026 08:01:59.567332 28033 net.cpp:208] This network produces output prob
I1026 08:01:59.567347 28033 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 08:01:59.567356 28033 net.cpp:219] Network initialization done.
I1026 08:01:59.567360 28033 net.cpp:220] Memory required for data: 1837200
I1026 08:58:23.999416 23697 convert_imageset.cpp:70] Shuffling data
I1026 08:58:24.714747 23697 convert_imageset.cpp:73] A total of 56898 images.
I1026 08:58:24.714826 23697 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1026 08:58:26.906839 23697 convert_imageset.cpp:177] Processed 1000 files.
E1026 08:58:28.839007 23697 convert_imageset.cpp:177] Processed 2000 files.
E1026 08:58:30.753778 23697 convert_imageset.cpp:177] Processed 3000 files.
E1026 08:58:32.600469 23697 convert_imageset.cpp:177] Processed 4000 files.
E1026 08:58:34.473732 23697 convert_imageset.cpp:177] Processed 5000 files.
E1026 08:58:36.199990 23697 convert_imageset.cpp:177] Processed 6000 files.
E1026 08:58:38.035033 23697 convert_imageset.cpp:177] Processed 7000 files.
E1026 08:58:39.715297 23697 convert_imageset.cpp:177] Processed 8000 files.
E1026 08:58:41.448268 23697 convert_imageset.cpp:177] Processed 9000 files.
E1026 08:58:43.111758 23697 convert_imageset.cpp:177] Processed 10000 files.
E1026 08:58:44.815256 23697 convert_imageset.cpp:177] Processed 11000 files.
E1026 08:58:46.447516 23697 convert_imageset.cpp:177] Processed 12000 files.
E1026 08:58:48.178748 23697 convert_imageset.cpp:177] Processed 13000 files.
E1026 08:58:49.932263 23697 convert_imageset.cpp:177] Processed 14000 files.
E1026 08:58:51.716460 23697 convert_imageset.cpp:177] Processed 15000 files.
E1026 08:58:53.319432 23697 convert_imageset.cpp:177] Processed 16000 files.
E1026 08:58:54.866566 23697 convert_imageset.cpp:177] Processed 17000 files.
E1026 08:58:56.448555 23697 convert_imageset.cpp:177] Processed 18000 files.
E1026 08:58:58.017302 23697 convert_imageset.cpp:177] Processed 19000 files.
E1026 08:58:59.661078 23697 convert_imageset.cpp:177] Processed 20000 files.
E1026 08:59:01.269045 23697 convert_imageset.cpp:177] Processed 21000 files.
E1026 08:59:02.879273 23697 convert_imageset.cpp:177] Processed 22000 files.
E1026 08:59:04.425310 23697 convert_imageset.cpp:177] Processed 23000 files.
E1026 08:59:06.008332 23697 convert_imageset.cpp:177] Processed 24000 files.
E1026 08:59:07.566998 23697 convert_imageset.cpp:177] Processed 25000 files.
E1026 08:59:09.041733 23697 convert_imageset.cpp:177] Processed 26000 files.
E1026 08:59:10.537948 23697 convert_imageset.cpp:177] Processed 27000 files.
E1026 08:59:12.083838 23697 convert_imageset.cpp:177] Processed 28000 files.
E1026 08:59:13.748574 23697 convert_imageset.cpp:177] Processed 29000 files.
E1026 08:59:15.319596 23697 convert_imageset.cpp:177] Processed 30000 files.
E1026 08:59:16.811060 23697 convert_imageset.cpp:177] Processed 31000 files.
E1026 08:59:18.343055 23697 convert_imageset.cpp:177] Processed 32000 files.
E1026 08:59:19.907747 23697 convert_imageset.cpp:177] Processed 33000 files.
E1026 08:59:21.537164 23697 convert_imageset.cpp:177] Processed 34000 files.
E1026 08:59:23.035051 23697 convert_imageset.cpp:177] Processed 35000 files.
E1026 08:59:24.569723 23697 convert_imageset.cpp:177] Processed 36000 files.
E1026 08:59:25.999752 23697 convert_imageset.cpp:177] Processed 37000 files.
E1026 08:59:27.540006 23697 convert_imageset.cpp:177] Processed 38000 files.
E1026 08:59:29.095932 23697 convert_imageset.cpp:177] Processed 39000 files.
E1026 08:59:30.617751 23697 convert_imageset.cpp:177] Processed 40000 files.
E1026 08:59:32.103766 23697 convert_imageset.cpp:177] Processed 41000 files.
E1026 08:59:33.479465 23697 convert_imageset.cpp:177] Processed 42000 files.
E1026 08:59:34.928995 23697 convert_imageset.cpp:177] Processed 43000 files.
E1026 08:59:36.381217 23697 convert_imageset.cpp:177] Processed 44000 files.
E1026 08:59:37.859369 23697 convert_imageset.cpp:177] Processed 45000 files.
E1026 08:59:39.290501 23697 convert_imageset.cpp:177] Processed 46000 files.
E1026 08:59:40.701968 23697 convert_imageset.cpp:177] Processed 47000 files.
E1026 08:59:42.147490 23697 convert_imageset.cpp:177] Processed 48000 files.
E1026 08:59:43.667896 23697 convert_imageset.cpp:177] Processed 49000 files.
E1026 08:59:45.088721 23697 convert_imageset.cpp:177] Processed 50000 files.
E1026 08:59:46.543419 23697 convert_imageset.cpp:177] Processed 51000 files.
E1026 08:59:47.970041 23697 convert_imageset.cpp:177] Processed 52000 files.
E1026 08:59:49.504583 23697 convert_imageset.cpp:177] Processed 53000 files.
E1026 08:59:51.035629 23697 convert_imageset.cpp:177] Processed 54000 files.
E1026 08:59:52.473291 23697 convert_imageset.cpp:177] Processed 55000 files.
E1026 08:59:53.936087 23697 convert_imageset.cpp:177] Processed 56000 files.
E1026 08:59:55.472743 23697 convert_imageset.cpp:193] Processed 56898 files.
I1026 08:59:55.766759 24283 caffe.cpp:99] Use GPU with device ID 0
I1026 08:59:56.236217 24283 caffe.cpp:107] Starting Optimization
I1026 08:59:56.236342 24283 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 75000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1026 08:59:56.236373 24283 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1026 08:59:56.247192 24283 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1026 08:59:56.247282 24283 net.cpp:67] Creating Layer mnist
I1026 08:59:56.247293 24283 net.cpp:356] mnist -> data
I1026 08:59:56.247310 24283 net.cpp:356] mnist -> label
I1026 08:59:56.247323 24283 net.cpp:96] Setting up mnist
I1026 08:59:56.254997 24283 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1026 08:59:56.255123 24283 data_layer.cpp:128] output data size: 64,1,50,180
I1026 08:59:56.256844 24283 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1026 08:59:56.256883 24283 net.cpp:103] Top shape: 64 1 1 1 (64)
I1026 08:59:56.256911 24283 net.cpp:67] Creating Layer conv1
I1026 08:59:56.256925 24283 net.cpp:394] conv1 <- data
I1026 08:59:56.256966 24283 net.cpp:356] conv1 -> conv1
I1026 08:59:56.256989 24283 net.cpp:96] Setting up conv1
I1026 08:59:56.257349 24283 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1026 08:59:56.257382 24283 net.cpp:67] Creating Layer pool1
I1026 08:59:56.257390 24283 net.cpp:394] pool1 <- conv1
I1026 08:59:56.257396 24283 net.cpp:356] pool1 -> pool1
I1026 08:59:56.257405 24283 net.cpp:96] Setting up pool1
I1026 08:59:56.257422 24283 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 08:59:56.257431 24283 net.cpp:67] Creating Layer relu1
I1026 08:59:56.257434 24283 net.cpp:394] relu1 <- pool1
I1026 08:59:56.257441 24283 net.cpp:345] relu1 -> pool1 (in-place)
I1026 08:59:56.257447 24283 net.cpp:96] Setting up relu1
I1026 08:59:56.257452 24283 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 08:59:56.257459 24283 net.cpp:67] Creating Layer drop1
I1026 08:59:56.257463 24283 net.cpp:394] drop1 <- pool1
I1026 08:59:56.257472 24283 net.cpp:345] drop1 -> pool1 (in-place)
I1026 08:59:56.257478 24283 net.cpp:96] Setting up drop1
I1026 08:59:56.257484 24283 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 08:59:56.257491 24283 net.cpp:67] Creating Layer conv2
I1026 08:59:56.257496 24283 net.cpp:394] conv2 <- pool1
I1026 08:59:56.257504 24283 net.cpp:356] conv2 -> conv2
I1026 08:59:56.257513 24283 net.cpp:96] Setting up conv2
I1026 08:59:56.258090 24283 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1026 08:59:56.258107 24283 net.cpp:67] Creating Layer pool2
I1026 08:59:56.258113 24283 net.cpp:394] pool2 <- conv2
I1026 08:59:56.258121 24283 net.cpp:356] pool2 -> pool2
I1026 08:59:56.258127 24283 net.cpp:96] Setting up pool2
I1026 08:59:56.258133 24283 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 08:59:56.258139 24283 net.cpp:67] Creating Layer relu2
I1026 08:59:56.258144 24283 net.cpp:394] relu2 <- pool2
I1026 08:59:56.258152 24283 net.cpp:345] relu2 -> pool2 (in-place)
I1026 08:59:56.258159 24283 net.cpp:96] Setting up relu2
I1026 08:59:56.258163 24283 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 08:59:56.258172 24283 net.cpp:67] Creating Layer drop2
I1026 08:59:56.258177 24283 net.cpp:394] drop2 <- pool2
I1026 08:59:56.258182 24283 net.cpp:345] drop2 -> pool2 (in-place)
I1026 08:59:56.258188 24283 net.cpp:96] Setting up drop2
I1026 08:59:56.258193 24283 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 08:59:56.258203 24283 net.cpp:67] Creating Layer conv3
I1026 08:59:56.258208 24283 net.cpp:394] conv3 <- pool2
I1026 08:59:56.258213 24283 net.cpp:356] conv3 -> conv3
I1026 08:59:56.258220 24283 net.cpp:96] Setting up conv3
I1026 08:59:56.261145 24283 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1026 08:59:56.261196 24283 net.cpp:67] Creating Layer pool3
I1026 08:59:56.261211 24283 net.cpp:394] pool3 <- conv3
I1026 08:59:56.261234 24283 net.cpp:356] pool3 -> pool3
I1026 08:59:56.261255 24283 net.cpp:96] Setting up pool3
I1026 08:59:56.261270 24283 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 08:59:56.261287 24283 net.cpp:67] Creating Layer relu3
I1026 08:59:56.261301 24283 net.cpp:394] relu3 <- pool3
I1026 08:59:56.261319 24283 net.cpp:345] relu3 -> pool3 (in-place)
I1026 08:59:56.261337 24283 net.cpp:96] Setting up relu3
I1026 08:59:56.261350 24283 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 08:59:56.261368 24283 net.cpp:67] Creating Layer drop3
I1026 08:59:56.261380 24283 net.cpp:394] drop3 <- pool3
I1026 08:59:56.261396 24283 net.cpp:345] drop3 -> pool3 (in-place)
I1026 08:59:56.261414 24283 net.cpp:96] Setting up drop3
I1026 08:59:56.261426 24283 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 08:59:56.261445 24283 net.cpp:67] Creating Layer ip1
I1026 08:59:56.261457 24283 net.cpp:394] ip1 <- pool3
I1026 08:59:56.261478 24283 net.cpp:356] ip1 -> ip1
I1026 08:59:56.261543 24283 net.cpp:96] Setting up ip1
I1026 08:59:56.765292 24283 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 08:59:56.765354 24283 net.cpp:67] Creating Layer relu4
I1026 08:59:56.765362 24283 net.cpp:394] relu4 <- ip1
I1026 08:59:56.765372 24283 net.cpp:345] relu4 -> ip1 (in-place)
I1026 08:59:56.765382 24283 net.cpp:96] Setting up relu4
I1026 08:59:56.765393 24283 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 08:59:56.765401 24283 net.cpp:67] Creating Layer drop4
I1026 08:59:56.765406 24283 net.cpp:394] drop4 <- ip1
I1026 08:59:56.765413 24283 net.cpp:345] drop4 -> ip1 (in-place)
I1026 08:59:56.765419 24283 net.cpp:96] Setting up drop4
I1026 08:59:56.765424 24283 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 08:59:56.765436 24283 net.cpp:67] Creating Layer ip2
I1026 08:59:56.765441 24283 net.cpp:394] ip2 <- ip1
I1026 08:59:56.765450 24283 net.cpp:356] ip2 -> ip2
I1026 08:59:56.765458 24283 net.cpp:96] Setting up ip2
I1026 08:59:56.775888 24283 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1026 08:59:56.775960 24283 net.cpp:67] Creating Layer loss
I1026 08:59:56.775967 24283 net.cpp:394] loss <- ip2
I1026 08:59:56.775975 24283 net.cpp:394] loss <- label
I1026 08:59:56.775982 24283 net.cpp:356] loss -> loss
I1026 08:59:56.775992 24283 net.cpp:96] Setting up loss
I1026 08:59:56.776005 24283 net.cpp:103] Top shape: 1 1 1 1 (1)
I1026 08:59:56.776010 24283 net.cpp:109]     with loss weight 1
I1026 08:59:56.776051 24283 net.cpp:170] loss needs backward computation.
I1026 08:59:56.776057 24283 net.cpp:170] ip2 needs backward computation.
I1026 08:59:56.776062 24283 net.cpp:170] drop4 needs backward computation.
I1026 08:59:56.776067 24283 net.cpp:170] relu4 needs backward computation.
I1026 08:59:56.776070 24283 net.cpp:170] ip1 needs backward computation.
I1026 08:59:56.776074 24283 net.cpp:170] drop3 needs backward computation.
I1026 08:59:56.776079 24283 net.cpp:170] relu3 needs backward computation.
I1026 08:59:56.776083 24283 net.cpp:170] pool3 needs backward computation.
I1026 08:59:56.776087 24283 net.cpp:170] conv3 needs backward computation.
I1026 08:59:56.776093 24283 net.cpp:170] drop2 needs backward computation.
I1026 08:59:56.776098 24283 net.cpp:170] relu2 needs backward computation.
I1026 08:59:56.776101 24283 net.cpp:170] pool2 needs backward computation.
I1026 08:59:56.776105 24283 net.cpp:170] conv2 needs backward computation.
I1026 08:59:56.776110 24283 net.cpp:170] drop1 needs backward computation.
I1026 08:59:56.776114 24283 net.cpp:170] relu1 needs backward computation.
I1026 08:59:56.776119 24283 net.cpp:170] pool1 needs backward computation.
I1026 08:59:56.776124 24283 net.cpp:170] conv1 needs backward computation.
I1026 08:59:56.776129 24283 net.cpp:172] mnist does not need backward computation.
I1026 08:59:56.776132 24283 net.cpp:208] This network produces output loss
I1026 08:59:56.776144 24283 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 08:59:56.776150 24283 net.cpp:219] Network initialization done.
I1026 08:59:56.776154 24283 net.cpp:220] Memory required for data: 119788292
I1026 08:59:56.776217 24283 solver.cpp:41] Solver scaffolding done.
I1026 08:59:56.776223 24283 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_50000.solverstate
I1026 08:59:56.776228 24283 solver.cpp:160] Solving Captcha
I1026 08:59:56.776248 24283 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_50000.solverstate
I1026 09:00:02.844331 24283 solver.cpp:502] SGDSolver: restoring history
I1026 09:00:03.859380 24283 solver.cpp:191] Iteration 50000, loss = 3.22673
I1026 09:00:03.859434 24283 solver.cpp:206]     Train net output #0: loss = 3.22673 (* 1 = 3.22673 loss)
I1026 09:00:03.859448 24283 solver.cpp:403] Iteration 50000, lr = 0.00260847
I1026 09:08:29.082031 24283 solver.cpp:191] Iteration 51000, loss = 2.95093
I1026 09:08:29.082886 24283 solver.cpp:206]     Train net output #0: loss = 2.95093 (* 1 = 2.95093 loss)
I1026 09:08:29.082919 24283 solver.cpp:403] Iteration 51000, lr = 0.00257634
I1026 09:16:55.847178 24283 solver.cpp:191] Iteration 52000, loss = 2.91887
I1026 09:16:55.847774 24283 solver.cpp:206]     Train net output #0: loss = 2.91887 (* 1 = 2.91887 loss)
I1026 09:16:55.847806 24283 solver.cpp:403] Iteration 52000, lr = 0.00254511
I1026 09:25:19.684522 24283 solver.cpp:191] Iteration 53000, loss = 2.95954
I1026 09:25:19.685355 24283 solver.cpp:206]     Train net output #0: loss = 2.95954 (* 1 = 2.95954 loss)
I1026 09:25:19.685389 24283 solver.cpp:403] Iteration 53000, lr = 0.00251475
I1026 09:33:45.688292 24283 solver.cpp:191] Iteration 54000, loss = 2.81119
I1026 09:33:45.689515 24283 solver.cpp:206]     Train net output #0: loss = 2.81119 (* 1 = 2.81119 loss)
I1026 09:33:45.689548 24283 solver.cpp:403] Iteration 54000, lr = 0.00248522
I1026 09:42:11.356464 24283 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_55000.caffemodel
I1026 09:42:15.361332 24283 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_55000.solverstate
I1026 09:42:18.929381 24283 solver.cpp:191] Iteration 55000, loss = 2.50184
I1026 09:42:18.929941 24283 solver.cpp:206]     Train net output #0: loss = 2.50184 (* 1 = 2.50184 loss)
I1026 09:42:18.929980 24283 solver.cpp:403] Iteration 55000, lr = 0.00245649
I1026 09:50:44.879510 24283 solver.cpp:191] Iteration 56000, loss = 2.70689
I1026 09:50:44.880100 24283 solver.cpp:206]     Train net output #0: loss = 2.70689 (* 1 = 2.70689 loss)
I1026 09:50:44.880132 24283 solver.cpp:403] Iteration 56000, lr = 0.00242852
I1026 09:59:10.166391 24283 solver.cpp:191] Iteration 57000, loss = 2.51014
I1026 09:59:10.166914 24283 solver.cpp:206]     Train net output #0: loss = 2.51014 (* 1 = 2.51014 loss)
I1026 09:59:10.166944 24283 solver.cpp:403] Iteration 57000, lr = 0.00240129
I1026 10:05:21.256325 24283 solver.cpp:191] Iteration 58000, loss = 2.4387
I1026 10:05:21.257160 24283 solver.cpp:206]     Train net output #0: loss = 2.4387 (* 1 = 2.4387 loss)
I1026 10:05:21.257194 24283 solver.cpp:403] Iteration 58000, lr = 0.00237475
I1026 10:09:22.179539 24283 solver.cpp:191] Iteration 59000, loss = 2.66194
I1026 10:09:22.180093 24283 solver.cpp:206]     Train net output #0: loss = 2.66194 (* 1 = 2.66194 loss)
I1026 10:09:22.180112 24283 solver.cpp:403] Iteration 59000, lr = 0.00234889
I1026 10:13:23.567131 24283 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_60000.caffemodel
I1026 10:13:29.320266 24283 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_60000.solverstate
I1026 10:13:32.718935 24283 solver.cpp:191] Iteration 60000, loss = 2.72962
I1026 10:13:32.719693 24283 solver.cpp:206]     Train net output #0: loss = 2.72962 (* 1 = 2.72962 loss)
I1026 10:13:32.719739 24283 solver.cpp:403] Iteration 60000, lr = 0.00232368
I1026 10:21:57.533771 24283 solver.cpp:191] Iteration 61000, loss = 3.00289
I1026 10:21:57.534642 24283 solver.cpp:206]     Train net output #0: loss = 3.00289 (* 1 = 3.00289 loss)
I1026 10:21:57.534678 24283 solver.cpp:403] Iteration 61000, lr = 0.00229909
I1026 10:30:23.660725 24283 solver.cpp:191] Iteration 62000, loss = 2.48806
I1026 10:30:23.661521 24283 solver.cpp:206]     Train net output #0: loss = 2.48806 (* 1 = 2.48806 loss)
I1026 10:30:23.661556 24283 solver.cpp:403] Iteration 62000, lr = 0.0022751
I1026 10:38:49.899992 24283 solver.cpp:191] Iteration 63000, loss = 2.57702
I1026 10:38:49.900617 24283 solver.cpp:206]     Train net output #0: loss = 2.57702 (* 1 = 2.57702 loss)
I1026 10:38:49.900650 24283 solver.cpp:403] Iteration 63000, lr = 0.00225169
I1026 10:47:14.892946 24283 solver.cpp:191] Iteration 64000, loss = 2.59436
I1026 10:47:14.893770 24283 solver.cpp:206]     Train net output #0: loss = 2.59436 (* 1 = 2.59436 loss)
I1026 10:47:14.893805 24283 solver.cpp:403] Iteration 64000, lr = 0.00222883
I1026 10:55:39.746675 24283 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_65000.caffemodel
I1026 10:55:44.446779 24283 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_65000.solverstate
I1026 10:55:49.759532 24283 solver.cpp:191] Iteration 65000, loss = 2.57828
I1026 10:55:49.760224 24283 solver.cpp:206]     Train net output #0: loss = 2.57828 (* 1 = 2.57828 loss)
I1026 10:55:49.760244 24283 solver.cpp:403] Iteration 65000, lr = 0.0022065
I1026 11:04:15.768111 24283 solver.cpp:191] Iteration 66000, loss = 2.39935
I1026 11:04:15.768889 24283 solver.cpp:206]     Train net output #0: loss = 2.39935 (* 1 = 2.39935 loss)
I1026 11:04:15.768926 24283 solver.cpp:403] Iteration 66000, lr = 0.00218469
I1026 11:12:41.585726 24283 solver.cpp:191] Iteration 67000, loss = 2.60418
I1026 11:12:41.586544 24283 solver.cpp:206]     Train net output #0: loss = 2.60418 (* 1 = 2.60418 loss)
I1026 11:12:41.586576 24283 solver.cpp:403] Iteration 67000, lr = 0.00216338
I1026 11:21:06.454509 24283 solver.cpp:191] Iteration 68000, loss = 2.44529
I1026 11:21:06.457923 24283 solver.cpp:206]     Train net output #0: loss = 2.44529 (* 1 = 2.44529 loss)
I1026 11:21:06.457955 24283 solver.cpp:403] Iteration 68000, lr = 0.00214254
I1026 11:29:31.252843 24283 solver.cpp:191] Iteration 69000, loss = 2.55185
I1026 11:29:31.253557 24283 solver.cpp:206]     Train net output #0: loss = 2.55185 (* 1 = 2.55185 loss)
I1026 11:29:31.253592 24283 solver.cpp:403] Iteration 69000, lr = 0.00212217
I1026 11:37:55.841635 24283 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_70000.caffemodel
I1026 11:38:00.366142 24283 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_70000.solverstate
I1026 11:38:04.284359 24283 solver.cpp:191] Iteration 70000, loss = 2.50625
I1026 11:38:04.284929 24283 solver.cpp:206]     Train net output #0: loss = 2.50625 (* 1 = 2.50625 loss)
I1026 11:38:04.284960 24283 solver.cpp:403] Iteration 70000, lr = 0.00210224
I1026 11:46:28.982767 24283 solver.cpp:191] Iteration 71000, loss = 2.56683
I1026 11:46:28.983595 24283 solver.cpp:206]     Train net output #0: loss = 2.56683 (* 1 = 2.56683 loss)
I1026 11:46:28.983628 24283 solver.cpp:403] Iteration 71000, lr = 0.00208275
I1026 11:54:53.494683 24283 solver.cpp:191] Iteration 72000, loss = 2.41358
I1026 11:54:53.495465 24283 solver.cpp:206]     Train net output #0: loss = 2.41358 (* 1 = 2.41358 loss)
I1026 11:54:53.495497 24283 solver.cpp:403] Iteration 72000, lr = 0.00206367
I1026 12:03:19.387290 24283 solver.cpp:191] Iteration 73000, loss = 2.58714
I1026 12:03:19.387912 24283 solver.cpp:206]     Train net output #0: loss = 2.58714 (* 1 = 2.58714 loss)
I1026 12:03:19.387949 24283 solver.cpp:403] Iteration 73000, lr = 0.00204499
I1026 12:11:43.938361 24283 solver.cpp:191] Iteration 74000, loss = 2.58832
I1026 12:11:43.939162 24283 solver.cpp:206]     Train net output #0: loss = 2.58832 (* 1 = 2.58832 loss)
I1026 12:11:43.939193 24283 solver.cpp:403] Iteration 74000, lr = 0.00202671
I1026 12:20:10.178328 24283 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_75000.caffemodel
I1026 12:20:16.344630 24283 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_75000.solverstate
I1026 12:20:19.778106 24283 solver.cpp:228] Iteration 75000, loss = 2.48318
I1026 12:20:19.778574 24283 solver.cpp:233] Optimization Done.
I1026 12:20:19.778597 24283 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1026 12:43:32.861497  4990 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 12:43:32.861605  4990 net.cpp:358] Input 0 -> data
I1026 12:43:32.861630  4990 net.cpp:67] Creating Layer conv1
I1026 12:43:32.861635  4990 net.cpp:394] conv1 <- data
I1026 12:43:32.861642  4990 net.cpp:356] conv1 -> conv1
I1026 12:43:32.861652  4990 net.cpp:96] Setting up conv1
I1026 12:43:32.861975  4990 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 12:43:32.861994  4990 net.cpp:67] Creating Layer pool1
I1026 12:43:32.861999  4990 net.cpp:394] pool1 <- conv1
I1026 12:43:32.862004  4990 net.cpp:356] pool1 -> pool1
I1026 12:43:32.862011  4990 net.cpp:96] Setting up pool1
I1026 12:43:32.862027  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:43:32.862035  4990 net.cpp:67] Creating Layer relu1
I1026 12:43:32.862038  4990 net.cpp:394] relu1 <- pool1
I1026 12:43:32.862043  4990 net.cpp:345] relu1 -> pool1 (in-place)
I1026 12:43:32.862049  4990 net.cpp:96] Setting up relu1
I1026 12:43:32.862053  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:43:32.862059  4990 net.cpp:67] Creating Layer drop1
I1026 12:43:32.862063  4990 net.cpp:394] drop1 <- pool1
I1026 12:43:32.862071  4990 net.cpp:345] drop1 -> pool1 (in-place)
I1026 12:43:32.862076  4990 net.cpp:96] Setting up drop1
I1026 12:43:32.862082  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:43:32.862088  4990 net.cpp:67] Creating Layer conv2
I1026 12:43:32.862092  4990 net.cpp:394] conv2 <- pool1
I1026 12:43:32.862099  4990 net.cpp:356] conv2 -> conv2
I1026 12:43:32.862107  4990 net.cpp:96] Setting up conv2
I1026 12:43:32.862655  4990 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 12:43:32.862671  4990 net.cpp:67] Creating Layer pool2
I1026 12:43:32.862676  4990 net.cpp:394] pool2 <- conv2
I1026 12:43:32.862681  4990 net.cpp:356] pool2 -> pool2
I1026 12:43:32.862689  4990 net.cpp:96] Setting up pool2
I1026 12:43:32.862694  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:43:32.862701  4990 net.cpp:67] Creating Layer relu2
I1026 12:43:32.862707  4990 net.cpp:394] relu2 <- pool2
I1026 12:43:32.862714  4990 net.cpp:345] relu2 -> pool2 (in-place)
I1026 12:43:32.862720  4990 net.cpp:96] Setting up relu2
I1026 12:43:32.862723  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:43:32.862728  4990 net.cpp:67] Creating Layer drop2
I1026 12:43:32.862732  4990 net.cpp:394] drop2 <- pool2
I1026 12:43:32.862740  4990 net.cpp:345] drop2 -> pool2 (in-place)
I1026 12:43:32.862745  4990 net.cpp:96] Setting up drop2
I1026 12:43:32.862750  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:43:32.862756  4990 net.cpp:67] Creating Layer conv3
I1026 12:43:32.862761  4990 net.cpp:394] conv3 <- pool2
I1026 12:43:32.862766  4990 net.cpp:356] conv3 -> conv3
I1026 12:43:32.862772  4990 net.cpp:96] Setting up conv3
I1026 12:43:32.864225  4990 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 12:43:32.864241  4990 net.cpp:67] Creating Layer pool3
I1026 12:43:32.864246  4990 net.cpp:394] pool3 <- conv3
I1026 12:43:32.864254  4990 net.cpp:356] pool3 -> pool3
I1026 12:43:32.864261  4990 net.cpp:96] Setting up pool3
I1026 12:43:32.864266  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:43:32.864272  4990 net.cpp:67] Creating Layer relu3
I1026 12:43:32.864276  4990 net.cpp:394] relu3 <- pool3
I1026 12:43:32.864282  4990 net.cpp:345] relu3 -> pool3 (in-place)
I1026 12:43:32.864289  4990 net.cpp:96] Setting up relu3
I1026 12:43:32.864292  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:43:32.864297  4990 net.cpp:67] Creating Layer drop3
I1026 12:43:32.864301  4990 net.cpp:394] drop3 <- pool3
I1026 12:43:32.864306  4990 net.cpp:345] drop3 -> pool3 (in-place)
I1026 12:43:32.864312  4990 net.cpp:96] Setting up drop3
I1026 12:43:32.864316  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:43:32.864322  4990 net.cpp:67] Creating Layer ip1
I1026 12:43:32.864326  4990 net.cpp:394] ip1 <- pool3
I1026 12:43:32.864334  4990 net.cpp:356] ip1 -> ip1
I1026 12:43:32.864341  4990 net.cpp:96] Setting up ip1
I1026 12:43:33.402326  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:43:33.402390  4990 net.cpp:67] Creating Layer relu4
I1026 12:43:33.402398  4990 net.cpp:394] relu4 <- ip1
I1026 12:43:33.402407  4990 net.cpp:345] relu4 -> ip1 (in-place)
I1026 12:43:33.402417  4990 net.cpp:96] Setting up relu4
I1026 12:43:33.402422  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:43:33.402429  4990 net.cpp:67] Creating Layer drop4
I1026 12:43:33.402433  4990 net.cpp:394] drop4 <- ip1
I1026 12:43:33.402438  4990 net.cpp:345] drop4 -> ip1 (in-place)
I1026 12:43:33.402446  4990 net.cpp:96] Setting up drop4
I1026 12:43:33.402451  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:43:33.402459  4990 net.cpp:67] Creating Layer ip2
I1026 12:43:33.402463  4990 net.cpp:394] ip2 <- ip1
I1026 12:43:33.402470  4990 net.cpp:356] ip2 -> ip2
I1026 12:43:33.402484  4990 net.cpp:96] Setting up ip2
I1026 12:43:33.412035  4990 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 12:43:33.412108  4990 net.cpp:67] Creating Layer prob
I1026 12:43:33.412114  4990 net.cpp:394] prob <- ip2
I1026 12:43:33.412123  4990 net.cpp:356] prob -> prob
I1026 12:43:33.412133  4990 net.cpp:96] Setting up prob
I1026 12:43:33.412140  4990 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 12:43:33.412144  4990 net.cpp:172] prob does not need backward computation.
I1026 12:43:33.412148  4990 net.cpp:172] ip2 does not need backward computation.
I1026 12:43:33.412152  4990 net.cpp:172] drop4 does not need backward computation.
I1026 12:43:33.412155  4990 net.cpp:172] relu4 does not need backward computation.
I1026 12:43:33.412159  4990 net.cpp:172] ip1 does not need backward computation.
I1026 12:43:33.412163  4990 net.cpp:172] drop3 does not need backward computation.
I1026 12:43:33.412166  4990 net.cpp:172] relu3 does not need backward computation.
I1026 12:43:33.412169  4990 net.cpp:172] pool3 does not need backward computation.
I1026 12:43:33.412173  4990 net.cpp:172] conv3 does not need backward computation.
I1026 12:43:33.412176  4990 net.cpp:172] drop2 does not need backward computation.
I1026 12:43:33.412186  4990 net.cpp:172] relu2 does not need backward computation.
I1026 12:43:33.412191  4990 net.cpp:172] pool2 does not need backward computation.
I1026 12:43:33.412194  4990 net.cpp:172] conv2 does not need backward computation.
I1026 12:43:33.412199  4990 net.cpp:172] drop1 does not need backward computation.
I1026 12:43:33.412201  4990 net.cpp:172] relu1 does not need backward computation.
I1026 12:43:33.412204  4990 net.cpp:172] pool1 does not need backward computation.
I1026 12:43:33.412209  4990 net.cpp:172] conv1 does not need backward computation.
I1026 12:43:33.412211  4990 net.cpp:208] This network produces output prob
I1026 12:43:33.412225  4990 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 12:43:33.412232  4990 net.cpp:219] Network initialization done.
I1026 12:43:33.412236  4990 net.cpp:220] Memory required for data: 1837200
I1026 12:44:41.615506  4990 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 12:44:41.616104  4990 net.cpp:358] Input 0 -> data
I1026 12:44:41.616160  4990 net.cpp:67] Creating Layer conv1
I1026 12:44:41.616176  4990 net.cpp:394] conv1 <- data
I1026 12:44:41.616194  4990 net.cpp:356] conv1 -> conv1
I1026 12:44:41.616219  4990 net.cpp:96] Setting up conv1
I1026 12:44:41.616282  4990 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 12:44:41.616318  4990 net.cpp:67] Creating Layer pool1
I1026 12:44:41.616346  4990 net.cpp:394] pool1 <- conv1
I1026 12:44:41.616364  4990 net.cpp:356] pool1 -> pool1
I1026 12:44:41.616384  4990 net.cpp:96] Setting up pool1
I1026 12:44:41.616401  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:44:41.616461  4990 net.cpp:67] Creating Layer relu1
I1026 12:44:41.616489  4990 net.cpp:394] relu1 <- pool1
I1026 12:44:41.616509  4990 net.cpp:345] relu1 -> pool1 (in-place)
I1026 12:44:41.616528  4990 net.cpp:96] Setting up relu1
I1026 12:44:41.616540  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:44:41.616556  4990 net.cpp:67] Creating Layer drop1
I1026 12:44:41.616569  4990 net.cpp:394] drop1 <- pool1
I1026 12:44:41.616583  4990 net.cpp:345] drop1 -> pool1 (in-place)
I1026 12:44:41.616600  4990 net.cpp:96] Setting up drop1
I1026 12:44:41.616612  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:44:41.616632  4990 net.cpp:67] Creating Layer conv2
I1026 12:44:41.616644  4990 net.cpp:394] conv2 <- pool1
I1026 12:44:41.616662  4990 net.cpp:356] conv2 -> conv2
I1026 12:44:41.616680  4990 net.cpp:96] Setting up conv2
I1026 12:44:41.618057  4990 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 12:44:41.618093  4990 net.cpp:67] Creating Layer pool2
I1026 12:44:41.618105  4990 net.cpp:394] pool2 <- conv2
I1026 12:44:41.618122  4990 net.cpp:356] pool2 -> pool2
I1026 12:44:41.618141  4990 net.cpp:96] Setting up pool2
I1026 12:44:41.618157  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:44:41.618173  4990 net.cpp:67] Creating Layer relu2
I1026 12:44:41.618185  4990 net.cpp:394] relu2 <- pool2
I1026 12:44:41.618201  4990 net.cpp:345] relu2 -> pool2 (in-place)
I1026 12:44:41.618216  4990 net.cpp:96] Setting up relu2
I1026 12:44:41.618227  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:44:41.618242  4990 net.cpp:67] Creating Layer drop2
I1026 12:44:41.618253  4990 net.cpp:394] drop2 <- pool2
I1026 12:44:41.618268  4990 net.cpp:345] drop2 -> pool2 (in-place)
I1026 12:44:41.618283  4990 net.cpp:96] Setting up drop2
I1026 12:44:41.618296  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:44:41.618315  4990 net.cpp:67] Creating Layer conv3
I1026 12:44:41.618327  4990 net.cpp:394] conv3 <- pool2
I1026 12:44:41.618345  4990 net.cpp:356] conv3 -> conv3
I1026 12:44:41.618363  4990 net.cpp:96] Setting up conv3
I1026 12:44:41.621779  4990 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 12:44:41.621822  4990 net.cpp:67] Creating Layer pool3
I1026 12:44:41.621836  4990 net.cpp:394] pool3 <- conv3
I1026 12:44:41.621853  4990 net.cpp:356] pool3 -> pool3
I1026 12:44:41.621872  4990 net.cpp:96] Setting up pool3
I1026 12:44:41.621887  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:44:41.621903  4990 net.cpp:67] Creating Layer relu3
I1026 12:44:41.621914  4990 net.cpp:394] relu3 <- pool3
I1026 12:44:41.621929  4990 net.cpp:345] relu3 -> pool3 (in-place)
I1026 12:44:41.621944  4990 net.cpp:96] Setting up relu3
I1026 12:44:41.621956  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:44:41.621971  4990 net.cpp:67] Creating Layer drop3
I1026 12:44:41.621983  4990 net.cpp:394] drop3 <- pool3
I1026 12:44:41.621997  4990 net.cpp:345] drop3 -> pool3 (in-place)
I1026 12:44:41.622014  4990 net.cpp:96] Setting up drop3
I1026 12:44:41.622025  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:44:41.622042  4990 net.cpp:67] Creating Layer ip1
I1026 12:44:41.622055  4990 net.cpp:394] ip1 <- pool3
I1026 12:44:41.622071  4990 net.cpp:356] ip1 -> ip1
I1026 12:44:41.622090  4990 net.cpp:96] Setting up ip1
I1026 12:44:42.077707  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:44:42.077767  4990 net.cpp:67] Creating Layer relu4
I1026 12:44:42.077775  4990 net.cpp:394] relu4 <- ip1
I1026 12:44:42.077785  4990 net.cpp:345] relu4 -> ip1 (in-place)
I1026 12:44:42.077796  4990 net.cpp:96] Setting up relu4
I1026 12:44:42.077801  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:44:42.077811  4990 net.cpp:67] Creating Layer drop4
I1026 12:44:42.077814  4990 net.cpp:394] drop4 <- ip1
I1026 12:44:42.077821  4990 net.cpp:345] drop4 -> ip1 (in-place)
I1026 12:44:42.077838  4990 net.cpp:96] Setting up drop4
I1026 12:44:42.077844  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:44:42.077854  4990 net.cpp:67] Creating Layer ip2
I1026 12:44:42.077859  4990 net.cpp:394] ip2 <- ip1
I1026 12:44:42.077867  4990 net.cpp:356] ip2 -> ip2
I1026 12:44:42.077882  4990 net.cpp:96] Setting up ip2
I1026 12:44:42.085443  4990 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 12:44:42.085503  4990 net.cpp:67] Creating Layer prob
I1026 12:44:42.085511  4990 net.cpp:394] prob <- ip2
I1026 12:44:42.085520  4990 net.cpp:356] prob -> prob
I1026 12:44:42.085531  4990 net.cpp:96] Setting up prob
I1026 12:44:42.085539  4990 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 12:44:42.085544  4990 net.cpp:172] prob does not need backward computation.
I1026 12:44:42.085548  4990 net.cpp:172] ip2 does not need backward computation.
I1026 12:44:42.085552  4990 net.cpp:172] drop4 does not need backward computation.
I1026 12:44:42.085556  4990 net.cpp:172] relu4 does not need backward computation.
I1026 12:44:42.085561  4990 net.cpp:172] ip1 does not need backward computation.
I1026 12:44:42.085563  4990 net.cpp:172] drop3 does not need backward computation.
I1026 12:44:42.085567  4990 net.cpp:172] relu3 does not need backward computation.
I1026 12:44:42.085572  4990 net.cpp:172] pool3 does not need backward computation.
I1026 12:44:42.085575  4990 net.cpp:172] conv3 does not need backward computation.
I1026 12:44:42.085579  4990 net.cpp:172] drop2 does not need backward computation.
I1026 12:44:42.085583  4990 net.cpp:172] relu2 does not need backward computation.
I1026 12:44:42.085587  4990 net.cpp:172] pool2 does not need backward computation.
I1026 12:44:42.085592  4990 net.cpp:172] conv2 does not need backward computation.
I1026 12:44:42.085595  4990 net.cpp:172] drop1 does not need backward computation.
I1026 12:44:42.085599  4990 net.cpp:172] relu1 does not need backward computation.
I1026 12:44:42.085603  4990 net.cpp:172] pool1 does not need backward computation.
I1026 12:44:42.085607  4990 net.cpp:172] conv1 does not need backward computation.
I1026 12:44:42.085610  4990 net.cpp:208] This network produces output prob
I1026 12:44:42.085625  4990 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 12:44:42.085635  4990 net.cpp:219] Network initialization done.
I1026 12:44:42.085639  4990 net.cpp:220] Memory required for data: 1837200
I1026 12:45:41.591130  4990 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 12:45:41.591629  4990 net.cpp:358] Input 0 -> data
I1026 12:45:41.591684  4990 net.cpp:67] Creating Layer conv1
I1026 12:45:41.591699  4990 net.cpp:394] conv1 <- data
I1026 12:45:41.591717  4990 net.cpp:356] conv1 -> conv1
I1026 12:45:41.591742  4990 net.cpp:96] Setting up conv1
I1026 12:45:41.591807  4990 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 12:45:41.591842  4990 net.cpp:67] Creating Layer pool1
I1026 12:45:41.591856  4990 net.cpp:394] pool1 <- conv1
I1026 12:45:41.591871  4990 net.cpp:356] pool1 -> pool1
I1026 12:45:41.591891  4990 net.cpp:96] Setting up pool1
I1026 12:45:41.591908  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:45:41.591927  4990 net.cpp:67] Creating Layer relu1
I1026 12:45:41.591938  4990 net.cpp:394] relu1 <- pool1
I1026 12:45:41.591953  4990 net.cpp:345] relu1 -> pool1 (in-place)
I1026 12:45:41.591969  4990 net.cpp:96] Setting up relu1
I1026 12:45:41.591981  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:45:41.591996  4990 net.cpp:67] Creating Layer drop1
I1026 12:45:41.592008  4990 net.cpp:394] drop1 <- pool1
I1026 12:45:41.592023  4990 net.cpp:345] drop1 -> pool1 (in-place)
I1026 12:45:41.592039  4990 net.cpp:96] Setting up drop1
I1026 12:45:41.592052  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:45:41.592072  4990 net.cpp:67] Creating Layer conv2
I1026 12:45:41.592083  4990 net.cpp:394] conv2 <- pool1
I1026 12:45:41.592100  4990 net.cpp:356] conv2 -> conv2
I1026 12:45:41.592119  4990 net.cpp:96] Setting up conv2
I1026 12:45:41.593576  4990 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 12:45:41.593618  4990 net.cpp:67] Creating Layer pool2
I1026 12:45:41.593632  4990 net.cpp:394] pool2 <- conv2
I1026 12:45:41.593649  4990 net.cpp:356] pool2 -> pool2
I1026 12:45:41.593670  4990 net.cpp:96] Setting up pool2
I1026 12:45:41.593688  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:45:41.593703  4990 net.cpp:67] Creating Layer relu2
I1026 12:45:41.593714  4990 net.cpp:394] relu2 <- pool2
I1026 12:45:41.593729  4990 net.cpp:345] relu2 -> pool2 (in-place)
I1026 12:45:41.593744  4990 net.cpp:96] Setting up relu2
I1026 12:45:41.593756  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:45:41.593771  4990 net.cpp:67] Creating Layer drop2
I1026 12:45:41.593782  4990 net.cpp:394] drop2 <- pool2
I1026 12:45:41.593797  4990 net.cpp:345] drop2 -> pool2 (in-place)
I1026 12:45:41.593813  4990 net.cpp:96] Setting up drop2
I1026 12:45:41.593825  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:45:41.593847  4990 net.cpp:67] Creating Layer conv3
I1026 12:45:41.593858  4990 net.cpp:394] conv3 <- pool2
I1026 12:45:41.593874  4990 net.cpp:356] conv3 -> conv3
I1026 12:45:41.593894  4990 net.cpp:96] Setting up conv3
I1026 12:45:41.597141  4990 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 12:45:41.597167  4990 net.cpp:67] Creating Layer pool3
I1026 12:45:41.597173  4990 net.cpp:394] pool3 <- conv3
I1026 12:45:41.597182  4990 net.cpp:356] pool3 -> pool3
I1026 12:45:41.597196  4990 net.cpp:96] Setting up pool3
I1026 12:45:41.597205  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:45:41.597213  4990 net.cpp:67] Creating Layer relu3
I1026 12:45:41.597218  4990 net.cpp:394] relu3 <- pool3
I1026 12:45:41.597225  4990 net.cpp:345] relu3 -> pool3 (in-place)
I1026 12:45:41.597234  4990 net.cpp:96] Setting up relu3
I1026 12:45:41.597239  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:45:41.597246  4990 net.cpp:67] Creating Layer drop3
I1026 12:45:41.597252  4990 net.cpp:394] drop3 <- pool3
I1026 12:45:41.597259  4990 net.cpp:345] drop3 -> pool3 (in-place)
I1026 12:45:41.597267  4990 net.cpp:96] Setting up drop3
I1026 12:45:41.597275  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:45:41.597283  4990 net.cpp:67] Creating Layer ip1
I1026 12:45:41.597290  4990 net.cpp:394] ip1 <- pool3
I1026 12:45:41.597297  4990 net.cpp:356] ip1 -> ip1
I1026 12:45:41.597307  4990 net.cpp:96] Setting up ip1
I1026 12:45:42.002499  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:45:42.002565  4990 net.cpp:67] Creating Layer relu4
I1026 12:45:42.002573  4990 net.cpp:394] relu4 <- ip1
I1026 12:45:42.002584  4990 net.cpp:345] relu4 -> ip1 (in-place)
I1026 12:45:42.002594  4990 net.cpp:96] Setting up relu4
I1026 12:45:42.002599  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:45:42.002609  4990 net.cpp:67] Creating Layer drop4
I1026 12:45:42.002614  4990 net.cpp:394] drop4 <- ip1
I1026 12:45:42.002619  4990 net.cpp:345] drop4 -> ip1 (in-place)
I1026 12:45:42.002627  4990 net.cpp:96] Setting up drop4
I1026 12:45:42.002634  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:45:42.002643  4990 net.cpp:67] Creating Layer ip2
I1026 12:45:42.002648  4990 net.cpp:394] ip2 <- ip1
I1026 12:45:42.002656  4990 net.cpp:356] ip2 -> ip2
I1026 12:45:42.002670  4990 net.cpp:96] Setting up ip2
I1026 12:45:42.010263  4990 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 12:45:42.010329  4990 net.cpp:67] Creating Layer prob
I1026 12:45:42.010336  4990 net.cpp:394] prob <- ip2
I1026 12:45:42.010345  4990 net.cpp:356] prob -> prob
I1026 12:45:42.010356  4990 net.cpp:96] Setting up prob
I1026 12:45:42.010365  4990 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 12:45:42.010368  4990 net.cpp:172] prob does not need backward computation.
I1026 12:45:42.010372  4990 net.cpp:172] ip2 does not need backward computation.
I1026 12:45:42.010376  4990 net.cpp:172] drop4 does not need backward computation.
I1026 12:45:42.010380  4990 net.cpp:172] relu4 does not need backward computation.
I1026 12:45:42.010385  4990 net.cpp:172] ip1 does not need backward computation.
I1026 12:45:42.010390  4990 net.cpp:172] drop3 does not need backward computation.
I1026 12:45:42.010392  4990 net.cpp:172] relu3 does not need backward computation.
I1026 12:45:42.010396  4990 net.cpp:172] pool3 does not need backward computation.
I1026 12:45:42.010401  4990 net.cpp:172] conv3 does not need backward computation.
I1026 12:45:42.010406  4990 net.cpp:172] drop2 does not need backward computation.
I1026 12:45:42.010409  4990 net.cpp:172] relu2 does not need backward computation.
I1026 12:45:42.010413  4990 net.cpp:172] pool2 does not need backward computation.
I1026 12:45:42.010417  4990 net.cpp:172] conv2 does not need backward computation.
I1026 12:45:42.010421  4990 net.cpp:172] drop1 does not need backward computation.
I1026 12:45:42.010424  4990 net.cpp:172] relu1 does not need backward computation.
I1026 12:45:42.010428  4990 net.cpp:172] pool1 does not need backward computation.
I1026 12:45:42.010432  4990 net.cpp:172] conv1 does not need backward computation.
I1026 12:45:42.010437  4990 net.cpp:208] This network produces output prob
I1026 12:45:42.010452  4990 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 12:45:42.010462  4990 net.cpp:219] Network initialization done.
I1026 12:45:42.010465  4990 net.cpp:220] Memory required for data: 1837200
I1026 12:46:45.900360  4990 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 12:46:45.900931  4990 net.cpp:358] Input 0 -> data
I1026 12:46:45.900990  4990 net.cpp:67] Creating Layer conv1
I1026 12:46:45.901005  4990 net.cpp:394] conv1 <- data
I1026 12:46:45.901024  4990 net.cpp:356] conv1 -> conv1
I1026 12:46:45.901048  4990 net.cpp:96] Setting up conv1
I1026 12:46:45.901114  4990 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 12:46:45.901150  4990 net.cpp:67] Creating Layer pool1
I1026 12:46:45.901163  4990 net.cpp:394] pool1 <- conv1
I1026 12:46:45.901180  4990 net.cpp:356] pool1 -> pool1
I1026 12:46:45.901199  4990 net.cpp:96] Setting up pool1
I1026 12:46:45.901216  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:46:45.901234  4990 net.cpp:67] Creating Layer relu1
I1026 12:46:45.901247  4990 net.cpp:394] relu1 <- pool1
I1026 12:46:45.901262  4990 net.cpp:345] relu1 -> pool1 (in-place)
I1026 12:46:45.901278  4990 net.cpp:96] Setting up relu1
I1026 12:46:45.901289  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:46:45.901305  4990 net.cpp:67] Creating Layer drop1
I1026 12:46:45.901316  4990 net.cpp:394] drop1 <- pool1
I1026 12:46:45.901331  4990 net.cpp:345] drop1 -> pool1 (in-place)
I1026 12:46:45.901347  4990 net.cpp:96] Setting up drop1
I1026 12:46:45.901360  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:46:45.901379  4990 net.cpp:67] Creating Layer conv2
I1026 12:46:45.901391  4990 net.cpp:394] conv2 <- pool1
I1026 12:46:45.901408  4990 net.cpp:356] conv2 -> conv2
I1026 12:46:45.901434  4990 net.cpp:96] Setting up conv2
I1026 12:46:45.902812  4990 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 12:46:45.902848  4990 net.cpp:67] Creating Layer pool2
I1026 12:46:45.902861  4990 net.cpp:394] pool2 <- conv2
I1026 12:46:45.902878  4990 net.cpp:356] pool2 -> pool2
I1026 12:46:45.902897  4990 net.cpp:96] Setting up pool2
I1026 12:46:45.902914  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:46:45.902930  4990 net.cpp:67] Creating Layer relu2
I1026 12:46:45.902940  4990 net.cpp:394] relu2 <- pool2
I1026 12:46:45.902956  4990 net.cpp:345] relu2 -> pool2 (in-place)
I1026 12:46:45.902971  4990 net.cpp:96] Setting up relu2
I1026 12:46:45.902982  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:46:45.902998  4990 net.cpp:67] Creating Layer drop2
I1026 12:46:45.903009  4990 net.cpp:394] drop2 <- pool2
I1026 12:46:45.903024  4990 net.cpp:345] drop2 -> pool2 (in-place)
I1026 12:46:45.903039  4990 net.cpp:96] Setting up drop2
I1026 12:46:45.903053  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:46:45.903072  4990 net.cpp:67] Creating Layer conv3
I1026 12:46:45.903084  4990 net.cpp:394] conv3 <- pool2
I1026 12:46:45.903101  4990 net.cpp:356] conv3 -> conv3
I1026 12:46:45.903120  4990 net.cpp:96] Setting up conv3
I1026 12:46:45.906792  4990 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 12:46:45.906831  4990 net.cpp:67] Creating Layer pool3
I1026 12:46:45.906844  4990 net.cpp:394] pool3 <- conv3
I1026 12:46:45.906862  4990 net.cpp:356] pool3 -> pool3
I1026 12:46:45.906879  4990 net.cpp:96] Setting up pool3
I1026 12:46:45.906893  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:46:45.906910  4990 net.cpp:67] Creating Layer relu3
I1026 12:46:45.906921  4990 net.cpp:394] relu3 <- pool3
I1026 12:46:45.906936  4990 net.cpp:345] relu3 -> pool3 (in-place)
I1026 12:46:45.906951  4990 net.cpp:96] Setting up relu3
I1026 12:46:45.906963  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:46:45.906978  4990 net.cpp:67] Creating Layer drop3
I1026 12:46:45.906990  4990 net.cpp:394] drop3 <- pool3
I1026 12:46:45.907006  4990 net.cpp:345] drop3 -> pool3 (in-place)
I1026 12:46:45.907021  4990 net.cpp:96] Setting up drop3
I1026 12:46:45.907033  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:46:45.907052  4990 net.cpp:67] Creating Layer ip1
I1026 12:46:45.907063  4990 net.cpp:394] ip1 <- pool3
I1026 12:46:45.907079  4990 net.cpp:356] ip1 -> ip1
I1026 12:46:45.907099  4990 net.cpp:96] Setting up ip1
I1026 12:46:46.325124  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:46:46.325189  4990 net.cpp:67] Creating Layer relu4
I1026 12:46:46.325197  4990 net.cpp:394] relu4 <- ip1
I1026 12:46:46.325208  4990 net.cpp:345] relu4 -> ip1 (in-place)
I1026 12:46:46.325217  4990 net.cpp:96] Setting up relu4
I1026 12:46:46.325223  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:46:46.325232  4990 net.cpp:67] Creating Layer drop4
I1026 12:46:46.325237  4990 net.cpp:394] drop4 <- ip1
I1026 12:46:46.325243  4990 net.cpp:345] drop4 -> ip1 (in-place)
I1026 12:46:46.325250  4990 net.cpp:96] Setting up drop4
I1026 12:46:46.325256  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:46:46.325268  4990 net.cpp:67] Creating Layer ip2
I1026 12:46:46.325271  4990 net.cpp:394] ip2 <- ip1
I1026 12:46:46.325280  4990 net.cpp:356] ip2 -> ip2
I1026 12:46:46.325294  4990 net.cpp:96] Setting up ip2
I1026 12:46:46.332962  4990 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 12:46:46.333029  4990 net.cpp:67] Creating Layer prob
I1026 12:46:46.333036  4990 net.cpp:394] prob <- ip2
I1026 12:46:46.333046  4990 net.cpp:356] prob -> prob
I1026 12:46:46.333057  4990 net.cpp:96] Setting up prob
I1026 12:46:46.333066  4990 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 12:46:46.333071  4990 net.cpp:172] prob does not need backward computation.
I1026 12:46:46.333076  4990 net.cpp:172] ip2 does not need backward computation.
I1026 12:46:46.333080  4990 net.cpp:172] drop4 does not need backward computation.
I1026 12:46:46.333083  4990 net.cpp:172] relu4 does not need backward computation.
I1026 12:46:46.333097  4990 net.cpp:172] ip1 does not need backward computation.
I1026 12:46:46.333101  4990 net.cpp:172] drop3 does not need backward computation.
I1026 12:46:46.333106  4990 net.cpp:172] relu3 does not need backward computation.
I1026 12:46:46.333111  4990 net.cpp:172] pool3 does not need backward computation.
I1026 12:46:46.333113  4990 net.cpp:172] conv3 does not need backward computation.
I1026 12:46:46.333117  4990 net.cpp:172] drop2 does not need backward computation.
I1026 12:46:46.333122  4990 net.cpp:172] relu2 does not need backward computation.
I1026 12:46:46.333125  4990 net.cpp:172] pool2 does not need backward computation.
I1026 12:46:46.333129  4990 net.cpp:172] conv2 does not need backward computation.
I1026 12:46:46.333132  4990 net.cpp:172] drop1 does not need backward computation.
I1026 12:46:46.333137  4990 net.cpp:172] relu1 does not need backward computation.
I1026 12:46:46.333140  4990 net.cpp:172] pool1 does not need backward computation.
I1026 12:46:46.333143  4990 net.cpp:172] conv1 does not need backward computation.
I1026 12:46:46.333148  4990 net.cpp:208] This network produces output prob
I1026 12:46:46.333163  4990 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 12:46:46.333173  4990 net.cpp:219] Network initialization done.
I1026 12:46:46.333176  4990 net.cpp:220] Memory required for data: 1837200
I1026 12:47:45.703749  4990 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 12:47:45.704275  4990 net.cpp:358] Input 0 -> data
I1026 12:47:45.704320  4990 net.cpp:67] Creating Layer conv1
I1026 12:47:45.704331  4990 net.cpp:394] conv1 <- data
I1026 12:47:45.704344  4990 net.cpp:356] conv1 -> conv1
I1026 12:47:45.704361  4990 net.cpp:96] Setting up conv1
I1026 12:47:45.704411  4990 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 12:47:45.704491  4990 net.cpp:67] Creating Layer pool1
I1026 12:47:45.704506  4990 net.cpp:394] pool1 <- conv1
I1026 12:47:45.704520  4990 net.cpp:356] pool1 -> pool1
I1026 12:47:45.704538  4990 net.cpp:96] Setting up pool1
I1026 12:47:45.704553  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:47:45.704568  4990 net.cpp:67] Creating Layer relu1
I1026 12:47:45.704577  4990 net.cpp:394] relu1 <- pool1
I1026 12:47:45.704591  4990 net.cpp:345] relu1 -> pool1 (in-place)
I1026 12:47:45.704603  4990 net.cpp:96] Setting up relu1
I1026 12:47:45.704613  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:47:45.704627  4990 net.cpp:67] Creating Layer drop1
I1026 12:47:45.704637  4990 net.cpp:394] drop1 <- pool1
I1026 12:47:45.704648  4990 net.cpp:345] drop1 -> pool1 (in-place)
I1026 12:47:45.704663  4990 net.cpp:96] Setting up drop1
I1026 12:47:45.704674  4990 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 12:47:45.704689  4990 net.cpp:67] Creating Layer conv2
I1026 12:47:45.704699  4990 net.cpp:394] conv2 <- pool1
I1026 12:47:45.704712  4990 net.cpp:356] conv2 -> conv2
I1026 12:47:45.704730  4990 net.cpp:96] Setting up conv2
I1026 12:47:45.705862  4990 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 12:47:45.705891  4990 net.cpp:67] Creating Layer pool2
I1026 12:47:45.705903  4990 net.cpp:394] pool2 <- conv2
I1026 12:47:45.705916  4990 net.cpp:356] pool2 -> pool2
I1026 12:47:45.705932  4990 net.cpp:96] Setting up pool2
I1026 12:47:45.705945  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:47:45.705958  4990 net.cpp:67] Creating Layer relu2
I1026 12:47:45.705968  4990 net.cpp:394] relu2 <- pool2
I1026 12:47:45.705981  4990 net.cpp:345] relu2 -> pool2 (in-place)
I1026 12:47:45.706001  4990 net.cpp:96] Setting up relu2
I1026 12:47:45.706014  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:47:45.706029  4990 net.cpp:67] Creating Layer drop2
I1026 12:47:45.706040  4990 net.cpp:394] drop2 <- pool2
I1026 12:47:45.706055  4990 net.cpp:345] drop2 -> pool2 (in-place)
I1026 12:47:45.706073  4990 net.cpp:96] Setting up drop2
I1026 12:47:45.706085  4990 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 12:47:45.706105  4990 net.cpp:67] Creating Layer conv3
I1026 12:47:45.706117  4990 net.cpp:394] conv3 <- pool2
I1026 12:47:45.706135  4990 net.cpp:356] conv3 -> conv3
I1026 12:47:45.706153  4990 net.cpp:96] Setting up conv3
I1026 12:47:45.709008  4990 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 12:47:45.709028  4990 net.cpp:67] Creating Layer pool3
I1026 12:47:45.709033  4990 net.cpp:394] pool3 <- conv3
I1026 12:47:45.709038  4990 net.cpp:356] pool3 -> pool3
I1026 12:47:45.709046  4990 net.cpp:96] Setting up pool3
I1026 12:47:45.709053  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:47:45.709058  4990 net.cpp:67] Creating Layer relu3
I1026 12:47:45.709061  4990 net.cpp:394] relu3 <- pool3
I1026 12:47:45.709067  4990 net.cpp:345] relu3 -> pool3 (in-place)
I1026 12:47:45.709072  4990 net.cpp:96] Setting up relu3
I1026 12:47:45.709077  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:47:45.709084  4990 net.cpp:67] Creating Layer drop3
I1026 12:47:45.709087  4990 net.cpp:394] drop3 <- pool3
I1026 12:47:45.709092  4990 net.cpp:345] drop3 -> pool3 (in-place)
I1026 12:47:45.709098  4990 net.cpp:96] Setting up drop3
I1026 12:47:45.709103  4990 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 12:47:45.709110  4990 net.cpp:67] Creating Layer ip1
I1026 12:47:45.709115  4990 net.cpp:394] ip1 <- pool3
I1026 12:47:45.709120  4990 net.cpp:356] ip1 -> ip1
I1026 12:47:45.709128  4990 net.cpp:96] Setting up ip1
I1026 12:47:46.105743  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:47:46.105819  4990 net.cpp:67] Creating Layer relu4
I1026 12:47:46.105829  4990 net.cpp:394] relu4 <- ip1
I1026 12:47:46.105839  4990 net.cpp:345] relu4 -> ip1 (in-place)
I1026 12:47:46.105849  4990 net.cpp:96] Setting up relu4
I1026 12:47:46.105855  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:47:46.105865  4990 net.cpp:67] Creating Layer drop4
I1026 12:47:46.105868  4990 net.cpp:394] drop4 <- ip1
I1026 12:47:46.105876  4990 net.cpp:345] drop4 -> ip1 (in-place)
I1026 12:47:46.105883  4990 net.cpp:96] Setting up drop4
I1026 12:47:46.105890  4990 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 12:47:46.105901  4990 net.cpp:67] Creating Layer ip2
I1026 12:47:46.105904  4990 net.cpp:394] ip2 <- ip1
I1026 12:47:46.105913  4990 net.cpp:356] ip2 -> ip2
I1026 12:47:46.105928  4990 net.cpp:96] Setting up ip2
I1026 12:47:46.113545  4990 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 12:47:46.113611  4990 net.cpp:67] Creating Layer prob
I1026 12:47:46.113620  4990 net.cpp:394] prob <- ip2
I1026 12:47:46.113629  4990 net.cpp:356] prob -> prob
I1026 12:47:46.113639  4990 net.cpp:96] Setting up prob
I1026 12:47:46.113647  4990 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 12:47:46.113652  4990 net.cpp:172] prob does not need backward computation.
I1026 12:47:46.113656  4990 net.cpp:172] ip2 does not need backward computation.
I1026 12:47:46.113661  4990 net.cpp:172] drop4 does not need backward computation.
I1026 12:47:46.113664  4990 net.cpp:172] relu4 does not need backward computation.
I1026 12:47:46.113668  4990 net.cpp:172] ip1 does not need backward computation.
I1026 12:47:46.113672  4990 net.cpp:172] drop3 does not need backward computation.
I1026 12:47:46.113677  4990 net.cpp:172] relu3 does not need backward computation.
I1026 12:47:46.113680  4990 net.cpp:172] pool3 does not need backward computation.
I1026 12:47:46.113684  4990 net.cpp:172] conv3 does not need backward computation.
I1026 12:47:46.113688  4990 net.cpp:172] drop2 does not need backward computation.
I1026 12:47:46.113692  4990 net.cpp:172] relu2 does not need backward computation.
I1026 12:47:46.113697  4990 net.cpp:172] pool2 does not need backward computation.
I1026 12:47:46.113700  4990 net.cpp:172] conv2 does not need backward computation.
I1026 12:47:46.113704  4990 net.cpp:172] drop1 does not need backward computation.
I1026 12:47:46.113708  4990 net.cpp:172] relu1 does not need backward computation.
I1026 12:47:46.113711  4990 net.cpp:172] pool1 does not need backward computation.
I1026 12:47:46.113715  4990 net.cpp:172] conv1 does not need backward computation.
I1026 12:47:46.113719  4990 net.cpp:208] This network produces output prob
I1026 12:47:46.113734  4990 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 12:47:46.113744  4990 net.cpp:219] Network initialization done.
I1026 12:47:46.113749  4990 net.cpp:220] Memory required for data: 1837200
I1026 13:27:51.358314 14636 convert_imageset.cpp:70] Shuffling data
I1026 13:27:51.844007 14636 convert_imageset.cpp:73] A total of 60000 images.
I1026 13:27:51.844084 14636 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1026 13:27:53.983291 14636 convert_imageset.cpp:177] Processed 1000 files.
E1026 13:27:56.190399 14636 convert_imageset.cpp:177] Processed 2000 files.
E1026 13:27:58.151140 14636 convert_imageset.cpp:177] Processed 3000 files.
E1026 13:28:00.142457 14636 convert_imageset.cpp:177] Processed 4000 files.
E1026 13:28:02.197055 14636 convert_imageset.cpp:177] Processed 5000 files.
E1026 13:28:04.067044 14636 convert_imageset.cpp:177] Processed 6000 files.
E1026 13:28:05.826082 14636 convert_imageset.cpp:177] Processed 7000 files.
E1026 13:28:07.662904 14636 convert_imageset.cpp:177] Processed 8000 files.
E1026 13:28:09.430810 14636 convert_imageset.cpp:177] Processed 9000 files.
E1026 13:28:11.202168 14636 convert_imageset.cpp:177] Processed 10000 files.
E1026 13:28:12.882025 14636 convert_imageset.cpp:177] Processed 11000 files.
E1026 13:28:14.717087 14636 convert_imageset.cpp:177] Processed 12000 files.
E1026 13:28:16.424546 14636 convert_imageset.cpp:177] Processed 13000 files.
E1026 13:28:18.250041 14636 convert_imageset.cpp:177] Processed 14000 files.
E1026 13:28:20.003113 14636 convert_imageset.cpp:177] Processed 15000 files.
E1026 13:28:21.800767 14636 convert_imageset.cpp:177] Processed 16000 files.
E1026 13:28:23.457785 14636 convert_imageset.cpp:177] Processed 17000 files.
E1026 13:28:25.260321 14636 convert_imageset.cpp:177] Processed 18000 files.
E1026 13:28:27.003255 14636 convert_imageset.cpp:177] Processed 19000 files.
E1026 13:28:28.576833 14636 convert_imageset.cpp:177] Processed 20000 files.
E1026 13:28:30.246443 14636 convert_imageset.cpp:177] Processed 21000 files.
E1026 13:28:31.915824 14636 convert_imageset.cpp:177] Processed 22000 files.
E1026 13:28:33.543375 14636 convert_imageset.cpp:177] Processed 23000 files.
E1026 13:28:35.115583 14636 convert_imageset.cpp:177] Processed 24000 files.
E1026 13:28:36.788460 14636 convert_imageset.cpp:177] Processed 25000 files.
E1026 13:28:38.338377 14636 convert_imageset.cpp:177] Processed 26000 files.
E1026 13:28:39.930119 14636 convert_imageset.cpp:177] Processed 27000 files.
E1026 13:28:41.531703 14636 convert_imageset.cpp:177] Processed 28000 files.
E1026 13:28:43.160928 14636 convert_imageset.cpp:177] Processed 29000 files.
E1026 13:28:44.674358 14636 convert_imageset.cpp:177] Processed 30000 files.
E1026 13:28:46.229693 14636 convert_imageset.cpp:177] Processed 31000 files.
E1026 13:28:47.847164 14636 convert_imageset.cpp:177] Processed 32000 files.
E1026 13:28:49.467614 14636 convert_imageset.cpp:177] Processed 33000 files.
E1026 13:28:51.116472 14636 convert_imageset.cpp:177] Processed 34000 files.
E1026 13:28:52.732966 14636 convert_imageset.cpp:177] Processed 35000 files.
E1026 13:28:54.402137 14636 convert_imageset.cpp:177] Processed 36000 files.
E1026 13:28:55.969326 14636 convert_imageset.cpp:177] Processed 37000 files.
E1026 13:28:57.560238 14636 convert_imageset.cpp:177] Processed 38000 files.
E1026 13:28:59.822021 14636 convert_imageset.cpp:177] Processed 39000 files.
E1026 13:29:01.257212 14636 convert_imageset.cpp:177] Processed 40000 files.
E1026 13:29:02.837648 14636 convert_imageset.cpp:177] Processed 41000 files.
E1026 13:29:04.421723 14636 convert_imageset.cpp:177] Processed 42000 files.
E1026 13:29:05.984817 14636 convert_imageset.cpp:177] Processed 43000 files.
E1026 13:29:07.475186 14636 convert_imageset.cpp:177] Processed 44000 files.
E1026 13:29:09.084262 14636 convert_imageset.cpp:177] Processed 45000 files.
E1026 13:29:10.598713 14636 convert_imageset.cpp:177] Processed 46000 files.
E1026 13:29:12.054168 14636 convert_imageset.cpp:177] Processed 47000 files.
E1026 13:29:13.675771 14636 convert_imageset.cpp:177] Processed 48000 files.
E1026 13:29:15.220585 14636 convert_imageset.cpp:177] Processed 49000 files.
E1026 13:29:16.791677 14636 convert_imageset.cpp:177] Processed 50000 files.
E1026 13:29:18.326097 14636 convert_imageset.cpp:177] Processed 51000 files.
E1026 13:29:19.851411 14636 convert_imageset.cpp:177] Processed 52000 files.
E1026 13:29:21.574429 14636 convert_imageset.cpp:177] Processed 53000 files.
E1026 13:29:23.417407 14636 convert_imageset.cpp:177] Processed 54000 files.
E1026 13:29:25.218473 14636 convert_imageset.cpp:177] Processed 55000 files.
E1026 13:29:27.020056 14636 convert_imageset.cpp:177] Processed 56000 files.
E1026 13:29:28.776890 14636 convert_imageset.cpp:177] Processed 57000 files.
E1026 13:29:30.595990 14636 convert_imageset.cpp:177] Processed 58000 files.
E1026 13:29:32.381126 14636 convert_imageset.cpp:177] Processed 59000 files.
E1026 13:29:33.815353 14636 convert_imageset.cpp:177] Processed 60000 files.
I1026 13:29:34.470187 14644 caffe.cpp:99] Use GPU with device ID 0
I1026 13:29:34.827119 14644 caffe.cpp:107] Starting Optimization
I1026 13:29:34.827237 14644 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 100000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1026 13:29:34.827270 14644 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1026 13:29:34.838805 14644 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1026 13:29:34.839012 14644 net.cpp:67] Creating Layer mnist
I1026 13:29:34.839037 14644 net.cpp:356] mnist -> data
I1026 13:29:34.839073 14644 net.cpp:356] mnist -> label
I1026 13:29:34.839105 14644 net.cpp:96] Setting up mnist
I1026 13:29:34.846281 14644 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1026 13:29:34.846375 14644 data_layer.cpp:128] output data size: 64,1,50,180
I1026 13:29:34.846890 14644 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1026 13:29:34.846917 14644 net.cpp:103] Top shape: 64 1 1 1 (64)
I1026 13:29:34.846930 14644 net.cpp:67] Creating Layer conv1
I1026 13:29:34.846940 14644 net.cpp:394] conv1 <- data
I1026 13:29:34.846956 14644 net.cpp:356] conv1 -> conv1
I1026 13:29:34.846969 14644 net.cpp:96] Setting up conv1
I1026 13:29:34.847337 14644 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1026 13:29:34.847373 14644 net.cpp:67] Creating Layer pool1
I1026 13:29:34.847379 14644 net.cpp:394] pool1 <- conv1
I1026 13:29:34.847386 14644 net.cpp:356] pool1 -> pool1
I1026 13:29:34.847393 14644 net.cpp:96] Setting up pool1
I1026 13:29:34.847410 14644 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 13:29:34.847419 14644 net.cpp:67] Creating Layer relu1
I1026 13:29:34.847424 14644 net.cpp:394] relu1 <- pool1
I1026 13:29:34.847430 14644 net.cpp:345] relu1 -> pool1 (in-place)
I1026 13:29:34.847436 14644 net.cpp:96] Setting up relu1
I1026 13:29:34.847441 14644 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 13:29:34.847448 14644 net.cpp:67] Creating Layer drop1
I1026 13:29:34.847453 14644 net.cpp:394] drop1 <- pool1
I1026 13:29:34.847460 14644 net.cpp:345] drop1 -> pool1 (in-place)
I1026 13:29:34.847465 14644 net.cpp:96] Setting up drop1
I1026 13:29:34.847471 14644 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 13:29:34.847479 14644 net.cpp:67] Creating Layer conv2
I1026 13:29:34.847484 14644 net.cpp:394] conv2 <- pool1
I1026 13:29:34.847492 14644 net.cpp:356] conv2 -> conv2
I1026 13:29:34.847501 14644 net.cpp:96] Setting up conv2
I1026 13:29:34.848068 14644 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1026 13:29:34.848088 14644 net.cpp:67] Creating Layer pool2
I1026 13:29:34.848093 14644 net.cpp:394] pool2 <- conv2
I1026 13:29:34.848099 14644 net.cpp:356] pool2 -> pool2
I1026 13:29:34.848106 14644 net.cpp:96] Setting up pool2
I1026 13:29:34.848112 14644 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 13:29:34.848119 14644 net.cpp:67] Creating Layer relu2
I1026 13:29:34.848124 14644 net.cpp:394] relu2 <- pool2
I1026 13:29:34.848130 14644 net.cpp:345] relu2 -> pool2 (in-place)
I1026 13:29:34.848136 14644 net.cpp:96] Setting up relu2
I1026 13:29:34.848141 14644 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 13:29:34.848148 14644 net.cpp:67] Creating Layer drop2
I1026 13:29:34.848152 14644 net.cpp:394] drop2 <- pool2
I1026 13:29:34.848158 14644 net.cpp:345] drop2 -> pool2 (in-place)
I1026 13:29:34.848165 14644 net.cpp:96] Setting up drop2
I1026 13:29:34.848170 14644 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 13:29:34.848178 14644 net.cpp:67] Creating Layer conv3
I1026 13:29:34.848183 14644 net.cpp:394] conv3 <- pool2
I1026 13:29:34.848189 14644 net.cpp:356] conv3 -> conv3
I1026 13:29:34.848196 14644 net.cpp:96] Setting up conv3
I1026 13:29:34.851387 14644 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1026 13:29:34.851439 14644 net.cpp:67] Creating Layer pool3
I1026 13:29:34.851454 14644 net.cpp:394] pool3 <- conv3
I1026 13:29:34.851475 14644 net.cpp:356] pool3 -> pool3
I1026 13:29:34.851496 14644 net.cpp:96] Setting up pool3
I1026 13:29:34.851512 14644 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 13:29:34.851528 14644 net.cpp:67] Creating Layer relu3
I1026 13:29:34.851541 14644 net.cpp:394] relu3 <- pool3
I1026 13:29:34.851557 14644 net.cpp:345] relu3 -> pool3 (in-place)
I1026 13:29:34.851572 14644 net.cpp:96] Setting up relu3
I1026 13:29:34.851585 14644 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 13:29:34.851605 14644 net.cpp:67] Creating Layer drop3
I1026 13:29:34.851618 14644 net.cpp:394] drop3 <- pool3
I1026 13:29:34.851634 14644 net.cpp:345] drop3 -> pool3 (in-place)
I1026 13:29:34.851650 14644 net.cpp:96] Setting up drop3
I1026 13:29:34.851663 14644 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 13:29:34.851681 14644 net.cpp:67] Creating Layer ip1
I1026 13:29:34.851693 14644 net.cpp:394] ip1 <- pool3
I1026 13:29:34.851714 14644 net.cpp:356] ip1 -> ip1
I1026 13:29:34.851779 14644 net.cpp:96] Setting up ip1
I1026 13:29:35.287190 14644 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 13:29:35.287251 14644 net.cpp:67] Creating Layer relu4
I1026 13:29:35.287259 14644 net.cpp:394] relu4 <- ip1
I1026 13:29:35.287278 14644 net.cpp:345] relu4 -> ip1 (in-place)
I1026 13:29:35.287288 14644 net.cpp:96] Setting up relu4
I1026 13:29:35.287293 14644 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 13:29:35.287300 14644 net.cpp:67] Creating Layer drop4
I1026 13:29:35.287304 14644 net.cpp:394] drop4 <- ip1
I1026 13:29:35.287310 14644 net.cpp:345] drop4 -> ip1 (in-place)
I1026 13:29:35.287317 14644 net.cpp:96] Setting up drop4
I1026 13:29:35.287322 14644 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 13:29:35.287334 14644 net.cpp:67] Creating Layer ip2
I1026 13:29:35.287339 14644 net.cpp:394] ip2 <- ip1
I1026 13:29:35.287345 14644 net.cpp:356] ip2 -> ip2
I1026 13:29:35.287353 14644 net.cpp:96] Setting up ip2
I1026 13:29:35.295510 14644 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1026 13:29:35.295577 14644 net.cpp:67] Creating Layer loss
I1026 13:29:35.295583 14644 net.cpp:394] loss <- ip2
I1026 13:29:35.295591 14644 net.cpp:394] loss <- label
I1026 13:29:35.295598 14644 net.cpp:356] loss -> loss
I1026 13:29:35.295608 14644 net.cpp:96] Setting up loss
I1026 13:29:35.295620 14644 net.cpp:103] Top shape: 1 1 1 1 (1)
I1026 13:29:35.295625 14644 net.cpp:109]     with loss weight 1
I1026 13:29:35.295668 14644 net.cpp:170] loss needs backward computation.
I1026 13:29:35.295673 14644 net.cpp:170] ip2 needs backward computation.
I1026 13:29:35.295677 14644 net.cpp:170] drop4 needs backward computation.
I1026 13:29:35.295681 14644 net.cpp:170] relu4 needs backward computation.
I1026 13:29:35.295686 14644 net.cpp:170] ip1 needs backward computation.
I1026 13:29:35.295691 14644 net.cpp:170] drop3 needs backward computation.
I1026 13:29:35.295694 14644 net.cpp:170] relu3 needs backward computation.
I1026 13:29:35.295698 14644 net.cpp:170] pool3 needs backward computation.
I1026 13:29:35.295703 14644 net.cpp:170] conv3 needs backward computation.
I1026 13:29:35.295707 14644 net.cpp:170] drop2 needs backward computation.
I1026 13:29:35.295712 14644 net.cpp:170] relu2 needs backward computation.
I1026 13:29:35.295717 14644 net.cpp:170] pool2 needs backward computation.
I1026 13:29:35.295720 14644 net.cpp:170] conv2 needs backward computation.
I1026 13:29:35.295724 14644 net.cpp:170] drop1 needs backward computation.
I1026 13:29:35.295729 14644 net.cpp:170] relu1 needs backward computation.
I1026 13:29:35.295733 14644 net.cpp:170] pool1 needs backward computation.
I1026 13:29:35.295737 14644 net.cpp:170] conv1 needs backward computation.
I1026 13:29:35.295742 14644 net.cpp:172] mnist does not need backward computation.
I1026 13:29:35.295747 14644 net.cpp:208] This network produces output loss
I1026 13:29:35.295756 14644 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 13:29:35.295763 14644 net.cpp:219] Network initialization done.
I1026 13:29:35.295768 14644 net.cpp:220] Memory required for data: 119788292
I1026 13:29:35.295830 14644 solver.cpp:41] Solver scaffolding done.
I1026 13:29:35.295837 14644 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_75000.solverstate
I1026 13:29:35.295841 14644 solver.cpp:160] Solving Captcha
I1026 13:29:35.295861 14644 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_75000.solverstate
I1026 13:29:42.107789 14644 solver.cpp:502] SGDSolver: restoring history
I1026 13:29:42.837018 14644 solver.cpp:191] Iteration 75000, loss = 2.83826
I1026 13:29:42.837074 14644 solver.cpp:206]     Train net output #0: loss = 2.83826 (* 1 = 2.83826 loss)
I1026 13:29:42.837088 14644 solver.cpp:403] Iteration 75000, lr = 0.0020088
I1026 13:33:44.608501 14644 solver.cpp:191] Iteration 76000, loss = 2.59315
I1026 13:33:44.609218 14644 solver.cpp:206]     Train net output #0: loss = 2.59315 (* 1 = 2.59315 loss)
I1026 13:33:44.609251 14644 solver.cpp:403] Iteration 76000, lr = 0.00199125
I1026 13:37:45.790143 14644 solver.cpp:191] Iteration 77000, loss = 2.52124
I1026 13:37:45.790791 14644 solver.cpp:206]     Train net output #0: loss = 2.52124 (* 1 = 2.52124 loss)
I1026 13:37:45.790832 14644 solver.cpp:403] Iteration 77000, lr = 0.00197406
I1026 13:41:46.959369 14644 solver.cpp:191] Iteration 78000, loss = 2.54339
I1026 13:41:46.959949 14644 solver.cpp:206]     Train net output #0: loss = 2.54339 (* 1 = 2.54339 loss)
I1026 13:41:46.959985 14644 solver.cpp:403] Iteration 78000, lr = 0.00195721
I1026 13:45:48.119843 14644 solver.cpp:191] Iteration 79000, loss = 2.60715
I1026 13:45:48.120587 14644 solver.cpp:206]     Train net output #0: loss = 2.60715 (* 1 = 2.60715 loss)
I1026 13:45:48.120620 14644 solver.cpp:403] Iteration 79000, lr = 0.0019407
I1026 13:49:49.932543 14644 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_80000.caffemodel
I1026 13:49:55.004258 14644 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_80000.solverstate
I1026 13:49:59.531031 14644 solver.cpp:191] Iteration 80000, loss = 2.36737
I1026 13:49:59.531591 14644 solver.cpp:206]     Train net output #0: loss = 2.36737 (* 1 = 2.36737 loss)
I1026 13:49:59.531627 14644 solver.cpp:403] Iteration 80000, lr = 0.0019245
I1026 13:54:00.826822 14644 solver.cpp:191] Iteration 81000, loss = 2.98846
I1026 13:54:00.827544 14644 solver.cpp:206]     Train net output #0: loss = 2.98846 (* 1 = 2.98846 loss)
I1026 13:54:00.827576 14644 solver.cpp:403] Iteration 81000, lr = 0.00190862
I1026 13:58:02.016655 14644 solver.cpp:191] Iteration 82000, loss = 2.93413
I1026 13:58:02.017189 14644 solver.cpp:206]     Train net output #0: loss = 2.93413 (* 1 = 2.93413 loss)
I1026 13:58:02.017220 14644 solver.cpp:403] Iteration 82000, lr = 0.00189304
I1026 14:02:03.208837 14644 solver.cpp:191] Iteration 83000, loss = 2.59978
I1026 14:02:03.209503 14644 solver.cpp:206]     Train net output #0: loss = 2.59978 (* 1 = 2.59978 loss)
I1026 14:02:03.209535 14644 solver.cpp:403] Iteration 83000, lr = 0.00187775
I1026 14:06:04.350965 14644 solver.cpp:191] Iteration 84000, loss = 2.45338
I1026 14:06:04.351738 14644 solver.cpp:206]     Train net output #0: loss = 2.45338 (* 1 = 2.45338 loss)
I1026 14:06:04.351771 14644 solver.cpp:403] Iteration 84000, lr = 0.00186275
I1026 14:10:05.998123 14644 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_85000.caffemodel
I1026 14:10:10.083176 14644 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_85000.solverstate
I1026 14:10:13.755468 14644 solver.cpp:191] Iteration 85000, loss = 2.59451
I1026 14:10:13.756237 14644 solver.cpp:206]     Train net output #0: loss = 2.59451 (* 1 = 2.59451 loss)
I1026 14:10:13.756268 14644 solver.cpp:403] Iteration 85000, lr = 0.00184802
I1026 14:14:14.950708 14644 solver.cpp:191] Iteration 86000, loss = 2.37298
I1026 14:14:14.951532 14644 solver.cpp:206]     Train net output #0: loss = 2.37298 (* 1 = 2.37298 loss)
I1026 14:14:14.951565 14644 solver.cpp:403] Iteration 86000, lr = 0.00183357
I1026 14:18:16.113428 14644 solver.cpp:191] Iteration 87000, loss = 2.55267
I1026 14:18:16.114229 14644 solver.cpp:206]     Train net output #0: loss = 2.55267 (* 1 = 2.55267 loss)
I1026 14:18:16.114261 14644 solver.cpp:403] Iteration 87000, lr = 0.00181937
I1026 14:22:17.315186 14644 solver.cpp:191] Iteration 88000, loss = 2.54486
I1026 14:22:17.316068 14644 solver.cpp:206]     Train net output #0: loss = 2.54486 (* 1 = 2.54486 loss)
I1026 14:22:17.316099 14644 solver.cpp:403] Iteration 88000, lr = 0.00180543
I1026 14:26:18.541203 14644 solver.cpp:191] Iteration 89000, loss = 2.37284
I1026 14:26:18.541833 14644 solver.cpp:206]     Train net output #0: loss = 2.37284 (* 1 = 2.37284 loss)
I1026 14:26:18.541868 14644 solver.cpp:403] Iteration 89000, lr = 0.00179173
I1026 14:30:20.136257 14644 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_90000.caffemodel
I1026 14:30:24.962013 14644 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_90000.solverstate
I1026 14:30:28.848733 14644 solver.cpp:191] Iteration 90000, loss = 2.57827
I1026 14:30:28.849239 14644 solver.cpp:206]     Train net output #0: loss = 2.57827 (* 1 = 2.57827 loss)
I1026 14:30:28.849277 14644 solver.cpp:403] Iteration 90000, lr = 0.00177828
I1026 14:34:30.030964 14644 solver.cpp:191] Iteration 91000, loss = 2.45611
I1026 14:34:30.031575 14644 solver.cpp:206]     Train net output #0: loss = 2.45611 (* 1 = 2.45611 loss)
I1026 14:34:30.031607 14644 solver.cpp:403] Iteration 91000, lr = 0.00176506
I1026 14:38:31.229538 14644 solver.cpp:191] Iteration 92000, loss = 2.4096
I1026 14:38:31.230195 14644 solver.cpp:206]     Train net output #0: loss = 2.4096 (* 1 = 2.4096 loss)
I1026 14:38:31.230227 14644 solver.cpp:403] Iteration 92000, lr = 0.00175206
I1026 14:42:32.412832 14644 solver.cpp:191] Iteration 93000, loss = 2.44973
I1026 14:42:32.413496 14644 solver.cpp:206]     Train net output #0: loss = 2.44973 (* 1 = 2.44973 loss)
I1026 14:42:32.413527 14644 solver.cpp:403] Iteration 93000, lr = 0.00173929
I1026 14:46:33.547859 14644 solver.cpp:191] Iteration 94000, loss = 2.53621
I1026 14:46:33.548504 14644 solver.cpp:206]     Train net output #0: loss = 2.53621 (* 1 = 2.53621 loss)
I1026 14:46:33.548537 14644 solver.cpp:403] Iteration 94000, lr = 0.00172673
I1026 14:50:35.198808 14644 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_95000.caffemodel
I1026 14:50:40.313164 14644 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_95000.solverstate
I1026 14:50:45.001420 14644 solver.cpp:191] Iteration 95000, loss = 2.41656
I1026 14:50:45.002039 14644 solver.cpp:206]     Train net output #0: loss = 2.41656 (* 1 = 2.41656 loss)
I1026 14:50:45.002073 14644 solver.cpp:403] Iteration 95000, lr = 0.00171438
I1026 14:54:46.169569 14644 solver.cpp:191] Iteration 96000, loss = 2.72166
I1026 14:54:46.170125 14644 solver.cpp:206]     Train net output #0: loss = 2.72166 (* 1 = 2.72166 loss)
I1026 14:54:46.170159 14644 solver.cpp:403] Iteration 96000, lr = 0.00170224
I1026 14:58:47.329290 14644 solver.cpp:191] Iteration 97000, loss = 2.39377
I1026 14:58:47.329972 14644 solver.cpp:206]     Train net output #0: loss = 2.39377 (* 1 = 2.39377 loss)
I1026 14:58:47.330003 14644 solver.cpp:403] Iteration 97000, lr = 0.00169029
I1026 15:02:48.511368 14644 solver.cpp:191] Iteration 98000, loss = 2.50878
I1026 15:02:48.512006 14644 solver.cpp:206]     Train net output #0: loss = 2.50878 (* 1 = 2.50878 loss)
I1026 15:02:48.512037 14644 solver.cpp:403] Iteration 98000, lr = 0.00167854
I1026 15:06:49.697233 14644 solver.cpp:191] Iteration 99000, loss = 2.30857
I1026 15:06:49.697813 14644 solver.cpp:206]     Train net output #0: loss = 2.30857 (* 1 = 2.30857 loss)
I1026 15:06:49.697844 14644 solver.cpp:403] Iteration 99000, lr = 0.00166698
I1026 15:10:51.307498 14644 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_100000.caffemodel
I1026 15:10:55.283846 14644 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_100000.solverstate
I1026 15:10:58.754847 14644 solver.cpp:228] Iteration 100000, loss = 2.44754
I1026 15:10:58.755573 14644 solver.cpp:233] Optimization Done.
I1026 15:10:58.755596 14644 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1026 15:34:13.698487 10978 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 15:34:13.699082 10978 net.cpp:358] Input 0 -> data
I1026 15:34:13.699111 10978 net.cpp:67] Creating Layer conv1
I1026 15:34:13.699116 10978 net.cpp:394] conv1 <- data
I1026 15:34:13.699123 10978 net.cpp:356] conv1 -> conv1
I1026 15:34:13.699133 10978 net.cpp:96] Setting up conv1
I1026 15:34:13.699450 10978 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 15:34:13.699470 10978 net.cpp:67] Creating Layer pool1
I1026 15:34:13.699473 10978 net.cpp:394] pool1 <- conv1
I1026 15:34:13.699479 10978 net.cpp:356] pool1 -> pool1
I1026 15:34:13.699486 10978 net.cpp:96] Setting up pool1
I1026 15:34:13.699502 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:34:13.699509 10978 net.cpp:67] Creating Layer relu1
I1026 15:34:13.699513 10978 net.cpp:394] relu1 <- pool1
I1026 15:34:13.699518 10978 net.cpp:345] relu1 -> pool1 (in-place)
I1026 15:34:13.699524 10978 net.cpp:96] Setting up relu1
I1026 15:34:13.699528 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:34:13.699537 10978 net.cpp:67] Creating Layer drop1
I1026 15:34:13.699540 10978 net.cpp:394] drop1 <- pool1
I1026 15:34:13.699547 10978 net.cpp:345] drop1 -> pool1 (in-place)
I1026 15:34:13.699553 10978 net.cpp:96] Setting up drop1
I1026 15:34:13.699556 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:34:13.699563 10978 net.cpp:67] Creating Layer conv2
I1026 15:34:13.699566 10978 net.cpp:394] conv2 <- pool1
I1026 15:34:13.699573 10978 net.cpp:356] conv2 -> conv2
I1026 15:34:13.699579 10978 net.cpp:96] Setting up conv2
I1026 15:34:13.700139 10978 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 15:34:13.700153 10978 net.cpp:67] Creating Layer pool2
I1026 15:34:13.700157 10978 net.cpp:394] pool2 <- conv2
I1026 15:34:13.700162 10978 net.cpp:356] pool2 -> pool2
I1026 15:34:13.700170 10978 net.cpp:96] Setting up pool2
I1026 15:34:13.700176 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:34:13.700186 10978 net.cpp:67] Creating Layer relu2
I1026 15:34:13.700189 10978 net.cpp:394] relu2 <- pool2
I1026 15:34:13.700196 10978 net.cpp:345] relu2 -> pool2 (in-place)
I1026 15:34:13.700201 10978 net.cpp:96] Setting up relu2
I1026 15:34:13.700204 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:34:13.700211 10978 net.cpp:67] Creating Layer drop2
I1026 15:34:13.700214 10978 net.cpp:394] drop2 <- pool2
I1026 15:34:13.700219 10978 net.cpp:345] drop2 -> pool2 (in-place)
I1026 15:34:13.700225 10978 net.cpp:96] Setting up drop2
I1026 15:34:13.700229 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:34:13.700235 10978 net.cpp:67] Creating Layer conv3
I1026 15:34:13.700239 10978 net.cpp:394] conv3 <- pool2
I1026 15:34:13.700248 10978 net.cpp:356] conv3 -> conv3
I1026 15:34:13.700254 10978 net.cpp:96] Setting up conv3
I1026 15:34:13.701756 10978 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 15:34:13.701778 10978 net.cpp:67] Creating Layer pool3
I1026 15:34:13.701783 10978 net.cpp:394] pool3 <- conv3
I1026 15:34:13.701789 10978 net.cpp:356] pool3 -> pool3
I1026 15:34:13.701795 10978 net.cpp:96] Setting up pool3
I1026 15:34:13.701800 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:34:13.701807 10978 net.cpp:67] Creating Layer relu3
I1026 15:34:13.701812 10978 net.cpp:394] relu3 <- pool3
I1026 15:34:13.701817 10978 net.cpp:345] relu3 -> pool3 (in-place)
I1026 15:34:13.701822 10978 net.cpp:96] Setting up relu3
I1026 15:34:13.701827 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:34:13.701831 10978 net.cpp:67] Creating Layer drop3
I1026 15:34:13.701835 10978 net.cpp:394] drop3 <- pool3
I1026 15:34:13.701839 10978 net.cpp:345] drop3 -> pool3 (in-place)
I1026 15:34:13.701845 10978 net.cpp:96] Setting up drop3
I1026 15:34:13.701849 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:34:13.701858 10978 net.cpp:67] Creating Layer ip1
I1026 15:34:13.701861 10978 net.cpp:394] ip1 <- pool3
I1026 15:34:13.701867 10978 net.cpp:356] ip1 -> ip1
I1026 15:34:13.701874 10978 net.cpp:96] Setting up ip1
I1026 15:34:14.202811 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:34:14.202872 10978 net.cpp:67] Creating Layer relu4
I1026 15:34:14.202879 10978 net.cpp:394] relu4 <- ip1
I1026 15:34:14.202888 10978 net.cpp:345] relu4 -> ip1 (in-place)
I1026 15:34:14.202896 10978 net.cpp:96] Setting up relu4
I1026 15:34:14.202901 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:34:14.202908 10978 net.cpp:67] Creating Layer drop4
I1026 15:34:14.202913 10978 net.cpp:394] drop4 <- ip1
I1026 15:34:14.202922 10978 net.cpp:345] drop4 -> ip1 (in-place)
I1026 15:34:14.202929 10978 net.cpp:96] Setting up drop4
I1026 15:34:14.202934 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:34:14.202942 10978 net.cpp:67] Creating Layer ip2
I1026 15:34:14.202946 10978 net.cpp:394] ip2 <- ip1
I1026 15:34:14.202955 10978 net.cpp:356] ip2 -> ip2
I1026 15:34:14.202966 10978 net.cpp:96] Setting up ip2
I1026 15:34:14.212731 10978 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 15:34:14.212800 10978 net.cpp:67] Creating Layer prob
I1026 15:34:14.212806 10978 net.cpp:394] prob <- ip2
I1026 15:34:14.212815 10978 net.cpp:356] prob -> prob
I1026 15:34:14.212824 10978 net.cpp:96] Setting up prob
I1026 15:34:14.212831 10978 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 15:34:14.212834 10978 net.cpp:172] prob does not need backward computation.
I1026 15:34:14.212838 10978 net.cpp:172] ip2 does not need backward computation.
I1026 15:34:14.212842 10978 net.cpp:172] drop4 does not need backward computation.
I1026 15:34:14.212846 10978 net.cpp:172] relu4 does not need backward computation.
I1026 15:34:14.212849 10978 net.cpp:172] ip1 does not need backward computation.
I1026 15:34:14.212852 10978 net.cpp:172] drop3 does not need backward computation.
I1026 15:34:14.212855 10978 net.cpp:172] relu3 does not need backward computation.
I1026 15:34:14.212859 10978 net.cpp:172] pool3 does not need backward computation.
I1026 15:34:14.212862 10978 net.cpp:172] conv3 does not need backward computation.
I1026 15:34:14.212873 10978 net.cpp:172] drop2 does not need backward computation.
I1026 15:34:14.212877 10978 net.cpp:172] relu2 does not need backward computation.
I1026 15:34:14.212882 10978 net.cpp:172] pool2 does not need backward computation.
I1026 15:34:14.212884 10978 net.cpp:172] conv2 does not need backward computation.
I1026 15:34:14.212888 10978 net.cpp:172] drop1 does not need backward computation.
I1026 15:34:14.212891 10978 net.cpp:172] relu1 does not need backward computation.
I1026 15:34:14.212894 10978 net.cpp:172] pool1 does not need backward computation.
I1026 15:34:14.212898 10978 net.cpp:172] conv1 does not need backward computation.
I1026 15:34:14.212901 10978 net.cpp:208] This network produces output prob
I1026 15:34:14.212914 10978 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 15:34:14.212923 10978 net.cpp:219] Network initialization done.
I1026 15:34:14.212926 10978 net.cpp:220] Memory required for data: 1837200
I1026 15:34:57.030092 10978 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 15:34:57.030629 10978 net.cpp:358] Input 0 -> data
I1026 15:34:57.030660 10978 net.cpp:67] Creating Layer conv1
I1026 15:34:57.030666 10978 net.cpp:394] conv1 <- data
I1026 15:34:57.030673 10978 net.cpp:356] conv1 -> conv1
I1026 15:34:57.030683 10978 net.cpp:96] Setting up conv1
I1026 15:34:57.030714 10978 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 15:34:57.030740 10978 net.cpp:67] Creating Layer pool1
I1026 15:34:57.030745 10978 net.cpp:394] pool1 <- conv1
I1026 15:34:57.030750 10978 net.cpp:356] pool1 -> pool1
I1026 15:34:57.030757 10978 net.cpp:96] Setting up pool1
I1026 15:34:57.030766 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:34:57.030771 10978 net.cpp:67] Creating Layer relu1
I1026 15:34:57.030776 10978 net.cpp:394] relu1 <- pool1
I1026 15:34:57.030781 10978 net.cpp:345] relu1 -> pool1 (in-place)
I1026 15:34:57.030786 10978 net.cpp:96] Setting up relu1
I1026 15:34:57.030791 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:34:57.030797 10978 net.cpp:67] Creating Layer drop1
I1026 15:34:57.030800 10978 net.cpp:394] drop1 <- pool1
I1026 15:34:57.030805 10978 net.cpp:345] drop1 -> pool1 (in-place)
I1026 15:34:57.030812 10978 net.cpp:96] Setting up drop1
I1026 15:34:57.030817 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:34:57.030823 10978 net.cpp:67] Creating Layer conv2
I1026 15:34:57.030827 10978 net.cpp:394] conv2 <- pool1
I1026 15:34:57.030833 10978 net.cpp:356] conv2 -> conv2
I1026 15:34:57.030839 10978 net.cpp:96] Setting up conv2
I1026 15:34:57.031343 10978 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 15:34:57.031358 10978 net.cpp:67] Creating Layer pool2
I1026 15:34:57.031361 10978 net.cpp:394] pool2 <- conv2
I1026 15:34:57.031368 10978 net.cpp:356] pool2 -> pool2
I1026 15:34:57.031374 10978 net.cpp:96] Setting up pool2
I1026 15:34:57.031380 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:34:57.031386 10978 net.cpp:67] Creating Layer relu2
I1026 15:34:57.031389 10978 net.cpp:394] relu2 <- pool2
I1026 15:34:57.031394 10978 net.cpp:345] relu2 -> pool2 (in-place)
I1026 15:34:57.031400 10978 net.cpp:96] Setting up relu2
I1026 15:34:57.031404 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:34:57.031409 10978 net.cpp:67] Creating Layer drop2
I1026 15:34:57.031414 10978 net.cpp:394] drop2 <- pool2
I1026 15:34:57.031419 10978 net.cpp:345] drop2 -> pool2 (in-place)
I1026 15:34:57.031424 10978 net.cpp:96] Setting up drop2
I1026 15:34:57.031430 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:34:57.031436 10978 net.cpp:67] Creating Layer conv3
I1026 15:34:57.031440 10978 net.cpp:394] conv3 <- pool2
I1026 15:34:57.031446 10978 net.cpp:356] conv3 -> conv3
I1026 15:34:57.031453 10978 net.cpp:96] Setting up conv3
I1026 15:34:57.032814 10978 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 15:34:57.032832 10978 net.cpp:67] Creating Layer pool3
I1026 15:34:57.032837 10978 net.cpp:394] pool3 <- conv3
I1026 15:34:57.032843 10978 net.cpp:356] pool3 -> pool3
I1026 15:34:57.032850 10978 net.cpp:96] Setting up pool3
I1026 15:34:57.032856 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:34:57.032861 10978 net.cpp:67] Creating Layer relu3
I1026 15:34:57.032866 10978 net.cpp:394] relu3 <- pool3
I1026 15:34:57.032871 10978 net.cpp:345] relu3 -> pool3 (in-place)
I1026 15:34:57.032876 10978 net.cpp:96] Setting up relu3
I1026 15:34:57.032881 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:34:57.032886 10978 net.cpp:67] Creating Layer drop3
I1026 15:34:57.032889 10978 net.cpp:394] drop3 <- pool3
I1026 15:34:57.032894 10978 net.cpp:345] drop3 -> pool3 (in-place)
I1026 15:34:57.032901 10978 net.cpp:96] Setting up drop3
I1026 15:34:57.032904 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:34:57.032910 10978 net.cpp:67] Creating Layer ip1
I1026 15:34:57.032914 10978 net.cpp:394] ip1 <- pool3
I1026 15:34:57.032922 10978 net.cpp:356] ip1 -> ip1
I1026 15:34:57.032928 10978 net.cpp:96] Setting up ip1
I1026 15:34:57.460530 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:34:57.460592 10978 net.cpp:67] Creating Layer relu4
I1026 15:34:57.460602 10978 net.cpp:394] relu4 <- ip1
I1026 15:34:57.460613 10978 net.cpp:345] relu4 -> ip1 (in-place)
I1026 15:34:57.460621 10978 net.cpp:96] Setting up relu4
I1026 15:34:57.460628 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:34:57.460635 10978 net.cpp:67] Creating Layer drop4
I1026 15:34:57.460640 10978 net.cpp:394] drop4 <- ip1
I1026 15:34:57.460657 10978 net.cpp:345] drop4 -> ip1 (in-place)
I1026 15:34:57.460664 10978 net.cpp:96] Setting up drop4
I1026 15:34:57.460669 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:34:57.460680 10978 net.cpp:67] Creating Layer ip2
I1026 15:34:57.460683 10978 net.cpp:394] ip2 <- ip1
I1026 15:34:57.460691 10978 net.cpp:356] ip2 -> ip2
I1026 15:34:57.460705 10978 net.cpp:96] Setting up ip2
I1026 15:34:57.468209 10978 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 15:34:57.468271 10978 net.cpp:67] Creating Layer prob
I1026 15:34:57.468278 10978 net.cpp:394] prob <- ip2
I1026 15:34:57.468288 10978 net.cpp:356] prob -> prob
I1026 15:34:57.468298 10978 net.cpp:96] Setting up prob
I1026 15:34:57.468307 10978 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 15:34:57.468312 10978 net.cpp:172] prob does not need backward computation.
I1026 15:34:57.468317 10978 net.cpp:172] ip2 does not need backward computation.
I1026 15:34:57.468319 10978 net.cpp:172] drop4 does not need backward computation.
I1026 15:34:57.468323 10978 net.cpp:172] relu4 does not need backward computation.
I1026 15:34:57.468327 10978 net.cpp:172] ip1 does not need backward computation.
I1026 15:34:57.468332 10978 net.cpp:172] drop3 does not need backward computation.
I1026 15:34:57.468334 10978 net.cpp:172] relu3 does not need backward computation.
I1026 15:34:57.468338 10978 net.cpp:172] pool3 does not need backward computation.
I1026 15:34:57.468343 10978 net.cpp:172] conv3 does not need backward computation.
I1026 15:34:57.468345 10978 net.cpp:172] drop2 does not need backward computation.
I1026 15:34:57.468349 10978 net.cpp:172] relu2 does not need backward computation.
I1026 15:34:57.468353 10978 net.cpp:172] pool2 does not need backward computation.
I1026 15:34:57.468358 10978 net.cpp:172] conv2 does not need backward computation.
I1026 15:34:57.468360 10978 net.cpp:172] drop1 does not need backward computation.
I1026 15:34:57.468364 10978 net.cpp:172] relu1 does not need backward computation.
I1026 15:34:57.468368 10978 net.cpp:172] pool1 does not need backward computation.
I1026 15:34:57.468371 10978 net.cpp:172] conv1 does not need backward computation.
I1026 15:34:57.468375 10978 net.cpp:208] This network produces output prob
I1026 15:34:57.468391 10978 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 15:34:57.468401 10978 net.cpp:219] Network initialization done.
I1026 15:34:57.468405 10978 net.cpp:220] Memory required for data: 1837200
I1026 15:35:33.043728 10978 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 15:35:33.044323 10978 net.cpp:358] Input 0 -> data
I1026 15:35:33.044353 10978 net.cpp:67] Creating Layer conv1
I1026 15:35:33.044359 10978 net.cpp:394] conv1 <- data
I1026 15:35:33.044366 10978 net.cpp:356] conv1 -> conv1
I1026 15:35:33.044376 10978 net.cpp:96] Setting up conv1
I1026 15:35:33.044407 10978 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 15:35:33.044442 10978 net.cpp:67] Creating Layer pool1
I1026 15:35:33.044450 10978 net.cpp:394] pool1 <- conv1
I1026 15:35:33.044457 10978 net.cpp:356] pool1 -> pool1
I1026 15:35:33.044466 10978 net.cpp:96] Setting up pool1
I1026 15:35:33.044473 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:35:33.044481 10978 net.cpp:67] Creating Layer relu1
I1026 15:35:33.044484 10978 net.cpp:394] relu1 <- pool1
I1026 15:35:33.044491 10978 net.cpp:345] relu1 -> pool1 (in-place)
I1026 15:35:33.044497 10978 net.cpp:96] Setting up relu1
I1026 15:35:33.044502 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:35:33.044507 10978 net.cpp:67] Creating Layer drop1
I1026 15:35:33.044512 10978 net.cpp:394] drop1 <- pool1
I1026 15:35:33.044517 10978 net.cpp:345] drop1 -> pool1 (in-place)
I1026 15:35:33.044523 10978 net.cpp:96] Setting up drop1
I1026 15:35:33.044528 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:35:33.044540 10978 net.cpp:67] Creating Layer conv2
I1026 15:35:33.044544 10978 net.cpp:394] conv2 <- pool1
I1026 15:35:33.044551 10978 net.cpp:356] conv2 -> conv2
I1026 15:35:33.044559 10978 net.cpp:96] Setting up conv2
I1026 15:35:33.045059 10978 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 15:35:33.045074 10978 net.cpp:67] Creating Layer pool2
I1026 15:35:33.045079 10978 net.cpp:394] pool2 <- conv2
I1026 15:35:33.045085 10978 net.cpp:356] pool2 -> pool2
I1026 15:35:33.045092 10978 net.cpp:96] Setting up pool2
I1026 15:35:33.045099 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:35:33.045104 10978 net.cpp:67] Creating Layer relu2
I1026 15:35:33.045107 10978 net.cpp:394] relu2 <- pool2
I1026 15:35:33.045114 10978 net.cpp:345] relu2 -> pool2 (in-place)
I1026 15:35:33.045119 10978 net.cpp:96] Setting up relu2
I1026 15:35:33.045123 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:35:33.045128 10978 net.cpp:67] Creating Layer drop2
I1026 15:35:33.045132 10978 net.cpp:394] drop2 <- pool2
I1026 15:35:33.045137 10978 net.cpp:345] drop2 -> pool2 (in-place)
I1026 15:35:33.045143 10978 net.cpp:96] Setting up drop2
I1026 15:35:33.045147 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:35:33.045155 10978 net.cpp:67] Creating Layer conv3
I1026 15:35:33.045159 10978 net.cpp:394] conv3 <- pool2
I1026 15:35:33.045166 10978 net.cpp:356] conv3 -> conv3
I1026 15:35:33.045172 10978 net.cpp:96] Setting up conv3
I1026 15:35:33.046514 10978 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 15:35:33.046530 10978 net.cpp:67] Creating Layer pool3
I1026 15:35:33.046538 10978 net.cpp:394] pool3 <- conv3
I1026 15:35:33.046545 10978 net.cpp:356] pool3 -> pool3
I1026 15:35:33.046551 10978 net.cpp:96] Setting up pool3
I1026 15:35:33.046557 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:35:33.046563 10978 net.cpp:67] Creating Layer relu3
I1026 15:35:33.046567 10978 net.cpp:394] relu3 <- pool3
I1026 15:35:33.046572 10978 net.cpp:345] relu3 -> pool3 (in-place)
I1026 15:35:33.046577 10978 net.cpp:96] Setting up relu3
I1026 15:35:33.046581 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:35:33.046587 10978 net.cpp:67] Creating Layer drop3
I1026 15:35:33.046591 10978 net.cpp:394] drop3 <- pool3
I1026 15:35:33.046597 10978 net.cpp:345] drop3 -> pool3 (in-place)
I1026 15:35:33.046602 10978 net.cpp:96] Setting up drop3
I1026 15:35:33.046607 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:35:33.046612 10978 net.cpp:67] Creating Layer ip1
I1026 15:35:33.046617 10978 net.cpp:394] ip1 <- pool3
I1026 15:35:33.046623 10978 net.cpp:356] ip1 -> ip1
I1026 15:35:33.046630 10978 net.cpp:96] Setting up ip1
I1026 15:35:33.451561 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:35:33.451627 10978 net.cpp:67] Creating Layer relu4
I1026 15:35:33.451635 10978 net.cpp:394] relu4 <- ip1
I1026 15:35:33.451647 10978 net.cpp:345] relu4 -> ip1 (in-place)
I1026 15:35:33.451656 10978 net.cpp:96] Setting up relu4
I1026 15:35:33.451663 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:35:33.451670 10978 net.cpp:67] Creating Layer drop4
I1026 15:35:33.451674 10978 net.cpp:394] drop4 <- ip1
I1026 15:35:33.451681 10978 net.cpp:345] drop4 -> ip1 (in-place)
I1026 15:35:33.451688 10978 net.cpp:96] Setting up drop4
I1026 15:35:33.451694 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:35:33.451704 10978 net.cpp:67] Creating Layer ip2
I1026 15:35:33.451707 10978 net.cpp:394] ip2 <- ip1
I1026 15:35:33.451716 10978 net.cpp:356] ip2 -> ip2
I1026 15:35:33.451730 10978 net.cpp:96] Setting up ip2
I1026 15:35:33.459451 10978 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 15:35:33.459525 10978 net.cpp:67] Creating Layer prob
I1026 15:35:33.459533 10978 net.cpp:394] prob <- ip2
I1026 15:35:33.459543 10978 net.cpp:356] prob -> prob
I1026 15:35:33.459555 10978 net.cpp:96] Setting up prob
I1026 15:35:33.459563 10978 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 15:35:33.459568 10978 net.cpp:172] prob does not need backward computation.
I1026 15:35:33.459573 10978 net.cpp:172] ip2 does not need backward computation.
I1026 15:35:33.459578 10978 net.cpp:172] drop4 does not need backward computation.
I1026 15:35:33.459581 10978 net.cpp:172] relu4 does not need backward computation.
I1026 15:35:33.459585 10978 net.cpp:172] ip1 does not need backward computation.
I1026 15:35:33.459589 10978 net.cpp:172] drop3 does not need backward computation.
I1026 15:35:33.459594 10978 net.cpp:172] relu3 does not need backward computation.
I1026 15:35:33.459601 10978 net.cpp:172] pool3 does not need backward computation.
I1026 15:35:33.459604 10978 net.cpp:172] conv3 does not need backward computation.
I1026 15:35:33.459609 10978 net.cpp:172] drop2 does not need backward computation.
I1026 15:35:33.459612 10978 net.cpp:172] relu2 does not need backward computation.
I1026 15:35:33.459616 10978 net.cpp:172] pool2 does not need backward computation.
I1026 15:35:33.459620 10978 net.cpp:172] conv2 does not need backward computation.
I1026 15:35:33.459625 10978 net.cpp:172] drop1 does not need backward computation.
I1026 15:35:33.459628 10978 net.cpp:172] relu1 does not need backward computation.
I1026 15:35:33.459631 10978 net.cpp:172] pool1 does not need backward computation.
I1026 15:35:33.459635 10978 net.cpp:172] conv1 does not need backward computation.
I1026 15:35:33.459640 10978 net.cpp:208] This network produces output prob
I1026 15:35:33.459653 10978 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 15:35:33.459663 10978 net.cpp:219] Network initialization done.
I1026 15:35:33.459667 10978 net.cpp:220] Memory required for data: 1837200
I1026 15:36:10.064203 10978 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 15:36:10.064733 10978 net.cpp:358] Input 0 -> data
I1026 15:36:10.064780 10978 net.cpp:67] Creating Layer conv1
I1026 15:36:10.064793 10978 net.cpp:394] conv1 <- data
I1026 15:36:10.064808 10978 net.cpp:356] conv1 -> conv1
I1026 15:36:10.064828 10978 net.cpp:96] Setting up conv1
I1026 15:36:10.064883 10978 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 15:36:10.064913 10978 net.cpp:67] Creating Layer pool1
I1026 15:36:10.064924 10978 net.cpp:394] pool1 <- conv1
I1026 15:36:10.064939 10978 net.cpp:356] pool1 -> pool1
I1026 15:36:10.064954 10978 net.cpp:96] Setting up pool1
I1026 15:36:10.064970 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:36:10.064983 10978 net.cpp:67] Creating Layer relu1
I1026 15:36:10.064992 10978 net.cpp:394] relu1 <- pool1
I1026 15:36:10.065004 10978 net.cpp:345] relu1 -> pool1 (in-place)
I1026 15:36:10.065017 10978 net.cpp:96] Setting up relu1
I1026 15:36:10.065027 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:36:10.065039 10978 net.cpp:67] Creating Layer drop1
I1026 15:36:10.065048 10978 net.cpp:394] drop1 <- pool1
I1026 15:36:10.065060 10978 net.cpp:345] drop1 -> pool1 (in-place)
I1026 15:36:10.065073 10978 net.cpp:96] Setting up drop1
I1026 15:36:10.065084 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:36:10.065099 10978 net.cpp:67] Creating Layer conv2
I1026 15:36:10.065115 10978 net.cpp:394] conv2 <- pool1
I1026 15:36:10.065130 10978 net.cpp:356] conv2 -> conv2
I1026 15:36:10.065145 10978 net.cpp:96] Setting up conv2
I1026 15:36:10.066277 10978 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 15:36:10.066305 10978 net.cpp:67] Creating Layer pool2
I1026 15:36:10.066316 10978 net.cpp:394] pool2 <- conv2
I1026 15:36:10.066329 10978 net.cpp:356] pool2 -> pool2
I1026 15:36:10.066345 10978 net.cpp:96] Setting up pool2
I1026 15:36:10.066359 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:36:10.066371 10978 net.cpp:67] Creating Layer relu2
I1026 15:36:10.066380 10978 net.cpp:394] relu2 <- pool2
I1026 15:36:10.066391 10978 net.cpp:345] relu2 -> pool2 (in-place)
I1026 15:36:10.066404 10978 net.cpp:96] Setting up relu2
I1026 15:36:10.066413 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:36:10.066426 10978 net.cpp:67] Creating Layer drop2
I1026 15:36:10.066438 10978 net.cpp:394] drop2 <- pool2
I1026 15:36:10.066455 10978 net.cpp:345] drop2 -> pool2 (in-place)
I1026 15:36:10.066472 10978 net.cpp:96] Setting up drop2
I1026 15:36:10.066483 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:36:10.066503 10978 net.cpp:67] Creating Layer conv3
I1026 15:36:10.066514 10978 net.cpp:394] conv3 <- pool2
I1026 15:36:10.066530 10978 net.cpp:356] conv3 -> conv3
I1026 15:36:10.066550 10978 net.cpp:96] Setting up conv3
I1026 15:36:10.070220 10978 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 15:36:10.070263 10978 net.cpp:67] Creating Layer pool3
I1026 15:36:10.070276 10978 net.cpp:394] pool3 <- conv3
I1026 15:36:10.070293 10978 net.cpp:356] pool3 -> pool3
I1026 15:36:10.070312 10978 net.cpp:96] Setting up pool3
I1026 15:36:10.070327 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:36:10.070340 10978 net.cpp:67] Creating Layer relu3
I1026 15:36:10.070351 10978 net.cpp:394] relu3 <- pool3
I1026 15:36:10.070366 10978 net.cpp:345] relu3 -> pool3 (in-place)
I1026 15:36:10.070381 10978 net.cpp:96] Setting up relu3
I1026 15:36:10.070394 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:36:10.070408 10978 net.cpp:67] Creating Layer drop3
I1026 15:36:10.070420 10978 net.cpp:394] drop3 <- pool3
I1026 15:36:10.070435 10978 net.cpp:345] drop3 -> pool3 (in-place)
I1026 15:36:10.070449 10978 net.cpp:96] Setting up drop3
I1026 15:36:10.070461 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:36:10.070478 10978 net.cpp:67] Creating Layer ip1
I1026 15:36:10.070490 10978 net.cpp:394] ip1 <- pool3
I1026 15:36:10.070508 10978 net.cpp:356] ip1 -> ip1
I1026 15:36:10.070526 10978 net.cpp:96] Setting up ip1
I1026 15:36:10.490681 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:36:10.490747 10978 net.cpp:67] Creating Layer relu4
I1026 15:36:10.490756 10978 net.cpp:394] relu4 <- ip1
I1026 15:36:10.490767 10978 net.cpp:345] relu4 -> ip1 (in-place)
I1026 15:36:10.490777 10978 net.cpp:96] Setting up relu4
I1026 15:36:10.490783 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:36:10.490792 10978 net.cpp:67] Creating Layer drop4
I1026 15:36:10.490795 10978 net.cpp:394] drop4 <- ip1
I1026 15:36:10.490802 10978 net.cpp:345] drop4 -> ip1 (in-place)
I1026 15:36:10.490809 10978 net.cpp:96] Setting up drop4
I1026 15:36:10.490815 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:36:10.490825 10978 net.cpp:67] Creating Layer ip2
I1026 15:36:10.490829 10978 net.cpp:394] ip2 <- ip1
I1026 15:36:10.490839 10978 net.cpp:356] ip2 -> ip2
I1026 15:36:10.490854 10978 net.cpp:96] Setting up ip2
I1026 15:36:10.498424 10978 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 15:36:10.498486 10978 net.cpp:67] Creating Layer prob
I1026 15:36:10.498493 10978 net.cpp:394] prob <- ip2
I1026 15:36:10.498503 10978 net.cpp:356] prob -> prob
I1026 15:36:10.498515 10978 net.cpp:96] Setting up prob
I1026 15:36:10.498523 10978 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 15:36:10.498528 10978 net.cpp:172] prob does not need backward computation.
I1026 15:36:10.498533 10978 net.cpp:172] ip2 does not need backward computation.
I1026 15:36:10.498536 10978 net.cpp:172] drop4 does not need backward computation.
I1026 15:36:10.498551 10978 net.cpp:172] relu4 does not need backward computation.
I1026 15:36:10.498556 10978 net.cpp:172] ip1 does not need backward computation.
I1026 15:36:10.498560 10978 net.cpp:172] drop3 does not need backward computation.
I1026 15:36:10.498564 10978 net.cpp:172] relu3 does not need backward computation.
I1026 15:36:10.498569 10978 net.cpp:172] pool3 does not need backward computation.
I1026 15:36:10.498572 10978 net.cpp:172] conv3 does not need backward computation.
I1026 15:36:10.498575 10978 net.cpp:172] drop2 does not need backward computation.
I1026 15:36:10.498580 10978 net.cpp:172] relu2 does not need backward computation.
I1026 15:36:10.498584 10978 net.cpp:172] pool2 does not need backward computation.
I1026 15:36:10.498587 10978 net.cpp:172] conv2 does not need backward computation.
I1026 15:36:10.498591 10978 net.cpp:172] drop1 does not need backward computation.
I1026 15:36:10.498595 10978 net.cpp:172] relu1 does not need backward computation.
I1026 15:36:10.498599 10978 net.cpp:172] pool1 does not need backward computation.
I1026 15:36:10.498602 10978 net.cpp:172] conv1 does not need backward computation.
I1026 15:36:10.498606 10978 net.cpp:208] This network produces output prob
I1026 15:36:10.498621 10978 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 15:36:10.498631 10978 net.cpp:219] Network initialization done.
I1026 15:36:10.498636 10978 net.cpp:220] Memory required for data: 1837200
I1026 15:36:45.754250 10978 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 15:36:45.754420 10978 net.cpp:358] Input 0 -> data
I1026 15:36:45.754463 10978 net.cpp:67] Creating Layer conv1
I1026 15:36:45.754477 10978 net.cpp:394] conv1 <- data
I1026 15:36:45.754492 10978 net.cpp:356] conv1 -> conv1
I1026 15:36:45.754510 10978 net.cpp:96] Setting up conv1
I1026 15:36:45.754567 10978 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 15:36:45.754597 10978 net.cpp:67] Creating Layer pool1
I1026 15:36:45.754607 10978 net.cpp:394] pool1 <- conv1
I1026 15:36:45.754621 10978 net.cpp:356] pool1 -> pool1
I1026 15:36:45.754637 10978 net.cpp:96] Setting up pool1
I1026 15:36:45.754652 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:36:45.754665 10978 net.cpp:67] Creating Layer relu1
I1026 15:36:45.754674 10978 net.cpp:394] relu1 <- pool1
I1026 15:36:45.754686 10978 net.cpp:345] relu1 -> pool1 (in-place)
I1026 15:36:45.754699 10978 net.cpp:96] Setting up relu1
I1026 15:36:45.754709 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:36:45.754722 10978 net.cpp:67] Creating Layer drop1
I1026 15:36:45.754731 10978 net.cpp:394] drop1 <- pool1
I1026 15:36:45.754744 10978 net.cpp:345] drop1 -> pool1 (in-place)
I1026 15:36:45.754756 10978 net.cpp:96] Setting up drop1
I1026 15:36:45.754766 10978 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 15:36:45.754781 10978 net.cpp:67] Creating Layer conv2
I1026 15:36:45.754791 10978 net.cpp:394] conv2 <- pool1
I1026 15:36:45.754804 10978 net.cpp:356] conv2 -> conv2
I1026 15:36:45.754819 10978 net.cpp:96] Setting up conv2
I1026 15:36:45.756160 10978 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 15:36:45.756196 10978 net.cpp:67] Creating Layer pool2
I1026 15:36:45.756208 10978 net.cpp:394] pool2 <- conv2
I1026 15:36:45.756224 10978 net.cpp:356] pool2 -> pool2
I1026 15:36:45.756243 10978 net.cpp:96] Setting up pool2
I1026 15:36:45.756259 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:36:45.756274 10978 net.cpp:67] Creating Layer relu2
I1026 15:36:45.756285 10978 net.cpp:394] relu2 <- pool2
I1026 15:36:45.756299 10978 net.cpp:345] relu2 -> pool2 (in-place)
I1026 15:36:45.756314 10978 net.cpp:96] Setting up relu2
I1026 15:36:45.756325 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:36:45.756340 10978 net.cpp:67] Creating Layer drop2
I1026 15:36:45.756350 10978 net.cpp:394] drop2 <- pool2
I1026 15:36:45.756366 10978 net.cpp:345] drop2 -> pool2 (in-place)
I1026 15:36:45.756381 10978 net.cpp:96] Setting up drop2
I1026 15:36:45.756393 10978 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 15:36:45.756412 10978 net.cpp:67] Creating Layer conv3
I1026 15:36:45.756476 10978 net.cpp:394] conv3 <- pool2
I1026 15:36:45.756507 10978 net.cpp:356] conv3 -> conv3
I1026 15:36:45.756530 10978 net.cpp:96] Setting up conv3
I1026 15:36:45.760166 10978 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 15:36:45.760202 10978 net.cpp:67] Creating Layer pool3
I1026 15:36:45.760215 10978 net.cpp:394] pool3 <- conv3
I1026 15:36:45.760232 10978 net.cpp:356] pool3 -> pool3
I1026 15:36:45.760251 10978 net.cpp:96] Setting up pool3
I1026 15:36:45.760264 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:36:45.760280 10978 net.cpp:67] Creating Layer relu3
I1026 15:36:45.760292 10978 net.cpp:394] relu3 <- pool3
I1026 15:36:45.760305 10978 net.cpp:345] relu3 -> pool3 (in-place)
I1026 15:36:45.760321 10978 net.cpp:96] Setting up relu3
I1026 15:36:45.760332 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:36:45.760347 10978 net.cpp:67] Creating Layer drop3
I1026 15:36:45.760359 10978 net.cpp:394] drop3 <- pool3
I1026 15:36:45.760372 10978 net.cpp:345] drop3 -> pool3 (in-place)
I1026 15:36:45.760388 10978 net.cpp:96] Setting up drop3
I1026 15:36:45.760401 10978 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 15:36:45.760432 10978 net.cpp:67] Creating Layer ip1
I1026 15:36:45.760493 10978 net.cpp:394] ip1 <- pool3
I1026 15:36:45.760524 10978 net.cpp:356] ip1 -> ip1
I1026 15:36:45.760553 10978 net.cpp:96] Setting up ip1
I1026 15:36:46.167315 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:36:46.167992 10978 net.cpp:67] Creating Layer relu4
I1026 15:36:46.168015 10978 net.cpp:394] relu4 <- ip1
I1026 15:36:46.168035 10978 net.cpp:345] relu4 -> ip1 (in-place)
I1026 15:36:46.168054 10978 net.cpp:96] Setting up relu4
I1026 15:36:46.168066 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:36:46.168081 10978 net.cpp:67] Creating Layer drop4
I1026 15:36:46.168092 10978 net.cpp:394] drop4 <- ip1
I1026 15:36:46.168104 10978 net.cpp:345] drop4 -> ip1 (in-place)
I1026 15:36:46.168118 10978 net.cpp:96] Setting up drop4
I1026 15:36:46.168130 10978 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 15:36:46.168149 10978 net.cpp:67] Creating Layer ip2
I1026 15:36:46.168159 10978 net.cpp:394] ip2 <- ip1
I1026 15:36:46.168174 10978 net.cpp:356] ip2 -> ip2
I1026 15:36:46.168200 10978 net.cpp:96] Setting up ip2
I1026 15:36:46.180759 10978 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 15:36:46.180827 10978 net.cpp:67] Creating Layer prob
I1026 15:36:46.180835 10978 net.cpp:394] prob <- ip2
I1026 15:36:46.180845 10978 net.cpp:356] prob -> prob
I1026 15:36:46.180855 10978 net.cpp:96] Setting up prob
I1026 15:36:46.180863 10978 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 15:36:46.180868 10978 net.cpp:172] prob does not need backward computation.
I1026 15:36:46.180872 10978 net.cpp:172] ip2 does not need backward computation.
I1026 15:36:46.180876 10978 net.cpp:172] drop4 does not need backward computation.
I1026 15:36:46.180881 10978 net.cpp:172] relu4 does not need backward computation.
I1026 15:36:46.180884 10978 net.cpp:172] ip1 does not need backward computation.
I1026 15:36:46.180888 10978 net.cpp:172] drop3 does not need backward computation.
I1026 15:36:46.180892 10978 net.cpp:172] relu3 does not need backward computation.
I1026 15:36:46.180896 10978 net.cpp:172] pool3 does not need backward computation.
I1026 15:36:46.180901 10978 net.cpp:172] conv3 does not need backward computation.
I1026 15:36:46.180903 10978 net.cpp:172] drop2 does not need backward computation.
I1026 15:36:46.180907 10978 net.cpp:172] relu2 does not need backward computation.
I1026 15:36:46.180912 10978 net.cpp:172] pool2 does not need backward computation.
I1026 15:36:46.180915 10978 net.cpp:172] conv2 does not need backward computation.
I1026 15:36:46.180918 10978 net.cpp:172] drop1 does not need backward computation.
I1026 15:36:46.180922 10978 net.cpp:172] relu1 does not need backward computation.
I1026 15:36:46.180927 10978 net.cpp:172] pool1 does not need backward computation.
I1026 15:36:46.180930 10978 net.cpp:172] conv1 does not need backward computation.
I1026 15:36:46.180933 10978 net.cpp:208] This network produces output prob
I1026 15:36:46.180949 10978 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 15:36:46.180959 10978 net.cpp:219] Network initialization done.
I1026 15:36:46.180963 10978 net.cpp:220] Memory required for data: 1837200
I1026 16:09:56.226233 17654 convert_imageset.cpp:70] Shuffling data
I1026 16:09:56.776765 17654 convert_imageset.cpp:73] A total of 60000 images.
I1026 16:09:56.776845 17654 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1026 16:09:58.904460 17654 convert_imageset.cpp:177] Processed 1000 files.
E1026 16:10:01.027233 17654 convert_imageset.cpp:177] Processed 2000 files.
E1026 16:10:03.139226 17654 convert_imageset.cpp:177] Processed 3000 files.
E1026 16:10:05.170884 17654 convert_imageset.cpp:177] Processed 4000 files.
E1026 16:10:07.453454 17654 convert_imageset.cpp:177] Processed 5000 files.
E1026 16:10:09.247898 17654 convert_imageset.cpp:177] Processed 6000 files.
E1026 16:10:11.045248 17654 convert_imageset.cpp:177] Processed 7000 files.
E1026 16:10:13.007030 17654 convert_imageset.cpp:177] Processed 8000 files.
E1026 16:10:14.679143 17654 convert_imageset.cpp:177] Processed 9000 files.
E1026 16:10:16.365847 17654 convert_imageset.cpp:177] Processed 10000 files.
E1026 16:10:18.040870 17654 convert_imageset.cpp:177] Processed 11000 files.
E1026 16:10:19.756357 17654 convert_imageset.cpp:177] Processed 12000 files.
E1026 16:10:21.413084 17654 convert_imageset.cpp:177] Processed 13000 files.
E1026 16:10:23.060467 17654 convert_imageset.cpp:177] Processed 14000 files.
E1026 16:10:24.678354 17654 convert_imageset.cpp:177] Processed 15000 files.
E1026 16:10:26.331043 17654 convert_imageset.cpp:177] Processed 16000 files.
E1026 16:10:27.998280 17654 convert_imageset.cpp:177] Processed 17000 files.
E1026 16:10:29.838820 17654 convert_imageset.cpp:177] Processed 18000 files.
E1026 16:10:31.615068 17654 convert_imageset.cpp:177] Processed 19000 files.
E1026 16:10:33.242717 17654 convert_imageset.cpp:177] Processed 20000 files.
E1026 16:10:34.828586 17654 convert_imageset.cpp:177] Processed 21000 files.
E1026 16:10:36.375445 17654 convert_imageset.cpp:177] Processed 22000 files.
E1026 16:10:37.913943 17654 convert_imageset.cpp:177] Processed 23000 files.
E1026 16:10:39.682417 17654 convert_imageset.cpp:177] Processed 24000 files.
E1026 16:10:41.251618 17654 convert_imageset.cpp:177] Processed 25000 files.
E1026 16:10:42.847384 17654 convert_imageset.cpp:177] Processed 26000 files.
E1026 16:10:44.524161 17654 convert_imageset.cpp:177] Processed 27000 files.
E1026 16:10:46.142911 17654 convert_imageset.cpp:177] Processed 28000 files.
E1026 16:10:47.708104 17654 convert_imageset.cpp:177] Processed 29000 files.
E1026 16:10:49.288578 17654 convert_imageset.cpp:177] Processed 30000 files.
E1026 16:10:50.860893 17654 convert_imageset.cpp:177] Processed 31000 files.
E1026 16:10:52.343317 17654 convert_imageset.cpp:177] Processed 32000 files.
E1026 16:10:53.849547 17654 convert_imageset.cpp:177] Processed 33000 files.
E1026 16:10:55.374644 17654 convert_imageset.cpp:177] Processed 34000 files.
E1026 16:10:56.851935 17654 convert_imageset.cpp:177] Processed 35000 files.
E1026 16:10:58.426100 17654 convert_imageset.cpp:177] Processed 36000 files.
E1026 16:10:59.976207 17654 convert_imageset.cpp:177] Processed 37000 files.
E1026 16:11:01.555085 17654 convert_imageset.cpp:177] Processed 38000 files.
E1026 16:11:03.163795 17654 convert_imageset.cpp:177] Processed 39000 files.
E1026 16:11:04.761615 17654 convert_imageset.cpp:177] Processed 40000 files.
E1026 16:11:06.286659 17654 convert_imageset.cpp:177] Processed 41000 files.
E1026 16:11:07.875571 17654 convert_imageset.cpp:177] Processed 42000 files.
E1026 16:11:09.406725 17654 convert_imageset.cpp:177] Processed 43000 files.
E1026 16:11:10.899471 17654 convert_imageset.cpp:177] Processed 44000 files.
E1026 16:11:12.472038 17654 convert_imageset.cpp:177] Processed 45000 files.
E1026 16:11:14.010490 17654 convert_imageset.cpp:177] Processed 46000 files.
E1026 16:11:15.553576 17654 convert_imageset.cpp:177] Processed 47000 files.
E1026 16:11:17.074717 17654 convert_imageset.cpp:177] Processed 48000 files.
E1026 16:11:18.712838 17654 convert_imageset.cpp:177] Processed 49000 files.
E1026 16:11:20.294162 17654 convert_imageset.cpp:177] Processed 50000 files.
E1026 16:11:21.891487 17654 convert_imageset.cpp:177] Processed 51000 files.
E1026 16:11:23.341168 17654 convert_imageset.cpp:177] Processed 52000 files.
E1026 16:11:24.797184 17654 convert_imageset.cpp:177] Processed 53000 files.
E1026 16:11:26.265712 17654 convert_imageset.cpp:177] Processed 54000 files.
E1026 16:11:27.748528 17654 convert_imageset.cpp:177] Processed 55000 files.
E1026 16:11:29.314699 17654 convert_imageset.cpp:177] Processed 56000 files.
E1026 16:11:30.837602 17654 convert_imageset.cpp:177] Processed 57000 files.
E1026 16:11:32.284690 17654 convert_imageset.cpp:177] Processed 58000 files.
E1026 16:11:33.722923 17654 convert_imageset.cpp:177] Processed 59000 files.
E1026 16:11:35.210772 17654 convert_imageset.cpp:177] Processed 60000 files.
I1026 16:11:35.425680 17728 caffe.cpp:99] Use GPU with device ID 0
I1026 16:11:35.768556 17728 caffe.cpp:107] Starting Optimization
I1026 16:11:35.768676 17728 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 125000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1026 16:11:35.768707 17728 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1026 16:11:35.781599 17728 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1026 16:11:35.781689 17728 net.cpp:67] Creating Layer mnist
I1026 16:11:35.781699 17728 net.cpp:356] mnist -> data
I1026 16:11:35.781715 17728 net.cpp:356] mnist -> label
I1026 16:11:35.781729 17728 net.cpp:96] Setting up mnist
I1026 16:11:35.790009 17728 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1026 16:11:35.790117 17728 data_layer.cpp:128] output data size: 64,1,50,180
I1026 16:11:35.791218 17728 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1026 16:11:35.791254 17728 net.cpp:103] Top shape: 64 1 1 1 (64)
I1026 16:11:35.791276 17728 net.cpp:67] Creating Layer conv1
I1026 16:11:35.791285 17728 net.cpp:394] conv1 <- data
I1026 16:11:35.791332 17728 net.cpp:356] conv1 -> conv1
I1026 16:11:35.791364 17728 net.cpp:96] Setting up conv1
I1026 16:11:35.791738 17728 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1026 16:11:35.791774 17728 net.cpp:67] Creating Layer pool1
I1026 16:11:35.791779 17728 net.cpp:394] pool1 <- conv1
I1026 16:11:35.791785 17728 net.cpp:356] pool1 -> pool1
I1026 16:11:35.791793 17728 net.cpp:96] Setting up pool1
I1026 16:11:35.791810 17728 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 16:11:35.791818 17728 net.cpp:67] Creating Layer relu1
I1026 16:11:35.791823 17728 net.cpp:394] relu1 <- pool1
I1026 16:11:35.791829 17728 net.cpp:345] relu1 -> pool1 (in-place)
I1026 16:11:35.791836 17728 net.cpp:96] Setting up relu1
I1026 16:11:35.791841 17728 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 16:11:35.791847 17728 net.cpp:67] Creating Layer drop1
I1026 16:11:35.791852 17728 net.cpp:394] drop1 <- pool1
I1026 16:11:35.791858 17728 net.cpp:345] drop1 -> pool1 (in-place)
I1026 16:11:35.791864 17728 net.cpp:96] Setting up drop1
I1026 16:11:35.791869 17728 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 16:11:35.791878 17728 net.cpp:67] Creating Layer conv2
I1026 16:11:35.791883 17728 net.cpp:394] conv2 <- pool1
I1026 16:11:35.791889 17728 net.cpp:356] conv2 -> conv2
I1026 16:11:35.791898 17728 net.cpp:96] Setting up conv2
I1026 16:11:35.792536 17728 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1026 16:11:35.792564 17728 net.cpp:67] Creating Layer pool2
I1026 16:11:35.792573 17728 net.cpp:394] pool2 <- conv2
I1026 16:11:35.792584 17728 net.cpp:356] pool2 -> pool2
I1026 16:11:35.792595 17728 net.cpp:96] Setting up pool2
I1026 16:11:35.792604 17728 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 16:11:35.792614 17728 net.cpp:67] Creating Layer relu2
I1026 16:11:35.792621 17728 net.cpp:394] relu2 <- pool2
I1026 16:11:35.792630 17728 net.cpp:345] relu2 -> pool2 (in-place)
I1026 16:11:35.792640 17728 net.cpp:96] Setting up relu2
I1026 16:11:35.792647 17728 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 16:11:35.792661 17728 net.cpp:67] Creating Layer drop2
I1026 16:11:35.792670 17728 net.cpp:394] drop2 <- pool2
I1026 16:11:35.792678 17728 net.cpp:345] drop2 -> pool2 (in-place)
I1026 16:11:35.792690 17728 net.cpp:96] Setting up drop2
I1026 16:11:35.792697 17728 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 16:11:35.792711 17728 net.cpp:67] Creating Layer conv3
I1026 16:11:35.792718 17728 net.cpp:394] conv3 <- pool2
I1026 16:11:35.792728 17728 net.cpp:356] conv3 -> conv3
I1026 16:11:35.792740 17728 net.cpp:96] Setting up conv3
I1026 16:11:35.795207 17728 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1026 16:11:35.795236 17728 net.cpp:67] Creating Layer pool3
I1026 16:11:35.795244 17728 net.cpp:394] pool3 <- conv3
I1026 16:11:35.795255 17728 net.cpp:356] pool3 -> pool3
I1026 16:11:35.795266 17728 net.cpp:96] Setting up pool3
I1026 16:11:35.795279 17728 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 16:11:35.795290 17728 net.cpp:67] Creating Layer relu3
I1026 16:11:35.795297 17728 net.cpp:394] relu3 <- pool3
I1026 16:11:35.795306 17728 net.cpp:345] relu3 -> pool3 (in-place)
I1026 16:11:35.795316 17728 net.cpp:96] Setting up relu3
I1026 16:11:35.795323 17728 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 16:11:35.795336 17728 net.cpp:67] Creating Layer drop3
I1026 16:11:35.795344 17728 net.cpp:394] drop3 <- pool3
I1026 16:11:35.795353 17728 net.cpp:345] drop3 -> pool3 (in-place)
I1026 16:11:35.795363 17728 net.cpp:96] Setting up drop3
I1026 16:11:35.795372 17728 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 16:11:35.795382 17728 net.cpp:67] Creating Layer ip1
I1026 16:11:35.795389 17728 net.cpp:394] ip1 <- pool3
I1026 16:11:35.795402 17728 net.cpp:356] ip1 -> ip1
I1026 16:11:35.795449 17728 net.cpp:96] Setting up ip1
I1026 16:11:36.213717 17728 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 16:11:36.213778 17728 net.cpp:67] Creating Layer relu4
I1026 16:11:36.213791 17728 net.cpp:394] relu4 <- ip1
I1026 16:11:36.213803 17728 net.cpp:345] relu4 -> ip1 (in-place)
I1026 16:11:36.213811 17728 net.cpp:96] Setting up relu4
I1026 16:11:36.213817 17728 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 16:11:36.213824 17728 net.cpp:67] Creating Layer drop4
I1026 16:11:36.213829 17728 net.cpp:394] drop4 <- ip1
I1026 16:11:36.213834 17728 net.cpp:345] drop4 -> ip1 (in-place)
I1026 16:11:36.213840 17728 net.cpp:96] Setting up drop4
I1026 16:11:36.213845 17728 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 16:11:36.213857 17728 net.cpp:67] Creating Layer ip2
I1026 16:11:36.213861 17728 net.cpp:394] ip2 <- ip1
I1026 16:11:36.213868 17728 net.cpp:356] ip2 -> ip2
I1026 16:11:36.213876 17728 net.cpp:96] Setting up ip2
I1026 16:11:36.227211 17728 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1026 16:11:36.227277 17728 net.cpp:67] Creating Layer loss
I1026 16:11:36.227283 17728 net.cpp:394] loss <- ip2
I1026 16:11:36.227291 17728 net.cpp:394] loss <- label
I1026 16:11:36.227298 17728 net.cpp:356] loss -> loss
I1026 16:11:36.227308 17728 net.cpp:96] Setting up loss
I1026 16:11:36.227321 17728 net.cpp:103] Top shape: 1 1 1 1 (1)
I1026 16:11:36.227326 17728 net.cpp:109]     with loss weight 1
I1026 16:11:36.227361 17728 net.cpp:170] loss needs backward computation.
I1026 16:11:36.227366 17728 net.cpp:170] ip2 needs backward computation.
I1026 16:11:36.227371 17728 net.cpp:170] drop4 needs backward computation.
I1026 16:11:36.227375 17728 net.cpp:170] relu4 needs backward computation.
I1026 16:11:36.227380 17728 net.cpp:170] ip1 needs backward computation.
I1026 16:11:36.227383 17728 net.cpp:170] drop3 needs backward computation.
I1026 16:11:36.227387 17728 net.cpp:170] relu3 needs backward computation.
I1026 16:11:36.227391 17728 net.cpp:170] pool3 needs backward computation.
I1026 16:11:36.227396 17728 net.cpp:170] conv3 needs backward computation.
I1026 16:11:36.227401 17728 net.cpp:170] drop2 needs backward computation.
I1026 16:11:36.227406 17728 net.cpp:170] relu2 needs backward computation.
I1026 16:11:36.227409 17728 net.cpp:170] pool2 needs backward computation.
I1026 16:11:36.227413 17728 net.cpp:170] conv2 needs backward computation.
I1026 16:11:36.227417 17728 net.cpp:170] drop1 needs backward computation.
I1026 16:11:36.227422 17728 net.cpp:170] relu1 needs backward computation.
I1026 16:11:36.227427 17728 net.cpp:170] pool1 needs backward computation.
I1026 16:11:36.227430 17728 net.cpp:170] conv1 needs backward computation.
I1026 16:11:36.227434 17728 net.cpp:172] mnist does not need backward computation.
I1026 16:11:36.227439 17728 net.cpp:208] This network produces output loss
I1026 16:11:36.227449 17728 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 16:11:36.227457 17728 net.cpp:219] Network initialization done.
I1026 16:11:36.227460 17728 net.cpp:220] Memory required for data: 119788292
I1026 16:11:36.227520 17728 solver.cpp:41] Solver scaffolding done.
I1026 16:11:36.227527 17728 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_100000.solverstate
I1026 16:11:36.227531 17728 solver.cpp:160] Solving Captcha
I1026 16:11:36.227550 17728 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_100000.solverstate
I1026 16:11:42.329149 17728 solver.cpp:502] SGDSolver: restoring history
I1026 16:11:43.232647 17728 solver.cpp:191] Iteration 100000, loss = 3.00862
I1026 16:11:43.232707 17728 solver.cpp:206]     Train net output #0: loss = 3.00862 (* 1 = 3.00862 loss)
I1026 16:11:43.232722 17728 solver.cpp:403] Iteration 100000, lr = 0.0016556
I1026 16:15:44.663235 17728 solver.cpp:191] Iteration 101000, loss = 2.60389
I1026 16:15:44.664191 17728 solver.cpp:206]     Train net output #0: loss = 2.60389 (* 1 = 2.60389 loss)
I1026 16:15:44.664216 17728 solver.cpp:403] Iteration 101000, lr = 0.0016444
I1026 16:19:45.664676 17728 solver.cpp:191] Iteration 102000, loss = 2.49742
I1026 16:19:45.665529 17728 solver.cpp:206]     Train net output #0: loss = 2.49742 (* 1 = 2.49742 loss)
I1026 16:19:45.665570 17728 solver.cpp:403] Iteration 102000, lr = 0.00163338
I1026 16:23:46.730255 17728 solver.cpp:191] Iteration 103000, loss = 2.65046
I1026 16:23:46.730835 17728 solver.cpp:206]     Train net output #0: loss = 2.65046 (* 1 = 2.65046 loss)
I1026 16:23:46.730871 17728 solver.cpp:403] Iteration 103000, lr = 0.00162252
I1026 16:27:47.845319 17728 solver.cpp:191] Iteration 104000, loss = 2.44867
I1026 16:27:47.846019 17728 solver.cpp:206]     Train net output #0: loss = 2.44867 (* 1 = 2.44867 loss)
I1026 16:27:47.846051 17728 solver.cpp:403] Iteration 104000, lr = 0.00161184
I1026 16:31:49.518329 17728 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_105000.caffemodel
I1026 16:31:54.132014 17728 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_105000.solverstate
I1026 16:31:57.817478 17728 solver.cpp:191] Iteration 105000, loss = 2.66962
I1026 16:31:57.817987 17728 solver.cpp:206]     Train net output #0: loss = 2.66962 (* 1 = 2.66962 loss)
I1026 16:31:57.818027 17728 solver.cpp:403] Iteration 105000, lr = 0.00160131
I1026 16:35:58.954809 17728 solver.cpp:191] Iteration 106000, loss = 2.45612
I1026 16:35:58.955363 17728 solver.cpp:206]     Train net output #0: loss = 2.45612 (* 1 = 2.45612 loss)
I1026 16:35:58.955396 17728 solver.cpp:403] Iteration 106000, lr = 0.00159095
I1026 16:40:00.114303 17728 solver.cpp:191] Iteration 107000, loss = 2.46026
I1026 16:40:00.114960 17728 solver.cpp:206]     Train net output #0: loss = 2.46026 (* 1 = 2.46026 loss)
I1026 16:40:00.114980 17728 solver.cpp:403] Iteration 107000, lr = 0.00158074
I1026 16:44:01.216774 17728 solver.cpp:191] Iteration 108000, loss = 2.3431
I1026 16:44:01.217572 17728 solver.cpp:206]     Train net output #0: loss = 2.3431 (* 1 = 2.3431 loss)
I1026 16:44:01.217608 17728 solver.cpp:403] Iteration 108000, lr = 0.00157068
I1026 16:48:02.377435 17728 solver.cpp:191] Iteration 109000, loss = 2.70414
I1026 16:48:02.378070 17728 solver.cpp:206]     Train net output #0: loss = 2.70414 (* 1 = 2.70414 loss)
I1026 16:48:02.378103 17728 solver.cpp:403] Iteration 109000, lr = 0.00156077
I1026 16:52:04.090284 17728 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_110000.caffemodel
I1026 16:52:08.497946 17728 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_110000.solverstate
I1026 16:52:12.335659 17728 solver.cpp:191] Iteration 110000, loss = 2.4043
I1026 16:52:12.336272 17728 solver.cpp:206]     Train net output #0: loss = 2.4043 (* 1 = 2.4043 loss)
I1026 16:52:12.336305 17728 solver.cpp:403] Iteration 110000, lr = 0.00155101
I1026 16:56:13.497934 17728 solver.cpp:191] Iteration 111000, loss = 2.60744
I1026 16:56:13.498558 17728 solver.cpp:206]     Train net output #0: loss = 2.60744 (* 1 = 2.60744 loss)
I1026 16:56:13.498589 17728 solver.cpp:403] Iteration 111000, lr = 0.00154138
I1026 17:00:14.712263 17728 solver.cpp:191] Iteration 112000, loss = 2.54528
I1026 17:00:14.712883 17728 solver.cpp:206]     Train net output #0: loss = 2.54528 (* 1 = 2.54528 loss)
I1026 17:00:14.712898 17728 solver.cpp:403] Iteration 112000, lr = 0.0015319
I1026 17:04:15.915418 17728 solver.cpp:191] Iteration 113000, loss = 2.65994
I1026 17:04:15.916021 17728 solver.cpp:206]     Train net output #0: loss = 2.65994 (* 1 = 2.65994 loss)
I1026 17:04:15.916057 17728 solver.cpp:403] Iteration 113000, lr = 0.00152255
I1026 17:08:17.249203 17728 solver.cpp:191] Iteration 114000, loss = 2.58967
I1026 17:08:17.249728 17728 solver.cpp:206]     Train net output #0: loss = 2.58967 (* 1 = 2.58967 loss)
I1026 17:08:17.249765 17728 solver.cpp:403] Iteration 114000, lr = 0.00151333
I1026 17:12:19.154561 17728 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_115000.caffemodel
I1026 17:12:24.072187 17728 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_115000.solverstate
I1026 17:12:28.240353 17728 solver.cpp:191] Iteration 115000, loss = 2.42596
I1026 17:12:28.240859 17728 solver.cpp:206]     Train net output #0: loss = 2.42596 (* 1 = 2.42596 loss)
I1026 17:12:28.240895 17728 solver.cpp:403] Iteration 115000, lr = 0.00150424
I1026 17:16:29.546883 17728 solver.cpp:191] Iteration 116000, loss = 2.35
I1026 17:16:29.547472 17728 solver.cpp:206]     Train net output #0: loss = 2.35 (* 1 = 2.35 loss)
I1026 17:16:29.547493 17728 solver.cpp:403] Iteration 116000, lr = 0.00149528
I1026 17:20:30.738665 17728 solver.cpp:191] Iteration 117000, loss = 2.34122
I1026 17:20:30.739297 17728 solver.cpp:206]     Train net output #0: loss = 2.34122 (* 1 = 2.34122 loss)
I1026 17:20:30.739337 17728 solver.cpp:403] Iteration 117000, lr = 0.00148644
I1026 17:24:31.816406 17728 solver.cpp:191] Iteration 118000, loss = 2.47413
I1026 17:24:31.816988 17728 solver.cpp:206]     Train net output #0: loss = 2.47413 (* 1 = 2.47413 loss)
I1026 17:24:31.817023 17728 solver.cpp:403] Iteration 118000, lr = 0.00147772
I1026 17:28:33.006209 17728 solver.cpp:191] Iteration 119000, loss = 2.61067
I1026 17:28:33.006736 17728 solver.cpp:206]     Train net output #0: loss = 2.61067 (* 1 = 2.61067 loss)
I1026 17:28:33.006768 17728 solver.cpp:403] Iteration 119000, lr = 0.00146912
I1026 17:32:34.735448 17728 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_120000.caffemodel
I1026 17:32:39.082415 17728 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_120000.solverstate
I1026 17:32:42.850127 17728 solver.cpp:191] Iteration 120000, loss = 2.51655
I1026 17:32:42.850579 17728 solver.cpp:206]     Train net output #0: loss = 2.51655 (* 1 = 2.51655 loss)
I1026 17:32:42.850612 17728 solver.cpp:403] Iteration 120000, lr = 0.00146064
I1026 17:36:44.147750 17728 solver.cpp:191] Iteration 121000, loss = 2.304
I1026 17:36:44.148538 17728 solver.cpp:206]     Train net output #0: loss = 2.304 (* 1 = 2.304 loss)
I1026 17:36:44.148573 17728 solver.cpp:403] Iteration 121000, lr = 0.00145227
I1026 17:40:45.543979 17728 solver.cpp:191] Iteration 122000, loss = 2.38789
I1026 17:40:45.544554 17728 solver.cpp:206]     Train net output #0: loss = 2.38789 (* 1 = 2.38789 loss)
I1026 17:40:45.544587 17728 solver.cpp:403] Iteration 122000, lr = 0.00144401
I1026 17:44:46.893220 17728 solver.cpp:191] Iteration 123000, loss = 2.4501
I1026 17:44:46.894173 17728 solver.cpp:206]     Train net output #0: loss = 2.4501 (* 1 = 2.4501 loss)
I1026 17:44:46.894206 17728 solver.cpp:403] Iteration 123000, lr = 0.00143586
I1026 17:48:48.249804 17728 solver.cpp:191] Iteration 124000, loss = 2.37438
I1026 17:48:48.250363 17728 solver.cpp:206]     Train net output #0: loss = 2.37438 (* 1 = 2.37438 loss)
I1026 17:48:48.250401 17728 solver.cpp:403] Iteration 124000, lr = 0.00142781
I1026 17:52:50.020134 17728 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_125000.caffemodel
I1026 17:52:54.073612 17728 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_125000.solverstate
I1026 17:52:57.514288 17728 solver.cpp:228] Iteration 125000, loss = 2.60615
I1026 17:52:57.514770 17728 solver.cpp:233] Optimization Done.
I1026 17:52:57.514794 17728 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1026 18:15:19.664386 14090 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 18:15:19.664568 14090 net.cpp:358] Input 0 -> data
I1026 18:15:19.664604 14090 net.cpp:67] Creating Layer conv1
I1026 18:15:19.664613 14090 net.cpp:394] conv1 <- data
I1026 18:15:19.664625 14090 net.cpp:356] conv1 -> conv1
I1026 18:15:19.664641 14090 net.cpp:96] Setting up conv1
I1026 18:15:19.665220 14090 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 18:15:19.665253 14090 net.cpp:67] Creating Layer pool1
I1026 18:15:19.665262 14090 net.cpp:394] pool1 <- conv1
I1026 18:15:19.665272 14090 net.cpp:356] pool1 -> pool1
I1026 18:15:19.665285 14090 net.cpp:96] Setting up pool1
I1026 18:15:19.665304 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:15:19.665318 14090 net.cpp:67] Creating Layer relu1
I1026 18:15:19.665324 14090 net.cpp:394] relu1 <- pool1
I1026 18:15:19.665333 14090 net.cpp:345] relu1 -> pool1 (in-place)
I1026 18:15:19.665343 14090 net.cpp:96] Setting up relu1
I1026 18:15:19.665352 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:15:19.665361 14090 net.cpp:67] Creating Layer drop1
I1026 18:15:19.665369 14090 net.cpp:394] drop1 <- pool1
I1026 18:15:19.665381 14090 net.cpp:345] drop1 -> pool1 (in-place)
I1026 18:15:19.665392 14090 net.cpp:96] Setting up drop1
I1026 18:15:19.665401 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:15:19.665413 14090 net.cpp:67] Creating Layer conv2
I1026 18:15:19.665421 14090 net.cpp:394] conv2 <- pool1
I1026 18:15:19.665433 14090 net.cpp:356] conv2 -> conv2
I1026 18:15:19.665446 14090 net.cpp:96] Setting up conv2
I1026 18:15:19.666434 14090 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 18:15:19.666460 14090 net.cpp:67] Creating Layer pool2
I1026 18:15:19.666468 14090 net.cpp:394] pool2 <- conv2
I1026 18:15:19.666478 14090 net.cpp:356] pool2 -> pool2
I1026 18:15:19.666494 14090 net.cpp:96] Setting up pool2
I1026 18:15:19.666504 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:15:19.666514 14090 net.cpp:67] Creating Layer relu2
I1026 18:15:19.666522 14090 net.cpp:394] relu2 <- pool2
I1026 18:15:19.666534 14090 net.cpp:345] relu2 -> pool2 (in-place)
I1026 18:15:19.666544 14090 net.cpp:96] Setting up relu2
I1026 18:15:19.666553 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:15:19.666561 14090 net.cpp:67] Creating Layer drop2
I1026 18:15:19.666568 14090 net.cpp:394] drop2 <- pool2
I1026 18:15:19.666579 14090 net.cpp:345] drop2 -> pool2 (in-place)
I1026 18:15:19.666589 14090 net.cpp:96] Setting up drop2
I1026 18:15:19.666595 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:15:19.666610 14090 net.cpp:67] Creating Layer conv3
I1026 18:15:19.666618 14090 net.cpp:394] conv3 <- pool2
I1026 18:15:19.666628 14090 net.cpp:356] conv3 -> conv3
I1026 18:15:19.666640 14090 net.cpp:96] Setting up conv3
I1026 18:15:19.668964 14090 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 18:15:19.668982 14090 net.cpp:67] Creating Layer pool3
I1026 18:15:19.668987 14090 net.cpp:394] pool3 <- conv3
I1026 18:15:19.668993 14090 net.cpp:356] pool3 -> pool3
I1026 18:15:19.669000 14090 net.cpp:96] Setting up pool3
I1026 18:15:19.669005 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:15:19.669011 14090 net.cpp:67] Creating Layer relu3
I1026 18:15:19.669014 14090 net.cpp:394] relu3 <- pool3
I1026 18:15:19.669020 14090 net.cpp:345] relu3 -> pool3 (in-place)
I1026 18:15:19.669028 14090 net.cpp:96] Setting up relu3
I1026 18:15:19.669031 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:15:19.669036 14090 net.cpp:67] Creating Layer drop3
I1026 18:15:19.669040 14090 net.cpp:394] drop3 <- pool3
I1026 18:15:19.669045 14090 net.cpp:345] drop3 -> pool3 (in-place)
I1026 18:15:19.669051 14090 net.cpp:96] Setting up drop3
I1026 18:15:19.669055 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:15:19.669061 14090 net.cpp:67] Creating Layer ip1
I1026 18:15:19.669065 14090 net.cpp:394] ip1 <- pool3
I1026 18:15:19.669073 14090 net.cpp:356] ip1 -> ip1
I1026 18:15:19.669080 14090 net.cpp:96] Setting up ip1
I1026 18:15:20.143435 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:15:20.143492 14090 net.cpp:67] Creating Layer relu4
I1026 18:15:20.143499 14090 net.cpp:394] relu4 <- ip1
I1026 18:15:20.143510 14090 net.cpp:345] relu4 -> ip1 (in-place)
I1026 18:15:20.143520 14090 net.cpp:96] Setting up relu4
I1026 18:15:20.143525 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:15:20.143532 14090 net.cpp:67] Creating Layer drop4
I1026 18:15:20.143537 14090 net.cpp:394] drop4 <- ip1
I1026 18:15:20.143543 14090 net.cpp:345] drop4 -> ip1 (in-place)
I1026 18:15:20.143548 14090 net.cpp:96] Setting up drop4
I1026 18:15:20.143553 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:15:20.143560 14090 net.cpp:67] Creating Layer ip2
I1026 18:15:20.143564 14090 net.cpp:394] ip2 <- ip1
I1026 18:15:20.143575 14090 net.cpp:356] ip2 -> ip2
I1026 18:15:20.143587 14090 net.cpp:96] Setting up ip2
I1026 18:15:20.152576 14090 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 18:15:20.152642 14090 net.cpp:67] Creating Layer prob
I1026 18:15:20.152648 14090 net.cpp:394] prob <- ip2
I1026 18:15:20.152657 14090 net.cpp:356] prob -> prob
I1026 18:15:20.152667 14090 net.cpp:96] Setting up prob
I1026 18:15:20.152673 14090 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 18:15:20.152676 14090 net.cpp:172] prob does not need backward computation.
I1026 18:15:20.152680 14090 net.cpp:172] ip2 does not need backward computation.
I1026 18:15:20.152684 14090 net.cpp:172] drop4 does not need backward computation.
I1026 18:15:20.152688 14090 net.cpp:172] relu4 does not need backward computation.
I1026 18:15:20.152691 14090 net.cpp:172] ip1 does not need backward computation.
I1026 18:15:20.152695 14090 net.cpp:172] drop3 does not need backward computation.
I1026 18:15:20.152698 14090 net.cpp:172] relu3 does not need backward computation.
I1026 18:15:20.152703 14090 net.cpp:172] pool3 does not need backward computation.
I1026 18:15:20.152715 14090 net.cpp:172] conv3 does not need backward computation.
I1026 18:15:20.152719 14090 net.cpp:172] drop2 does not need backward computation.
I1026 18:15:20.152722 14090 net.cpp:172] relu2 does not need backward computation.
I1026 18:15:20.152726 14090 net.cpp:172] pool2 does not need backward computation.
I1026 18:15:20.152730 14090 net.cpp:172] conv2 does not need backward computation.
I1026 18:15:20.152734 14090 net.cpp:172] drop1 does not need backward computation.
I1026 18:15:20.152737 14090 net.cpp:172] relu1 does not need backward computation.
I1026 18:15:20.152740 14090 net.cpp:172] pool1 does not need backward computation.
I1026 18:15:20.152745 14090 net.cpp:172] conv1 does not need backward computation.
I1026 18:15:20.152747 14090 net.cpp:208] This network produces output prob
I1026 18:15:20.152760 14090 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 18:15:20.152768 14090 net.cpp:219] Network initialization done.
I1026 18:15:20.152772 14090 net.cpp:220] Memory required for data: 1837200
I1026 18:16:03.596482 14090 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 18:16:03.597107 14090 net.cpp:358] Input 0 -> data
I1026 18:16:03.597156 14090 net.cpp:67] Creating Layer conv1
I1026 18:16:03.597168 14090 net.cpp:394] conv1 <- data
I1026 18:16:03.597184 14090 net.cpp:356] conv1 -> conv1
I1026 18:16:03.597215 14090 net.cpp:96] Setting up conv1
I1026 18:16:03.597272 14090 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 18:16:03.597303 14090 net.cpp:67] Creating Layer pool1
I1026 18:16:03.597314 14090 net.cpp:394] pool1 <- conv1
I1026 18:16:03.597327 14090 net.cpp:356] pool1 -> pool1
I1026 18:16:03.597343 14090 net.cpp:96] Setting up pool1
I1026 18:16:03.597358 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:16:03.597373 14090 net.cpp:67] Creating Layer relu1
I1026 18:16:03.597383 14090 net.cpp:394] relu1 <- pool1
I1026 18:16:03.597394 14090 net.cpp:345] relu1 -> pool1 (in-place)
I1026 18:16:03.597407 14090 net.cpp:96] Setting up relu1
I1026 18:16:03.597417 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:16:03.597431 14090 net.cpp:67] Creating Layer drop1
I1026 18:16:03.597440 14090 net.cpp:394] drop1 <- pool1
I1026 18:16:03.597452 14090 net.cpp:345] drop1 -> pool1 (in-place)
I1026 18:16:03.597465 14090 net.cpp:96] Setting up drop1
I1026 18:16:03.597476 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:16:03.597491 14090 net.cpp:67] Creating Layer conv2
I1026 18:16:03.597501 14090 net.cpp:394] conv2 <- pool1
I1026 18:16:03.597514 14090 net.cpp:356] conv2 -> conv2
I1026 18:16:03.597529 14090 net.cpp:96] Setting up conv2
I1026 18:16:03.598667 14090 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 18:16:03.598696 14090 net.cpp:67] Creating Layer pool2
I1026 18:16:03.598707 14090 net.cpp:394] pool2 <- conv2
I1026 18:16:03.598721 14090 net.cpp:356] pool2 -> pool2
I1026 18:16:03.598737 14090 net.cpp:96] Setting up pool2
I1026 18:16:03.598749 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:16:03.598762 14090 net.cpp:67] Creating Layer relu2
I1026 18:16:03.598773 14090 net.cpp:394] relu2 <- pool2
I1026 18:16:03.598784 14090 net.cpp:345] relu2 -> pool2 (in-place)
I1026 18:16:03.598796 14090 net.cpp:96] Setting up relu2
I1026 18:16:03.598806 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:16:03.598819 14090 net.cpp:67] Creating Layer drop2
I1026 18:16:03.598827 14090 net.cpp:394] drop2 <- pool2
I1026 18:16:03.598840 14090 net.cpp:345] drop2 -> pool2 (in-place)
I1026 18:16:03.598862 14090 net.cpp:96] Setting up drop2
I1026 18:16:03.598875 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:16:03.598894 14090 net.cpp:67] Creating Layer conv3
I1026 18:16:03.598906 14090 net.cpp:394] conv3 <- pool2
I1026 18:16:03.598923 14090 net.cpp:356] conv3 -> conv3
I1026 18:16:03.598942 14090 net.cpp:96] Setting up conv3
I1026 18:16:03.602695 14090 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 18:16:03.602738 14090 net.cpp:67] Creating Layer pool3
I1026 18:16:03.602752 14090 net.cpp:394] pool3 <- conv3
I1026 18:16:03.602769 14090 net.cpp:356] pool3 -> pool3
I1026 18:16:03.602788 14090 net.cpp:96] Setting up pool3
I1026 18:16:03.602803 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:16:03.602819 14090 net.cpp:67] Creating Layer relu3
I1026 18:16:03.602830 14090 net.cpp:394] relu3 <- pool3
I1026 18:16:03.602845 14090 net.cpp:345] relu3 -> pool3 (in-place)
I1026 18:16:03.602861 14090 net.cpp:96] Setting up relu3
I1026 18:16:03.602874 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:16:03.602888 14090 net.cpp:67] Creating Layer drop3
I1026 18:16:03.602900 14090 net.cpp:394] drop3 <- pool3
I1026 18:16:03.602916 14090 net.cpp:345] drop3 -> pool3 (in-place)
I1026 18:16:03.602931 14090 net.cpp:96] Setting up drop3
I1026 18:16:03.602944 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:16:03.602962 14090 net.cpp:67] Creating Layer ip1
I1026 18:16:03.602973 14090 net.cpp:394] ip1 <- pool3
I1026 18:16:03.602989 14090 net.cpp:356] ip1 -> ip1
I1026 18:16:03.603008 14090 net.cpp:96] Setting up ip1
I1026 18:16:04.027781 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:16:04.027845 14090 net.cpp:67] Creating Layer relu4
I1026 18:16:04.027853 14090 net.cpp:394] relu4 <- ip1
I1026 18:16:04.027864 14090 net.cpp:345] relu4 -> ip1 (in-place)
I1026 18:16:04.027873 14090 net.cpp:96] Setting up relu4
I1026 18:16:04.027879 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:16:04.027899 14090 net.cpp:67] Creating Layer drop4
I1026 18:16:04.027904 14090 net.cpp:394] drop4 <- ip1
I1026 18:16:04.027910 14090 net.cpp:345] drop4 -> ip1 (in-place)
I1026 18:16:04.027917 14090 net.cpp:96] Setting up drop4
I1026 18:16:04.027923 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:16:04.027933 14090 net.cpp:67] Creating Layer ip2
I1026 18:16:04.027937 14090 net.cpp:394] ip2 <- ip1
I1026 18:16:04.027945 14090 net.cpp:356] ip2 -> ip2
I1026 18:16:04.027958 14090 net.cpp:96] Setting up ip2
I1026 18:16:04.035575 14090 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 18:16:04.035639 14090 net.cpp:67] Creating Layer prob
I1026 18:16:04.035646 14090 net.cpp:394] prob <- ip2
I1026 18:16:04.035655 14090 net.cpp:356] prob -> prob
I1026 18:16:04.035665 14090 net.cpp:96] Setting up prob
I1026 18:16:04.035672 14090 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 18:16:04.035676 14090 net.cpp:172] prob does not need backward computation.
I1026 18:16:04.035681 14090 net.cpp:172] ip2 does not need backward computation.
I1026 18:16:04.035686 14090 net.cpp:172] drop4 does not need backward computation.
I1026 18:16:04.035689 14090 net.cpp:172] relu4 does not need backward computation.
I1026 18:16:04.035693 14090 net.cpp:172] ip1 does not need backward computation.
I1026 18:16:04.035697 14090 net.cpp:172] drop3 does not need backward computation.
I1026 18:16:04.035701 14090 net.cpp:172] relu3 does not need backward computation.
I1026 18:16:04.035706 14090 net.cpp:172] pool3 does not need backward computation.
I1026 18:16:04.035711 14090 net.cpp:172] conv3 does not need backward computation.
I1026 18:16:04.035714 14090 net.cpp:172] drop2 does not need backward computation.
I1026 18:16:04.035718 14090 net.cpp:172] relu2 does not need backward computation.
I1026 18:16:04.035722 14090 net.cpp:172] pool2 does not need backward computation.
I1026 18:16:04.035727 14090 net.cpp:172] conv2 does not need backward computation.
I1026 18:16:04.035730 14090 net.cpp:172] drop1 does not need backward computation.
I1026 18:16:04.035734 14090 net.cpp:172] relu1 does not need backward computation.
I1026 18:16:04.035738 14090 net.cpp:172] pool1 does not need backward computation.
I1026 18:16:04.035742 14090 net.cpp:172] conv1 does not need backward computation.
I1026 18:16:04.035747 14090 net.cpp:208] This network produces output prob
I1026 18:16:04.035763 14090 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 18:16:04.035773 14090 net.cpp:219] Network initialization done.
I1026 18:16:04.035776 14090 net.cpp:220] Memory required for data: 1837200
I1026 18:16:41.516629 14090 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 18:16:41.517132 14090 net.cpp:358] Input 0 -> data
I1026 18:16:41.517163 14090 net.cpp:67] Creating Layer conv1
I1026 18:16:41.517168 14090 net.cpp:394] conv1 <- data
I1026 18:16:41.517175 14090 net.cpp:356] conv1 -> conv1
I1026 18:16:41.517185 14090 net.cpp:96] Setting up conv1
I1026 18:16:41.517215 14090 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 18:16:41.517232 14090 net.cpp:67] Creating Layer pool1
I1026 18:16:41.517237 14090 net.cpp:394] pool1 <- conv1
I1026 18:16:41.517242 14090 net.cpp:356] pool1 -> pool1
I1026 18:16:41.517251 14090 net.cpp:96] Setting up pool1
I1026 18:16:41.517257 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:16:41.517264 14090 net.cpp:67] Creating Layer relu1
I1026 18:16:41.517268 14090 net.cpp:394] relu1 <- pool1
I1026 18:16:41.517273 14090 net.cpp:345] relu1 -> pool1 (in-place)
I1026 18:16:41.517279 14090 net.cpp:96] Setting up relu1
I1026 18:16:41.517283 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:16:41.517289 14090 net.cpp:67] Creating Layer drop1
I1026 18:16:41.517293 14090 net.cpp:394] drop1 <- pool1
I1026 18:16:41.517298 14090 net.cpp:345] drop1 -> pool1 (in-place)
I1026 18:16:41.517304 14090 net.cpp:96] Setting up drop1
I1026 18:16:41.517309 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:16:41.517316 14090 net.cpp:67] Creating Layer conv2
I1026 18:16:41.517320 14090 net.cpp:394] conv2 <- pool1
I1026 18:16:41.517326 14090 net.cpp:356] conv2 -> conv2
I1026 18:16:41.517333 14090 net.cpp:96] Setting up conv2
I1026 18:16:41.517834 14090 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 18:16:41.517849 14090 net.cpp:67] Creating Layer pool2
I1026 18:16:41.517853 14090 net.cpp:394] pool2 <- conv2
I1026 18:16:41.517859 14090 net.cpp:356] pool2 -> pool2
I1026 18:16:41.517866 14090 net.cpp:96] Setting up pool2
I1026 18:16:41.517873 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:16:41.517879 14090 net.cpp:67] Creating Layer relu2
I1026 18:16:41.517882 14090 net.cpp:394] relu2 <- pool2
I1026 18:16:41.517887 14090 net.cpp:345] relu2 -> pool2 (in-place)
I1026 18:16:41.517892 14090 net.cpp:96] Setting up relu2
I1026 18:16:41.517896 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:16:41.517901 14090 net.cpp:67] Creating Layer drop2
I1026 18:16:41.517905 14090 net.cpp:394] drop2 <- pool2
I1026 18:16:41.517910 14090 net.cpp:345] drop2 -> pool2 (in-place)
I1026 18:16:41.517916 14090 net.cpp:96] Setting up drop2
I1026 18:16:41.517920 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:16:41.517927 14090 net.cpp:67] Creating Layer conv3
I1026 18:16:41.517931 14090 net.cpp:394] conv3 <- pool2
I1026 18:16:41.517937 14090 net.cpp:356] conv3 -> conv3
I1026 18:16:41.517945 14090 net.cpp:96] Setting up conv3
I1026 18:16:41.519276 14090 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 18:16:41.519291 14090 net.cpp:67] Creating Layer pool3
I1026 18:16:41.519296 14090 net.cpp:394] pool3 <- conv3
I1026 18:16:41.519302 14090 net.cpp:356] pool3 -> pool3
I1026 18:16:41.519309 14090 net.cpp:96] Setting up pool3
I1026 18:16:41.519315 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:16:41.519320 14090 net.cpp:67] Creating Layer relu3
I1026 18:16:41.519323 14090 net.cpp:394] relu3 <- pool3
I1026 18:16:41.519330 14090 net.cpp:345] relu3 -> pool3 (in-place)
I1026 18:16:41.519335 14090 net.cpp:96] Setting up relu3
I1026 18:16:41.519338 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:16:41.519345 14090 net.cpp:67] Creating Layer drop3
I1026 18:16:41.519348 14090 net.cpp:394] drop3 <- pool3
I1026 18:16:41.519353 14090 net.cpp:345] drop3 -> pool3 (in-place)
I1026 18:16:41.519359 14090 net.cpp:96] Setting up drop3
I1026 18:16:41.519363 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:16:41.519369 14090 net.cpp:67] Creating Layer ip1
I1026 18:16:41.519373 14090 net.cpp:394] ip1 <- pool3
I1026 18:16:41.519379 14090 net.cpp:356] ip1 -> ip1
I1026 18:16:41.519387 14090 net.cpp:96] Setting up ip1
I1026 18:16:41.933370 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:16:41.933435 14090 net.cpp:67] Creating Layer relu4
I1026 18:16:41.933444 14090 net.cpp:394] relu4 <- ip1
I1026 18:16:41.933454 14090 net.cpp:345] relu4 -> ip1 (in-place)
I1026 18:16:41.933465 14090 net.cpp:96] Setting up relu4
I1026 18:16:41.933470 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:16:41.933478 14090 net.cpp:67] Creating Layer drop4
I1026 18:16:41.933482 14090 net.cpp:394] drop4 <- ip1
I1026 18:16:41.933490 14090 net.cpp:345] drop4 -> ip1 (in-place)
I1026 18:16:41.933496 14090 net.cpp:96] Setting up drop4
I1026 18:16:41.933503 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:16:41.933513 14090 net.cpp:67] Creating Layer ip2
I1026 18:16:41.933517 14090 net.cpp:394] ip2 <- ip1
I1026 18:16:41.933526 14090 net.cpp:356] ip2 -> ip2
I1026 18:16:41.933539 14090 net.cpp:96] Setting up ip2
I1026 18:16:41.941166 14090 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 18:16:41.941234 14090 net.cpp:67] Creating Layer prob
I1026 18:16:41.941243 14090 net.cpp:394] prob <- ip2
I1026 18:16:41.941253 14090 net.cpp:356] prob -> prob
I1026 18:16:41.941265 14090 net.cpp:96] Setting up prob
I1026 18:16:41.941273 14090 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 18:16:41.941278 14090 net.cpp:172] prob does not need backward computation.
I1026 18:16:41.941282 14090 net.cpp:172] ip2 does not need backward computation.
I1026 18:16:41.941287 14090 net.cpp:172] drop4 does not need backward computation.
I1026 18:16:41.941290 14090 net.cpp:172] relu4 does not need backward computation.
I1026 18:16:41.941294 14090 net.cpp:172] ip1 does not need backward computation.
I1026 18:16:41.941298 14090 net.cpp:172] drop3 does not need backward computation.
I1026 18:16:41.941303 14090 net.cpp:172] relu3 does not need backward computation.
I1026 18:16:41.941306 14090 net.cpp:172] pool3 does not need backward computation.
I1026 18:16:41.941310 14090 net.cpp:172] conv3 does not need backward computation.
I1026 18:16:41.941314 14090 net.cpp:172] drop2 does not need backward computation.
I1026 18:16:41.941318 14090 net.cpp:172] relu2 does not need backward computation.
I1026 18:16:41.941323 14090 net.cpp:172] pool2 does not need backward computation.
I1026 18:16:41.941328 14090 net.cpp:172] conv2 does not need backward computation.
I1026 18:16:41.941331 14090 net.cpp:172] drop1 does not need backward computation.
I1026 18:16:41.941335 14090 net.cpp:172] relu1 does not need backward computation.
I1026 18:16:41.941339 14090 net.cpp:172] pool1 does not need backward computation.
I1026 18:16:41.941342 14090 net.cpp:172] conv1 does not need backward computation.
I1026 18:16:41.941346 14090 net.cpp:208] This network produces output prob
I1026 18:16:41.941361 14090 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 18:16:41.941371 14090 net.cpp:219] Network initialization done.
I1026 18:16:41.941385 14090 net.cpp:220] Memory required for data: 1837200
I1026 18:17:21.698356 14090 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 18:17:21.699034 14090 net.cpp:358] Input 0 -> data
I1026 18:17:21.699089 14090 net.cpp:67] Creating Layer conv1
I1026 18:17:21.699103 14090 net.cpp:394] conv1 <- data
I1026 18:17:21.699122 14090 net.cpp:356] conv1 -> conv1
I1026 18:17:21.699146 14090 net.cpp:96] Setting up conv1
I1026 18:17:21.699213 14090 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 18:17:21.699249 14090 net.cpp:67] Creating Layer pool1
I1026 18:17:21.699264 14090 net.cpp:394] pool1 <- conv1
I1026 18:17:21.699280 14090 net.cpp:356] pool1 -> pool1
I1026 18:17:21.699300 14090 net.cpp:96] Setting up pool1
I1026 18:17:21.699317 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:17:21.699334 14090 net.cpp:67] Creating Layer relu1
I1026 18:17:21.699347 14090 net.cpp:394] relu1 <- pool1
I1026 18:17:21.699362 14090 net.cpp:345] relu1 -> pool1 (in-place)
I1026 18:17:21.699378 14090 net.cpp:96] Setting up relu1
I1026 18:17:21.699390 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:17:21.699405 14090 net.cpp:67] Creating Layer drop1
I1026 18:17:21.699416 14090 net.cpp:394] drop1 <- pool1
I1026 18:17:21.699431 14090 net.cpp:345] drop1 -> pool1 (in-place)
I1026 18:17:21.699447 14090 net.cpp:96] Setting up drop1
I1026 18:17:21.699473 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:17:21.699493 14090 net.cpp:67] Creating Layer conv2
I1026 18:17:21.699506 14090 net.cpp:394] conv2 <- pool1
I1026 18:17:21.699522 14090 net.cpp:356] conv2 -> conv2
I1026 18:17:21.699542 14090 net.cpp:96] Setting up conv2
I1026 18:17:21.700973 14090 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 18:17:21.701014 14090 net.cpp:67] Creating Layer pool2
I1026 18:17:21.701027 14090 net.cpp:394] pool2 <- conv2
I1026 18:17:21.701043 14090 net.cpp:356] pool2 -> pool2
I1026 18:17:21.701063 14090 net.cpp:96] Setting up pool2
I1026 18:17:21.701079 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:17:21.701095 14090 net.cpp:67] Creating Layer relu2
I1026 18:17:21.701107 14090 net.cpp:394] relu2 <- pool2
I1026 18:17:21.701122 14090 net.cpp:345] relu2 -> pool2 (in-place)
I1026 18:17:21.701136 14090 net.cpp:96] Setting up relu2
I1026 18:17:21.701148 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:17:21.701164 14090 net.cpp:67] Creating Layer drop2
I1026 18:17:21.701175 14090 net.cpp:394] drop2 <- pool2
I1026 18:17:21.701190 14090 net.cpp:345] drop2 -> pool2 (in-place)
I1026 18:17:21.701206 14090 net.cpp:96] Setting up drop2
I1026 18:17:21.701220 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:17:21.701238 14090 net.cpp:67] Creating Layer conv3
I1026 18:17:21.701251 14090 net.cpp:394] conv3 <- pool2
I1026 18:17:21.701267 14090 net.cpp:356] conv3 -> conv3
I1026 18:17:21.701285 14090 net.cpp:96] Setting up conv3
I1026 18:17:21.704968 14090 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 18:17:21.705011 14090 net.cpp:67] Creating Layer pool3
I1026 18:17:21.705025 14090 net.cpp:394] pool3 <- conv3
I1026 18:17:21.705042 14090 net.cpp:356] pool3 -> pool3
I1026 18:17:21.705060 14090 net.cpp:96] Setting up pool3
I1026 18:17:21.705075 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:17:21.705091 14090 net.cpp:67] Creating Layer relu3
I1026 18:17:21.705102 14090 net.cpp:394] relu3 <- pool3
I1026 18:17:21.705117 14090 net.cpp:345] relu3 -> pool3 (in-place)
I1026 18:17:21.705133 14090 net.cpp:96] Setting up relu3
I1026 18:17:21.705145 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:17:21.705160 14090 net.cpp:67] Creating Layer drop3
I1026 18:17:21.705171 14090 net.cpp:394] drop3 <- pool3
I1026 18:17:21.705186 14090 net.cpp:345] drop3 -> pool3 (in-place)
I1026 18:17:21.705202 14090 net.cpp:96] Setting up drop3
I1026 18:17:21.705215 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:17:21.705232 14090 net.cpp:67] Creating Layer ip1
I1026 18:17:21.705243 14090 net.cpp:394] ip1 <- pool3
I1026 18:17:21.705260 14090 net.cpp:356] ip1 -> ip1
I1026 18:17:21.705279 14090 net.cpp:96] Setting up ip1
I1026 18:17:22.115598 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:17:22.115665 14090 net.cpp:67] Creating Layer relu4
I1026 18:17:22.115674 14090 net.cpp:394] relu4 <- ip1
I1026 18:17:22.115684 14090 net.cpp:345] relu4 -> ip1 (in-place)
I1026 18:17:22.115694 14090 net.cpp:96] Setting up relu4
I1026 18:17:22.115699 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:17:22.115706 14090 net.cpp:67] Creating Layer drop4
I1026 18:17:22.115711 14090 net.cpp:394] drop4 <- ip1
I1026 18:17:22.115718 14090 net.cpp:345] drop4 -> ip1 (in-place)
I1026 18:17:22.115726 14090 net.cpp:96] Setting up drop4
I1026 18:17:22.115732 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:17:22.115742 14090 net.cpp:67] Creating Layer ip2
I1026 18:17:22.115746 14090 net.cpp:394] ip2 <- ip1
I1026 18:17:22.115754 14090 net.cpp:356] ip2 -> ip2
I1026 18:17:22.115768 14090 net.cpp:96] Setting up ip2
I1026 18:17:22.123358 14090 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 18:17:22.123421 14090 net.cpp:67] Creating Layer prob
I1026 18:17:22.123430 14090 net.cpp:394] prob <- ip2
I1026 18:17:22.123440 14090 net.cpp:356] prob -> prob
I1026 18:17:22.123450 14090 net.cpp:96] Setting up prob
I1026 18:17:22.123458 14090 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 18:17:22.123463 14090 net.cpp:172] prob does not need backward computation.
I1026 18:17:22.123479 14090 net.cpp:172] ip2 does not need backward computation.
I1026 18:17:22.123484 14090 net.cpp:172] drop4 does not need backward computation.
I1026 18:17:22.123488 14090 net.cpp:172] relu4 does not need backward computation.
I1026 18:17:22.123493 14090 net.cpp:172] ip1 does not need backward computation.
I1026 18:17:22.123497 14090 net.cpp:172] drop3 does not need backward computation.
I1026 18:17:22.123502 14090 net.cpp:172] relu3 does not need backward computation.
I1026 18:17:22.123505 14090 net.cpp:172] pool3 does not need backward computation.
I1026 18:17:22.123509 14090 net.cpp:172] conv3 does not need backward computation.
I1026 18:17:22.123513 14090 net.cpp:172] drop2 does not need backward computation.
I1026 18:17:22.123517 14090 net.cpp:172] relu2 does not need backward computation.
I1026 18:17:22.123522 14090 net.cpp:172] pool2 does not need backward computation.
I1026 18:17:22.123527 14090 net.cpp:172] conv2 does not need backward computation.
I1026 18:17:22.123530 14090 net.cpp:172] drop1 does not need backward computation.
I1026 18:17:22.123534 14090 net.cpp:172] relu1 does not need backward computation.
I1026 18:17:22.123538 14090 net.cpp:172] pool1 does not need backward computation.
I1026 18:17:22.123543 14090 net.cpp:172] conv1 does not need backward computation.
I1026 18:17:22.123546 14090 net.cpp:208] This network produces output prob
I1026 18:17:22.123563 14090 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 18:17:22.123572 14090 net.cpp:219] Network initialization done.
I1026 18:17:22.123576 14090 net.cpp:220] Memory required for data: 1837200
I1026 18:17:59.421789 14090 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 18:17:59.422421 14090 net.cpp:358] Input 0 -> data
I1026 18:17:59.422476 14090 net.cpp:67] Creating Layer conv1
I1026 18:17:59.422492 14090 net.cpp:394] conv1 <- data
I1026 18:17:59.422510 14090 net.cpp:356] conv1 -> conv1
I1026 18:17:59.422535 14090 net.cpp:96] Setting up conv1
I1026 18:17:59.422600 14090 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 18:17:59.422636 14090 net.cpp:67] Creating Layer pool1
I1026 18:17:59.422649 14090 net.cpp:394] pool1 <- conv1
I1026 18:17:59.422667 14090 net.cpp:356] pool1 -> pool1
I1026 18:17:59.422687 14090 net.cpp:96] Setting up pool1
I1026 18:17:59.422703 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:17:59.422721 14090 net.cpp:67] Creating Layer relu1
I1026 18:17:59.422734 14090 net.cpp:394] relu1 <- pool1
I1026 18:17:59.422747 14090 net.cpp:345] relu1 -> pool1 (in-place)
I1026 18:17:59.422763 14090 net.cpp:96] Setting up relu1
I1026 18:17:59.422776 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:17:59.422792 14090 net.cpp:67] Creating Layer drop1
I1026 18:17:59.422803 14090 net.cpp:394] drop1 <- pool1
I1026 18:17:59.422818 14090 net.cpp:345] drop1 -> pool1 (in-place)
I1026 18:17:59.422834 14090 net.cpp:96] Setting up drop1
I1026 18:17:59.422847 14090 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 18:17:59.422866 14090 net.cpp:67] Creating Layer conv2
I1026 18:17:59.422878 14090 net.cpp:394] conv2 <- pool1
I1026 18:17:59.422894 14090 net.cpp:356] conv2 -> conv2
I1026 18:17:59.422914 14090 net.cpp:96] Setting up conv2
I1026 18:17:59.424299 14090 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 18:17:59.424332 14090 net.cpp:67] Creating Layer pool2
I1026 18:17:59.424346 14090 net.cpp:394] pool2 <- conv2
I1026 18:17:59.424361 14090 net.cpp:356] pool2 -> pool2
I1026 18:17:59.424381 14090 net.cpp:96] Setting up pool2
I1026 18:17:59.424396 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:17:59.424412 14090 net.cpp:67] Creating Layer relu2
I1026 18:17:59.424474 14090 net.cpp:394] relu2 <- pool2
I1026 18:17:59.424502 14090 net.cpp:345] relu2 -> pool2 (in-place)
I1026 18:17:59.424520 14090 net.cpp:96] Setting up relu2
I1026 18:17:59.424533 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:17:59.424549 14090 net.cpp:67] Creating Layer drop2
I1026 18:17:59.424561 14090 net.cpp:394] drop2 <- pool2
I1026 18:17:59.424576 14090 net.cpp:345] drop2 -> pool2 (in-place)
I1026 18:17:59.424592 14090 net.cpp:96] Setting up drop2
I1026 18:17:59.424605 14090 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 18:17:59.424625 14090 net.cpp:67] Creating Layer conv3
I1026 18:17:59.424638 14090 net.cpp:394] conv3 <- pool2
I1026 18:17:59.424655 14090 net.cpp:356] conv3 -> conv3
I1026 18:17:59.424674 14090 net.cpp:96] Setting up conv3
I1026 18:17:59.428303 14090 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 18:17:59.428340 14090 net.cpp:67] Creating Layer pool3
I1026 18:17:59.428354 14090 net.cpp:394] pool3 <- conv3
I1026 18:17:59.428371 14090 net.cpp:356] pool3 -> pool3
I1026 18:17:59.428390 14090 net.cpp:96] Setting up pool3
I1026 18:17:59.428403 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:17:59.428436 14090 net.cpp:67] Creating Layer relu3
I1026 18:17:59.428495 14090 net.cpp:394] relu3 <- pool3
I1026 18:17:59.428522 14090 net.cpp:345] relu3 -> pool3 (in-place)
I1026 18:17:59.428540 14090 net.cpp:96] Setting up relu3
I1026 18:17:59.428552 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:17:59.428568 14090 net.cpp:67] Creating Layer drop3
I1026 18:17:59.428580 14090 net.cpp:394] drop3 <- pool3
I1026 18:17:59.428596 14090 net.cpp:345] drop3 -> pool3 (in-place)
I1026 18:17:59.428612 14090 net.cpp:96] Setting up drop3
I1026 18:17:59.428624 14090 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 18:17:59.428642 14090 net.cpp:67] Creating Layer ip1
I1026 18:17:59.428661 14090 net.cpp:394] ip1 <- pool3
I1026 18:17:59.428680 14090 net.cpp:356] ip1 -> ip1
I1026 18:17:59.428700 14090 net.cpp:96] Setting up ip1
I1026 18:17:59.843894 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:17:59.843961 14090 net.cpp:67] Creating Layer relu4
I1026 18:17:59.843969 14090 net.cpp:394] relu4 <- ip1
I1026 18:17:59.843981 14090 net.cpp:345] relu4 -> ip1 (in-place)
I1026 18:17:59.843991 14090 net.cpp:96] Setting up relu4
I1026 18:17:59.843996 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:17:59.844004 14090 net.cpp:67] Creating Layer drop4
I1026 18:17:59.844009 14090 net.cpp:394] drop4 <- ip1
I1026 18:17:59.844017 14090 net.cpp:345] drop4 -> ip1 (in-place)
I1026 18:17:59.844023 14090 net.cpp:96] Setting up drop4
I1026 18:17:59.844029 14090 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 18:17:59.844040 14090 net.cpp:67] Creating Layer ip2
I1026 18:17:59.844045 14090 net.cpp:394] ip2 <- ip1
I1026 18:17:59.844053 14090 net.cpp:356] ip2 -> ip2
I1026 18:17:59.844069 14090 net.cpp:96] Setting up ip2
I1026 18:17:59.851608 14090 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 18:17:59.851670 14090 net.cpp:67] Creating Layer prob
I1026 18:17:59.851677 14090 net.cpp:394] prob <- ip2
I1026 18:17:59.851686 14090 net.cpp:356] prob -> prob
I1026 18:17:59.851697 14090 net.cpp:96] Setting up prob
I1026 18:17:59.851706 14090 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 18:17:59.851711 14090 net.cpp:172] prob does not need backward computation.
I1026 18:17:59.851714 14090 net.cpp:172] ip2 does not need backward computation.
I1026 18:17:59.851718 14090 net.cpp:172] drop4 does not need backward computation.
I1026 18:17:59.851723 14090 net.cpp:172] relu4 does not need backward computation.
I1026 18:17:59.851727 14090 net.cpp:172] ip1 does not need backward computation.
I1026 18:17:59.851732 14090 net.cpp:172] drop3 does not need backward computation.
I1026 18:17:59.851735 14090 net.cpp:172] relu3 does not need backward computation.
I1026 18:17:59.851739 14090 net.cpp:172] pool3 does not need backward computation.
I1026 18:17:59.851743 14090 net.cpp:172] conv3 does not need backward computation.
I1026 18:17:59.851747 14090 net.cpp:172] drop2 does not need backward computation.
I1026 18:17:59.851752 14090 net.cpp:172] relu2 does not need backward computation.
I1026 18:17:59.851757 14090 net.cpp:172] pool2 does not need backward computation.
I1026 18:17:59.851760 14090 net.cpp:172] conv2 does not need backward computation.
I1026 18:17:59.851764 14090 net.cpp:172] drop1 does not need backward computation.
I1026 18:17:59.851768 14090 net.cpp:172] relu1 does not need backward computation.
I1026 18:17:59.851773 14090 net.cpp:172] pool1 does not need backward computation.
I1026 18:17:59.851776 14090 net.cpp:172] conv1 does not need backward computation.
I1026 18:17:59.851780 14090 net.cpp:208] This network produces output prob
I1026 18:17:59.851796 14090 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 18:17:59.851806 14090 net.cpp:219] Network initialization done.
I1026 18:17:59.851811 14090 net.cpp:220] Memory required for data: 1837200
I1026 18:50:50.227735 20754 convert_imageset.cpp:70] Shuffling data
I1026 18:50:50.815485 20754 convert_imageset.cpp:73] A total of 60000 images.
I1026 18:50:50.815562 20754 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1026 18:50:52.833663 20754 convert_imageset.cpp:177] Processed 1000 files.
E1026 18:50:54.773180 20754 convert_imageset.cpp:177] Processed 2000 files.
E1026 18:50:56.662374 20754 convert_imageset.cpp:177] Processed 3000 files.
E1026 18:50:58.590061 20754 convert_imageset.cpp:177] Processed 4000 files.
E1026 18:51:00.412489 20754 convert_imageset.cpp:177] Processed 5000 files.
E1026 18:51:02.206877 20754 convert_imageset.cpp:177] Processed 6000 files.
E1026 18:51:03.919185 20754 convert_imageset.cpp:177] Processed 7000 files.
E1026 18:51:05.610187 20754 convert_imageset.cpp:177] Processed 8000 files.
E1026 18:51:07.349557 20754 convert_imageset.cpp:177] Processed 9000 files.
E1026 18:51:09.113513 20754 convert_imageset.cpp:177] Processed 10000 files.
E1026 18:51:10.917961 20754 convert_imageset.cpp:177] Processed 11000 files.
E1026 18:51:12.646006 20754 convert_imageset.cpp:177] Processed 12000 files.
E1026 18:51:14.411514 20754 convert_imageset.cpp:177] Processed 13000 files.
E1026 18:51:16.038877 20754 convert_imageset.cpp:177] Processed 14000 files.
E1026 18:51:17.830135 20754 convert_imageset.cpp:177] Processed 15000 files.
E1026 18:51:19.484006 20754 convert_imageset.cpp:177] Processed 16000 files.
E1026 18:51:21.146026 20754 convert_imageset.cpp:177] Processed 17000 files.
E1026 18:51:22.862260 20754 convert_imageset.cpp:177] Processed 18000 files.
E1026 18:51:24.421696 20754 convert_imageset.cpp:177] Processed 19000 files.
E1026 18:51:26.025012 20754 convert_imageset.cpp:177] Processed 20000 files.
E1026 18:51:27.685497 20754 convert_imageset.cpp:177] Processed 21000 files.
E1026 18:51:29.277277 20754 convert_imageset.cpp:177] Processed 22000 files.
E1026 18:51:30.890322 20754 convert_imageset.cpp:177] Processed 23000 files.
E1026 18:51:32.589630 20754 convert_imageset.cpp:177] Processed 24000 files.
E1026 18:51:34.150112 20754 convert_imageset.cpp:177] Processed 25000 files.
E1026 18:51:35.825043 20754 convert_imageset.cpp:177] Processed 26000 files.
E1026 18:51:37.386442 20754 convert_imageset.cpp:177] Processed 27000 files.
E1026 18:51:38.925073 20754 convert_imageset.cpp:177] Processed 28000 files.
E1026 18:51:40.518164 20754 convert_imageset.cpp:177] Processed 29000 files.
E1026 18:51:42.038275 20754 convert_imageset.cpp:177] Processed 30000 files.
E1026 18:51:43.596802 20754 convert_imageset.cpp:177] Processed 31000 files.
E1026 18:51:45.148494 20754 convert_imageset.cpp:177] Processed 32000 files.
E1026 18:51:46.750533 20754 convert_imageset.cpp:177] Processed 33000 files.
E1026 18:51:48.328577 20754 convert_imageset.cpp:177] Processed 34000 files.
E1026 18:51:49.987987 20754 convert_imageset.cpp:177] Processed 35000 files.
E1026 18:51:51.527273 20754 convert_imageset.cpp:177] Processed 36000 files.
E1026 18:51:53.062860 20754 convert_imageset.cpp:177] Processed 37000 files.
E1026 18:51:54.551302 20754 convert_imageset.cpp:177] Processed 38000 files.
E1026 18:51:56.017513 20754 convert_imageset.cpp:177] Processed 39000 files.
E1026 18:51:57.513185 20754 convert_imageset.cpp:177] Processed 40000 files.
E1026 18:51:59.042485 20754 convert_imageset.cpp:177] Processed 41000 files.
E1026 18:52:00.526857 20754 convert_imageset.cpp:177] Processed 42000 files.
E1026 18:52:02.189317 20754 convert_imageset.cpp:177] Processed 43000 files.
E1026 18:52:03.721539 20754 convert_imageset.cpp:177] Processed 44000 files.
E1026 18:52:05.248738 20754 convert_imageset.cpp:177] Processed 45000 files.
E1026 18:52:06.740859 20754 convert_imageset.cpp:177] Processed 46000 files.
E1026 18:52:08.248261 20754 convert_imageset.cpp:177] Processed 47000 files.
E1026 18:52:09.733664 20754 convert_imageset.cpp:177] Processed 48000 files.
E1026 18:52:11.167793 20754 convert_imageset.cpp:177] Processed 49000 files.
E1026 18:52:12.657938 20754 convert_imageset.cpp:177] Processed 50000 files.
E1026 18:52:14.115128 20754 convert_imageset.cpp:177] Processed 51000 files.
E1026 18:52:15.576958 20754 convert_imageset.cpp:177] Processed 52000 files.
E1026 18:52:17.143189 20754 convert_imageset.cpp:177] Processed 53000 files.
E1026 18:52:18.612480 20754 convert_imageset.cpp:177] Processed 54000 files.
E1026 18:52:20.109194 20754 convert_imageset.cpp:177] Processed 55000 files.
E1026 18:52:21.587718 20754 convert_imageset.cpp:177] Processed 56000 files.
E1026 18:52:23.046406 20754 convert_imageset.cpp:177] Processed 57000 files.
E1026 18:52:24.432117 20754 convert_imageset.cpp:177] Processed 58000 files.
E1026 18:52:25.853924 20754 convert_imageset.cpp:177] Processed 59000 files.
E1026 18:52:27.275701 20754 convert_imageset.cpp:177] Processed 60000 files.
I1026 18:52:27.493718 20808 caffe.cpp:99] Use GPU with device ID 0
I1026 18:52:27.832373 20808 caffe.cpp:107] Starting Optimization
I1026 18:52:27.832515 20808 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 150000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1026 18:52:27.832541 20808 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1026 18:52:27.844388 20808 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1026 18:52:27.844493 20808 net.cpp:67] Creating Layer mnist
I1026 18:52:27.844506 20808 net.cpp:356] mnist -> data
I1026 18:52:27.844522 20808 net.cpp:356] mnist -> label
I1026 18:52:27.844535 20808 net.cpp:96] Setting up mnist
I1026 18:52:27.851449 20808 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1026 18:52:27.851577 20808 data_layer.cpp:128] output data size: 64,1,50,180
I1026 18:52:27.853262 20808 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1026 18:52:27.853299 20808 net.cpp:103] Top shape: 64 1 1 1 (64)
I1026 18:52:27.853329 20808 net.cpp:67] Creating Layer conv1
I1026 18:52:27.853343 20808 net.cpp:394] conv1 <- data
I1026 18:52:27.853381 20808 net.cpp:356] conv1 -> conv1
I1026 18:52:27.853406 20808 net.cpp:96] Setting up conv1
I1026 18:52:27.853768 20808 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1026 18:52:27.853799 20808 net.cpp:67] Creating Layer pool1
I1026 18:52:27.853806 20808 net.cpp:394] pool1 <- conv1
I1026 18:52:27.853813 20808 net.cpp:356] pool1 -> pool1
I1026 18:52:27.853821 20808 net.cpp:96] Setting up pool1
I1026 18:52:27.853837 20808 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 18:52:27.853844 20808 net.cpp:67] Creating Layer relu1
I1026 18:52:27.853849 20808 net.cpp:394] relu1 <- pool1
I1026 18:52:27.853857 20808 net.cpp:345] relu1 -> pool1 (in-place)
I1026 18:52:27.853863 20808 net.cpp:96] Setting up relu1
I1026 18:52:27.853868 20808 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 18:52:27.853875 20808 net.cpp:67] Creating Layer drop1
I1026 18:52:27.853880 20808 net.cpp:394] drop1 <- pool1
I1026 18:52:27.853885 20808 net.cpp:345] drop1 -> pool1 (in-place)
I1026 18:52:27.853891 20808 net.cpp:96] Setting up drop1
I1026 18:52:27.853898 20808 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 18:52:27.853904 20808 net.cpp:67] Creating Layer conv2
I1026 18:52:27.853909 20808 net.cpp:394] conv2 <- pool1
I1026 18:52:27.853917 20808 net.cpp:356] conv2 -> conv2
I1026 18:52:27.853925 20808 net.cpp:96] Setting up conv2
I1026 18:52:27.854504 20808 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1026 18:52:27.854522 20808 net.cpp:67] Creating Layer pool2
I1026 18:52:27.854527 20808 net.cpp:394] pool2 <- conv2
I1026 18:52:27.854535 20808 net.cpp:356] pool2 -> pool2
I1026 18:52:27.854542 20808 net.cpp:96] Setting up pool2
I1026 18:52:27.854548 20808 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 18:52:27.854554 20808 net.cpp:67] Creating Layer relu2
I1026 18:52:27.854559 20808 net.cpp:394] relu2 <- pool2
I1026 18:52:27.854565 20808 net.cpp:345] relu2 -> pool2 (in-place)
I1026 18:52:27.854570 20808 net.cpp:96] Setting up relu2
I1026 18:52:27.854575 20808 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 18:52:27.854583 20808 net.cpp:67] Creating Layer drop2
I1026 18:52:27.854586 20808 net.cpp:394] drop2 <- pool2
I1026 18:52:27.854594 20808 net.cpp:345] drop2 -> pool2 (in-place)
I1026 18:52:27.854601 20808 net.cpp:96] Setting up drop2
I1026 18:52:27.854605 20808 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 18:52:27.854614 20808 net.cpp:67] Creating Layer conv3
I1026 18:52:27.854617 20808 net.cpp:394] conv3 <- pool2
I1026 18:52:27.854626 20808 net.cpp:356] conv3 -> conv3
I1026 18:52:27.854634 20808 net.cpp:96] Setting up conv3
I1026 18:52:27.857453 20808 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1026 18:52:27.857506 20808 net.cpp:67] Creating Layer pool3
I1026 18:52:27.857522 20808 net.cpp:394] pool3 <- conv3
I1026 18:52:27.857538 20808 net.cpp:356] pool3 -> pool3
I1026 18:52:27.857558 20808 net.cpp:96] Setting up pool3
I1026 18:52:27.857573 20808 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 18:52:27.857592 20808 net.cpp:67] Creating Layer relu3
I1026 18:52:27.857605 20808 net.cpp:394] relu3 <- pool3
I1026 18:52:27.857622 20808 net.cpp:345] relu3 -> pool3 (in-place)
I1026 18:52:27.857640 20808 net.cpp:96] Setting up relu3
I1026 18:52:27.857651 20808 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 18:52:27.857667 20808 net.cpp:67] Creating Layer drop3
I1026 18:52:27.857679 20808 net.cpp:394] drop3 <- pool3
I1026 18:52:27.857699 20808 net.cpp:345] drop3 -> pool3 (in-place)
I1026 18:52:27.857717 20808 net.cpp:96] Setting up drop3
I1026 18:52:27.857729 20808 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 18:52:27.857748 20808 net.cpp:67] Creating Layer ip1
I1026 18:52:27.857760 20808 net.cpp:394] ip1 <- pool3
I1026 18:52:27.857777 20808 net.cpp:356] ip1 -> ip1
I1026 18:52:27.857841 20808 net.cpp:96] Setting up ip1
I1026 18:52:28.266881 20808 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 18:52:28.266942 20808 net.cpp:67] Creating Layer relu4
I1026 18:52:28.266948 20808 net.cpp:394] relu4 <- ip1
I1026 18:52:28.266959 20808 net.cpp:345] relu4 -> ip1 (in-place)
I1026 18:52:28.266968 20808 net.cpp:96] Setting up relu4
I1026 18:52:28.266973 20808 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 18:52:28.266980 20808 net.cpp:67] Creating Layer drop4
I1026 18:52:28.266985 20808 net.cpp:394] drop4 <- ip1
I1026 18:52:28.266990 20808 net.cpp:345] drop4 -> ip1 (in-place)
I1026 18:52:28.266996 20808 net.cpp:96] Setting up drop4
I1026 18:52:28.267001 20808 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 18:52:28.267012 20808 net.cpp:67] Creating Layer ip2
I1026 18:52:28.267016 20808 net.cpp:394] ip2 <- ip1
I1026 18:52:28.267025 20808 net.cpp:356] ip2 -> ip2
I1026 18:52:28.267035 20808 net.cpp:96] Setting up ip2
I1026 18:52:28.276959 20808 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1026 18:52:28.277021 20808 net.cpp:67] Creating Layer loss
I1026 18:52:28.277029 20808 net.cpp:394] loss <- ip2
I1026 18:52:28.277036 20808 net.cpp:394] loss <- label
I1026 18:52:28.277042 20808 net.cpp:356] loss -> loss
I1026 18:52:28.277052 20808 net.cpp:96] Setting up loss
I1026 18:52:28.277062 20808 net.cpp:103] Top shape: 1 1 1 1 (1)
I1026 18:52:28.277066 20808 net.cpp:109]     with loss weight 1
I1026 18:52:28.277101 20808 net.cpp:170] loss needs backward computation.
I1026 18:52:28.277107 20808 net.cpp:170] ip2 needs backward computation.
I1026 18:52:28.277112 20808 net.cpp:170] drop4 needs backward computation.
I1026 18:52:28.277115 20808 net.cpp:170] relu4 needs backward computation.
I1026 18:52:28.277119 20808 net.cpp:170] ip1 needs backward computation.
I1026 18:52:28.277124 20808 net.cpp:170] drop3 needs backward computation.
I1026 18:52:28.277128 20808 net.cpp:170] relu3 needs backward computation.
I1026 18:52:28.277132 20808 net.cpp:170] pool3 needs backward computation.
I1026 18:52:28.277137 20808 net.cpp:170] conv3 needs backward computation.
I1026 18:52:28.277142 20808 net.cpp:170] drop2 needs backward computation.
I1026 18:52:28.277146 20808 net.cpp:170] relu2 needs backward computation.
I1026 18:52:28.277150 20808 net.cpp:170] pool2 needs backward computation.
I1026 18:52:28.277155 20808 net.cpp:170] conv2 needs backward computation.
I1026 18:52:28.277159 20808 net.cpp:170] drop1 needs backward computation.
I1026 18:52:28.277164 20808 net.cpp:170] relu1 needs backward computation.
I1026 18:52:28.277168 20808 net.cpp:170] pool1 needs backward computation.
I1026 18:52:28.277173 20808 net.cpp:170] conv1 needs backward computation.
I1026 18:52:28.277179 20808 net.cpp:172] mnist does not need backward computation.
I1026 18:52:28.277184 20808 net.cpp:208] This network produces output loss
I1026 18:52:28.277195 20808 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 18:52:28.277204 20808 net.cpp:219] Network initialization done.
I1026 18:52:28.277207 20808 net.cpp:220] Memory required for data: 119788292
I1026 18:52:28.277267 20808 solver.cpp:41] Solver scaffolding done.
I1026 18:52:28.277274 20808 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_125000.solverstate
I1026 18:52:28.277278 20808 solver.cpp:160] Solving Captcha
I1026 18:52:28.277297 20808 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_125000.solverstate
I1026 18:52:33.062429 20808 solver.cpp:502] SGDSolver: restoring history
I1026 18:52:33.886677 20808 solver.cpp:191] Iteration 125000, loss = 2.58607
I1026 18:52:33.886734 20808 solver.cpp:206]     Train net output #0: loss = 2.58607 (* 1 = 2.58607 loss)
I1026 18:52:33.886749 20808 solver.cpp:403] Iteration 125000, lr = 0.00141987
I1026 18:56:35.832350 20808 solver.cpp:191] Iteration 126000, loss = 2.69597
I1026 18:56:35.833061 20808 solver.cpp:206]     Train net output #0: loss = 2.69597 (* 1 = 2.69597 loss)
I1026 18:56:35.833094 20808 solver.cpp:403] Iteration 126000, lr = 0.00141204
I1026 19:00:37.303335 20808 solver.cpp:191] Iteration 127000, loss = 2.50844
I1026 19:00:37.303959 20808 solver.cpp:206]     Train net output #0: loss = 2.50844 (* 1 = 2.50844 loss)
I1026 19:00:37.303992 20808 solver.cpp:403] Iteration 127000, lr = 0.0014043
I1026 19:04:38.780514 20808 solver.cpp:191] Iteration 128000, loss = 2.57122
I1026 19:04:38.781457 20808 solver.cpp:206]     Train net output #0: loss = 2.57122 (* 1 = 2.57122 loss)
I1026 19:04:38.781471 20808 solver.cpp:403] Iteration 128000, lr = 0.00139666
I1026 19:08:40.280625 20808 solver.cpp:191] Iteration 129000, loss = 2.474
I1026 19:08:40.281214 20808 solver.cpp:206]     Train net output #0: loss = 2.474 (* 1 = 2.474 loss)
I1026 19:08:40.281247 20808 solver.cpp:403] Iteration 129000, lr = 0.00138912
I1026 19:12:42.499560 20808 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_130000.caffemodel
I1026 19:12:47.089951 20808 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_130000.solverstate
I1026 19:12:50.654670 20808 solver.cpp:191] Iteration 130000, loss = 2.56629
I1026 19:12:50.655351 20808 solver.cpp:206]     Train net output #0: loss = 2.56629 (* 1 = 2.56629 loss)
I1026 19:12:50.655385 20808 solver.cpp:403] Iteration 130000, lr = 0.00138167
I1026 19:16:52.167238 20808 solver.cpp:191] Iteration 131000, loss = 2.62325
I1026 19:16:52.167819 20808 solver.cpp:206]     Train net output #0: loss = 2.62325 (* 1 = 2.62325 loss)
I1026 19:16:52.167850 20808 solver.cpp:403] Iteration 131000, lr = 0.00137431
I1026 19:20:53.625495 20808 solver.cpp:191] Iteration 132000, loss = 2.60198
I1026 19:20:53.626152 20808 solver.cpp:206]     Train net output #0: loss = 2.60198 (* 1 = 2.60198 loss)
I1026 19:20:53.626183 20808 solver.cpp:403] Iteration 132000, lr = 0.00136705
I1026 19:24:55.142158 20808 solver.cpp:191] Iteration 133000, loss = 2.55756
I1026 19:24:55.142781 20808 solver.cpp:206]     Train net output #0: loss = 2.55756 (* 1 = 2.55756 loss)
I1026 19:24:55.142817 20808 solver.cpp:403] Iteration 133000, lr = 0.00135987
I1026 19:28:56.560282 20808 solver.cpp:191] Iteration 134000, loss = 2.41105
I1026 19:28:56.560878 20808 solver.cpp:206]     Train net output #0: loss = 2.41105 (* 1 = 2.41105 loss)
I1026 19:28:56.560909 20808 solver.cpp:403] Iteration 134000, lr = 0.00135278
I1026 19:32:58.616116 20808 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_135000.caffemodel
I1026 19:33:02.988035 20808 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_135000.solverstate
I1026 19:33:06.863386 20808 solver.cpp:191] Iteration 135000, loss = 2.43795
I1026 19:33:06.864032 20808 solver.cpp:206]     Train net output #0: loss = 2.43795 (* 1 = 2.43795 loss)
I1026 19:33:06.864070 20808 solver.cpp:403] Iteration 135000, lr = 0.00134578
I1026 19:37:08.312330 20808 solver.cpp:191] Iteration 136000, loss = 2.55402
I1026 19:37:08.313117 20808 solver.cpp:206]     Train net output #0: loss = 2.55402 (* 1 = 2.55402 loss)
I1026 19:37:08.313149 20808 solver.cpp:403] Iteration 136000, lr = 0.00133886
I1026 19:41:09.744149 20808 solver.cpp:191] Iteration 137000, loss = 2.56219
I1026 19:41:09.744825 20808 solver.cpp:206]     Train net output #0: loss = 2.56219 (* 1 = 2.56219 loss)
I1026 19:41:09.744858 20808 solver.cpp:403] Iteration 137000, lr = 0.00133202
I1026 19:45:11.209162 20808 solver.cpp:191] Iteration 138000, loss = 2.25586
I1026 19:45:11.209861 20808 solver.cpp:206]     Train net output #0: loss = 2.25586 (* 1 = 2.25586 loss)
I1026 19:45:11.209892 20808 solver.cpp:403] Iteration 138000, lr = 0.00132527
I1026 19:49:12.617810 20808 solver.cpp:191] Iteration 139000, loss = 2.57779
I1026 19:49:12.618567 20808 solver.cpp:206]     Train net output #0: loss = 2.57779 (* 1 = 2.57779 loss)
I1026 19:49:12.618602 20808 solver.cpp:403] Iteration 139000, lr = 0.00131859
I1026 19:53:14.548002 20808 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_140000.caffemodel
I1026 19:53:18.948329 20808 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_140000.solverstate
I1026 19:53:22.398670 20808 solver.cpp:191] Iteration 140000, loss = 2.45399
I1026 19:53:22.399245 20808 solver.cpp:206]     Train net output #0: loss = 2.45399 (* 1 = 2.45399 loss)
I1026 19:53:22.399272 20808 solver.cpp:403] Iteration 140000, lr = 0.00131199
I1026 19:57:23.962064 20808 solver.cpp:191] Iteration 141000, loss = 2.35046
I1026 19:57:23.962713 20808 solver.cpp:206]     Train net output #0: loss = 2.35046 (* 1 = 2.35046 loss)
I1026 19:57:23.962749 20808 solver.cpp:403] Iteration 141000, lr = 0.00130547
I1026 20:01:25.529853 20808 solver.cpp:191] Iteration 142000, loss = 2.31241
I1026 20:01:25.530447 20808 solver.cpp:206]     Train net output #0: loss = 2.31241 (* 1 = 2.31241 loss)
I1026 20:01:25.530479 20808 solver.cpp:403] Iteration 142000, lr = 0.00129902
I1026 20:05:27.169474 20808 solver.cpp:191] Iteration 143000, loss = 2.38337
I1026 20:05:27.170090 20808 solver.cpp:206]     Train net output #0: loss = 2.38337 (* 1 = 2.38337 loss)
I1026 20:05:27.170126 20808 solver.cpp:403] Iteration 143000, lr = 0.00129265
I1026 20:09:28.803730 20808 solver.cpp:191] Iteration 144000, loss = 2.28736
I1026 20:09:28.804347 20808 solver.cpp:206]     Train net output #0: loss = 2.28736 (* 1 = 2.28736 loss)
I1026 20:09:28.804365 20808 solver.cpp:403] Iteration 144000, lr = 0.00128635
I1026 20:13:30.958528 20808 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_145000.caffemodel
I1026 20:13:35.568778 20808 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_145000.solverstate
I1026 20:13:39.275436 20808 solver.cpp:191] Iteration 145000, loss = 2.43234
I1026 20:13:39.276089 20808 solver.cpp:206]     Train net output #0: loss = 2.43234 (* 1 = 2.43234 loss)
I1026 20:13:39.276124 20808 solver.cpp:403] Iteration 145000, lr = 0.00128012
I1026 20:17:40.857264 20808 solver.cpp:191] Iteration 146000, loss = 2.30706
I1026 20:17:40.857945 20808 solver.cpp:206]     Train net output #0: loss = 2.30706 (* 1 = 2.30706 loss)
I1026 20:17:40.857978 20808 solver.cpp:403] Iteration 146000, lr = 0.00127396
I1026 20:21:42.487674 20808 solver.cpp:191] Iteration 147000, loss = 2.42802
I1026 20:21:42.488281 20808 solver.cpp:206]     Train net output #0: loss = 2.42802 (* 1 = 2.42802 loss)
I1026 20:21:42.488312 20808 solver.cpp:403] Iteration 147000, lr = 0.00126787
I1026 20:25:44.117084 20808 solver.cpp:191] Iteration 148000, loss = 2.44471
I1026 20:25:44.117717 20808 solver.cpp:206]     Train net output #0: loss = 2.44471 (* 1 = 2.44471 loss)
I1026 20:25:44.117750 20808 solver.cpp:403] Iteration 148000, lr = 0.00126185
I1026 20:29:45.680933 20808 solver.cpp:191] Iteration 149000, loss = 2.60881
I1026 20:29:45.681762 20808 solver.cpp:206]     Train net output #0: loss = 2.60881 (* 1 = 2.60881 loss)
I1026 20:29:45.681794 20808 solver.cpp:403] Iteration 149000, lr = 0.00125589
I1026 20:33:47.718422 20808 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_150000.caffemodel
I1026 20:33:52.081540 20808 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_150000.solverstate
I1026 20:33:55.393230 20808 solver.cpp:228] Iteration 150000, loss = 2.46988
I1026 20:33:55.393796 20808 solver.cpp:233] Optimization Done.
I1026 20:33:55.393821 20808 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1026 20:56:13.247745 17108 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 20:56:13.247859 17108 net.cpp:358] Input 0 -> data
I1026 20:56:13.247890 17108 net.cpp:67] Creating Layer conv1
I1026 20:56:13.247895 17108 net.cpp:394] conv1 <- data
I1026 20:56:13.247902 17108 net.cpp:356] conv1 -> conv1
I1026 20:56:13.247912 17108 net.cpp:96] Setting up conv1
I1026 20:56:13.248236 17108 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 20:56:13.248255 17108 net.cpp:67] Creating Layer pool1
I1026 20:56:13.248260 17108 net.cpp:394] pool1 <- conv1
I1026 20:56:13.248266 17108 net.cpp:356] pool1 -> pool1
I1026 20:56:13.248273 17108 net.cpp:96] Setting up pool1
I1026 20:56:13.248286 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:56:13.248293 17108 net.cpp:67] Creating Layer relu1
I1026 20:56:13.248297 17108 net.cpp:394] relu1 <- pool1
I1026 20:56:13.248302 17108 net.cpp:345] relu1 -> pool1 (in-place)
I1026 20:56:13.248307 17108 net.cpp:96] Setting up relu1
I1026 20:56:13.248312 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:56:13.248319 17108 net.cpp:67] Creating Layer drop1
I1026 20:56:13.248324 17108 net.cpp:394] drop1 <- pool1
I1026 20:56:13.248329 17108 net.cpp:345] drop1 -> pool1 (in-place)
I1026 20:56:13.248335 17108 net.cpp:96] Setting up drop1
I1026 20:56:13.248340 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:56:13.248347 17108 net.cpp:67] Creating Layer conv2
I1026 20:56:13.248353 17108 net.cpp:394] conv2 <- pool1
I1026 20:56:13.248358 17108 net.cpp:356] conv2 -> conv2
I1026 20:56:13.248365 17108 net.cpp:96] Setting up conv2
I1026 20:56:13.248951 17108 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 20:56:13.248970 17108 net.cpp:67] Creating Layer pool2
I1026 20:56:13.248975 17108 net.cpp:394] pool2 <- conv2
I1026 20:56:13.248980 17108 net.cpp:356] pool2 -> pool2
I1026 20:56:13.248987 17108 net.cpp:96] Setting up pool2
I1026 20:56:13.248993 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:56:13.249002 17108 net.cpp:67] Creating Layer relu2
I1026 20:56:13.249007 17108 net.cpp:394] relu2 <- pool2
I1026 20:56:13.249012 17108 net.cpp:345] relu2 -> pool2 (in-place)
I1026 20:56:13.249017 17108 net.cpp:96] Setting up relu2
I1026 20:56:13.249022 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:56:13.249027 17108 net.cpp:67] Creating Layer drop2
I1026 20:56:13.249030 17108 net.cpp:394] drop2 <- pool2
I1026 20:56:13.249037 17108 net.cpp:345] drop2 -> pool2 (in-place)
I1026 20:56:13.249043 17108 net.cpp:96] Setting up drop2
I1026 20:56:13.249047 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:56:13.249054 17108 net.cpp:67] Creating Layer conv3
I1026 20:56:13.249058 17108 net.cpp:394] conv3 <- pool2
I1026 20:56:13.249066 17108 net.cpp:356] conv3 -> conv3
I1026 20:56:13.249073 17108 net.cpp:96] Setting up conv3
I1026 20:56:13.250524 17108 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 20:56:13.250542 17108 net.cpp:67] Creating Layer pool3
I1026 20:56:13.250547 17108 net.cpp:394] pool3 <- conv3
I1026 20:56:13.250553 17108 net.cpp:356] pool3 -> pool3
I1026 20:56:13.250560 17108 net.cpp:96] Setting up pool3
I1026 20:56:13.250566 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:56:13.250571 17108 net.cpp:67] Creating Layer relu3
I1026 20:56:13.250576 17108 net.cpp:394] relu3 <- pool3
I1026 20:56:13.250581 17108 net.cpp:345] relu3 -> pool3 (in-place)
I1026 20:56:13.250586 17108 net.cpp:96] Setting up relu3
I1026 20:56:13.250591 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:56:13.250596 17108 net.cpp:67] Creating Layer drop3
I1026 20:56:13.250599 17108 net.cpp:394] drop3 <- pool3
I1026 20:56:13.250604 17108 net.cpp:345] drop3 -> pool3 (in-place)
I1026 20:56:13.250610 17108 net.cpp:96] Setting up drop3
I1026 20:56:13.250614 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:56:13.250622 17108 net.cpp:67] Creating Layer ip1
I1026 20:56:13.250627 17108 net.cpp:394] ip1 <- pool3
I1026 20:56:13.250633 17108 net.cpp:356] ip1 -> ip1
I1026 20:56:13.250639 17108 net.cpp:96] Setting up ip1
I1026 20:56:13.734086 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:56:13.734156 17108 net.cpp:67] Creating Layer relu4
I1026 20:56:13.734164 17108 net.cpp:394] relu4 <- ip1
I1026 20:56:13.734174 17108 net.cpp:345] relu4 -> ip1 (in-place)
I1026 20:56:13.734184 17108 net.cpp:96] Setting up relu4
I1026 20:56:13.734189 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:56:13.734196 17108 net.cpp:67] Creating Layer drop4
I1026 20:56:13.734200 17108 net.cpp:394] drop4 <- ip1
I1026 20:56:13.734207 17108 net.cpp:345] drop4 -> ip1 (in-place)
I1026 20:56:13.734215 17108 net.cpp:96] Setting up drop4
I1026 20:56:13.734220 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:56:13.734227 17108 net.cpp:67] Creating Layer ip2
I1026 20:56:13.734231 17108 net.cpp:394] ip2 <- ip1
I1026 20:56:13.734239 17108 net.cpp:356] ip2 -> ip2
I1026 20:56:13.734251 17108 net.cpp:96] Setting up ip2
I1026 20:56:13.742393 17108 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 20:56:13.742462 17108 net.cpp:67] Creating Layer prob
I1026 20:56:13.742470 17108 net.cpp:394] prob <- ip2
I1026 20:56:13.742477 17108 net.cpp:356] prob -> prob
I1026 20:56:13.742487 17108 net.cpp:96] Setting up prob
I1026 20:56:13.742493 17108 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 20:56:13.742498 17108 net.cpp:172] prob does not need backward computation.
I1026 20:56:13.742502 17108 net.cpp:172] ip2 does not need backward computation.
I1026 20:56:13.742506 17108 net.cpp:172] drop4 does not need backward computation.
I1026 20:56:13.742509 17108 net.cpp:172] relu4 does not need backward computation.
I1026 20:56:13.742512 17108 net.cpp:172] ip1 does not need backward computation.
I1026 20:56:13.742516 17108 net.cpp:172] drop3 does not need backward computation.
I1026 20:56:13.742527 17108 net.cpp:172] relu3 does not need backward computation.
I1026 20:56:13.742530 17108 net.cpp:172] pool3 does not need backward computation.
I1026 20:56:13.742533 17108 net.cpp:172] conv3 does not need backward computation.
I1026 20:56:13.742537 17108 net.cpp:172] drop2 does not need backward computation.
I1026 20:56:13.742542 17108 net.cpp:172] relu2 does not need backward computation.
I1026 20:56:13.742544 17108 net.cpp:172] pool2 does not need backward computation.
I1026 20:56:13.742548 17108 net.cpp:172] conv2 does not need backward computation.
I1026 20:56:13.742552 17108 net.cpp:172] drop1 does not need backward computation.
I1026 20:56:13.742555 17108 net.cpp:172] relu1 does not need backward computation.
I1026 20:56:13.742558 17108 net.cpp:172] pool1 does not need backward computation.
I1026 20:56:13.742563 17108 net.cpp:172] conv1 does not need backward computation.
I1026 20:56:13.742565 17108 net.cpp:208] This network produces output prob
I1026 20:56:13.742579 17108 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 20:56:13.742588 17108 net.cpp:219] Network initialization done.
I1026 20:56:13.742591 17108 net.cpp:220] Memory required for data: 1837200
I1026 20:56:54.173877 17108 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 20:56:54.174453 17108 net.cpp:358] Input 0 -> data
I1026 20:56:54.174515 17108 net.cpp:67] Creating Layer conv1
I1026 20:56:54.174527 17108 net.cpp:394] conv1 <- data
I1026 20:56:54.174542 17108 net.cpp:356] conv1 -> conv1
I1026 20:56:54.174562 17108 net.cpp:96] Setting up conv1
I1026 20:56:54.174618 17108 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 20:56:54.174649 17108 net.cpp:67] Creating Layer pool1
I1026 20:56:54.174659 17108 net.cpp:394] pool1 <- conv1
I1026 20:56:54.174674 17108 net.cpp:356] pool1 -> pool1
I1026 20:56:54.174690 17108 net.cpp:96] Setting up pool1
I1026 20:56:54.174705 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:56:54.174720 17108 net.cpp:67] Creating Layer relu1
I1026 20:56:54.174728 17108 net.cpp:394] relu1 <- pool1
I1026 20:56:54.174741 17108 net.cpp:345] relu1 -> pool1 (in-place)
I1026 20:56:54.174754 17108 net.cpp:96] Setting up relu1
I1026 20:56:54.174763 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:56:54.174777 17108 net.cpp:67] Creating Layer drop1
I1026 20:56:54.174785 17108 net.cpp:394] drop1 <- pool1
I1026 20:56:54.174798 17108 net.cpp:345] drop1 -> pool1 (in-place)
I1026 20:56:54.174810 17108 net.cpp:96] Setting up drop1
I1026 20:56:54.174821 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:56:54.174836 17108 net.cpp:67] Creating Layer conv2
I1026 20:56:54.174845 17108 net.cpp:394] conv2 <- pool1
I1026 20:56:54.174860 17108 net.cpp:356] conv2 -> conv2
I1026 20:56:54.174875 17108 net.cpp:96] Setting up conv2
I1026 20:56:54.176010 17108 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 20:56:54.176039 17108 net.cpp:67] Creating Layer pool2
I1026 20:56:54.176049 17108 net.cpp:394] pool2 <- conv2
I1026 20:56:54.176064 17108 net.cpp:356] pool2 -> pool2
I1026 20:56:54.176079 17108 net.cpp:96] Setting up pool2
I1026 20:56:54.176092 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:56:54.176105 17108 net.cpp:67] Creating Layer relu2
I1026 20:56:54.176115 17108 net.cpp:394] relu2 <- pool2
I1026 20:56:54.176126 17108 net.cpp:345] relu2 -> pool2 (in-place)
I1026 20:56:54.176139 17108 net.cpp:96] Setting up relu2
I1026 20:56:54.176148 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:56:54.176161 17108 net.cpp:67] Creating Layer drop2
I1026 20:56:54.176169 17108 net.cpp:394] drop2 <- pool2
I1026 20:56:54.176182 17108 net.cpp:345] drop2 -> pool2 (in-place)
I1026 20:56:54.176194 17108 net.cpp:96] Setting up drop2
I1026 20:56:54.176210 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:56:54.176233 17108 net.cpp:67] Creating Layer conv3
I1026 20:56:54.176244 17108 net.cpp:394] conv3 <- pool2
I1026 20:56:54.176260 17108 net.cpp:356] conv3 -> conv3
I1026 20:56:54.176280 17108 net.cpp:96] Setting up conv3
I1026 20:56:54.179973 17108 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 20:56:54.180014 17108 net.cpp:67] Creating Layer pool3
I1026 20:56:54.180028 17108 net.cpp:394] pool3 <- conv3
I1026 20:56:54.180044 17108 net.cpp:356] pool3 -> pool3
I1026 20:56:54.180063 17108 net.cpp:96] Setting up pool3
I1026 20:56:54.180078 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:56:54.180093 17108 net.cpp:67] Creating Layer relu3
I1026 20:56:54.180104 17108 net.cpp:394] relu3 <- pool3
I1026 20:56:54.180119 17108 net.cpp:345] relu3 -> pool3 (in-place)
I1026 20:56:54.180133 17108 net.cpp:96] Setting up relu3
I1026 20:56:54.180145 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:56:54.180160 17108 net.cpp:67] Creating Layer drop3
I1026 20:56:54.180171 17108 net.cpp:394] drop3 <- pool3
I1026 20:56:54.180186 17108 net.cpp:345] drop3 -> pool3 (in-place)
I1026 20:56:54.180202 17108 net.cpp:96] Setting up drop3
I1026 20:56:54.180214 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:56:54.180232 17108 net.cpp:67] Creating Layer ip1
I1026 20:56:54.180243 17108 net.cpp:394] ip1 <- pool3
I1026 20:56:54.180259 17108 net.cpp:356] ip1 -> ip1
I1026 20:56:54.180279 17108 net.cpp:96] Setting up ip1
I1026 20:56:54.596083 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:56:54.596148 17108 net.cpp:67] Creating Layer relu4
I1026 20:56:54.596155 17108 net.cpp:394] relu4 <- ip1
I1026 20:56:54.596179 17108 net.cpp:345] relu4 -> ip1 (in-place)
I1026 20:56:54.596189 17108 net.cpp:96] Setting up relu4
I1026 20:56:54.596194 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:56:54.596201 17108 net.cpp:67] Creating Layer drop4
I1026 20:56:54.596205 17108 net.cpp:394] drop4 <- ip1
I1026 20:56:54.596212 17108 net.cpp:345] drop4 -> ip1 (in-place)
I1026 20:56:54.596220 17108 net.cpp:96] Setting up drop4
I1026 20:56:54.596225 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:56:54.596235 17108 net.cpp:67] Creating Layer ip2
I1026 20:56:54.596240 17108 net.cpp:394] ip2 <- ip1
I1026 20:56:54.596246 17108 net.cpp:356] ip2 -> ip2
I1026 20:56:54.596261 17108 net.cpp:96] Setting up ip2
I1026 20:56:54.603873 17108 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 20:56:54.603937 17108 net.cpp:67] Creating Layer prob
I1026 20:56:54.603945 17108 net.cpp:394] prob <- ip2
I1026 20:56:54.603955 17108 net.cpp:356] prob -> prob
I1026 20:56:54.603965 17108 net.cpp:96] Setting up prob
I1026 20:56:54.603973 17108 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 20:56:54.603977 17108 net.cpp:172] prob does not need backward computation.
I1026 20:56:54.603982 17108 net.cpp:172] ip2 does not need backward computation.
I1026 20:56:54.603986 17108 net.cpp:172] drop4 does not need backward computation.
I1026 20:56:54.603991 17108 net.cpp:172] relu4 does not need backward computation.
I1026 20:56:54.603993 17108 net.cpp:172] ip1 does not need backward computation.
I1026 20:56:54.603997 17108 net.cpp:172] drop3 does not need backward computation.
I1026 20:56:54.604001 17108 net.cpp:172] relu3 does not need backward computation.
I1026 20:56:54.604006 17108 net.cpp:172] pool3 does not need backward computation.
I1026 20:56:54.604009 17108 net.cpp:172] conv3 does not need backward computation.
I1026 20:56:54.604013 17108 net.cpp:172] drop2 does not need backward computation.
I1026 20:56:54.604017 17108 net.cpp:172] relu2 does not need backward computation.
I1026 20:56:54.604022 17108 net.cpp:172] pool2 does not need backward computation.
I1026 20:56:54.604025 17108 net.cpp:172] conv2 does not need backward computation.
I1026 20:56:54.604028 17108 net.cpp:172] drop1 does not need backward computation.
I1026 20:56:54.604032 17108 net.cpp:172] relu1 does not need backward computation.
I1026 20:56:54.604037 17108 net.cpp:172] pool1 does not need backward computation.
I1026 20:56:54.604040 17108 net.cpp:172] conv1 does not need backward computation.
I1026 20:56:54.604044 17108 net.cpp:208] This network produces output prob
I1026 20:56:54.604059 17108 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 20:56:54.604069 17108 net.cpp:219] Network initialization done.
I1026 20:56:54.604074 17108 net.cpp:220] Memory required for data: 1837200
I1026 20:57:29.520860 17108 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 20:57:29.521055 17108 net.cpp:358] Input 0 -> data
I1026 20:57:29.521105 17108 net.cpp:67] Creating Layer conv1
I1026 20:57:29.521118 17108 net.cpp:394] conv1 <- data
I1026 20:57:29.521137 17108 net.cpp:356] conv1 -> conv1
I1026 20:57:29.521162 17108 net.cpp:96] Setting up conv1
I1026 20:57:29.521225 17108 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 20:57:29.521261 17108 net.cpp:67] Creating Layer pool1
I1026 20:57:29.521275 17108 net.cpp:394] pool1 <- conv1
I1026 20:57:29.521291 17108 net.cpp:356] pool1 -> pool1
I1026 20:57:29.521311 17108 net.cpp:96] Setting up pool1
I1026 20:57:29.521327 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:57:29.521345 17108 net.cpp:67] Creating Layer relu1
I1026 20:57:29.521358 17108 net.cpp:394] relu1 <- pool1
I1026 20:57:29.521371 17108 net.cpp:345] relu1 -> pool1 (in-place)
I1026 20:57:29.521387 17108 net.cpp:96] Setting up relu1
I1026 20:57:29.521399 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:57:29.521414 17108 net.cpp:67] Creating Layer drop1
I1026 20:57:29.521425 17108 net.cpp:394] drop1 <- pool1
I1026 20:57:29.521440 17108 net.cpp:345] drop1 -> pool1 (in-place)
I1026 20:57:29.521456 17108 net.cpp:96] Setting up drop1
I1026 20:57:29.521469 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:57:29.521488 17108 net.cpp:67] Creating Layer conv2
I1026 20:57:29.521500 17108 net.cpp:394] conv2 <- pool1
I1026 20:57:29.521517 17108 net.cpp:356] conv2 -> conv2
I1026 20:57:29.521535 17108 net.cpp:96] Setting up conv2
I1026 20:57:29.522920 17108 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 20:57:29.522955 17108 net.cpp:67] Creating Layer pool2
I1026 20:57:29.522969 17108 net.cpp:394] pool2 <- conv2
I1026 20:57:29.522984 17108 net.cpp:356] pool2 -> pool2
I1026 20:57:29.523005 17108 net.cpp:96] Setting up pool2
I1026 20:57:29.523020 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:57:29.523036 17108 net.cpp:67] Creating Layer relu2
I1026 20:57:29.523046 17108 net.cpp:394] relu2 <- pool2
I1026 20:57:29.523061 17108 net.cpp:345] relu2 -> pool2 (in-place)
I1026 20:57:29.523077 17108 net.cpp:96] Setting up relu2
I1026 20:57:29.523087 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:57:29.523102 17108 net.cpp:67] Creating Layer drop2
I1026 20:57:29.523113 17108 net.cpp:394] drop2 <- pool2
I1026 20:57:29.523128 17108 net.cpp:345] drop2 -> pool2 (in-place)
I1026 20:57:29.523144 17108 net.cpp:96] Setting up drop2
I1026 20:57:29.523156 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:57:29.523175 17108 net.cpp:67] Creating Layer conv3
I1026 20:57:29.523187 17108 net.cpp:394] conv3 <- pool2
I1026 20:57:29.523211 17108 net.cpp:356] conv3 -> conv3
I1026 20:57:29.523232 17108 net.cpp:96] Setting up conv3
I1026 20:57:29.526891 17108 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 20:57:29.526929 17108 net.cpp:67] Creating Layer pool3
I1026 20:57:29.526942 17108 net.cpp:394] pool3 <- conv3
I1026 20:57:29.526959 17108 net.cpp:356] pool3 -> pool3
I1026 20:57:29.526976 17108 net.cpp:96] Setting up pool3
I1026 20:57:29.526991 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:57:29.527006 17108 net.cpp:67] Creating Layer relu3
I1026 20:57:29.527017 17108 net.cpp:394] relu3 <- pool3
I1026 20:57:29.527031 17108 net.cpp:345] relu3 -> pool3 (in-place)
I1026 20:57:29.527047 17108 net.cpp:96] Setting up relu3
I1026 20:57:29.527058 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:57:29.527075 17108 net.cpp:67] Creating Layer drop3
I1026 20:57:29.527084 17108 net.cpp:394] drop3 <- pool3
I1026 20:57:29.527101 17108 net.cpp:345] drop3 -> pool3 (in-place)
I1026 20:57:29.527115 17108 net.cpp:96] Setting up drop3
I1026 20:57:29.527128 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:57:29.527145 17108 net.cpp:67] Creating Layer ip1
I1026 20:57:29.527156 17108 net.cpp:394] ip1 <- pool3
I1026 20:57:29.527173 17108 net.cpp:356] ip1 -> ip1
I1026 20:57:29.527192 17108 net.cpp:96] Setting up ip1
I1026 20:57:29.931749 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:57:29.931813 17108 net.cpp:67] Creating Layer relu4
I1026 20:57:29.931821 17108 net.cpp:394] relu4 <- ip1
I1026 20:57:29.931831 17108 net.cpp:345] relu4 -> ip1 (in-place)
I1026 20:57:29.931841 17108 net.cpp:96] Setting up relu4
I1026 20:57:29.931846 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:57:29.931855 17108 net.cpp:67] Creating Layer drop4
I1026 20:57:29.931859 17108 net.cpp:394] drop4 <- ip1
I1026 20:57:29.931866 17108 net.cpp:345] drop4 -> ip1 (in-place)
I1026 20:57:29.931874 17108 net.cpp:96] Setting up drop4
I1026 20:57:29.931879 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:57:29.931890 17108 net.cpp:67] Creating Layer ip2
I1026 20:57:29.931895 17108 net.cpp:394] ip2 <- ip1
I1026 20:57:29.931901 17108 net.cpp:356] ip2 -> ip2
I1026 20:57:29.931915 17108 net.cpp:96] Setting up ip2
I1026 20:57:29.939479 17108 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 20:57:29.939543 17108 net.cpp:67] Creating Layer prob
I1026 20:57:29.939549 17108 net.cpp:394] prob <- ip2
I1026 20:57:29.939559 17108 net.cpp:356] prob -> prob
I1026 20:57:29.939570 17108 net.cpp:96] Setting up prob
I1026 20:57:29.939579 17108 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 20:57:29.939584 17108 net.cpp:172] prob does not need backward computation.
I1026 20:57:29.939589 17108 net.cpp:172] ip2 does not need backward computation.
I1026 20:57:29.939592 17108 net.cpp:172] drop4 does not need backward computation.
I1026 20:57:29.939596 17108 net.cpp:172] relu4 does not need backward computation.
I1026 20:57:29.939600 17108 net.cpp:172] ip1 does not need backward computation.
I1026 20:57:29.939604 17108 net.cpp:172] drop3 does not need backward computation.
I1026 20:57:29.939607 17108 net.cpp:172] relu3 does not need backward computation.
I1026 20:57:29.939612 17108 net.cpp:172] pool3 does not need backward computation.
I1026 20:57:29.939615 17108 net.cpp:172] conv3 does not need backward computation.
I1026 20:57:29.939620 17108 net.cpp:172] drop2 does not need backward computation.
I1026 20:57:29.939623 17108 net.cpp:172] relu2 does not need backward computation.
I1026 20:57:29.939627 17108 net.cpp:172] pool2 does not need backward computation.
I1026 20:57:29.939631 17108 net.cpp:172] conv2 does not need backward computation.
I1026 20:57:29.939635 17108 net.cpp:172] drop1 does not need backward computation.
I1026 20:57:29.939640 17108 net.cpp:172] relu1 does not need backward computation.
I1026 20:57:29.939643 17108 net.cpp:172] pool1 does not need backward computation.
I1026 20:57:29.939647 17108 net.cpp:172] conv1 does not need backward computation.
I1026 20:57:29.939651 17108 net.cpp:208] This network produces output prob
I1026 20:57:29.939677 17108 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 20:57:29.939687 17108 net.cpp:219] Network initialization done.
I1026 20:57:29.939692 17108 net.cpp:220] Memory required for data: 1837200
I1026 20:58:05.996682 17108 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 20:58:05.997275 17108 net.cpp:358] Input 0 -> data
I1026 20:58:05.997333 17108 net.cpp:67] Creating Layer conv1
I1026 20:58:05.997347 17108 net.cpp:394] conv1 <- data
I1026 20:58:05.997366 17108 net.cpp:356] conv1 -> conv1
I1026 20:58:05.997390 17108 net.cpp:96] Setting up conv1
I1026 20:58:05.997457 17108 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 20:58:05.997491 17108 net.cpp:67] Creating Layer pool1
I1026 20:58:05.997504 17108 net.cpp:394] pool1 <- conv1
I1026 20:58:05.997520 17108 net.cpp:356] pool1 -> pool1
I1026 20:58:05.997540 17108 net.cpp:96] Setting up pool1
I1026 20:58:05.997557 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:58:05.997576 17108 net.cpp:67] Creating Layer relu1
I1026 20:58:05.997588 17108 net.cpp:394] relu1 <- pool1
I1026 20:58:05.997602 17108 net.cpp:345] relu1 -> pool1 (in-place)
I1026 20:58:05.997619 17108 net.cpp:96] Setting up relu1
I1026 20:58:05.997632 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:58:05.997648 17108 net.cpp:67] Creating Layer drop1
I1026 20:58:05.997658 17108 net.cpp:394] drop1 <- pool1
I1026 20:58:05.997686 17108 net.cpp:345] drop1 -> pool1 (in-place)
I1026 20:58:05.997704 17108 net.cpp:96] Setting up drop1
I1026 20:58:05.997717 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:58:05.997737 17108 net.cpp:67] Creating Layer conv2
I1026 20:58:05.997750 17108 net.cpp:394] conv2 <- pool1
I1026 20:58:05.997766 17108 net.cpp:356] conv2 -> conv2
I1026 20:58:05.997786 17108 net.cpp:96] Setting up conv2
I1026 20:58:05.999167 17108 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 20:58:05.999203 17108 net.cpp:67] Creating Layer pool2
I1026 20:58:05.999215 17108 net.cpp:394] pool2 <- conv2
I1026 20:58:05.999233 17108 net.cpp:356] pool2 -> pool2
I1026 20:58:05.999251 17108 net.cpp:96] Setting up pool2
I1026 20:58:05.999269 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:58:05.999284 17108 net.cpp:67] Creating Layer relu2
I1026 20:58:05.999294 17108 net.cpp:394] relu2 <- pool2
I1026 20:58:05.999310 17108 net.cpp:345] relu2 -> pool2 (in-place)
I1026 20:58:05.999325 17108 net.cpp:96] Setting up relu2
I1026 20:58:05.999336 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:58:05.999351 17108 net.cpp:67] Creating Layer drop2
I1026 20:58:05.999362 17108 net.cpp:394] drop2 <- pool2
I1026 20:58:05.999377 17108 net.cpp:345] drop2 -> pool2 (in-place)
I1026 20:58:05.999393 17108 net.cpp:96] Setting up drop2
I1026 20:58:05.999405 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:58:05.999425 17108 net.cpp:67] Creating Layer conv3
I1026 20:58:05.999438 17108 net.cpp:394] conv3 <- pool2
I1026 20:58:05.999454 17108 net.cpp:356] conv3 -> conv3
I1026 20:58:05.999475 17108 net.cpp:96] Setting up conv3
I1026 20:58:06.003157 17108 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 20:58:06.003201 17108 net.cpp:67] Creating Layer pool3
I1026 20:58:06.003214 17108 net.cpp:394] pool3 <- conv3
I1026 20:58:06.003232 17108 net.cpp:356] pool3 -> pool3
I1026 20:58:06.003250 17108 net.cpp:96] Setting up pool3
I1026 20:58:06.003265 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:58:06.003281 17108 net.cpp:67] Creating Layer relu3
I1026 20:58:06.003293 17108 net.cpp:394] relu3 <- pool3
I1026 20:58:06.003307 17108 net.cpp:345] relu3 -> pool3 (in-place)
I1026 20:58:06.003322 17108 net.cpp:96] Setting up relu3
I1026 20:58:06.003334 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:58:06.003350 17108 net.cpp:67] Creating Layer drop3
I1026 20:58:06.003360 17108 net.cpp:394] drop3 <- pool3
I1026 20:58:06.003376 17108 net.cpp:345] drop3 -> pool3 (in-place)
I1026 20:58:06.003391 17108 net.cpp:96] Setting up drop3
I1026 20:58:06.003404 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:58:06.003422 17108 net.cpp:67] Creating Layer ip1
I1026 20:58:06.003433 17108 net.cpp:394] ip1 <- pool3
I1026 20:58:06.003450 17108 net.cpp:356] ip1 -> ip1
I1026 20:58:06.003469 17108 net.cpp:96] Setting up ip1
I1026 20:58:06.413030 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:58:06.413095 17108 net.cpp:67] Creating Layer relu4
I1026 20:58:06.413103 17108 net.cpp:394] relu4 <- ip1
I1026 20:58:06.413113 17108 net.cpp:345] relu4 -> ip1 (in-place)
I1026 20:58:06.413123 17108 net.cpp:96] Setting up relu4
I1026 20:58:06.413130 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:58:06.413137 17108 net.cpp:67] Creating Layer drop4
I1026 20:58:06.413142 17108 net.cpp:394] drop4 <- ip1
I1026 20:58:06.413149 17108 net.cpp:345] drop4 -> ip1 (in-place)
I1026 20:58:06.413156 17108 net.cpp:96] Setting up drop4
I1026 20:58:06.413162 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:58:06.413172 17108 net.cpp:67] Creating Layer ip2
I1026 20:58:06.413177 17108 net.cpp:394] ip2 <- ip1
I1026 20:58:06.413185 17108 net.cpp:356] ip2 -> ip2
I1026 20:58:06.413199 17108 net.cpp:96] Setting up ip2
I1026 20:58:06.420775 17108 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 20:58:06.420840 17108 net.cpp:67] Creating Layer prob
I1026 20:58:06.420847 17108 net.cpp:394] prob <- ip2
I1026 20:58:06.420857 17108 net.cpp:356] prob -> prob
I1026 20:58:06.420868 17108 net.cpp:96] Setting up prob
I1026 20:58:06.420887 17108 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 20:58:06.420892 17108 net.cpp:172] prob does not need backward computation.
I1026 20:58:06.420897 17108 net.cpp:172] ip2 does not need backward computation.
I1026 20:58:06.420900 17108 net.cpp:172] drop4 does not need backward computation.
I1026 20:58:06.420904 17108 net.cpp:172] relu4 does not need backward computation.
I1026 20:58:06.420908 17108 net.cpp:172] ip1 does not need backward computation.
I1026 20:58:06.420912 17108 net.cpp:172] drop3 does not need backward computation.
I1026 20:58:06.420917 17108 net.cpp:172] relu3 does not need backward computation.
I1026 20:58:06.420920 17108 net.cpp:172] pool3 does not need backward computation.
I1026 20:58:06.420924 17108 net.cpp:172] conv3 does not need backward computation.
I1026 20:58:06.420928 17108 net.cpp:172] drop2 does not need backward computation.
I1026 20:58:06.420933 17108 net.cpp:172] relu2 does not need backward computation.
I1026 20:58:06.420936 17108 net.cpp:172] pool2 does not need backward computation.
I1026 20:58:06.420940 17108 net.cpp:172] conv2 does not need backward computation.
I1026 20:58:06.420943 17108 net.cpp:172] drop1 does not need backward computation.
I1026 20:58:06.420948 17108 net.cpp:172] relu1 does not need backward computation.
I1026 20:58:06.420951 17108 net.cpp:172] pool1 does not need backward computation.
I1026 20:58:06.420955 17108 net.cpp:172] conv1 does not need backward computation.
I1026 20:58:06.420959 17108 net.cpp:208] This network produces output prob
I1026 20:58:06.420974 17108 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 20:58:06.420984 17108 net.cpp:219] Network initialization done.
I1026 20:58:06.420989 17108 net.cpp:220] Memory required for data: 1837200
I1026 20:58:41.612284 17108 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 20:58:41.612547 17108 net.cpp:358] Input 0 -> data
I1026 20:58:41.612584 17108 net.cpp:67] Creating Layer conv1
I1026 20:58:41.612591 17108 net.cpp:394] conv1 <- data
I1026 20:58:41.612598 17108 net.cpp:356] conv1 -> conv1
I1026 20:58:41.612608 17108 net.cpp:96] Setting up conv1
I1026 20:58:41.612638 17108 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 20:58:41.612654 17108 net.cpp:67] Creating Layer pool1
I1026 20:58:41.612659 17108 net.cpp:394] pool1 <- conv1
I1026 20:58:41.612665 17108 net.cpp:356] pool1 -> pool1
I1026 20:58:41.612673 17108 net.cpp:96] Setting up pool1
I1026 20:58:41.612680 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:58:41.612687 17108 net.cpp:67] Creating Layer relu1
I1026 20:58:41.612691 17108 net.cpp:394] relu1 <- pool1
I1026 20:58:41.612696 17108 net.cpp:345] relu1 -> pool1 (in-place)
I1026 20:58:41.612702 17108 net.cpp:96] Setting up relu1
I1026 20:58:41.612707 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:58:41.612714 17108 net.cpp:67] Creating Layer drop1
I1026 20:58:41.612717 17108 net.cpp:394] drop1 <- pool1
I1026 20:58:41.612722 17108 net.cpp:345] drop1 -> pool1 (in-place)
I1026 20:58:41.612728 17108 net.cpp:96] Setting up drop1
I1026 20:58:41.612733 17108 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 20:58:41.612740 17108 net.cpp:67] Creating Layer conv2
I1026 20:58:41.612745 17108 net.cpp:394] conv2 <- pool1
I1026 20:58:41.612751 17108 net.cpp:356] conv2 -> conv2
I1026 20:58:41.612758 17108 net.cpp:96] Setting up conv2
I1026 20:58:41.613260 17108 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 20:58:41.613275 17108 net.cpp:67] Creating Layer pool2
I1026 20:58:41.613279 17108 net.cpp:394] pool2 <- conv2
I1026 20:58:41.613286 17108 net.cpp:356] pool2 -> pool2
I1026 20:58:41.613292 17108 net.cpp:96] Setting up pool2
I1026 20:58:41.613298 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:58:41.613304 17108 net.cpp:67] Creating Layer relu2
I1026 20:58:41.613308 17108 net.cpp:394] relu2 <- pool2
I1026 20:58:41.613313 17108 net.cpp:345] relu2 -> pool2 (in-place)
I1026 20:58:41.613319 17108 net.cpp:96] Setting up relu2
I1026 20:58:41.613323 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:58:41.613328 17108 net.cpp:67] Creating Layer drop2
I1026 20:58:41.613332 17108 net.cpp:394] drop2 <- pool2
I1026 20:58:41.613338 17108 net.cpp:345] drop2 -> pool2 (in-place)
I1026 20:58:41.613343 17108 net.cpp:96] Setting up drop2
I1026 20:58:41.613348 17108 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 20:58:41.613355 17108 net.cpp:67] Creating Layer conv3
I1026 20:58:41.613359 17108 net.cpp:394] conv3 <- pool2
I1026 20:58:41.613365 17108 net.cpp:356] conv3 -> conv3
I1026 20:58:41.613373 17108 net.cpp:96] Setting up conv3
I1026 20:58:41.614694 17108 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 20:58:41.614711 17108 net.cpp:67] Creating Layer pool3
I1026 20:58:41.614716 17108 net.cpp:394] pool3 <- conv3
I1026 20:58:41.614722 17108 net.cpp:356] pool3 -> pool3
I1026 20:58:41.614728 17108 net.cpp:96] Setting up pool3
I1026 20:58:41.614733 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:58:41.614739 17108 net.cpp:67] Creating Layer relu3
I1026 20:58:41.614743 17108 net.cpp:394] relu3 <- pool3
I1026 20:58:41.614748 17108 net.cpp:345] relu3 -> pool3 (in-place)
I1026 20:58:41.614754 17108 net.cpp:96] Setting up relu3
I1026 20:58:41.614759 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:58:41.614764 17108 net.cpp:67] Creating Layer drop3
I1026 20:58:41.614768 17108 net.cpp:394] drop3 <- pool3
I1026 20:58:41.614773 17108 net.cpp:345] drop3 -> pool3 (in-place)
I1026 20:58:41.614784 17108 net.cpp:96] Setting up drop3
I1026 20:58:41.614789 17108 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 20:58:41.614795 17108 net.cpp:67] Creating Layer ip1
I1026 20:58:41.614799 17108 net.cpp:394] ip1 <- pool3
I1026 20:58:41.614805 17108 net.cpp:356] ip1 -> ip1
I1026 20:58:41.614812 17108 net.cpp:96] Setting up ip1
I1026 20:58:42.035392 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:58:42.036087 17108 net.cpp:67] Creating Layer relu4
I1026 20:58:42.036110 17108 net.cpp:394] relu4 <- ip1
I1026 20:58:42.036131 17108 net.cpp:345] relu4 -> ip1 (in-place)
I1026 20:58:42.036149 17108 net.cpp:96] Setting up relu4
I1026 20:58:42.036160 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:58:42.036177 17108 net.cpp:67] Creating Layer drop4
I1026 20:58:42.036186 17108 net.cpp:394] drop4 <- ip1
I1026 20:58:42.036201 17108 net.cpp:345] drop4 -> ip1 (in-place)
I1026 20:58:42.036216 17108 net.cpp:96] Setting up drop4
I1026 20:58:42.036227 17108 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 20:58:42.036247 17108 net.cpp:67] Creating Layer ip2
I1026 20:58:42.036255 17108 net.cpp:394] ip2 <- ip1
I1026 20:58:42.036272 17108 net.cpp:356] ip2 -> ip2
I1026 20:58:42.036296 17108 net.cpp:96] Setting up ip2
I1026 20:58:42.051489 17108 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 20:58:42.051556 17108 net.cpp:67] Creating Layer prob
I1026 20:58:42.051565 17108 net.cpp:394] prob <- ip2
I1026 20:58:42.051575 17108 net.cpp:356] prob -> prob
I1026 20:58:42.051585 17108 net.cpp:96] Setting up prob
I1026 20:58:42.051594 17108 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 20:58:42.051599 17108 net.cpp:172] prob does not need backward computation.
I1026 20:58:42.051602 17108 net.cpp:172] ip2 does not need backward computation.
I1026 20:58:42.051607 17108 net.cpp:172] drop4 does not need backward computation.
I1026 20:58:42.051611 17108 net.cpp:172] relu4 does not need backward computation.
I1026 20:58:42.051615 17108 net.cpp:172] ip1 does not need backward computation.
I1026 20:58:42.051620 17108 net.cpp:172] drop3 does not need backward computation.
I1026 20:58:42.051623 17108 net.cpp:172] relu3 does not need backward computation.
I1026 20:58:42.051627 17108 net.cpp:172] pool3 does not need backward computation.
I1026 20:58:42.051631 17108 net.cpp:172] conv3 does not need backward computation.
I1026 20:58:42.051635 17108 net.cpp:172] drop2 does not need backward computation.
I1026 20:58:42.051640 17108 net.cpp:172] relu2 does not need backward computation.
I1026 20:58:42.051643 17108 net.cpp:172] pool2 does not need backward computation.
I1026 20:58:42.051647 17108 net.cpp:172] conv2 does not need backward computation.
I1026 20:58:42.051651 17108 net.cpp:172] drop1 does not need backward computation.
I1026 20:58:42.051656 17108 net.cpp:172] relu1 does not need backward computation.
I1026 20:58:42.051659 17108 net.cpp:172] pool1 does not need backward computation.
I1026 20:58:42.051663 17108 net.cpp:172] conv1 does not need backward computation.
I1026 20:58:42.051667 17108 net.cpp:208] This network produces output prob
I1026 20:58:42.051682 17108 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 20:58:42.051692 17108 net.cpp:219] Network initialization done.
I1026 20:58:42.051697 17108 net.cpp:220] Memory required for data: 1837200
I1026 21:31:18.139924 23841 convert_imageset.cpp:70] Shuffling data
I1026 21:31:18.869990 23841 convert_imageset.cpp:73] A total of 60000 images.
I1026 21:31:18.870069 23841 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1026 21:31:21.084909 23841 convert_imageset.cpp:177] Processed 1000 files.
E1026 21:31:23.218439 23841 convert_imageset.cpp:177] Processed 2000 files.
E1026 21:31:25.414381 23841 convert_imageset.cpp:177] Processed 3000 files.
E1026 21:31:27.501302 23841 convert_imageset.cpp:177] Processed 4000 files.
E1026 21:31:29.635654 23841 convert_imageset.cpp:177] Processed 5000 files.
E1026 21:31:31.630625 23841 convert_imageset.cpp:177] Processed 6000 files.
E1026 21:31:33.563668 23841 convert_imageset.cpp:177] Processed 7000 files.
E1026 21:31:35.550304 23841 convert_imageset.cpp:177] Processed 8000 files.
E1026 21:31:37.525706 23841 convert_imageset.cpp:177] Processed 9000 files.
E1026 21:31:39.473187 23841 convert_imageset.cpp:177] Processed 10000 files.
E1026 21:31:41.413892 23841 convert_imageset.cpp:177] Processed 11000 files.
E1026 21:31:43.149420 23841 convert_imageset.cpp:177] Processed 12000 files.
E1026 21:31:45.021134 23841 convert_imageset.cpp:177] Processed 13000 files.
E1026 21:31:46.773423 23841 convert_imageset.cpp:177] Processed 14000 files.
E1026 21:31:48.664975 23841 convert_imageset.cpp:177] Processed 15000 files.
E1026 21:31:50.489399 23841 convert_imageset.cpp:177] Processed 16000 files.
E1026 21:31:52.312230 23841 convert_imageset.cpp:177] Processed 17000 files.
E1026 21:31:54.194463 23841 convert_imageset.cpp:177] Processed 18000 files.
E1026 21:31:56.010759 23841 convert_imageset.cpp:177] Processed 19000 files.
E1026 21:31:57.845701 23841 convert_imageset.cpp:177] Processed 20000 files.
E1026 21:31:59.598248 23841 convert_imageset.cpp:177] Processed 21000 files.
E1026 21:32:01.348057 23841 convert_imageset.cpp:177] Processed 22000 files.
E1026 21:32:02.974447 23841 convert_imageset.cpp:177] Processed 23000 files.
E1026 21:32:04.765367 23841 convert_imageset.cpp:177] Processed 24000 files.
E1026 21:32:06.480474 23841 convert_imageset.cpp:177] Processed 25000 files.
E1026 21:32:08.123929 23841 convert_imageset.cpp:177] Processed 26000 files.
E1026 21:32:09.849850 23841 convert_imageset.cpp:177] Processed 27000 files.
E1026 21:32:11.514695 23841 convert_imageset.cpp:177] Processed 28000 files.
E1026 21:32:13.259088 23841 convert_imageset.cpp:177] Processed 29000 files.
E1026 21:32:14.837365 23841 convert_imageset.cpp:177] Processed 30000 files.
E1026 21:32:16.466647 23841 convert_imageset.cpp:177] Processed 31000 files.
E1026 21:32:18.180965 23841 convert_imageset.cpp:177] Processed 32000 files.
E1026 21:32:19.892071 23841 convert_imageset.cpp:177] Processed 33000 files.
E1026 21:32:21.567687 23841 convert_imageset.cpp:177] Processed 34000 files.
E1026 21:32:23.222442 23841 convert_imageset.cpp:177] Processed 35000 files.
E1026 21:32:24.868981 23841 convert_imageset.cpp:177] Processed 36000 files.
E1026 21:32:26.536492 23841 convert_imageset.cpp:177] Processed 37000 files.
E1026 21:32:28.187115 23841 convert_imageset.cpp:177] Processed 38000 files.
E1026 21:32:29.677319 23841 convert_imageset.cpp:177] Processed 39000 files.
E1026 21:32:31.310998 23841 convert_imageset.cpp:177] Processed 40000 files.
E1026 21:32:32.962764 23841 convert_imageset.cpp:177] Processed 41000 files.
E1026 21:32:34.580088 23841 convert_imageset.cpp:177] Processed 42000 files.
E1026 21:32:36.210075 23841 convert_imageset.cpp:177] Processed 43000 files.
E1026 21:32:37.882093 23841 convert_imageset.cpp:177] Processed 44000 files.
E1026 21:32:39.444377 23841 convert_imageset.cpp:177] Processed 45000 files.
E1026 21:32:41.037576 23841 convert_imageset.cpp:177] Processed 46000 files.
E1026 21:32:42.713532 23841 convert_imageset.cpp:177] Processed 47000 files.
E1026 21:32:44.289710 23841 convert_imageset.cpp:177] Processed 48000 files.
E1026 21:32:45.960993 23841 convert_imageset.cpp:177] Processed 49000 files.
E1026 21:32:47.612458 23841 convert_imageset.cpp:177] Processed 50000 files.
E1026 21:32:49.220923 23841 convert_imageset.cpp:177] Processed 51000 files.
E1026 21:32:50.860924 23841 convert_imageset.cpp:177] Processed 52000 files.
E1026 21:32:52.342653 23841 convert_imageset.cpp:177] Processed 53000 files.
E1026 21:32:53.807837 23841 convert_imageset.cpp:177] Processed 54000 files.
E1026 21:32:55.384434 23841 convert_imageset.cpp:177] Processed 55000 files.
E1026 21:32:56.924844 23841 convert_imageset.cpp:177] Processed 56000 files.
E1026 21:32:58.454924 23841 convert_imageset.cpp:177] Processed 57000 files.
E1026 21:33:00.030833 23841 convert_imageset.cpp:177] Processed 58000 files.
E1026 21:33:01.683883 23841 convert_imageset.cpp:177] Processed 59000 files.
E1026 21:33:03.322366 23841 convert_imageset.cpp:177] Processed 60000 files.
I1026 21:33:03.480072 23909 caffe.cpp:99] Use GPU with device ID 0
I1026 21:33:03.821384 23909 caffe.cpp:107] Starting Optimization
I1026 21:33:03.821506 23909 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 175000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1026 21:33:03.821530 23909 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1026 21:33:03.824507 23909 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1026 21:33:03.824720 23909 net.cpp:67] Creating Layer mnist
I1026 21:33:03.824745 23909 net.cpp:356] mnist -> data
I1026 21:33:03.824781 23909 net.cpp:356] mnist -> label
I1026 21:33:03.824813 23909 net.cpp:96] Setting up mnist
I1026 21:33:03.832571 23909 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1026 21:33:03.832684 23909 data_layer.cpp:128] output data size: 64,1,50,180
I1026 21:33:03.834089 23909 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1026 21:33:03.834120 23909 net.cpp:103] Top shape: 64 1 1 1 (64)
I1026 21:33:03.834146 23909 net.cpp:67] Creating Layer conv1
I1026 21:33:03.834158 23909 net.cpp:394] conv1 <- data
I1026 21:33:03.834193 23909 net.cpp:356] conv1 -> conv1
I1026 21:33:03.834216 23909 net.cpp:96] Setting up conv1
I1026 21:33:03.834988 23909 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1026 21:33:03.835047 23909 net.cpp:67] Creating Layer pool1
I1026 21:33:03.835059 23909 net.cpp:394] pool1 <- conv1
I1026 21:33:03.835074 23909 net.cpp:356] pool1 -> pool1
I1026 21:33:03.835090 23909 net.cpp:96] Setting up pool1
I1026 21:33:03.835119 23909 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 21:33:03.835134 23909 net.cpp:67] Creating Layer relu1
I1026 21:33:03.835144 23909 net.cpp:394] relu1 <- pool1
I1026 21:33:03.835161 23909 net.cpp:345] relu1 -> pool1 (in-place)
I1026 21:33:03.835176 23909 net.cpp:96] Setting up relu1
I1026 21:33:03.835186 23909 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 21:33:03.835202 23909 net.cpp:67] Creating Layer drop1
I1026 21:33:03.835212 23909 net.cpp:394] drop1 <- pool1
I1026 21:33:03.835225 23909 net.cpp:345] drop1 -> pool1 (in-place)
I1026 21:33:03.835239 23909 net.cpp:96] Setting up drop1
I1026 21:33:03.835252 23909 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1026 21:33:03.835269 23909 net.cpp:67] Creating Layer conv2
I1026 21:33:03.835280 23909 net.cpp:394] conv2 <- pool1
I1026 21:33:03.835294 23909 net.cpp:356] conv2 -> conv2
I1026 21:33:03.835311 23909 net.cpp:96] Setting up conv2
I1026 21:33:03.836613 23909 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1026 21:33:03.836648 23909 net.cpp:67] Creating Layer pool2
I1026 21:33:03.836660 23909 net.cpp:394] pool2 <- conv2
I1026 21:33:03.836678 23909 net.cpp:356] pool2 -> pool2
I1026 21:33:03.836694 23909 net.cpp:96] Setting up pool2
I1026 21:33:03.836707 23909 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 21:33:03.836721 23909 net.cpp:67] Creating Layer relu2
I1026 21:33:03.836730 23909 net.cpp:394] relu2 <- pool2
I1026 21:33:03.836745 23909 net.cpp:345] relu2 -> pool2 (in-place)
I1026 21:33:03.836757 23909 net.cpp:96] Setting up relu2
I1026 21:33:03.836767 23909 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 21:33:03.836786 23909 net.cpp:67] Creating Layer drop2
I1026 21:33:03.836797 23909 net.cpp:394] drop2 <- pool2
I1026 21:33:03.836810 23909 net.cpp:345] drop2 -> pool2 (in-place)
I1026 21:33:03.836824 23909 net.cpp:96] Setting up drop2
I1026 21:33:03.836835 23909 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1026 21:33:03.836851 23909 net.cpp:67] Creating Layer conv3
I1026 21:33:03.836861 23909 net.cpp:394] conv3 <- pool2
I1026 21:33:03.836879 23909 net.cpp:356] conv3 -> conv3
I1026 21:33:03.836895 23909 net.cpp:96] Setting up conv3
I1026 21:33:03.840219 23909 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1026 21:33:03.840265 23909 net.cpp:67] Creating Layer pool3
I1026 21:33:03.840277 23909 net.cpp:394] pool3 <- conv3
I1026 21:33:03.840292 23909 net.cpp:356] pool3 -> pool3
I1026 21:33:03.840308 23909 net.cpp:96] Setting up pool3
I1026 21:33:03.840324 23909 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 21:33:03.840338 23909 net.cpp:67] Creating Layer relu3
I1026 21:33:03.840348 23909 net.cpp:394] relu3 <- pool3
I1026 21:33:03.840360 23909 net.cpp:345] relu3 -> pool3 (in-place)
I1026 21:33:03.840374 23909 net.cpp:96] Setting up relu3
I1026 21:33:03.840384 23909 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 21:33:03.840397 23909 net.cpp:67] Creating Layer drop3
I1026 21:33:03.840407 23909 net.cpp:394] drop3 <- pool3
I1026 21:33:03.840474 23909 net.cpp:345] drop3 -> pool3 (in-place)
I1026 21:33:03.840500 23909 net.cpp:96] Setting up drop3
I1026 21:33:03.840512 23909 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1026 21:33:03.840528 23909 net.cpp:67] Creating Layer ip1
I1026 21:33:03.840539 23909 net.cpp:394] ip1 <- pool3
I1026 21:33:03.840546 23909 net.cpp:356] ip1 -> ip1
I1026 21:33:03.840584 23909 net.cpp:96] Setting up ip1
I1026 21:33:04.263566 23909 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 21:33:04.263628 23909 net.cpp:67] Creating Layer relu4
I1026 21:33:04.263635 23909 net.cpp:394] relu4 <- ip1
I1026 21:33:04.263645 23909 net.cpp:345] relu4 -> ip1 (in-place)
I1026 21:33:04.263654 23909 net.cpp:96] Setting up relu4
I1026 21:33:04.263659 23909 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 21:33:04.263669 23909 net.cpp:67] Creating Layer drop4
I1026 21:33:04.263674 23909 net.cpp:394] drop4 <- ip1
I1026 21:33:04.263680 23909 net.cpp:345] drop4 -> ip1 (in-place)
I1026 21:33:04.263687 23909 net.cpp:96] Setting up drop4
I1026 21:33:04.263692 23909 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1026 21:33:04.263705 23909 net.cpp:67] Creating Layer ip2
I1026 21:33:04.263710 23909 net.cpp:394] ip2 <- ip1
I1026 21:33:04.263717 23909 net.cpp:356] ip2 -> ip2
I1026 21:33:04.263725 23909 net.cpp:96] Setting up ip2
I1026 21:33:04.272850 23909 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1026 21:33:04.272912 23909 net.cpp:67] Creating Layer loss
I1026 21:33:04.272918 23909 net.cpp:394] loss <- ip2
I1026 21:33:04.272927 23909 net.cpp:394] loss <- label
I1026 21:33:04.272933 23909 net.cpp:356] loss -> loss
I1026 21:33:04.272943 23909 net.cpp:96] Setting up loss
I1026 21:33:04.272955 23909 net.cpp:103] Top shape: 1 1 1 1 (1)
I1026 21:33:04.272960 23909 net.cpp:109]     with loss weight 1
I1026 21:33:04.272994 23909 net.cpp:170] loss needs backward computation.
I1026 21:33:04.273000 23909 net.cpp:170] ip2 needs backward computation.
I1026 21:33:04.273005 23909 net.cpp:170] drop4 needs backward computation.
I1026 21:33:04.273008 23909 net.cpp:170] relu4 needs backward computation.
I1026 21:33:04.273013 23909 net.cpp:170] ip1 needs backward computation.
I1026 21:33:04.273017 23909 net.cpp:170] drop3 needs backward computation.
I1026 21:33:04.273021 23909 net.cpp:170] relu3 needs backward computation.
I1026 21:33:04.273025 23909 net.cpp:170] pool3 needs backward computation.
I1026 21:33:04.273030 23909 net.cpp:170] conv3 needs backward computation.
I1026 21:33:04.273035 23909 net.cpp:170] drop2 needs backward computation.
I1026 21:33:04.273039 23909 net.cpp:170] relu2 needs backward computation.
I1026 21:33:04.273043 23909 net.cpp:170] pool2 needs backward computation.
I1026 21:33:04.273047 23909 net.cpp:170] conv2 needs backward computation.
I1026 21:33:04.273052 23909 net.cpp:170] drop1 needs backward computation.
I1026 21:33:04.273056 23909 net.cpp:170] relu1 needs backward computation.
I1026 21:33:04.273061 23909 net.cpp:170] pool1 needs backward computation.
I1026 21:33:04.273066 23909 net.cpp:170] conv1 needs backward computation.
I1026 21:33:04.273069 23909 net.cpp:172] mnist does not need backward computation.
I1026 21:33:04.273074 23909 net.cpp:208] This network produces output loss
I1026 21:33:04.273087 23909 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 21:33:04.273095 23909 net.cpp:219] Network initialization done.
I1026 21:33:04.273099 23909 net.cpp:220] Memory required for data: 119788292
I1026 21:33:04.273160 23909 solver.cpp:41] Solver scaffolding done.
I1026 21:33:04.273166 23909 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_150000.solverstate
I1026 21:33:04.273171 23909 solver.cpp:160] Solving Captcha
I1026 21:33:04.273190 23909 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_150000.solverstate
I1026 21:33:08.493736 23909 solver.cpp:502] SGDSolver: restoring history
I1026 21:33:09.296495 23909 solver.cpp:191] Iteration 150000, loss = 2.58587
I1026 21:33:09.296550 23909 solver.cpp:206]     Train net output #0: loss = 2.58587 (* 1 = 2.58587 loss)
I1026 21:33:09.296566 23909 solver.cpp:403] Iteration 150000, lr = 0.00125
I1026 21:37:11.175904 23909 solver.cpp:191] Iteration 151000, loss = 2.64886
I1026 21:37:11.176556 23909 solver.cpp:206]     Train net output #0: loss = 2.64886 (* 1 = 2.64886 loss)
I1026 21:37:11.176594 23909 solver.cpp:403] Iteration 151000, lr = 0.00124417
I1026 21:41:12.399065 23909 solver.cpp:191] Iteration 152000, loss = 2.57009
I1026 21:41:12.399682 23909 solver.cpp:206]     Train net output #0: loss = 2.57009 (* 1 = 2.57009 loss)
I1026 21:41:12.399715 23909 solver.cpp:403] Iteration 152000, lr = 0.00123841
I1026 21:45:13.726769 23909 solver.cpp:191] Iteration 153000, loss = 2.43566
I1026 21:45:13.727540 23909 solver.cpp:206]     Train net output #0: loss = 2.43566 (* 1 = 2.43566 loss)
I1026 21:45:13.727571 23909 solver.cpp:403] Iteration 153000, lr = 0.00123271
I1026 21:49:14.962193 23909 solver.cpp:191] Iteration 154000, loss = 2.79401
I1026 21:49:14.962882 23909 solver.cpp:206]     Train net output #0: loss = 2.79401 (* 1 = 2.79401 loss)
I1026 21:49:14.962914 23909 solver.cpp:403] Iteration 154000, lr = 0.00122706
I1026 21:53:16.764664 23909 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_155000.caffemodel
I1026 21:53:21.105864 23909 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_155000.solverstate
I1026 21:53:24.638284 23909 solver.cpp:191] Iteration 155000, loss = 2.28344
I1026 21:53:24.638769 23909 solver.cpp:206]     Train net output #0: loss = 2.28344 (* 1 = 2.28344 loss)
I1026 21:53:24.638797 23909 solver.cpp:403] Iteration 155000, lr = 0.00122148
I1026 21:57:26.116128 23909 solver.cpp:191] Iteration 156000, loss = 2.59752
I1026 21:57:26.116689 23909 solver.cpp:206]     Train net output #0: loss = 2.59752 (* 1 = 2.59752 loss)
I1026 21:57:26.116715 23909 solver.cpp:403] Iteration 156000, lr = 0.00121596
I1026 22:01:27.345921 23909 solver.cpp:191] Iteration 157000, loss = 2.42984
I1026 22:01:27.346650 23909 solver.cpp:206]     Train net output #0: loss = 2.42984 (* 1 = 2.42984 loss)
I1026 22:01:27.346664 23909 solver.cpp:403] Iteration 157000, lr = 0.00121049
I1026 22:05:28.584123 23909 solver.cpp:191] Iteration 158000, loss = 2.41604
I1026 22:05:28.584787 23909 solver.cpp:206]     Train net output #0: loss = 2.41604 (* 1 = 2.41604 loss)
I1026 22:05:28.584820 23909 solver.cpp:403] Iteration 158000, lr = 0.00120509
I1026 22:09:29.859413 23909 solver.cpp:191] Iteration 159000, loss = 2.51545
I1026 22:09:29.860093 23909 solver.cpp:206]     Train net output #0: loss = 2.51545 (* 1 = 2.51545 loss)
I1026 22:09:29.860126 23909 solver.cpp:403] Iteration 159000, lr = 0.00119973
I1026 22:13:31.704002 23909 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_160000.caffemodel
I1026 22:13:36.108664 23909 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_160000.solverstate
I1026 22:13:39.807970 23909 solver.cpp:191] Iteration 160000, loss = 2.48161
I1026 22:13:39.808650 23909 solver.cpp:206]     Train net output #0: loss = 2.48161 (* 1 = 2.48161 loss)
I1026 22:13:39.808691 23909 solver.cpp:403] Iteration 160000, lr = 0.00119444
I1026 22:17:41.092546 23909 solver.cpp:191] Iteration 161000, loss = 2.43102
I1026 22:17:41.093152 23909 solver.cpp:206]     Train net output #0: loss = 2.43102 (* 1 = 2.43102 loss)
I1026 22:17:41.093184 23909 solver.cpp:403] Iteration 161000, lr = 0.00118919
I1026 22:21:42.577121 23909 solver.cpp:191] Iteration 162000, loss = 2.45797
I1026 22:21:42.577760 23909 solver.cpp:206]     Train net output #0: loss = 2.45797 (* 1 = 2.45797 loss)
I1026 22:21:42.577795 23909 solver.cpp:403] Iteration 162000, lr = 0.00118401
I1026 22:25:43.957455 23909 solver.cpp:191] Iteration 163000, loss = 2.36278
I1026 22:25:43.958070 23909 solver.cpp:206]     Train net output #0: loss = 2.36278 (* 1 = 2.36278 loss)
I1026 22:25:43.958103 23909 solver.cpp:403] Iteration 163000, lr = 0.00117887
I1026 22:29:45.159250 23909 solver.cpp:191] Iteration 164000, loss = 2.48137
I1026 22:29:45.159999 23909 solver.cpp:206]     Train net output #0: loss = 2.48137 (* 1 = 2.48137 loss)
I1026 22:29:45.160044 23909 solver.cpp:403] Iteration 164000, lr = 0.00117378
I1026 22:33:46.823112 23909 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_165000.caffemodel
I1026 22:33:51.245792 23909 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_165000.solverstate
I1026 22:33:55.229373 23909 solver.cpp:191] Iteration 165000, loss = 2.41321
I1026 22:33:55.229856 23909 solver.cpp:206]     Train net output #0: loss = 2.41321 (* 1 = 2.41321 loss)
I1026 22:33:55.229892 23909 solver.cpp:403] Iteration 165000, lr = 0.00116875
I1026 22:37:56.495100 23909 solver.cpp:191] Iteration 166000, loss = 2.31907
I1026 22:37:56.495692 23909 solver.cpp:206]     Train net output #0: loss = 2.31907 (* 1 = 2.31907 loss)
I1026 22:37:56.495723 23909 solver.cpp:403] Iteration 166000, lr = 0.00116377
I1026 22:41:57.702677 23909 solver.cpp:191] Iteration 167000, loss = 2.39601
I1026 22:41:57.703279 23909 solver.cpp:206]     Train net output #0: loss = 2.39601 (* 1 = 2.39601 loss)
I1026 22:41:57.703311 23909 solver.cpp:403] Iteration 167000, lr = 0.00115883
I1026 22:45:58.955174 23909 solver.cpp:191] Iteration 168000, loss = 2.3449
I1026 22:45:58.955976 23909 solver.cpp:206]     Train net output #0: loss = 2.3449 (* 1 = 2.3449 loss)
I1026 22:45:58.956009 23909 solver.cpp:403] Iteration 168000, lr = 0.00115394
I1026 22:50:00.137301 23909 solver.cpp:191] Iteration 169000, loss = 2.47481
I1026 22:50:00.137912 23909 solver.cpp:206]     Train net output #0: loss = 2.47481 (* 1 = 2.47481 loss)
I1026 22:50:00.137944 23909 solver.cpp:403] Iteration 169000, lr = 0.00114911
I1026 22:54:01.806490 23909 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_170000.caffemodel
I1026 22:54:05.981266 23909 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_170000.solverstate
I1026 22:54:09.720190 23909 solver.cpp:191] Iteration 170000, loss = 2.42915
I1026 22:54:09.720664 23909 solver.cpp:206]     Train net output #0: loss = 2.42915 (* 1 = 2.42915 loss)
I1026 22:54:09.720696 23909 solver.cpp:403] Iteration 170000, lr = 0.00114431
I1026 22:58:10.932025 23909 solver.cpp:191] Iteration 171000, loss = 2.48413
I1026 22:58:10.932756 23909 solver.cpp:206]     Train net output #0: loss = 2.48413 (* 1 = 2.48413 loss)
I1026 22:58:10.932788 23909 solver.cpp:403] Iteration 171000, lr = 0.00113957
I1026 23:02:12.135046 23909 solver.cpp:191] Iteration 172000, loss = 2.34242
I1026 23:02:12.135843 23909 solver.cpp:206]     Train net output #0: loss = 2.34242 (* 1 = 2.34242 loss)
I1026 23:02:12.135877 23909 solver.cpp:403] Iteration 172000, lr = 0.00113487
I1026 23:06:13.366322 23909 solver.cpp:191] Iteration 173000, loss = 2.4386
I1026 23:06:13.367087 23909 solver.cpp:206]     Train net output #0: loss = 2.4386 (* 1 = 2.4386 loss)
I1026 23:06:13.367120 23909 solver.cpp:403] Iteration 173000, lr = 0.00113022
I1026 23:10:14.632311 23909 solver.cpp:191] Iteration 174000, loss = 2.32563
I1026 23:10:14.633431 23909 solver.cpp:206]     Train net output #0: loss = 2.32563 (* 1 = 2.32563 loss)
I1026 23:10:14.633463 23909 solver.cpp:403] Iteration 174000, lr = 0.00112561
I1026 23:14:16.248644 23909 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_175000.caffemodel
I1026 23:14:20.627261 23909 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_175000.solverstate
I1026 23:14:24.296550 23909 solver.cpp:228] Iteration 175000, loss = 2.36685
I1026 23:14:24.296996 23909 solver.cpp:233] Optimization Done.
I1026 23:14:24.297022 23909 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1026 23:39:44.853751 20311 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 23:39:44.853850 20311 net.cpp:358] Input 0 -> data
I1026 23:39:44.853875 20311 net.cpp:67] Creating Layer conv1
I1026 23:39:44.853880 20311 net.cpp:394] conv1 <- data
I1026 23:39:44.853888 20311 net.cpp:356] conv1 -> conv1
I1026 23:39:44.853896 20311 net.cpp:96] Setting up conv1
I1026 23:39:44.854212 20311 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 23:39:44.854230 20311 net.cpp:67] Creating Layer pool1
I1026 23:39:44.854235 20311 net.cpp:394] pool1 <- conv1
I1026 23:39:44.854240 20311 net.cpp:356] pool1 -> pool1
I1026 23:39:44.854249 20311 net.cpp:96] Setting up pool1
I1026 23:39:44.854264 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:39:44.854271 20311 net.cpp:67] Creating Layer relu1
I1026 23:39:44.854275 20311 net.cpp:394] relu1 <- pool1
I1026 23:39:44.854280 20311 net.cpp:345] relu1 -> pool1 (in-place)
I1026 23:39:44.854286 20311 net.cpp:96] Setting up relu1
I1026 23:39:44.854290 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:39:44.854298 20311 net.cpp:67] Creating Layer drop1
I1026 23:39:44.854303 20311 net.cpp:394] drop1 <- pool1
I1026 23:39:44.854308 20311 net.cpp:345] drop1 -> pool1 (in-place)
I1026 23:39:44.854315 20311 net.cpp:96] Setting up drop1
I1026 23:39:44.854320 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:39:44.854326 20311 net.cpp:67] Creating Layer conv2
I1026 23:39:44.854331 20311 net.cpp:394] conv2 <- pool1
I1026 23:39:44.854336 20311 net.cpp:356] conv2 -> conv2
I1026 23:39:44.854346 20311 net.cpp:96] Setting up conv2
I1026 23:39:44.854905 20311 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 23:39:44.854920 20311 net.cpp:67] Creating Layer pool2
I1026 23:39:44.854924 20311 net.cpp:394] pool2 <- conv2
I1026 23:39:44.854930 20311 net.cpp:356] pool2 -> pool2
I1026 23:39:44.854936 20311 net.cpp:96] Setting up pool2
I1026 23:39:44.854943 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:39:44.854950 20311 net.cpp:67] Creating Layer relu2
I1026 23:39:44.854954 20311 net.cpp:394] relu2 <- pool2
I1026 23:39:44.854959 20311 net.cpp:345] relu2 -> pool2 (in-place)
I1026 23:39:44.854965 20311 net.cpp:96] Setting up relu2
I1026 23:39:44.854969 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:39:44.854974 20311 net.cpp:67] Creating Layer drop2
I1026 23:39:44.854979 20311 net.cpp:394] drop2 <- pool2
I1026 23:39:44.854985 20311 net.cpp:345] drop2 -> pool2 (in-place)
I1026 23:39:44.854990 20311 net.cpp:96] Setting up drop2
I1026 23:39:44.854995 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:39:44.855001 20311 net.cpp:67] Creating Layer conv3
I1026 23:39:44.855005 20311 net.cpp:394] conv3 <- pool2
I1026 23:39:44.855013 20311 net.cpp:356] conv3 -> conv3
I1026 23:39:44.855020 20311 net.cpp:96] Setting up conv3
I1026 23:39:44.856524 20311 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 23:39:44.856546 20311 net.cpp:67] Creating Layer pool3
I1026 23:39:44.856551 20311 net.cpp:394] pool3 <- conv3
I1026 23:39:44.856557 20311 net.cpp:356] pool3 -> pool3
I1026 23:39:44.856564 20311 net.cpp:96] Setting up pool3
I1026 23:39:44.856570 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:39:44.856578 20311 net.cpp:67] Creating Layer relu3
I1026 23:39:44.856582 20311 net.cpp:394] relu3 <- pool3
I1026 23:39:44.856587 20311 net.cpp:345] relu3 -> pool3 (in-place)
I1026 23:39:44.856592 20311 net.cpp:96] Setting up relu3
I1026 23:39:44.856596 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:39:44.856602 20311 net.cpp:67] Creating Layer drop3
I1026 23:39:44.856606 20311 net.cpp:394] drop3 <- pool3
I1026 23:39:44.856611 20311 net.cpp:345] drop3 -> pool3 (in-place)
I1026 23:39:44.856616 20311 net.cpp:96] Setting up drop3
I1026 23:39:44.856621 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:39:44.856628 20311 net.cpp:67] Creating Layer ip1
I1026 23:39:44.856633 20311 net.cpp:394] ip1 <- pool3
I1026 23:39:44.856638 20311 net.cpp:356] ip1 -> ip1
I1026 23:39:44.856645 20311 net.cpp:96] Setting up ip1
I1026 23:39:45.373419 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:39:45.373478 20311 net.cpp:67] Creating Layer relu4
I1026 23:39:45.373486 20311 net.cpp:394] relu4 <- ip1
I1026 23:39:45.373494 20311 net.cpp:345] relu4 -> ip1 (in-place)
I1026 23:39:45.373504 20311 net.cpp:96] Setting up relu4
I1026 23:39:45.373509 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:39:45.373517 20311 net.cpp:67] Creating Layer drop4
I1026 23:39:45.373520 20311 net.cpp:394] drop4 <- ip1
I1026 23:39:45.373529 20311 net.cpp:345] drop4 -> ip1 (in-place)
I1026 23:39:45.373536 20311 net.cpp:96] Setting up drop4
I1026 23:39:45.373541 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:39:45.373549 20311 net.cpp:67] Creating Layer ip2
I1026 23:39:45.373553 20311 net.cpp:394] ip2 <- ip1
I1026 23:39:45.373561 20311 net.cpp:356] ip2 -> ip2
I1026 23:39:45.373572 20311 net.cpp:96] Setting up ip2
I1026 23:39:45.383329 20311 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 23:39:45.383406 20311 net.cpp:67] Creating Layer prob
I1026 23:39:45.383414 20311 net.cpp:394] prob <- ip2
I1026 23:39:45.383422 20311 net.cpp:356] prob -> prob
I1026 23:39:45.383433 20311 net.cpp:96] Setting up prob
I1026 23:39:45.383442 20311 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 23:39:45.383447 20311 net.cpp:172] prob does not need backward computation.
I1026 23:39:45.383451 20311 net.cpp:172] ip2 does not need backward computation.
I1026 23:39:45.383455 20311 net.cpp:172] drop4 does not need backward computation.
I1026 23:39:45.383460 20311 net.cpp:172] relu4 does not need backward computation.
I1026 23:39:45.383469 20311 net.cpp:172] ip1 does not need backward computation.
I1026 23:39:45.383473 20311 net.cpp:172] drop3 does not need backward computation.
I1026 23:39:45.383477 20311 net.cpp:172] relu3 does not need backward computation.
I1026 23:39:45.383481 20311 net.cpp:172] pool3 does not need backward computation.
I1026 23:39:45.383484 20311 net.cpp:172] conv3 does not need backward computation.
I1026 23:39:45.383488 20311 net.cpp:172] drop2 does not need backward computation.
I1026 23:39:45.383492 20311 net.cpp:172] relu2 does not need backward computation.
I1026 23:39:45.383496 20311 net.cpp:172] pool2 does not need backward computation.
I1026 23:39:45.383499 20311 net.cpp:172] conv2 does not need backward computation.
I1026 23:39:45.383503 20311 net.cpp:172] drop1 does not need backward computation.
I1026 23:39:45.383507 20311 net.cpp:172] relu1 does not need backward computation.
I1026 23:39:45.383510 20311 net.cpp:172] pool1 does not need backward computation.
I1026 23:39:45.383514 20311 net.cpp:172] conv1 does not need backward computation.
I1026 23:39:45.383518 20311 net.cpp:208] This network produces output prob
I1026 23:39:45.383535 20311 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 23:39:45.383548 20311 net.cpp:219] Network initialization done.
I1026 23:39:45.383553 20311 net.cpp:220] Memory required for data: 1837200
I1026 23:40:29.454979 20311 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 23:40:29.455505 20311 net.cpp:358] Input 0 -> data
I1026 23:40:29.455538 20311 net.cpp:67] Creating Layer conv1
I1026 23:40:29.455543 20311 net.cpp:394] conv1 <- data
I1026 23:40:29.455550 20311 net.cpp:356] conv1 -> conv1
I1026 23:40:29.455561 20311 net.cpp:96] Setting up conv1
I1026 23:40:29.455592 20311 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 23:40:29.455608 20311 net.cpp:67] Creating Layer pool1
I1026 23:40:29.455612 20311 net.cpp:394] pool1 <- conv1
I1026 23:40:29.455618 20311 net.cpp:356] pool1 -> pool1
I1026 23:40:29.455626 20311 net.cpp:96] Setting up pool1
I1026 23:40:29.455634 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:40:29.455641 20311 net.cpp:67] Creating Layer relu1
I1026 23:40:29.455644 20311 net.cpp:394] relu1 <- pool1
I1026 23:40:29.455651 20311 net.cpp:345] relu1 -> pool1 (in-place)
I1026 23:40:29.455657 20311 net.cpp:96] Setting up relu1
I1026 23:40:29.455660 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:40:29.455667 20311 net.cpp:67] Creating Layer drop1
I1026 23:40:29.455670 20311 net.cpp:394] drop1 <- pool1
I1026 23:40:29.455677 20311 net.cpp:345] drop1 -> pool1 (in-place)
I1026 23:40:29.455682 20311 net.cpp:96] Setting up drop1
I1026 23:40:29.455687 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:40:29.455694 20311 net.cpp:67] Creating Layer conv2
I1026 23:40:29.455698 20311 net.cpp:394] conv2 <- pool1
I1026 23:40:29.455704 20311 net.cpp:356] conv2 -> conv2
I1026 23:40:29.455711 20311 net.cpp:96] Setting up conv2
I1026 23:40:29.456217 20311 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 23:40:29.456230 20311 net.cpp:67] Creating Layer pool2
I1026 23:40:29.456235 20311 net.cpp:394] pool2 <- conv2
I1026 23:40:29.456241 20311 net.cpp:356] pool2 -> pool2
I1026 23:40:29.456248 20311 net.cpp:96] Setting up pool2
I1026 23:40:29.456255 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:40:29.456260 20311 net.cpp:67] Creating Layer relu2
I1026 23:40:29.456264 20311 net.cpp:394] relu2 <- pool2
I1026 23:40:29.456269 20311 net.cpp:345] relu2 -> pool2 (in-place)
I1026 23:40:29.456275 20311 net.cpp:96] Setting up relu2
I1026 23:40:29.456279 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:40:29.456284 20311 net.cpp:67] Creating Layer drop2
I1026 23:40:29.456289 20311 net.cpp:394] drop2 <- pool2
I1026 23:40:29.456295 20311 net.cpp:345] drop2 -> pool2 (in-place)
I1026 23:40:29.456300 20311 net.cpp:96] Setting up drop2
I1026 23:40:29.456305 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:40:29.456311 20311 net.cpp:67] Creating Layer conv3
I1026 23:40:29.456315 20311 net.cpp:394] conv3 <- pool2
I1026 23:40:29.456322 20311 net.cpp:356] conv3 -> conv3
I1026 23:40:29.456329 20311 net.cpp:96] Setting up conv3
I1026 23:40:29.457687 20311 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 23:40:29.457705 20311 net.cpp:67] Creating Layer pool3
I1026 23:40:29.457710 20311 net.cpp:394] pool3 <- conv3
I1026 23:40:29.457715 20311 net.cpp:356] pool3 -> pool3
I1026 23:40:29.457722 20311 net.cpp:96] Setting up pool3
I1026 23:40:29.457728 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:40:29.457733 20311 net.cpp:67] Creating Layer relu3
I1026 23:40:29.457737 20311 net.cpp:394] relu3 <- pool3
I1026 23:40:29.457743 20311 net.cpp:345] relu3 -> pool3 (in-place)
I1026 23:40:29.457748 20311 net.cpp:96] Setting up relu3
I1026 23:40:29.457753 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:40:29.457758 20311 net.cpp:67] Creating Layer drop3
I1026 23:40:29.457762 20311 net.cpp:394] drop3 <- pool3
I1026 23:40:29.457768 20311 net.cpp:345] drop3 -> pool3 (in-place)
I1026 23:40:29.457773 20311 net.cpp:96] Setting up drop3
I1026 23:40:29.457778 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:40:29.457784 20311 net.cpp:67] Creating Layer ip1
I1026 23:40:29.457789 20311 net.cpp:394] ip1 <- pool3
I1026 23:40:29.457795 20311 net.cpp:356] ip1 -> ip1
I1026 23:40:29.457803 20311 net.cpp:96] Setting up ip1
I1026 23:40:29.866756 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:40:29.866829 20311 net.cpp:67] Creating Layer relu4
I1026 23:40:29.866838 20311 net.cpp:394] relu4 <- ip1
I1026 23:40:29.866849 20311 net.cpp:345] relu4 -> ip1 (in-place)
I1026 23:40:29.866858 20311 net.cpp:96] Setting up relu4
I1026 23:40:29.866864 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:40:29.866871 20311 net.cpp:67] Creating Layer drop4
I1026 23:40:29.866876 20311 net.cpp:394] drop4 <- ip1
I1026 23:40:29.866883 20311 net.cpp:345] drop4 -> ip1 (in-place)
I1026 23:40:29.866889 20311 net.cpp:96] Setting up drop4
I1026 23:40:29.866895 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:40:29.866906 20311 net.cpp:67] Creating Layer ip2
I1026 23:40:29.866910 20311 net.cpp:394] ip2 <- ip1
I1026 23:40:29.866919 20311 net.cpp:356] ip2 -> ip2
I1026 23:40:29.866932 20311 net.cpp:96] Setting up ip2
I1026 23:40:29.874546 20311 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 23:40:29.874611 20311 net.cpp:67] Creating Layer prob
I1026 23:40:29.874619 20311 net.cpp:394] prob <- ip2
I1026 23:40:29.874629 20311 net.cpp:356] prob -> prob
I1026 23:40:29.874640 20311 net.cpp:96] Setting up prob
I1026 23:40:29.874649 20311 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 23:40:29.874653 20311 net.cpp:172] prob does not need backward computation.
I1026 23:40:29.874657 20311 net.cpp:172] ip2 does not need backward computation.
I1026 23:40:29.874661 20311 net.cpp:172] drop4 does not need backward computation.
I1026 23:40:29.874665 20311 net.cpp:172] relu4 does not need backward computation.
I1026 23:40:29.874670 20311 net.cpp:172] ip1 does not need backward computation.
I1026 23:40:29.874672 20311 net.cpp:172] drop3 does not need backward computation.
I1026 23:40:29.874676 20311 net.cpp:172] relu3 does not need backward computation.
I1026 23:40:29.874680 20311 net.cpp:172] pool3 does not need backward computation.
I1026 23:40:29.874685 20311 net.cpp:172] conv3 does not need backward computation.
I1026 23:40:29.874688 20311 net.cpp:172] drop2 does not need backward computation.
I1026 23:40:29.874692 20311 net.cpp:172] relu2 does not need backward computation.
I1026 23:40:29.874696 20311 net.cpp:172] pool2 does not need backward computation.
I1026 23:40:29.874701 20311 net.cpp:172] conv2 does not need backward computation.
I1026 23:40:29.874704 20311 net.cpp:172] drop1 does not need backward computation.
I1026 23:40:29.874707 20311 net.cpp:172] relu1 does not need backward computation.
I1026 23:40:29.874711 20311 net.cpp:172] pool1 does not need backward computation.
I1026 23:40:29.874716 20311 net.cpp:172] conv1 does not need backward computation.
I1026 23:40:29.874718 20311 net.cpp:208] This network produces output prob
I1026 23:40:29.874734 20311 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 23:40:29.874743 20311 net.cpp:219] Network initialization done.
I1026 23:40:29.874747 20311 net.cpp:220] Memory required for data: 1837200
I1026 23:41:05.535537 20311 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 23:41:05.536062 20311 net.cpp:358] Input 0 -> data
I1026 23:41:05.536090 20311 net.cpp:67] Creating Layer conv1
I1026 23:41:05.536098 20311 net.cpp:394] conv1 <- data
I1026 23:41:05.536104 20311 net.cpp:356] conv1 -> conv1
I1026 23:41:05.536115 20311 net.cpp:96] Setting up conv1
I1026 23:41:05.536145 20311 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 23:41:05.536162 20311 net.cpp:67] Creating Layer pool1
I1026 23:41:05.536167 20311 net.cpp:394] pool1 <- conv1
I1026 23:41:05.536173 20311 net.cpp:356] pool1 -> pool1
I1026 23:41:05.536180 20311 net.cpp:96] Setting up pool1
I1026 23:41:05.536188 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:41:05.536195 20311 net.cpp:67] Creating Layer relu1
I1026 23:41:05.536200 20311 net.cpp:394] relu1 <- pool1
I1026 23:41:05.536205 20311 net.cpp:345] relu1 -> pool1 (in-place)
I1026 23:41:05.536211 20311 net.cpp:96] Setting up relu1
I1026 23:41:05.536216 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:41:05.536221 20311 net.cpp:67] Creating Layer drop1
I1026 23:41:05.536224 20311 net.cpp:394] drop1 <- pool1
I1026 23:41:05.536231 20311 net.cpp:345] drop1 -> pool1 (in-place)
I1026 23:41:05.536237 20311 net.cpp:96] Setting up drop1
I1026 23:41:05.536242 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:41:05.536248 20311 net.cpp:67] Creating Layer conv2
I1026 23:41:05.536252 20311 net.cpp:394] conv2 <- pool1
I1026 23:41:05.536258 20311 net.cpp:356] conv2 -> conv2
I1026 23:41:05.536267 20311 net.cpp:96] Setting up conv2
I1026 23:41:05.537447 20311 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 23:41:05.537487 20311 net.cpp:67] Creating Layer pool2
I1026 23:41:05.537502 20311 net.cpp:394] pool2 <- conv2
I1026 23:41:05.537518 20311 net.cpp:356] pool2 -> pool2
I1026 23:41:05.537539 20311 net.cpp:96] Setting up pool2
I1026 23:41:05.537555 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:41:05.537570 20311 net.cpp:67] Creating Layer relu2
I1026 23:41:05.537582 20311 net.cpp:394] relu2 <- pool2
I1026 23:41:05.537597 20311 net.cpp:345] relu2 -> pool2 (in-place)
I1026 23:41:05.537612 20311 net.cpp:96] Setting up relu2
I1026 23:41:05.537624 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:41:05.537639 20311 net.cpp:67] Creating Layer drop2
I1026 23:41:05.537652 20311 net.cpp:394] drop2 <- pool2
I1026 23:41:05.537665 20311 net.cpp:345] drop2 -> pool2 (in-place)
I1026 23:41:05.537681 20311 net.cpp:96] Setting up drop2
I1026 23:41:05.537701 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:41:05.537722 20311 net.cpp:67] Creating Layer conv3
I1026 23:41:05.537734 20311 net.cpp:394] conv3 <- pool2
I1026 23:41:05.537752 20311 net.cpp:356] conv3 -> conv3
I1026 23:41:05.537771 20311 net.cpp:96] Setting up conv3
I1026 23:41:05.541435 20311 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 23:41:05.541477 20311 net.cpp:67] Creating Layer pool3
I1026 23:41:05.541491 20311 net.cpp:394] pool3 <- conv3
I1026 23:41:05.541508 20311 net.cpp:356] pool3 -> pool3
I1026 23:41:05.541527 20311 net.cpp:96] Setting up pool3
I1026 23:41:05.541543 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:41:05.541558 20311 net.cpp:67] Creating Layer relu3
I1026 23:41:05.541569 20311 net.cpp:394] relu3 <- pool3
I1026 23:41:05.541584 20311 net.cpp:345] relu3 -> pool3 (in-place)
I1026 23:41:05.541600 20311 net.cpp:96] Setting up relu3
I1026 23:41:05.541612 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:41:05.541627 20311 net.cpp:67] Creating Layer drop3
I1026 23:41:05.541638 20311 net.cpp:394] drop3 <- pool3
I1026 23:41:05.541654 20311 net.cpp:345] drop3 -> pool3 (in-place)
I1026 23:41:05.541669 20311 net.cpp:96] Setting up drop3
I1026 23:41:05.541682 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:41:05.541699 20311 net.cpp:67] Creating Layer ip1
I1026 23:41:05.541712 20311 net.cpp:394] ip1 <- pool3
I1026 23:41:05.541728 20311 net.cpp:356] ip1 -> ip1
I1026 23:41:05.541749 20311 net.cpp:96] Setting up ip1
I1026 23:41:05.940963 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:41:05.941028 20311 net.cpp:67] Creating Layer relu4
I1026 23:41:05.941036 20311 net.cpp:394] relu4 <- ip1
I1026 23:41:05.941047 20311 net.cpp:345] relu4 -> ip1 (in-place)
I1026 23:41:05.941057 20311 net.cpp:96] Setting up relu4
I1026 23:41:05.941062 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:41:05.941071 20311 net.cpp:67] Creating Layer drop4
I1026 23:41:05.941076 20311 net.cpp:394] drop4 <- ip1
I1026 23:41:05.941083 20311 net.cpp:345] drop4 -> ip1 (in-place)
I1026 23:41:05.941090 20311 net.cpp:96] Setting up drop4
I1026 23:41:05.941097 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:41:05.941107 20311 net.cpp:67] Creating Layer ip2
I1026 23:41:05.941112 20311 net.cpp:394] ip2 <- ip1
I1026 23:41:05.941120 20311 net.cpp:356] ip2 -> ip2
I1026 23:41:05.941133 20311 net.cpp:96] Setting up ip2
I1026 23:41:05.948698 20311 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 23:41:05.948762 20311 net.cpp:67] Creating Layer prob
I1026 23:41:05.948770 20311 net.cpp:394] prob <- ip2
I1026 23:41:05.948781 20311 net.cpp:356] prob -> prob
I1026 23:41:05.948792 20311 net.cpp:96] Setting up prob
I1026 23:41:05.948801 20311 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 23:41:05.948806 20311 net.cpp:172] prob does not need backward computation.
I1026 23:41:05.948810 20311 net.cpp:172] ip2 does not need backward computation.
I1026 23:41:05.948814 20311 net.cpp:172] drop4 does not need backward computation.
I1026 23:41:05.948818 20311 net.cpp:172] relu4 does not need backward computation.
I1026 23:41:05.948822 20311 net.cpp:172] ip1 does not need backward computation.
I1026 23:41:05.948827 20311 net.cpp:172] drop3 does not need backward computation.
I1026 23:41:05.948830 20311 net.cpp:172] relu3 does not need backward computation.
I1026 23:41:05.948834 20311 net.cpp:172] pool3 does not need backward computation.
I1026 23:41:05.948838 20311 net.cpp:172] conv3 does not need backward computation.
I1026 23:41:05.948843 20311 net.cpp:172] drop2 does not need backward computation.
I1026 23:41:05.948846 20311 net.cpp:172] relu2 does not need backward computation.
I1026 23:41:05.948850 20311 net.cpp:172] pool2 does not need backward computation.
I1026 23:41:05.948854 20311 net.cpp:172] conv2 does not need backward computation.
I1026 23:41:05.948858 20311 net.cpp:172] drop1 does not need backward computation.
I1026 23:41:05.948863 20311 net.cpp:172] relu1 does not need backward computation.
I1026 23:41:05.948866 20311 net.cpp:172] pool1 does not need backward computation.
I1026 23:41:05.948881 20311 net.cpp:172] conv1 does not need backward computation.
I1026 23:41:05.948886 20311 net.cpp:208] This network produces output prob
I1026 23:41:05.948900 20311 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 23:41:05.948909 20311 net.cpp:219] Network initialization done.
I1026 23:41:05.948915 20311 net.cpp:220] Memory required for data: 1837200
I1026 23:41:43.148957 20311 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 23:41:43.149504 20311 net.cpp:358] Input 0 -> data
I1026 23:41:43.149554 20311 net.cpp:67] Creating Layer conv1
I1026 23:41:43.149565 20311 net.cpp:394] conv1 <- data
I1026 23:41:43.149581 20311 net.cpp:356] conv1 -> conv1
I1026 23:41:43.149601 20311 net.cpp:96] Setting up conv1
I1026 23:41:43.149657 20311 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 23:41:43.149687 20311 net.cpp:67] Creating Layer pool1
I1026 23:41:43.149698 20311 net.cpp:394] pool1 <- conv1
I1026 23:41:43.149710 20311 net.cpp:356] pool1 -> pool1
I1026 23:41:43.149727 20311 net.cpp:96] Setting up pool1
I1026 23:41:43.149742 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:41:43.149757 20311 net.cpp:67] Creating Layer relu1
I1026 23:41:43.149766 20311 net.cpp:394] relu1 <- pool1
I1026 23:41:43.149778 20311 net.cpp:345] relu1 -> pool1 (in-place)
I1026 23:41:43.149791 20311 net.cpp:96] Setting up relu1
I1026 23:41:43.149809 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:41:43.149822 20311 net.cpp:67] Creating Layer drop1
I1026 23:41:43.149832 20311 net.cpp:394] drop1 <- pool1
I1026 23:41:43.149844 20311 net.cpp:345] drop1 -> pool1 (in-place)
I1026 23:41:43.149858 20311 net.cpp:96] Setting up drop1
I1026 23:41:43.149869 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:41:43.149884 20311 net.cpp:67] Creating Layer conv2
I1026 23:41:43.149894 20311 net.cpp:394] conv2 <- pool1
I1026 23:41:43.149909 20311 net.cpp:356] conv2 -> conv2
I1026 23:41:43.149924 20311 net.cpp:96] Setting up conv2
I1026 23:41:43.151057 20311 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 23:41:43.151085 20311 net.cpp:67] Creating Layer pool2
I1026 23:41:43.151096 20311 net.cpp:394] pool2 <- conv2
I1026 23:41:43.151110 20311 net.cpp:356] pool2 -> pool2
I1026 23:41:43.151126 20311 net.cpp:96] Setting up pool2
I1026 23:41:43.151139 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:41:43.151152 20311 net.cpp:67] Creating Layer relu2
I1026 23:41:43.151161 20311 net.cpp:394] relu2 <- pool2
I1026 23:41:43.151173 20311 net.cpp:345] relu2 -> pool2 (in-place)
I1026 23:41:43.151186 20311 net.cpp:96] Setting up relu2
I1026 23:41:43.151196 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:41:43.151208 20311 net.cpp:67] Creating Layer drop2
I1026 23:41:43.151218 20311 net.cpp:394] drop2 <- pool2
I1026 23:41:43.151237 20311 net.cpp:345] drop2 -> pool2 (in-place)
I1026 23:41:43.151254 20311 net.cpp:96] Setting up drop2
I1026 23:41:43.151268 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:41:43.151286 20311 net.cpp:67] Creating Layer conv3
I1026 23:41:43.151298 20311 net.cpp:394] conv3 <- pool2
I1026 23:41:43.151315 20311 net.cpp:356] conv3 -> conv3
I1026 23:41:43.151335 20311 net.cpp:96] Setting up conv3
I1026 23:41:43.154997 20311 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 23:41:43.155035 20311 net.cpp:67] Creating Layer pool3
I1026 23:41:43.155048 20311 net.cpp:394] pool3 <- conv3
I1026 23:41:43.155064 20311 net.cpp:356] pool3 -> pool3
I1026 23:41:43.155082 20311 net.cpp:96] Setting up pool3
I1026 23:41:43.155097 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:41:43.155112 20311 net.cpp:67] Creating Layer relu3
I1026 23:41:43.155123 20311 net.cpp:394] relu3 <- pool3
I1026 23:41:43.155138 20311 net.cpp:345] relu3 -> pool3 (in-place)
I1026 23:41:43.155153 20311 net.cpp:96] Setting up relu3
I1026 23:41:43.155165 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:41:43.155180 20311 net.cpp:67] Creating Layer drop3
I1026 23:41:43.155191 20311 net.cpp:394] drop3 <- pool3
I1026 23:41:43.155206 20311 net.cpp:345] drop3 -> pool3 (in-place)
I1026 23:41:43.155222 20311 net.cpp:96] Setting up drop3
I1026 23:41:43.155235 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:41:43.155251 20311 net.cpp:67] Creating Layer ip1
I1026 23:41:43.155263 20311 net.cpp:394] ip1 <- pool3
I1026 23:41:43.155280 20311 net.cpp:356] ip1 -> ip1
I1026 23:41:43.155299 20311 net.cpp:96] Setting up ip1
I1026 23:41:43.549084 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:41:43.549150 20311 net.cpp:67] Creating Layer relu4
I1026 23:41:43.549160 20311 net.cpp:394] relu4 <- ip1
I1026 23:41:43.549170 20311 net.cpp:345] relu4 -> ip1 (in-place)
I1026 23:41:43.549181 20311 net.cpp:96] Setting up relu4
I1026 23:41:43.549186 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:41:43.549196 20311 net.cpp:67] Creating Layer drop4
I1026 23:41:43.549199 20311 net.cpp:394] drop4 <- ip1
I1026 23:41:43.549206 20311 net.cpp:345] drop4 -> ip1 (in-place)
I1026 23:41:43.549214 20311 net.cpp:96] Setting up drop4
I1026 23:41:43.549221 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:41:43.549232 20311 net.cpp:67] Creating Layer ip2
I1026 23:41:43.549235 20311 net.cpp:394] ip2 <- ip1
I1026 23:41:43.549244 20311 net.cpp:356] ip2 -> ip2
I1026 23:41:43.549258 20311 net.cpp:96] Setting up ip2
I1026 23:41:43.556823 20311 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 23:41:43.556886 20311 net.cpp:67] Creating Layer prob
I1026 23:41:43.556895 20311 net.cpp:394] prob <- ip2
I1026 23:41:43.556915 20311 net.cpp:356] prob -> prob
I1026 23:41:43.556927 20311 net.cpp:96] Setting up prob
I1026 23:41:43.556936 20311 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 23:41:43.556941 20311 net.cpp:172] prob does not need backward computation.
I1026 23:41:43.556944 20311 net.cpp:172] ip2 does not need backward computation.
I1026 23:41:43.556948 20311 net.cpp:172] drop4 does not need backward computation.
I1026 23:41:43.556953 20311 net.cpp:172] relu4 does not need backward computation.
I1026 23:41:43.556957 20311 net.cpp:172] ip1 does not need backward computation.
I1026 23:41:43.556962 20311 net.cpp:172] drop3 does not need backward computation.
I1026 23:41:43.556965 20311 net.cpp:172] relu3 does not need backward computation.
I1026 23:41:43.556969 20311 net.cpp:172] pool3 does not need backward computation.
I1026 23:41:43.556973 20311 net.cpp:172] conv3 does not need backward computation.
I1026 23:41:43.556977 20311 net.cpp:172] drop2 does not need backward computation.
I1026 23:41:43.556980 20311 net.cpp:172] relu2 does not need backward computation.
I1026 23:41:43.556985 20311 net.cpp:172] pool2 does not need backward computation.
I1026 23:41:43.556988 20311 net.cpp:172] conv2 does not need backward computation.
I1026 23:41:43.556993 20311 net.cpp:172] drop1 does not need backward computation.
I1026 23:41:43.556996 20311 net.cpp:172] relu1 does not need backward computation.
I1026 23:41:43.557000 20311 net.cpp:172] pool1 does not need backward computation.
I1026 23:41:43.557004 20311 net.cpp:172] conv1 does not need backward computation.
I1026 23:41:43.557008 20311 net.cpp:208] This network produces output prob
I1026 23:41:43.557024 20311 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 23:41:43.557034 20311 net.cpp:219] Network initialization done.
I1026 23:41:43.557037 20311 net.cpp:220] Memory required for data: 1837200
I1026 23:42:18.756770 20311 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1026 23:42:18.756881 20311 net.cpp:358] Input 0 -> data
I1026 23:42:18.756909 20311 net.cpp:67] Creating Layer conv1
I1026 23:42:18.756916 20311 net.cpp:394] conv1 <- data
I1026 23:42:18.756925 20311 net.cpp:356] conv1 -> conv1
I1026 23:42:18.756938 20311 net.cpp:96] Setting up conv1
I1026 23:42:18.756973 20311 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1026 23:42:18.756992 20311 net.cpp:67] Creating Layer pool1
I1026 23:42:18.756999 20311 net.cpp:394] pool1 <- conv1
I1026 23:42:18.757005 20311 net.cpp:356] pool1 -> pool1
I1026 23:42:18.757014 20311 net.cpp:96] Setting up pool1
I1026 23:42:18.757024 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:42:18.757032 20311 net.cpp:67] Creating Layer relu1
I1026 23:42:18.757037 20311 net.cpp:394] relu1 <- pool1
I1026 23:42:18.757045 20311 net.cpp:345] relu1 -> pool1 (in-place)
I1026 23:42:18.757051 20311 net.cpp:96] Setting up relu1
I1026 23:42:18.757057 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:42:18.757064 20311 net.cpp:67] Creating Layer drop1
I1026 23:42:18.757069 20311 net.cpp:394] drop1 <- pool1
I1026 23:42:18.757076 20311 net.cpp:345] drop1 -> pool1 (in-place)
I1026 23:42:18.757084 20311 net.cpp:96] Setting up drop1
I1026 23:42:18.757091 20311 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1026 23:42:18.757098 20311 net.cpp:67] Creating Layer conv2
I1026 23:42:18.757104 20311 net.cpp:394] conv2 <- pool1
I1026 23:42:18.757112 20311 net.cpp:356] conv2 -> conv2
I1026 23:42:18.757120 20311 net.cpp:96] Setting up conv2
I1026 23:42:18.757743 20311 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1026 23:42:18.757763 20311 net.cpp:67] Creating Layer pool2
I1026 23:42:18.757769 20311 net.cpp:394] pool2 <- conv2
I1026 23:42:18.757776 20311 net.cpp:356] pool2 -> pool2
I1026 23:42:18.757786 20311 net.cpp:96] Setting up pool2
I1026 23:42:18.757793 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:42:18.757800 20311 net.cpp:67] Creating Layer relu2
I1026 23:42:18.757805 20311 net.cpp:394] relu2 <- pool2
I1026 23:42:18.757812 20311 net.cpp:345] relu2 -> pool2 (in-place)
I1026 23:42:18.757818 20311 net.cpp:96] Setting up relu2
I1026 23:42:18.757823 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:42:18.757830 20311 net.cpp:67] Creating Layer drop2
I1026 23:42:18.757835 20311 net.cpp:394] drop2 <- pool2
I1026 23:42:18.757843 20311 net.cpp:345] drop2 -> pool2 (in-place)
I1026 23:42:18.757849 20311 net.cpp:96] Setting up drop2
I1026 23:42:18.757854 20311 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1026 23:42:18.757864 20311 net.cpp:67] Creating Layer conv3
I1026 23:42:18.757869 20311 net.cpp:394] conv3 <- pool2
I1026 23:42:18.757876 20311 net.cpp:356] conv3 -> conv3
I1026 23:42:18.757885 20311 net.cpp:96] Setting up conv3
I1026 23:42:18.759527 20311 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1026 23:42:18.759549 20311 net.cpp:67] Creating Layer pool3
I1026 23:42:18.759555 20311 net.cpp:394] pool3 <- conv3
I1026 23:42:18.759563 20311 net.cpp:356] pool3 -> pool3
I1026 23:42:18.759572 20311 net.cpp:96] Setting up pool3
I1026 23:42:18.759578 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:42:18.759585 20311 net.cpp:67] Creating Layer relu3
I1026 23:42:18.759590 20311 net.cpp:394] relu3 <- pool3
I1026 23:42:18.759598 20311 net.cpp:345] relu3 -> pool3 (in-place)
I1026 23:42:18.759604 20311 net.cpp:96] Setting up relu3
I1026 23:42:18.759609 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:42:18.759616 20311 net.cpp:67] Creating Layer drop3
I1026 23:42:18.759625 20311 net.cpp:394] drop3 <- pool3
I1026 23:42:18.759632 20311 net.cpp:345] drop3 -> pool3 (in-place)
I1026 23:42:18.759640 20311 net.cpp:96] Setting up drop3
I1026 23:42:18.759645 20311 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1026 23:42:18.759654 20311 net.cpp:67] Creating Layer ip1
I1026 23:42:18.759659 20311 net.cpp:394] ip1 <- pool3
I1026 23:42:18.759666 20311 net.cpp:356] ip1 -> ip1
I1026 23:42:18.759675 20311 net.cpp:96] Setting up ip1
I1026 23:42:19.159888 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:42:19.160512 20311 net.cpp:67] Creating Layer relu4
I1026 23:42:19.160542 20311 net.cpp:394] relu4 <- ip1
I1026 23:42:19.160565 20311 net.cpp:345] relu4 -> ip1 (in-place)
I1026 23:42:19.160588 20311 net.cpp:96] Setting up relu4
I1026 23:42:19.160600 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:42:19.160619 20311 net.cpp:67] Creating Layer drop4
I1026 23:42:19.160631 20311 net.cpp:394] drop4 <- ip1
I1026 23:42:19.160647 20311 net.cpp:345] drop4 -> ip1 (in-place)
I1026 23:42:19.160665 20311 net.cpp:96] Setting up drop4
I1026 23:42:19.160679 20311 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1026 23:42:19.160701 20311 net.cpp:67] Creating Layer ip2
I1026 23:42:19.160713 20311 net.cpp:394] ip2 <- ip1
I1026 23:42:19.160732 20311 net.cpp:356] ip2 -> ip2
I1026 23:42:19.160760 20311 net.cpp:96] Setting up ip2
I1026 23:42:19.170385 20311 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 23:42:19.170455 20311 net.cpp:67] Creating Layer prob
I1026 23:42:19.170464 20311 net.cpp:394] prob <- ip2
I1026 23:42:19.170474 20311 net.cpp:356] prob -> prob
I1026 23:42:19.170485 20311 net.cpp:96] Setting up prob
I1026 23:42:19.170493 20311 net.cpp:103] Top shape: 1 378 1 1 (378)
I1026 23:42:19.170498 20311 net.cpp:172] prob does not need backward computation.
I1026 23:42:19.170502 20311 net.cpp:172] ip2 does not need backward computation.
I1026 23:42:19.170506 20311 net.cpp:172] drop4 does not need backward computation.
I1026 23:42:19.170511 20311 net.cpp:172] relu4 does not need backward computation.
I1026 23:42:19.170514 20311 net.cpp:172] ip1 does not need backward computation.
I1026 23:42:19.170518 20311 net.cpp:172] drop3 does not need backward computation.
I1026 23:42:19.170522 20311 net.cpp:172] relu3 does not need backward computation.
I1026 23:42:19.170526 20311 net.cpp:172] pool3 does not need backward computation.
I1026 23:42:19.170531 20311 net.cpp:172] conv3 does not need backward computation.
I1026 23:42:19.170534 20311 net.cpp:172] drop2 does not need backward computation.
I1026 23:42:19.170538 20311 net.cpp:172] relu2 does not need backward computation.
I1026 23:42:19.170542 20311 net.cpp:172] pool2 does not need backward computation.
I1026 23:42:19.170547 20311 net.cpp:172] conv2 does not need backward computation.
I1026 23:42:19.170550 20311 net.cpp:172] drop1 does not need backward computation.
I1026 23:42:19.170554 20311 net.cpp:172] relu1 does not need backward computation.
I1026 23:42:19.170558 20311 net.cpp:172] pool1 does not need backward computation.
I1026 23:42:19.170562 20311 net.cpp:172] conv1 does not need backward computation.
I1026 23:42:19.170567 20311 net.cpp:208] This network produces output prob
I1026 23:42:19.170583 20311 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1026 23:42:19.170591 20311 net.cpp:219] Network initialization done.
I1026 23:42:19.170595 20311 net.cpp:220] Memory required for data: 1837200
I1027 00:21:36.043634 27289 convert_imageset.cpp:70] Shuffling data
I1027 00:21:36.720151 27289 convert_imageset.cpp:73] A total of 60000 images.
I1027 00:21:36.720229 27289 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1027 00:21:38.905305 27289 convert_imageset.cpp:177] Processed 1000 files.
E1027 00:21:41.468479 27289 convert_imageset.cpp:177] Processed 2000 files.
E1027 00:21:43.490236 27289 convert_imageset.cpp:177] Processed 3000 files.
E1027 00:21:45.485357 27289 convert_imageset.cpp:177] Processed 4000 files.
E1027 00:21:47.580374 27289 convert_imageset.cpp:177] Processed 5000 files.
E1027 00:21:49.874301 27289 convert_imageset.cpp:177] Processed 6000 files.
E1027 00:21:51.845954 27289 convert_imageset.cpp:177] Processed 7000 files.
E1027 00:21:53.712167 27289 convert_imageset.cpp:177] Processed 8000 files.
E1027 00:21:55.635587 27289 convert_imageset.cpp:177] Processed 9000 files.
E1027 00:21:58.054950 27289 convert_imageset.cpp:177] Processed 10000 files.
E1027 00:21:59.817052 27289 convert_imageset.cpp:177] Processed 11000 files.
E1027 00:22:01.735571 27289 convert_imageset.cpp:177] Processed 12000 files.
E1027 00:22:03.838699 27289 convert_imageset.cpp:177] Processed 13000 files.
E1027 00:22:05.793063 27289 convert_imageset.cpp:177] Processed 14000 files.
E1027 00:22:07.904366 27289 convert_imageset.cpp:177] Processed 15000 files.
E1027 00:22:09.532706 27289 convert_imageset.cpp:177] Processed 16000 files.
E1027 00:22:11.495781 27289 convert_imageset.cpp:177] Processed 17000 files.
E1027 00:22:13.462754 27289 convert_imageset.cpp:177] Processed 18000 files.
E1027 00:22:15.390969 27289 convert_imageset.cpp:177] Processed 19000 files.
E1027 00:22:17.350812 27289 convert_imageset.cpp:177] Processed 20000 files.
E1027 00:22:19.223621 27289 convert_imageset.cpp:177] Processed 21000 files.
E1027 00:22:20.994995 27289 convert_imageset.cpp:177] Processed 22000 files.
E1027 00:22:22.932265 27289 convert_imageset.cpp:177] Processed 23000 files.
E1027 00:22:24.729809 27289 convert_imageset.cpp:177] Processed 24000 files.
E1027 00:22:26.689776 27289 convert_imageset.cpp:177] Processed 25000 files.
E1027 00:22:28.504160 27289 convert_imageset.cpp:177] Processed 26000 files.
E1027 00:22:30.361296 27289 convert_imageset.cpp:177] Processed 27000 files.
E1027 00:22:32.160368 27289 convert_imageset.cpp:177] Processed 28000 files.
E1027 00:22:34.159111 27289 convert_imageset.cpp:177] Processed 29000 files.
E1027 00:22:35.913833 27289 convert_imageset.cpp:177] Processed 30000 files.
E1027 00:22:37.576701 27289 convert_imageset.cpp:177] Processed 31000 files.
E1027 00:22:39.301275 27289 convert_imageset.cpp:177] Processed 32000 files.
E1027 00:22:41.119406 27289 convert_imageset.cpp:177] Processed 33000 files.
E1027 00:22:42.931874 27289 convert_imageset.cpp:177] Processed 34000 files.
E1027 00:22:44.587718 27289 convert_imageset.cpp:177] Processed 35000 files.
E1027 00:22:46.402724 27289 convert_imageset.cpp:177] Processed 36000 files.
E1027 00:22:48.218894 27289 convert_imageset.cpp:177] Processed 37000 files.
E1027 00:22:49.893647 27289 convert_imageset.cpp:177] Processed 38000 files.
E1027 00:22:51.612488 27289 convert_imageset.cpp:177] Processed 39000 files.
E1027 00:22:53.460307 27289 convert_imageset.cpp:177] Processed 40000 files.
E1027 00:22:55.311144 27289 convert_imageset.cpp:177] Processed 41000 files.
E1027 00:22:56.822774 27289 convert_imageset.cpp:177] Processed 42000 files.
E1027 00:22:58.594730 27289 convert_imageset.cpp:177] Processed 43000 files.
E1027 00:23:00.326762 27289 convert_imageset.cpp:177] Processed 44000 files.
E1027 00:23:02.038801 27289 convert_imageset.cpp:177] Processed 45000 files.
E1027 00:23:03.706131 27289 convert_imageset.cpp:177] Processed 46000 files.
E1027 00:23:05.381108 27289 convert_imageset.cpp:177] Processed 47000 files.
E1027 00:23:07.212553 27289 convert_imageset.cpp:177] Processed 48000 files.
E1027 00:23:09.010118 27289 convert_imageset.cpp:177] Processed 49000 files.
E1027 00:23:10.735617 27289 convert_imageset.cpp:177] Processed 50000 files.
E1027 00:23:12.421330 27289 convert_imageset.cpp:177] Processed 51000 files.
E1027 00:23:13.973965 27289 convert_imageset.cpp:177] Processed 52000 files.
E1027 00:23:15.622684 27289 convert_imageset.cpp:177] Processed 53000 files.
E1027 00:23:17.367595 27289 convert_imageset.cpp:177] Processed 54000 files.
E1027 00:23:18.917623 27289 convert_imageset.cpp:177] Processed 55000 files.
E1027 00:23:20.400199 27289 convert_imageset.cpp:177] Processed 56000 files.
E1027 00:23:22.039793 27289 convert_imageset.cpp:177] Processed 57000 files.
E1027 00:23:23.687773 27289 convert_imageset.cpp:177] Processed 58000 files.
E1027 00:23:25.503942 27289 convert_imageset.cpp:177] Processed 59000 files.
E1027 00:23:27.276751 27289 convert_imageset.cpp:177] Processed 60000 files.
I1027 00:23:27.504032 27354 caffe.cpp:99] Use GPU with device ID 0
I1027 00:23:27.859324 27354 caffe.cpp:107] Starting Optimization
I1027 00:23:27.859431 27354 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 195000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1027 00:23:27.859455 27354 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1027 00:23:27.883276 27354 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1027 00:23:27.883370 27354 net.cpp:67] Creating Layer mnist
I1027 00:23:27.883380 27354 net.cpp:356] mnist -> data
I1027 00:23:27.883401 27354 net.cpp:356] mnist -> label
I1027 00:23:27.883415 27354 net.cpp:96] Setting up mnist
I1027 00:23:27.930963 27354 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1027 00:23:27.931094 27354 data_layer.cpp:128] output data size: 64,1,50,180
I1027 00:23:27.932747 27354 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1027 00:23:27.932770 27354 net.cpp:103] Top shape: 64 1 1 1 (64)
I1027 00:23:27.932783 27354 net.cpp:67] Creating Layer conv1
I1027 00:23:27.932790 27354 net.cpp:394] conv1 <- data
I1027 00:23:27.932803 27354 net.cpp:356] conv1 -> conv1
I1027 00:23:27.932813 27354 net.cpp:96] Setting up conv1
I1027 00:23:27.933197 27354 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1027 00:23:27.933231 27354 net.cpp:67] Creating Layer pool1
I1027 00:23:27.933238 27354 net.cpp:394] pool1 <- conv1
I1027 00:23:27.933243 27354 net.cpp:356] pool1 -> pool1
I1027 00:23:27.933254 27354 net.cpp:96] Setting up pool1
I1027 00:23:27.933269 27354 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 00:23:27.933276 27354 net.cpp:67] Creating Layer relu1
I1027 00:23:27.933280 27354 net.cpp:394] relu1 <- pool1
I1027 00:23:27.933286 27354 net.cpp:345] relu1 -> pool1 (in-place)
I1027 00:23:27.933293 27354 net.cpp:96] Setting up relu1
I1027 00:23:27.933298 27354 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 00:23:27.933305 27354 net.cpp:67] Creating Layer drop1
I1027 00:23:27.933310 27354 net.cpp:394] drop1 <- pool1
I1027 00:23:27.933318 27354 net.cpp:345] drop1 -> pool1 (in-place)
I1027 00:23:27.933325 27354 net.cpp:96] Setting up drop1
I1027 00:23:27.933331 27354 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 00:23:27.933337 27354 net.cpp:67] Creating Layer conv2
I1027 00:23:27.933341 27354 net.cpp:394] conv2 <- pool1
I1027 00:23:27.933351 27354 net.cpp:356] conv2 -> conv2
I1027 00:23:27.933358 27354 net.cpp:96] Setting up conv2
I1027 00:23:27.933951 27354 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1027 00:23:27.933971 27354 net.cpp:67] Creating Layer pool2
I1027 00:23:27.933976 27354 net.cpp:394] pool2 <- conv2
I1027 00:23:27.933984 27354 net.cpp:356] pool2 -> pool2
I1027 00:23:27.933990 27354 net.cpp:96] Setting up pool2
I1027 00:23:27.933997 27354 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 00:23:27.934003 27354 net.cpp:67] Creating Layer relu2
I1027 00:23:27.934007 27354 net.cpp:394] relu2 <- pool2
I1027 00:23:27.934015 27354 net.cpp:345] relu2 -> pool2 (in-place)
I1027 00:23:27.934021 27354 net.cpp:96] Setting up relu2
I1027 00:23:27.934026 27354 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 00:23:27.934033 27354 net.cpp:67] Creating Layer drop2
I1027 00:23:27.934038 27354 net.cpp:394] drop2 <- pool2
I1027 00:23:27.934043 27354 net.cpp:345] drop2 -> pool2 (in-place)
I1027 00:23:27.934051 27354 net.cpp:96] Setting up drop2
I1027 00:23:27.934054 27354 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 00:23:27.934064 27354 net.cpp:67] Creating Layer conv3
I1027 00:23:27.934068 27354 net.cpp:394] conv3 <- pool2
I1027 00:23:27.934075 27354 net.cpp:356] conv3 -> conv3
I1027 00:23:27.934082 27354 net.cpp:96] Setting up conv3
I1027 00:23:27.935614 27354 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1027 00:23:27.935641 27354 net.cpp:67] Creating Layer pool3
I1027 00:23:27.935647 27354 net.cpp:394] pool3 <- conv3
I1027 00:23:27.935655 27354 net.cpp:356] pool3 -> pool3
I1027 00:23:27.935662 27354 net.cpp:96] Setting up pool3
I1027 00:23:27.935669 27354 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 00:23:27.935675 27354 net.cpp:67] Creating Layer relu3
I1027 00:23:27.935679 27354 net.cpp:394] relu3 <- pool3
I1027 00:23:27.935686 27354 net.cpp:345] relu3 -> pool3 (in-place)
I1027 00:23:27.935693 27354 net.cpp:96] Setting up relu3
I1027 00:23:27.935698 27354 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 00:23:27.935703 27354 net.cpp:67] Creating Layer drop3
I1027 00:23:27.935708 27354 net.cpp:394] drop3 <- pool3
I1027 00:23:27.935714 27354 net.cpp:345] drop3 -> pool3 (in-place)
I1027 00:23:27.935719 27354 net.cpp:96] Setting up drop3
I1027 00:23:27.935730 27354 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 00:23:27.935739 27354 net.cpp:67] Creating Layer ip1
I1027 00:23:27.935742 27354 net.cpp:394] ip1 <- pool3
I1027 00:23:27.935750 27354 net.cpp:356] ip1 -> ip1
I1027 00:23:27.935788 27354 net.cpp:96] Setting up ip1
I1027 00:23:28.294942 27354 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 00:23:28.294997 27354 net.cpp:67] Creating Layer relu4
I1027 00:23:28.295004 27354 net.cpp:394] relu4 <- ip1
I1027 00:23:28.295013 27354 net.cpp:345] relu4 -> ip1 (in-place)
I1027 00:23:28.295022 27354 net.cpp:96] Setting up relu4
I1027 00:23:28.295027 27354 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 00:23:28.295034 27354 net.cpp:67] Creating Layer drop4
I1027 00:23:28.295038 27354 net.cpp:394] drop4 <- ip1
I1027 00:23:28.295044 27354 net.cpp:345] drop4 -> ip1 (in-place)
I1027 00:23:28.295050 27354 net.cpp:96] Setting up drop4
I1027 00:23:28.295055 27354 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 00:23:28.295068 27354 net.cpp:67] Creating Layer ip2
I1027 00:23:28.295073 27354 net.cpp:394] ip2 <- ip1
I1027 00:23:28.295080 27354 net.cpp:356] ip2 -> ip2
I1027 00:23:28.295089 27354 net.cpp:96] Setting up ip2
I1027 00:23:28.303475 27354 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1027 00:23:28.303542 27354 net.cpp:67] Creating Layer loss
I1027 00:23:28.303550 27354 net.cpp:394] loss <- ip2
I1027 00:23:28.303557 27354 net.cpp:394] loss <- label
I1027 00:23:28.303565 27354 net.cpp:356] loss -> loss
I1027 00:23:28.303573 27354 net.cpp:96] Setting up loss
I1027 00:23:28.303586 27354 net.cpp:103] Top shape: 1 1 1 1 (1)
I1027 00:23:28.303591 27354 net.cpp:109]     with loss weight 1
I1027 00:23:28.303627 27354 net.cpp:170] loss needs backward computation.
I1027 00:23:28.303632 27354 net.cpp:170] ip2 needs backward computation.
I1027 00:23:28.303637 27354 net.cpp:170] drop4 needs backward computation.
I1027 00:23:28.303640 27354 net.cpp:170] relu4 needs backward computation.
I1027 00:23:28.303644 27354 net.cpp:170] ip1 needs backward computation.
I1027 00:23:28.303649 27354 net.cpp:170] drop3 needs backward computation.
I1027 00:23:28.303653 27354 net.cpp:170] relu3 needs backward computation.
I1027 00:23:28.303658 27354 net.cpp:170] pool3 needs backward computation.
I1027 00:23:28.303663 27354 net.cpp:170] conv3 needs backward computation.
I1027 00:23:28.303668 27354 net.cpp:170] drop2 needs backward computation.
I1027 00:23:28.303671 27354 net.cpp:170] relu2 needs backward computation.
I1027 00:23:28.303675 27354 net.cpp:170] pool2 needs backward computation.
I1027 00:23:28.303680 27354 net.cpp:170] conv2 needs backward computation.
I1027 00:23:28.303684 27354 net.cpp:170] drop1 needs backward computation.
I1027 00:23:28.303689 27354 net.cpp:170] relu1 needs backward computation.
I1027 00:23:28.303692 27354 net.cpp:170] pool1 needs backward computation.
I1027 00:23:28.303697 27354 net.cpp:170] conv1 needs backward computation.
I1027 00:23:28.303701 27354 net.cpp:172] mnist does not need backward computation.
I1027 00:23:28.303706 27354 net.cpp:208] This network produces output loss
I1027 00:23:28.303716 27354 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 00:23:28.303724 27354 net.cpp:219] Network initialization done.
I1027 00:23:28.303727 27354 net.cpp:220] Memory required for data: 119788292
I1027 00:23:28.303787 27354 solver.cpp:41] Solver scaffolding done.
I1027 00:23:28.303793 27354 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_175000.solverstate
I1027 00:23:28.303797 27354 solver.cpp:160] Solving Captcha
I1027 00:23:28.303817 27354 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_175000.solverstate
I1027 00:23:35.808184 27354 solver.cpp:502] SGDSolver: restoring history
I1027 00:23:36.671350 27354 solver.cpp:191] Iteration 175000, loss = 2.40541
I1027 00:23:36.671404 27354 solver.cpp:206]     Train net output #0: loss = 2.40541 (* 1 = 2.40541 loss)
I1027 00:23:36.671424 27354 solver.cpp:403] Iteration 175000, lr = 0.00112104
I1027 00:27:38.594816 27354 solver.cpp:191] Iteration 176000, loss = 2.41972
I1027 00:27:38.595422 27354 solver.cpp:206]     Train net output #0: loss = 2.41972 (* 1 = 2.41972 loss)
I1027 00:27:38.595446 27354 solver.cpp:403] Iteration 176000, lr = 0.00111652
I1027 00:31:39.816974 27354 solver.cpp:191] Iteration 177000, loss = 2.44571
I1027 00:31:39.817548 27354 solver.cpp:206]     Train net output #0: loss = 2.44571 (* 1 = 2.44571 loss)
I1027 00:31:39.817584 27354 solver.cpp:403] Iteration 177000, lr = 0.00111204
I1027 00:35:41.182443 27354 solver.cpp:191] Iteration 178000, loss = 2.44895
I1027 00:35:41.182974 27354 solver.cpp:206]     Train net output #0: loss = 2.44895 (* 1 = 2.44895 loss)
I1027 00:35:41.183008 27354 solver.cpp:403] Iteration 178000, lr = 0.0011076
I1027 00:39:42.548163 27354 solver.cpp:191] Iteration 179000, loss = 2.47018
I1027 00:39:42.548769 27354 solver.cpp:206]     Train net output #0: loss = 2.47018 (* 1 = 2.47018 loss)
I1027 00:39:42.548806 27354 solver.cpp:403] Iteration 179000, lr = 0.0011032
I1027 00:43:44.445641 27354 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_180000.caffemodel
I1027 00:43:48.879976 27354 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_180000.solverstate
I1027 00:43:52.761579 27354 solver.cpp:191] Iteration 180000, loss = 2.57578
I1027 00:43:52.762224 27354 solver.cpp:206]     Train net output #0: loss = 2.57578 (* 1 = 2.57578 loss)
I1027 00:43:52.762259 27354 solver.cpp:403] Iteration 180000, lr = 0.00109884
I1027 00:47:54.148716 27354 solver.cpp:191] Iteration 181000, loss = 2.49435
I1027 00:47:54.149324 27354 solver.cpp:206]     Train net output #0: loss = 2.49435 (* 1 = 2.49435 loss)
I1027 00:47:54.149339 27354 solver.cpp:403] Iteration 181000, lr = 0.00109452
I1027 00:51:55.420711 27354 solver.cpp:191] Iteration 182000, loss = 2.37025
I1027 00:51:55.421310 27354 solver.cpp:206]     Train net output #0: loss = 2.37025 (* 1 = 2.37025 loss)
I1027 00:51:55.421344 27354 solver.cpp:403] Iteration 182000, lr = 0.00109024
I1027 00:55:56.657143 27354 solver.cpp:191] Iteration 183000, loss = 2.5966
I1027 00:55:56.657724 27354 solver.cpp:206]     Train net output #0: loss = 2.5966 (* 1 = 2.5966 loss)
I1027 00:55:56.657757 27354 solver.cpp:403] Iteration 183000, lr = 0.00108601
I1027 00:59:57.972609 27354 solver.cpp:191] Iteration 184000, loss = 2.36328
I1027 00:59:57.973186 27354 solver.cpp:206]     Train net output #0: loss = 2.36328 (* 1 = 2.36328 loss)
I1027 00:59:57.973219 27354 solver.cpp:403] Iteration 184000, lr = 0.0010818
I1027 01:03:59.816073 27354 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_185000.caffemodel
I1027 01:04:03.873417 27354 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_185000.solverstate
I1027 01:04:07.384829 27354 solver.cpp:191] Iteration 185000, loss = 2.39293
I1027 01:04:07.385352 27354 solver.cpp:206]     Train net output #0: loss = 2.39293 (* 1 = 2.39293 loss)
I1027 01:04:07.385390 27354 solver.cpp:403] Iteration 185000, lr = 0.00107764
I1027 01:08:08.815132 27354 solver.cpp:191] Iteration 186000, loss = 2.67078
I1027 01:08:08.815651 27354 solver.cpp:206]     Train net output #0: loss = 2.67078 (* 1 = 2.67078 loss)
I1027 01:08:08.815690 27354 solver.cpp:403] Iteration 186000, lr = 0.00107351
I1027 01:12:10.299300 27354 solver.cpp:191] Iteration 187000, loss = 2.39091
I1027 01:12:10.299885 27354 solver.cpp:206]     Train net output #0: loss = 2.39091 (* 1 = 2.39091 loss)
I1027 01:12:10.299917 27354 solver.cpp:403] Iteration 187000, lr = 0.00106943
I1027 01:16:11.693133 27354 solver.cpp:191] Iteration 188000, loss = 2.46511
I1027 01:16:11.693701 27354 solver.cpp:206]     Train net output #0: loss = 2.46511 (* 1 = 2.46511 loss)
I1027 01:16:11.693734 27354 solver.cpp:403] Iteration 188000, lr = 0.00106537
I1027 01:20:13.075718 27354 solver.cpp:191] Iteration 189000, loss = 2.38465
I1027 01:20:13.076380 27354 solver.cpp:206]     Train net output #0: loss = 2.38465 (* 1 = 2.38465 loss)
I1027 01:20:13.076416 27354 solver.cpp:403] Iteration 189000, lr = 0.00106135
I1027 01:24:14.902915 27354 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_190000.caffemodel
I1027 01:24:19.351012 27354 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_190000.solverstate
I1027 01:24:23.326575 27354 solver.cpp:191] Iteration 190000, loss = 2.2133
I1027 01:24:23.327047 27354 solver.cpp:206]     Train net output #0: loss = 2.2133 (* 1 = 2.2133 loss)
I1027 01:24:23.327083 27354 solver.cpp:403] Iteration 190000, lr = 0.00105737
I1027 01:28:24.684435 27354 solver.cpp:191] Iteration 191000, loss = 2.32857
I1027 01:28:24.685083 27354 solver.cpp:206]     Train net output #0: loss = 2.32857 (* 1 = 2.32857 loss)
I1027 01:28:24.685117 27354 solver.cpp:403] Iteration 191000, lr = 0.00105342
I1027 01:32:25.970144 27354 solver.cpp:191] Iteration 192000, loss = 2.41284
I1027 01:32:25.970743 27354 solver.cpp:206]     Train net output #0: loss = 2.41284 (* 1 = 2.41284 loss)
I1027 01:32:25.970775 27354 solver.cpp:403] Iteration 192000, lr = 0.00104951
I1027 01:36:27.214344 27354 solver.cpp:191] Iteration 193000, loss = 2.51028
I1027 01:36:27.214932 27354 solver.cpp:206]     Train net output #0: loss = 2.51028 (* 1 = 2.51028 loss)
I1027 01:36:27.214965 27354 solver.cpp:403] Iteration 193000, lr = 0.00104563
I1027 01:40:28.359094 27354 solver.cpp:191] Iteration 194000, loss = 2.51075
I1027 01:40:28.359642 27354 solver.cpp:206]     Train net output #0: loss = 2.51075 (* 1 = 2.51075 loss)
I1027 01:40:28.359679 27354 solver.cpp:403] Iteration 194000, lr = 0.00104178
I1027 01:44:29.965806 27354 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_195000.caffemodel
I1027 01:44:34.056651 27354 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_195000.solverstate
I1027 01:44:37.802712 27354 solver.cpp:228] Iteration 195000, loss = 2.33148
I1027 01:44:37.803246 27354 solver.cpp:233] Optimization Done.
I1027 01:44:37.803269 27354 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1027 02:15:43.675621 18255 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 02:15:43.675729 18255 net.cpp:358] Input 0 -> data
I1027 02:15:43.675758 18255 net.cpp:67] Creating Layer conv1
I1027 02:15:43.675763 18255 net.cpp:394] conv1 <- data
I1027 02:15:43.675770 18255 net.cpp:356] conv1 -> conv1
I1027 02:15:43.675781 18255 net.cpp:96] Setting up conv1
I1027 02:15:43.676105 18255 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 02:15:43.676126 18255 net.cpp:67] Creating Layer pool1
I1027 02:15:43.676131 18255 net.cpp:394] pool1 <- conv1
I1027 02:15:43.676137 18255 net.cpp:356] pool1 -> pool1
I1027 02:15:43.676144 18255 net.cpp:96] Setting up pool1
I1027 02:15:43.676156 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:15:43.676162 18255 net.cpp:67] Creating Layer relu1
I1027 02:15:43.676167 18255 net.cpp:394] relu1 <- pool1
I1027 02:15:43.676172 18255 net.cpp:345] relu1 -> pool1 (in-place)
I1027 02:15:43.676177 18255 net.cpp:96] Setting up relu1
I1027 02:15:43.676182 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:15:43.676187 18255 net.cpp:67] Creating Layer drop1
I1027 02:15:43.676192 18255 net.cpp:394] drop1 <- pool1
I1027 02:15:43.676198 18255 net.cpp:345] drop1 -> pool1 (in-place)
I1027 02:15:43.676205 18255 net.cpp:96] Setting up drop1
I1027 02:15:43.676210 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:15:43.676216 18255 net.cpp:67] Creating Layer conv2
I1027 02:15:43.676220 18255 net.cpp:394] conv2 <- pool1
I1027 02:15:43.676229 18255 net.cpp:356] conv2 -> conv2
I1027 02:15:43.676235 18255 net.cpp:96] Setting up conv2
I1027 02:15:43.676830 18255 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 02:15:43.676847 18255 net.cpp:67] Creating Layer pool2
I1027 02:15:43.676852 18255 net.cpp:394] pool2 <- conv2
I1027 02:15:43.676858 18255 net.cpp:356] pool2 -> pool2
I1027 02:15:43.676865 18255 net.cpp:96] Setting up pool2
I1027 02:15:43.676872 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:15:43.676877 18255 net.cpp:67] Creating Layer relu2
I1027 02:15:43.676880 18255 net.cpp:394] relu2 <- pool2
I1027 02:15:43.676887 18255 net.cpp:345] relu2 -> pool2 (in-place)
I1027 02:15:43.676893 18255 net.cpp:96] Setting up relu2
I1027 02:15:43.676898 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:15:43.676903 18255 net.cpp:67] Creating Layer drop2
I1027 02:15:43.676906 18255 net.cpp:394] drop2 <- pool2
I1027 02:15:43.676913 18255 net.cpp:345] drop2 -> pool2 (in-place)
I1027 02:15:43.676919 18255 net.cpp:96] Setting up drop2
I1027 02:15:43.676923 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:15:43.676930 18255 net.cpp:67] Creating Layer conv3
I1027 02:15:43.676934 18255 net.cpp:394] conv3 <- pool2
I1027 02:15:43.676940 18255 net.cpp:356] conv3 -> conv3
I1027 02:15:43.676947 18255 net.cpp:96] Setting up conv3
I1027 02:15:43.678416 18255 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 02:15:43.678431 18255 net.cpp:67] Creating Layer pool3
I1027 02:15:43.678436 18255 net.cpp:394] pool3 <- conv3
I1027 02:15:43.678447 18255 net.cpp:356] pool3 -> pool3
I1027 02:15:43.678453 18255 net.cpp:96] Setting up pool3
I1027 02:15:43.678458 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:15:43.678463 18255 net.cpp:67] Creating Layer relu3
I1027 02:15:43.678467 18255 net.cpp:394] relu3 <- pool3
I1027 02:15:43.678474 18255 net.cpp:345] relu3 -> pool3 (in-place)
I1027 02:15:43.678480 18255 net.cpp:96] Setting up relu3
I1027 02:15:43.678484 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:15:43.678489 18255 net.cpp:67] Creating Layer drop3
I1027 02:15:43.678493 18255 net.cpp:394] drop3 <- pool3
I1027 02:15:43.678498 18255 net.cpp:345] drop3 -> pool3 (in-place)
I1027 02:15:43.678504 18255 net.cpp:96] Setting up drop3
I1027 02:15:43.678508 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:15:43.678514 18255 net.cpp:67] Creating Layer ip1
I1027 02:15:43.678519 18255 net.cpp:394] ip1 <- pool3
I1027 02:15:43.678526 18255 net.cpp:356] ip1 -> ip1
I1027 02:15:43.678534 18255 net.cpp:96] Setting up ip1
I1027 02:15:44.211459 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:15:44.211518 18255 net.cpp:67] Creating Layer relu4
I1027 02:15:44.211526 18255 net.cpp:394] relu4 <- ip1
I1027 02:15:44.211535 18255 net.cpp:345] relu4 -> ip1 (in-place)
I1027 02:15:44.211545 18255 net.cpp:96] Setting up relu4
I1027 02:15:44.211550 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:15:44.211557 18255 net.cpp:67] Creating Layer drop4
I1027 02:15:44.211561 18255 net.cpp:394] drop4 <- ip1
I1027 02:15:44.211567 18255 net.cpp:345] drop4 -> ip1 (in-place)
I1027 02:15:44.211573 18255 net.cpp:96] Setting up drop4
I1027 02:15:44.211578 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:15:44.211591 18255 net.cpp:67] Creating Layer ip2
I1027 02:15:44.211596 18255 net.cpp:394] ip2 <- ip1
I1027 02:15:44.211601 18255 net.cpp:356] ip2 -> ip2
I1027 02:15:44.211616 18255 net.cpp:96] Setting up ip2
I1027 02:15:44.221848 18255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 02:15:44.221928 18255 net.cpp:67] Creating Layer prob
I1027 02:15:44.221937 18255 net.cpp:394] prob <- ip2
I1027 02:15:44.221946 18255 net.cpp:356] prob -> prob
I1027 02:15:44.221957 18255 net.cpp:96] Setting up prob
I1027 02:15:44.221964 18255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 02:15:44.221969 18255 net.cpp:172] prob does not need backward computation.
I1027 02:15:44.221973 18255 net.cpp:172] ip2 does not need backward computation.
I1027 02:15:44.221977 18255 net.cpp:172] drop4 does not need backward computation.
I1027 02:15:44.221982 18255 net.cpp:172] relu4 does not need backward computation.
I1027 02:15:44.221984 18255 net.cpp:172] ip1 does not need backward computation.
I1027 02:15:44.221988 18255 net.cpp:172] drop3 does not need backward computation.
I1027 02:15:44.221992 18255 net.cpp:172] relu3 does not need backward computation.
I1027 02:15:44.221997 18255 net.cpp:172] pool3 does not need backward computation.
I1027 02:15:44.221999 18255 net.cpp:172] conv3 does not need backward computation.
I1027 02:15:44.222003 18255 net.cpp:172] drop2 does not need backward computation.
I1027 02:15:44.222007 18255 net.cpp:172] relu2 does not need backward computation.
I1027 02:15:44.222012 18255 net.cpp:172] pool2 does not need backward computation.
I1027 02:15:44.222014 18255 net.cpp:172] conv2 does not need backward computation.
I1027 02:15:44.222018 18255 net.cpp:172] drop1 does not need backward computation.
I1027 02:15:44.222023 18255 net.cpp:172] relu1 does not need backward computation.
I1027 02:15:44.222025 18255 net.cpp:172] pool1 does not need backward computation.
I1027 02:15:44.222029 18255 net.cpp:172] conv1 does not need backward computation.
I1027 02:15:44.222033 18255 net.cpp:208] This network produces output prob
I1027 02:15:44.222048 18255 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 02:15:44.222055 18255 net.cpp:219] Network initialization done.
I1027 02:15:44.222059 18255 net.cpp:220] Memory required for data: 1837200
I1027 02:16:28.139638 18255 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 02:16:28.140215 18255 net.cpp:358] Input 0 -> data
I1027 02:16:28.140274 18255 net.cpp:67] Creating Layer conv1
I1027 02:16:28.140288 18255 net.cpp:394] conv1 <- data
I1027 02:16:28.140307 18255 net.cpp:356] conv1 -> conv1
I1027 02:16:28.140331 18255 net.cpp:96] Setting up conv1
I1027 02:16:28.140398 18255 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 02:16:28.140486 18255 net.cpp:67] Creating Layer pool1
I1027 02:16:28.140506 18255 net.cpp:394] pool1 <- conv1
I1027 02:16:28.140524 18255 net.cpp:356] pool1 -> pool1
I1027 02:16:28.140545 18255 net.cpp:96] Setting up pool1
I1027 02:16:28.140564 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:16:28.140583 18255 net.cpp:67] Creating Layer relu1
I1027 02:16:28.140594 18255 net.cpp:394] relu1 <- pool1
I1027 02:16:28.140609 18255 net.cpp:345] relu1 -> pool1 (in-place)
I1027 02:16:28.140625 18255 net.cpp:96] Setting up relu1
I1027 02:16:28.140638 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:16:28.140655 18255 net.cpp:67] Creating Layer drop1
I1027 02:16:28.140666 18255 net.cpp:394] drop1 <- pool1
I1027 02:16:28.140681 18255 net.cpp:345] drop1 -> pool1 (in-place)
I1027 02:16:28.140697 18255 net.cpp:96] Setting up drop1
I1027 02:16:28.140712 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:16:28.140732 18255 net.cpp:67] Creating Layer conv2
I1027 02:16:28.140743 18255 net.cpp:394] conv2 <- pool1
I1027 02:16:28.140768 18255 net.cpp:356] conv2 -> conv2
I1027 02:16:28.140787 18255 net.cpp:96] Setting up conv2
I1027 02:16:28.142164 18255 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 02:16:28.142199 18255 net.cpp:67] Creating Layer pool2
I1027 02:16:28.142213 18255 net.cpp:394] pool2 <- conv2
I1027 02:16:28.142230 18255 net.cpp:356] pool2 -> pool2
I1027 02:16:28.142249 18255 net.cpp:96] Setting up pool2
I1027 02:16:28.142266 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:16:28.142282 18255 net.cpp:67] Creating Layer relu2
I1027 02:16:28.142293 18255 net.cpp:394] relu2 <- pool2
I1027 02:16:28.142308 18255 net.cpp:345] relu2 -> pool2 (in-place)
I1027 02:16:28.142324 18255 net.cpp:96] Setting up relu2
I1027 02:16:28.142336 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:16:28.142351 18255 net.cpp:67] Creating Layer drop2
I1027 02:16:28.142362 18255 net.cpp:394] drop2 <- pool2
I1027 02:16:28.142377 18255 net.cpp:345] drop2 -> pool2 (in-place)
I1027 02:16:28.142393 18255 net.cpp:96] Setting up drop2
I1027 02:16:28.142406 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:16:28.142426 18255 net.cpp:67] Creating Layer conv3
I1027 02:16:28.142437 18255 net.cpp:394] conv3 <- pool2
I1027 02:16:28.142454 18255 net.cpp:356] conv3 -> conv3
I1027 02:16:28.142473 18255 net.cpp:96] Setting up conv3
I1027 02:16:28.146114 18255 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 02:16:28.146157 18255 net.cpp:67] Creating Layer pool3
I1027 02:16:28.146172 18255 net.cpp:394] pool3 <- conv3
I1027 02:16:28.146188 18255 net.cpp:356] pool3 -> pool3
I1027 02:16:28.146208 18255 net.cpp:96] Setting up pool3
I1027 02:16:28.146222 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:16:28.146237 18255 net.cpp:67] Creating Layer relu3
I1027 02:16:28.146250 18255 net.cpp:394] relu3 <- pool3
I1027 02:16:28.146265 18255 net.cpp:345] relu3 -> pool3 (in-place)
I1027 02:16:28.146280 18255 net.cpp:96] Setting up relu3
I1027 02:16:28.146291 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:16:28.146306 18255 net.cpp:67] Creating Layer drop3
I1027 02:16:28.146317 18255 net.cpp:394] drop3 <- pool3
I1027 02:16:28.146333 18255 net.cpp:345] drop3 -> pool3 (in-place)
I1027 02:16:28.146349 18255 net.cpp:96] Setting up drop3
I1027 02:16:28.146363 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:16:28.146379 18255 net.cpp:67] Creating Layer ip1
I1027 02:16:28.146391 18255 net.cpp:394] ip1 <- pool3
I1027 02:16:28.146409 18255 net.cpp:356] ip1 -> ip1
I1027 02:16:28.146426 18255 net.cpp:96] Setting up ip1
I1027 02:16:28.566788 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:16:28.566854 18255 net.cpp:67] Creating Layer relu4
I1027 02:16:28.566862 18255 net.cpp:394] relu4 <- ip1
I1027 02:16:28.566872 18255 net.cpp:345] relu4 -> ip1 (in-place)
I1027 02:16:28.566882 18255 net.cpp:96] Setting up relu4
I1027 02:16:28.566889 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:16:28.566896 18255 net.cpp:67] Creating Layer drop4
I1027 02:16:28.566901 18255 net.cpp:394] drop4 <- ip1
I1027 02:16:28.566908 18255 net.cpp:345] drop4 -> ip1 (in-place)
I1027 02:16:28.566915 18255 net.cpp:96] Setting up drop4
I1027 02:16:28.566921 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:16:28.566931 18255 net.cpp:67] Creating Layer ip2
I1027 02:16:28.566936 18255 net.cpp:394] ip2 <- ip1
I1027 02:16:28.566943 18255 net.cpp:356] ip2 -> ip2
I1027 02:16:28.566956 18255 net.cpp:96] Setting up ip2
I1027 02:16:28.574551 18255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 02:16:28.574615 18255 net.cpp:67] Creating Layer prob
I1027 02:16:28.574622 18255 net.cpp:394] prob <- ip2
I1027 02:16:28.574632 18255 net.cpp:356] prob -> prob
I1027 02:16:28.574643 18255 net.cpp:96] Setting up prob
I1027 02:16:28.574651 18255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 02:16:28.574656 18255 net.cpp:172] prob does not need backward computation.
I1027 02:16:28.574661 18255 net.cpp:172] ip2 does not need backward computation.
I1027 02:16:28.574666 18255 net.cpp:172] drop4 does not need backward computation.
I1027 02:16:28.574681 18255 net.cpp:172] relu4 does not need backward computation.
I1027 02:16:28.574686 18255 net.cpp:172] ip1 does not need backward computation.
I1027 02:16:28.574689 18255 net.cpp:172] drop3 does not need backward computation.
I1027 02:16:28.574693 18255 net.cpp:172] relu3 does not need backward computation.
I1027 02:16:28.574697 18255 net.cpp:172] pool3 does not need backward computation.
I1027 02:16:28.574702 18255 net.cpp:172] conv3 does not need backward computation.
I1027 02:16:28.574705 18255 net.cpp:172] drop2 does not need backward computation.
I1027 02:16:28.574709 18255 net.cpp:172] relu2 does not need backward computation.
I1027 02:16:28.574713 18255 net.cpp:172] pool2 does not need backward computation.
I1027 02:16:28.574717 18255 net.cpp:172] conv2 does not need backward computation.
I1027 02:16:28.574722 18255 net.cpp:172] drop1 does not need backward computation.
I1027 02:16:28.574725 18255 net.cpp:172] relu1 does not need backward computation.
I1027 02:16:28.574729 18255 net.cpp:172] pool1 does not need backward computation.
I1027 02:16:28.574733 18255 net.cpp:172] conv1 does not need backward computation.
I1027 02:16:28.574738 18255 net.cpp:208] This network produces output prob
I1027 02:16:28.574753 18255 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 02:16:28.574764 18255 net.cpp:219] Network initialization done.
I1027 02:16:28.574769 18255 net.cpp:220] Memory required for data: 1837200
I1027 02:17:04.616129 18255 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 02:17:04.616674 18255 net.cpp:358] Input 0 -> data
I1027 02:17:04.616730 18255 net.cpp:67] Creating Layer conv1
I1027 02:17:04.616746 18255 net.cpp:394] conv1 <- data
I1027 02:17:04.616765 18255 net.cpp:356] conv1 -> conv1
I1027 02:17:04.616791 18255 net.cpp:96] Setting up conv1
I1027 02:17:04.616855 18255 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 02:17:04.616893 18255 net.cpp:67] Creating Layer pool1
I1027 02:17:04.616906 18255 net.cpp:394] pool1 <- conv1
I1027 02:17:04.616922 18255 net.cpp:356] pool1 -> pool1
I1027 02:17:04.616943 18255 net.cpp:96] Setting up pool1
I1027 02:17:04.616961 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:17:04.616979 18255 net.cpp:67] Creating Layer relu1
I1027 02:17:04.616991 18255 net.cpp:394] relu1 <- pool1
I1027 02:17:04.617007 18255 net.cpp:345] relu1 -> pool1 (in-place)
I1027 02:17:04.617024 18255 net.cpp:96] Setting up relu1
I1027 02:17:04.617036 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:17:04.617053 18255 net.cpp:67] Creating Layer drop1
I1027 02:17:04.617064 18255 net.cpp:394] drop1 <- pool1
I1027 02:17:04.617079 18255 net.cpp:345] drop1 -> pool1 (in-place)
I1027 02:17:04.617096 18255 net.cpp:96] Setting up drop1
I1027 02:17:04.617110 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:17:04.617130 18255 net.cpp:67] Creating Layer conv2
I1027 02:17:04.617141 18255 net.cpp:394] conv2 <- pool1
I1027 02:17:04.617158 18255 net.cpp:356] conv2 -> conv2
I1027 02:17:04.617177 18255 net.cpp:96] Setting up conv2
I1027 02:17:04.618557 18255 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 02:17:04.618592 18255 net.cpp:67] Creating Layer pool2
I1027 02:17:04.618607 18255 net.cpp:394] pool2 <- conv2
I1027 02:17:04.618623 18255 net.cpp:356] pool2 -> pool2
I1027 02:17:04.618643 18255 net.cpp:96] Setting up pool2
I1027 02:17:04.618659 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:17:04.618675 18255 net.cpp:67] Creating Layer relu2
I1027 02:17:04.618687 18255 net.cpp:394] relu2 <- pool2
I1027 02:17:04.618703 18255 net.cpp:345] relu2 -> pool2 (in-place)
I1027 02:17:04.618718 18255 net.cpp:96] Setting up relu2
I1027 02:17:04.618731 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:17:04.618746 18255 net.cpp:67] Creating Layer drop2
I1027 02:17:04.618758 18255 net.cpp:394] drop2 <- pool2
I1027 02:17:04.618774 18255 net.cpp:345] drop2 -> pool2 (in-place)
I1027 02:17:04.618790 18255 net.cpp:96] Setting up drop2
I1027 02:17:04.618803 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:17:04.618824 18255 net.cpp:67] Creating Layer conv3
I1027 02:17:04.618836 18255 net.cpp:394] conv3 <- pool2
I1027 02:17:04.618854 18255 net.cpp:356] conv3 -> conv3
I1027 02:17:04.618872 18255 net.cpp:96] Setting up conv3
I1027 02:17:04.622539 18255 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 02:17:04.622577 18255 net.cpp:67] Creating Layer pool3
I1027 02:17:04.622592 18255 net.cpp:394] pool3 <- conv3
I1027 02:17:04.622609 18255 net.cpp:356] pool3 -> pool3
I1027 02:17:04.622628 18255 net.cpp:96] Setting up pool3
I1027 02:17:04.622643 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:17:04.622659 18255 net.cpp:67] Creating Layer relu3
I1027 02:17:04.622671 18255 net.cpp:394] relu3 <- pool3
I1027 02:17:04.622686 18255 net.cpp:345] relu3 -> pool3 (in-place)
I1027 02:17:04.622702 18255 net.cpp:96] Setting up relu3
I1027 02:17:04.622714 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:17:04.622730 18255 net.cpp:67] Creating Layer drop3
I1027 02:17:04.622741 18255 net.cpp:394] drop3 <- pool3
I1027 02:17:04.622756 18255 net.cpp:345] drop3 -> pool3 (in-place)
I1027 02:17:04.622772 18255 net.cpp:96] Setting up drop3
I1027 02:17:04.622786 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:17:04.622802 18255 net.cpp:67] Creating Layer ip1
I1027 02:17:04.622814 18255 net.cpp:394] ip1 <- pool3
I1027 02:17:04.622831 18255 net.cpp:356] ip1 -> ip1
I1027 02:17:04.622851 18255 net.cpp:96] Setting up ip1
I1027 02:17:05.021206 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:17:05.021271 18255 net.cpp:67] Creating Layer relu4
I1027 02:17:05.021280 18255 net.cpp:394] relu4 <- ip1
I1027 02:17:05.021289 18255 net.cpp:345] relu4 -> ip1 (in-place)
I1027 02:17:05.021301 18255 net.cpp:96] Setting up relu4
I1027 02:17:05.021306 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:17:05.021316 18255 net.cpp:67] Creating Layer drop4
I1027 02:17:05.021319 18255 net.cpp:394] drop4 <- ip1
I1027 02:17:05.021327 18255 net.cpp:345] drop4 -> ip1 (in-place)
I1027 02:17:05.021334 18255 net.cpp:96] Setting up drop4
I1027 02:17:05.021340 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:17:05.021352 18255 net.cpp:67] Creating Layer ip2
I1027 02:17:05.021355 18255 net.cpp:394] ip2 <- ip1
I1027 02:17:05.021364 18255 net.cpp:356] ip2 -> ip2
I1027 02:17:05.021378 18255 net.cpp:96] Setting up ip2
I1027 02:17:05.029006 18255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 02:17:05.029069 18255 net.cpp:67] Creating Layer prob
I1027 02:17:05.029078 18255 net.cpp:394] prob <- ip2
I1027 02:17:05.029088 18255 net.cpp:356] prob -> prob
I1027 02:17:05.029099 18255 net.cpp:96] Setting up prob
I1027 02:17:05.029106 18255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 02:17:05.029111 18255 net.cpp:172] prob does not need backward computation.
I1027 02:17:05.029116 18255 net.cpp:172] ip2 does not need backward computation.
I1027 02:17:05.029120 18255 net.cpp:172] drop4 does not need backward computation.
I1027 02:17:05.029124 18255 net.cpp:172] relu4 does not need backward computation.
I1027 02:17:05.029129 18255 net.cpp:172] ip1 does not need backward computation.
I1027 02:17:05.029134 18255 net.cpp:172] drop3 does not need backward computation.
I1027 02:17:05.029139 18255 net.cpp:172] relu3 does not need backward computation.
I1027 02:17:05.029145 18255 net.cpp:172] pool3 does not need backward computation.
I1027 02:17:05.029148 18255 net.cpp:172] conv3 does not need backward computation.
I1027 02:17:05.029152 18255 net.cpp:172] drop2 does not need backward computation.
I1027 02:17:05.029156 18255 net.cpp:172] relu2 does not need backward computation.
I1027 02:17:05.029160 18255 net.cpp:172] pool2 does not need backward computation.
I1027 02:17:05.029165 18255 net.cpp:172] conv2 does not need backward computation.
I1027 02:17:05.029168 18255 net.cpp:172] drop1 does not need backward computation.
I1027 02:17:05.029172 18255 net.cpp:172] relu1 does not need backward computation.
I1027 02:17:05.029176 18255 net.cpp:172] pool1 does not need backward computation.
I1027 02:17:05.029181 18255 net.cpp:172] conv1 does not need backward computation.
I1027 02:17:05.029184 18255 net.cpp:208] This network produces output prob
I1027 02:17:05.029199 18255 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 02:17:05.029209 18255 net.cpp:219] Network initialization done.
I1027 02:17:05.029213 18255 net.cpp:220] Memory required for data: 1837200
I1027 02:17:41.019861 18255 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 02:17:41.020570 18255 net.cpp:358] Input 0 -> data
I1027 02:17:41.020618 18255 net.cpp:67] Creating Layer conv1
I1027 02:17:41.020632 18255 net.cpp:394] conv1 <- data
I1027 02:17:41.020647 18255 net.cpp:356] conv1 -> conv1
I1027 02:17:41.020668 18255 net.cpp:96] Setting up conv1
I1027 02:17:41.020722 18255 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 02:17:41.020753 18255 net.cpp:67] Creating Layer pool1
I1027 02:17:41.020764 18255 net.cpp:394] pool1 <- conv1
I1027 02:17:41.020778 18255 net.cpp:356] pool1 -> pool1
I1027 02:17:41.020794 18255 net.cpp:96] Setting up pool1
I1027 02:17:41.020809 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:17:41.020825 18255 net.cpp:67] Creating Layer relu1
I1027 02:17:41.020835 18255 net.cpp:394] relu1 <- pool1
I1027 02:17:41.020848 18255 net.cpp:345] relu1 -> pool1 (in-place)
I1027 02:17:41.020861 18255 net.cpp:96] Setting up relu1
I1027 02:17:41.020871 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:17:41.020884 18255 net.cpp:67] Creating Layer drop1
I1027 02:17:41.020895 18255 net.cpp:394] drop1 <- pool1
I1027 02:17:41.020906 18255 net.cpp:345] drop1 -> pool1 (in-place)
I1027 02:17:41.020920 18255 net.cpp:96] Setting up drop1
I1027 02:17:41.020931 18255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 02:17:41.020946 18255 net.cpp:67] Creating Layer conv2
I1027 02:17:41.020956 18255 net.cpp:394] conv2 <- pool1
I1027 02:17:41.020970 18255 net.cpp:356] conv2 -> conv2
I1027 02:17:41.020987 18255 net.cpp:96] Setting up conv2
I1027 02:17:41.022120 18255 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 02:17:41.022150 18255 net.cpp:67] Creating Layer pool2
I1027 02:17:41.022162 18255 net.cpp:394] pool2 <- conv2
I1027 02:17:41.022176 18255 net.cpp:356] pool2 -> pool2
I1027 02:17:41.022192 18255 net.cpp:96] Setting up pool2
I1027 02:17:41.022205 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:17:41.022217 18255 net.cpp:67] Creating Layer relu2
I1027 02:17:41.022228 18255 net.cpp:394] relu2 <- pool2
I1027 02:17:41.022240 18255 net.cpp:345] relu2 -> pool2 (in-place)
I1027 02:17:41.022263 18255 net.cpp:96] Setting up relu2
I1027 02:17:41.022275 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:17:41.022290 18255 net.cpp:67] Creating Layer drop2
I1027 02:17:41.022302 18255 net.cpp:394] drop2 <- pool2
I1027 02:17:41.022317 18255 net.cpp:345] drop2 -> pool2 (in-place)
I1027 02:17:41.022341 18255 net.cpp:96] Setting up drop2
I1027 02:17:41.022354 18255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 02:17:41.022375 18255 net.cpp:67] Creating Layer conv3
I1027 02:17:41.022388 18255 net.cpp:394] conv3 <- pool2
I1027 02:17:41.022405 18255 net.cpp:356] conv3 -> conv3
I1027 02:17:41.022424 18255 net.cpp:96] Setting up conv3
I1027 02:17:41.026170 18255 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 02:17:41.026209 18255 net.cpp:67] Creating Layer pool3
I1027 02:17:41.026223 18255 net.cpp:394] pool3 <- conv3
I1027 02:17:41.026240 18255 net.cpp:356] pool3 -> pool3
I1027 02:17:41.026258 18255 net.cpp:96] Setting up pool3
I1027 02:17:41.026273 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:17:41.026289 18255 net.cpp:67] Creating Layer relu3
I1027 02:17:41.026301 18255 net.cpp:394] relu3 <- pool3
I1027 02:17:41.026316 18255 net.cpp:345] relu3 -> pool3 (in-place)
I1027 02:17:41.026332 18255 net.cpp:96] Setting up relu3
I1027 02:17:41.026345 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:17:41.026360 18255 net.cpp:67] Creating Layer drop3
I1027 02:17:41.026371 18255 net.cpp:394] drop3 <- pool3
I1027 02:17:41.026387 18255 net.cpp:345] drop3 -> pool3 (in-place)
I1027 02:17:41.026403 18255 net.cpp:96] Setting up drop3
I1027 02:17:41.026417 18255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 02:17:41.026434 18255 net.cpp:67] Creating Layer ip1
I1027 02:17:41.026448 18255 net.cpp:394] ip1 <- pool3
I1027 02:17:41.026464 18255 net.cpp:356] ip1 -> ip1
I1027 02:17:41.026484 18255 net.cpp:96] Setting up ip1
I1027 02:17:41.427383 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:17:41.427450 18255 net.cpp:67] Creating Layer relu4
I1027 02:17:41.427459 18255 net.cpp:394] relu4 <- ip1
I1027 02:17:41.427469 18255 net.cpp:345] relu4 -> ip1 (in-place)
I1027 02:17:41.427479 18255 net.cpp:96] Setting up relu4
I1027 02:17:41.427484 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:17:41.427494 18255 net.cpp:67] Creating Layer drop4
I1027 02:17:41.427497 18255 net.cpp:394] drop4 <- ip1
I1027 02:17:41.427505 18255 net.cpp:345] drop4 -> ip1 (in-place)
I1027 02:17:41.427512 18255 net.cpp:96] Setting up drop4
I1027 02:17:41.427518 18255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 02:17:41.427528 18255 net.cpp:67] Creating Layer ip2
I1027 02:17:41.427533 18255 net.cpp:394] ip2 <- ip1
I1027 02:17:41.427542 18255 net.cpp:356] ip2 -> ip2
I1027 02:17:41.427556 18255 net.cpp:96] Setting up ip2
I1027 02:17:41.435127 18255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 02:17:41.435189 18255 net.cpp:67] Creating Layer prob
I1027 02:17:41.435195 18255 net.cpp:394] prob <- ip2
I1027 02:17:41.435204 18255 net.cpp:356] prob -> prob
I1027 02:17:41.435214 18255 net.cpp:96] Setting up prob
I1027 02:17:41.435223 18255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 02:17:41.435228 18255 net.cpp:172] prob does not need backward computation.
I1027 02:17:41.435233 18255 net.cpp:172] ip2 does not need backward computation.
I1027 02:17:41.435237 18255 net.cpp:172] drop4 does not need backward computation.
I1027 02:17:41.435241 18255 net.cpp:172] relu4 does not need backward computation.
I1027 02:17:41.435245 18255 net.cpp:172] ip1 does not need backward computation.
I1027 02:17:41.435250 18255 net.cpp:172] drop3 does not need backward computation.
I1027 02:17:41.435253 18255 net.cpp:172] relu3 does not need backward computation.
I1027 02:17:41.435258 18255 net.cpp:172] pool3 does not need backward computation.
I1027 02:17:41.435262 18255 net.cpp:172] conv3 does not need backward computation.
I1027 02:17:41.435266 18255 net.cpp:172] drop2 does not need backward computation.
I1027 02:17:41.435271 18255 net.cpp:172] relu2 does not need backward computation.
I1027 02:17:41.435276 18255 net.cpp:172] pool2 does not need backward computation.
I1027 02:17:41.435279 18255 net.cpp:172] conv2 does not need backward computation.
I1027 02:17:41.435283 18255 net.cpp:172] drop1 does not need backward computation.
I1027 02:17:41.435287 18255 net.cpp:172] relu1 does not need backward computation.
I1027 02:17:41.435292 18255 net.cpp:172] pool1 does not need backward computation.
I1027 02:17:41.435307 18255 net.cpp:172] conv1 does not need backward computation.
I1027 02:17:41.435310 18255 net.cpp:208] This network produces output prob
I1027 02:17:41.435327 18255 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 02:17:41.435335 18255 net.cpp:219] Network initialization done.
I1027 02:17:41.435340 18255 net.cpp:220] Memory required for data: 1837200
I1027 03:01:28.064604 25266 convert_imageset.cpp:70] Shuffling data
I1027 03:01:28.897840 25266 convert_imageset.cpp:73] A total of 60000 images.
I1027 03:01:28.897920 25266 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1027 03:01:31.282163 25266 convert_imageset.cpp:177] Processed 1000 files.
E1027 03:01:33.436023 25266 convert_imageset.cpp:177] Processed 2000 files.
E1027 03:01:35.483078 25266 convert_imageset.cpp:177] Processed 3000 files.
E1027 03:01:37.694775 25266 convert_imageset.cpp:177] Processed 4000 files.
E1027 03:01:39.760457 25266 convert_imageset.cpp:177] Processed 5000 files.
E1027 03:01:41.822944 25266 convert_imageset.cpp:177] Processed 6000 files.
E1027 03:01:43.682531 25266 convert_imageset.cpp:177] Processed 7000 files.
E1027 03:01:45.519538 25266 convert_imageset.cpp:177] Processed 8000 files.
E1027 03:01:47.487704 25266 convert_imageset.cpp:177] Processed 9000 files.
E1027 03:01:49.342859 25266 convert_imageset.cpp:177] Processed 10000 files.
E1027 03:01:51.290618 25266 convert_imageset.cpp:177] Processed 11000 files.
E1027 03:01:53.018853 25266 convert_imageset.cpp:177] Processed 12000 files.
E1027 03:01:54.825353 25266 convert_imageset.cpp:177] Processed 13000 files.
E1027 03:01:56.573904 25266 convert_imageset.cpp:177] Processed 14000 files.
E1027 03:01:58.278815 25266 convert_imageset.cpp:177] Processed 15000 files.
E1027 03:02:00.038138 25266 convert_imageset.cpp:177] Processed 16000 files.
E1027 03:02:01.774495 25266 convert_imageset.cpp:177] Processed 17000 files.
E1027 03:02:03.648268 25266 convert_imageset.cpp:177] Processed 18000 files.
E1027 03:02:05.330639 25266 convert_imageset.cpp:177] Processed 19000 files.
E1027 03:02:07.077152 25266 convert_imageset.cpp:177] Processed 20000 files.
E1027 03:02:08.771585 25266 convert_imageset.cpp:177] Processed 21000 files.
E1027 03:02:10.438596 25266 convert_imageset.cpp:177] Processed 22000 files.
E1027 03:02:12.031586 25266 convert_imageset.cpp:177] Processed 23000 files.
E1027 03:02:13.871491 25266 convert_imageset.cpp:177] Processed 24000 files.
E1027 03:02:15.542357 25266 convert_imageset.cpp:177] Processed 25000 files.
E1027 03:02:17.130450 25266 convert_imageset.cpp:177] Processed 26000 files.
E1027 03:02:18.726336 25266 convert_imageset.cpp:177] Processed 27000 files.
E1027 03:02:20.270709 25266 convert_imageset.cpp:177] Processed 28000 files.
E1027 03:02:21.837134 25266 convert_imageset.cpp:177] Processed 29000 files.
E1027 03:02:23.509912 25266 convert_imageset.cpp:177] Processed 30000 files.
E1027 03:02:25.223865 25266 convert_imageset.cpp:177] Processed 31000 files.
E1027 03:02:26.819383 25266 convert_imageset.cpp:177] Processed 32000 files.
E1027 03:02:28.558955 25266 convert_imageset.cpp:177] Processed 33000 files.
E1027 03:02:30.259011 25266 convert_imageset.cpp:177] Processed 34000 files.
E1027 03:02:31.948964 25266 convert_imageset.cpp:177] Processed 35000 files.
E1027 03:02:33.552712 25266 convert_imageset.cpp:177] Processed 36000 files.
E1027 03:02:35.211027 25266 convert_imageset.cpp:177] Processed 37000 files.
E1027 03:02:36.863451 25266 convert_imageset.cpp:177] Processed 38000 files.
E1027 03:02:38.402943 25266 convert_imageset.cpp:177] Processed 39000 files.
E1027 03:02:39.951040 25266 convert_imageset.cpp:177] Processed 40000 files.
E1027 03:02:41.476963 25266 convert_imageset.cpp:177] Processed 41000 files.
E1027 03:02:43.044924 25266 convert_imageset.cpp:177] Processed 42000 files.
E1027 03:02:44.531605 25266 convert_imageset.cpp:177] Processed 43000 files.
E1027 03:02:46.202625 25266 convert_imageset.cpp:177] Processed 44000 files.
E1027 03:02:47.733096 25266 convert_imageset.cpp:177] Processed 45000 files.
E1027 03:02:49.250187 25266 convert_imageset.cpp:177] Processed 46000 files.
E1027 03:02:50.758502 25266 convert_imageset.cpp:177] Processed 47000 files.
E1027 03:02:52.447616 25266 convert_imageset.cpp:177] Processed 48000 files.
E1027 03:02:54.082954 25266 convert_imageset.cpp:177] Processed 49000 files.
E1027 03:02:55.638875 25266 convert_imageset.cpp:177] Processed 50000 files.
E1027 03:02:57.208466 25266 convert_imageset.cpp:177] Processed 51000 files.
E1027 03:02:58.833873 25266 convert_imageset.cpp:177] Processed 52000 files.
E1027 03:03:00.459466 25266 convert_imageset.cpp:177] Processed 53000 files.
E1027 03:03:02.032327 25266 convert_imageset.cpp:177] Processed 54000 files.
E1027 03:03:03.512054 25266 convert_imageset.cpp:177] Processed 55000 files.
E1027 03:03:05.177151 25266 convert_imageset.cpp:177] Processed 56000 files.
E1027 03:03:06.710007 25266 convert_imageset.cpp:177] Processed 57000 files.
E1027 03:03:08.366621 25266 convert_imageset.cpp:177] Processed 58000 files.
E1027 03:03:09.836334 25266 convert_imageset.cpp:177] Processed 59000 files.
E1027 03:03:11.324550 25266 convert_imageset.cpp:177] Processed 60000 files.
I1027 03:03:11.581271 25274 caffe.cpp:99] Use GPU with device ID 0
I1027 03:03:11.907024 25274 caffe.cpp:107] Starting Optimization
I1027 03:03:11.907136 25274 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 215000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1027 03:03:11.907165 25274 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1027 03:03:11.925581 25274 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1027 03:03:11.925717 25274 net.cpp:67] Creating Layer mnist
I1027 03:03:11.925731 25274 net.cpp:356] mnist -> data
I1027 03:03:11.925751 25274 net.cpp:356] mnist -> label
I1027 03:03:11.925770 25274 net.cpp:96] Setting up mnist
I1027 03:03:11.931354 25274 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1027 03:03:11.931447 25274 data_layer.cpp:128] output data size: 64,1,50,180
I1027 03:03:11.932232 25274 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1027 03:03:11.932265 25274 net.cpp:103] Top shape: 64 1 1 1 (64)
I1027 03:03:11.932287 25274 net.cpp:67] Creating Layer conv1
I1027 03:03:11.932298 25274 net.cpp:394] conv1 <- data
I1027 03:03:11.932323 25274 net.cpp:356] conv1 -> conv1
I1027 03:03:11.932343 25274 net.cpp:96] Setting up conv1
I1027 03:03:11.932982 25274 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1027 03:03:11.933030 25274 net.cpp:67] Creating Layer pool1
I1027 03:03:11.933042 25274 net.cpp:394] pool1 <- conv1
I1027 03:03:11.933051 25274 net.cpp:356] pool1 -> pool1
I1027 03:03:11.933063 25274 net.cpp:96] Setting up pool1
I1027 03:03:11.933085 25274 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 03:03:11.933101 25274 net.cpp:67] Creating Layer relu1
I1027 03:03:11.933110 25274 net.cpp:394] relu1 <- pool1
I1027 03:03:11.933120 25274 net.cpp:345] relu1 -> pool1 (in-place)
I1027 03:03:11.933130 25274 net.cpp:96] Setting up relu1
I1027 03:03:11.933137 25274 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 03:03:11.933148 25274 net.cpp:67] Creating Layer drop1
I1027 03:03:11.933156 25274 net.cpp:394] drop1 <- pool1
I1027 03:03:11.933166 25274 net.cpp:345] drop1 -> pool1 (in-place)
I1027 03:03:11.933176 25274 net.cpp:96] Setting up drop1
I1027 03:03:11.933183 25274 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 03:03:11.933199 25274 net.cpp:67] Creating Layer conv2
I1027 03:03:11.933207 25274 net.cpp:394] conv2 <- pool1
I1027 03:03:11.933218 25274 net.cpp:356] conv2 -> conv2
I1027 03:03:11.933233 25274 net.cpp:96] Setting up conv2
I1027 03:03:11.934231 25274 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1027 03:03:11.934265 25274 net.cpp:67] Creating Layer pool2
I1027 03:03:11.934275 25274 net.cpp:394] pool2 <- conv2
I1027 03:03:11.934288 25274 net.cpp:356] pool2 -> pool2
I1027 03:03:11.934299 25274 net.cpp:96] Setting up pool2
I1027 03:03:11.934309 25274 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 03:03:11.934319 25274 net.cpp:67] Creating Layer relu2
I1027 03:03:11.934326 25274 net.cpp:394] relu2 <- pool2
I1027 03:03:11.934335 25274 net.cpp:345] relu2 -> pool2 (in-place)
I1027 03:03:11.934345 25274 net.cpp:96] Setting up relu2
I1027 03:03:11.934352 25274 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 03:03:11.934367 25274 net.cpp:67] Creating Layer drop2
I1027 03:03:11.934376 25274 net.cpp:394] drop2 <- pool2
I1027 03:03:11.934384 25274 net.cpp:345] drop2 -> pool2 (in-place)
I1027 03:03:11.934396 25274 net.cpp:96] Setting up drop2
I1027 03:03:11.934403 25274 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 03:03:11.934419 25274 net.cpp:67] Creating Layer conv3
I1027 03:03:11.934432 25274 net.cpp:394] conv3 <- pool2
I1027 03:03:11.934438 25274 net.cpp:356] conv3 -> conv3
I1027 03:03:11.934447 25274 net.cpp:96] Setting up conv3
I1027 03:03:11.936269 25274 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1027 03:03:11.936305 25274 net.cpp:67] Creating Layer pool3
I1027 03:03:11.936312 25274 net.cpp:394] pool3 <- conv3
I1027 03:03:11.936322 25274 net.cpp:356] pool3 -> pool3
I1027 03:03:11.936333 25274 net.cpp:96] Setting up pool3
I1027 03:03:11.936344 25274 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 03:03:11.936354 25274 net.cpp:67] Creating Layer relu3
I1027 03:03:11.936362 25274 net.cpp:394] relu3 <- pool3
I1027 03:03:11.936370 25274 net.cpp:345] relu3 -> pool3 (in-place)
I1027 03:03:11.936379 25274 net.cpp:96] Setting up relu3
I1027 03:03:11.936385 25274 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 03:03:11.936396 25274 net.cpp:67] Creating Layer drop3
I1027 03:03:11.936403 25274 net.cpp:394] drop3 <- pool3
I1027 03:03:11.936411 25274 net.cpp:345] drop3 -> pool3 (in-place)
I1027 03:03:11.936527 25274 net.cpp:96] Setting up drop3
I1027 03:03:11.936539 25274 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 03:03:11.936549 25274 net.cpp:67] Creating Layer ip1
I1027 03:03:11.936555 25274 net.cpp:394] ip1 <- pool3
I1027 03:03:11.936568 25274 net.cpp:356] ip1 -> ip1
I1027 03:03:11.936596 25274 net.cpp:96] Setting up ip1
I1027 03:03:12.303138 25274 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 03:03:12.303195 25274 net.cpp:67] Creating Layer relu4
I1027 03:03:12.303202 25274 net.cpp:394] relu4 <- ip1
I1027 03:03:12.303213 25274 net.cpp:345] relu4 -> ip1 (in-place)
I1027 03:03:12.303223 25274 net.cpp:96] Setting up relu4
I1027 03:03:12.303228 25274 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 03:03:12.303236 25274 net.cpp:67] Creating Layer drop4
I1027 03:03:12.303241 25274 net.cpp:394] drop4 <- ip1
I1027 03:03:12.303246 25274 net.cpp:345] drop4 -> ip1 (in-place)
I1027 03:03:12.303252 25274 net.cpp:96] Setting up drop4
I1027 03:03:12.303257 25274 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 03:03:12.303269 25274 net.cpp:67] Creating Layer ip2
I1027 03:03:12.303274 25274 net.cpp:394] ip2 <- ip1
I1027 03:03:12.303282 25274 net.cpp:356] ip2 -> ip2
I1027 03:03:12.303289 25274 net.cpp:96] Setting up ip2
I1027 03:03:12.311404 25274 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1027 03:03:12.311461 25274 net.cpp:67] Creating Layer loss
I1027 03:03:12.311468 25274 net.cpp:394] loss <- ip2
I1027 03:03:12.311476 25274 net.cpp:394] loss <- label
I1027 03:03:12.311483 25274 net.cpp:356] loss -> loss
I1027 03:03:12.311491 25274 net.cpp:96] Setting up loss
I1027 03:03:12.311507 25274 net.cpp:103] Top shape: 1 1 1 1 (1)
I1027 03:03:12.311513 25274 net.cpp:109]     with loss weight 1
I1027 03:03:12.311548 25274 net.cpp:170] loss needs backward computation.
I1027 03:03:12.311553 25274 net.cpp:170] ip2 needs backward computation.
I1027 03:03:12.311558 25274 net.cpp:170] drop4 needs backward computation.
I1027 03:03:12.311563 25274 net.cpp:170] relu4 needs backward computation.
I1027 03:03:12.311566 25274 net.cpp:170] ip1 needs backward computation.
I1027 03:03:12.311571 25274 net.cpp:170] drop3 needs backward computation.
I1027 03:03:12.311575 25274 net.cpp:170] relu3 needs backward computation.
I1027 03:03:12.311579 25274 net.cpp:170] pool3 needs backward computation.
I1027 03:03:12.311584 25274 net.cpp:170] conv3 needs backward computation.
I1027 03:03:12.311589 25274 net.cpp:170] drop2 needs backward computation.
I1027 03:03:12.311594 25274 net.cpp:170] relu2 needs backward computation.
I1027 03:03:12.311599 25274 net.cpp:170] pool2 needs backward computation.
I1027 03:03:12.311602 25274 net.cpp:170] conv2 needs backward computation.
I1027 03:03:12.311607 25274 net.cpp:170] drop1 needs backward computation.
I1027 03:03:12.311611 25274 net.cpp:170] relu1 needs backward computation.
I1027 03:03:12.311615 25274 net.cpp:170] pool1 needs backward computation.
I1027 03:03:12.311620 25274 net.cpp:170] conv1 needs backward computation.
I1027 03:03:12.311625 25274 net.cpp:172] mnist does not need backward computation.
I1027 03:03:12.311635 25274 net.cpp:208] This network produces output loss
I1027 03:03:12.311646 25274 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 03:03:12.311653 25274 net.cpp:219] Network initialization done.
I1027 03:03:12.311657 25274 net.cpp:220] Memory required for data: 119788292
I1027 03:03:12.311717 25274 solver.cpp:41] Solver scaffolding done.
I1027 03:03:12.311722 25274 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_195000.solverstate
I1027 03:03:12.311727 25274 solver.cpp:160] Solving Captcha
I1027 03:03:12.311745 25274 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_195000.solverstate
I1027 03:03:18.381175 25274 solver.cpp:502] SGDSolver: restoring history
I1027 03:03:19.255058 25274 solver.cpp:191] Iteration 195000, loss = 2.83561
I1027 03:03:19.255115 25274 solver.cpp:206]     Train net output #0: loss = 2.83561 (* 1 = 2.83561 loss)
I1027 03:03:19.255130 25274 solver.cpp:403] Iteration 195000, lr = 0.00103797
I1027 03:07:21.156461 25274 solver.cpp:191] Iteration 196000, loss = 2.59299
I1027 03:07:21.157604 25274 solver.cpp:206]     Train net output #0: loss = 2.59299 (* 1 = 2.59299 loss)
I1027 03:07:21.157637 25274 solver.cpp:403] Iteration 196000, lr = 0.00103419
I1027 03:11:22.493075 25274 solver.cpp:191] Iteration 197000, loss = 2.50961
I1027 03:11:22.493713 25274 solver.cpp:206]     Train net output #0: loss = 2.50961 (* 1 = 2.50961 loss)
I1027 03:11:22.493741 25274 solver.cpp:403] Iteration 197000, lr = 0.00103044
I1027 03:15:23.944886 25274 solver.cpp:191] Iteration 198000, loss = 2.38261
I1027 03:15:23.945565 25274 solver.cpp:206]     Train net output #0: loss = 2.38261 (* 1 = 2.38261 loss)
I1027 03:15:23.945597 25274 solver.cpp:403] Iteration 198000, lr = 0.00102672
I1027 03:19:25.448920 25274 solver.cpp:191] Iteration 199000, loss = 2.36144
I1027 03:19:25.449451 25274 solver.cpp:206]     Train net output #0: loss = 2.36144 (* 1 = 2.36144 loss)
I1027 03:19:25.449486 25274 solver.cpp:403] Iteration 199000, lr = 0.00102303
I1027 03:23:27.650607 25274 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_200000.caffemodel
I1027 03:23:33.338158 25274 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_200000.solverstate
I1027 03:23:38.393045 25274 solver.cpp:191] Iteration 200000, loss = 2.45995
I1027 03:23:38.393743 25274 solver.cpp:206]     Train net output #0: loss = 2.45995 (* 1 = 2.45995 loss)
I1027 03:23:38.393779 25274 solver.cpp:403] Iteration 200000, lr = 0.00101938
I1027 03:27:40.037592 25274 solver.cpp:191] Iteration 201000, loss = 2.34339
I1027 03:27:40.038238 25274 solver.cpp:206]     Train net output #0: loss = 2.34339 (* 1 = 2.34339 loss)
I1027 03:27:40.038270 25274 solver.cpp:403] Iteration 201000, lr = 0.00101575
I1027 03:31:41.526412 25274 solver.cpp:191] Iteration 202000, loss = 2.44787
I1027 03:31:41.526983 25274 solver.cpp:206]     Train net output #0: loss = 2.44787 (* 1 = 2.44787 loss)
I1027 03:31:41.527016 25274 solver.cpp:403] Iteration 202000, lr = 0.00101216
I1027 03:35:43.008194 25274 solver.cpp:191] Iteration 203000, loss = 2.53644
I1027 03:35:43.008781 25274 solver.cpp:206]     Train net output #0: loss = 2.53644 (* 1 = 2.53644 loss)
I1027 03:35:43.008815 25274 solver.cpp:403] Iteration 203000, lr = 0.00100859
I1027 03:39:44.482017 25274 solver.cpp:191] Iteration 204000, loss = 2.44586
I1027 03:39:44.482540 25274 solver.cpp:206]     Train net output #0: loss = 2.44586 (* 1 = 2.44586 loss)
I1027 03:39:44.482559 25274 solver.cpp:403] Iteration 204000, lr = 0.00100505
I1027 03:43:46.421057 25274 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_205000.caffemodel
I1027 03:43:50.991958 25274 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_205000.solverstate
I1027 03:43:54.915091 25274 solver.cpp:191] Iteration 205000, loss = 2.48352
I1027 03:43:54.915611 25274 solver.cpp:206]     Train net output #0: loss = 2.48352 (* 1 = 2.48352 loss)
I1027 03:43:54.915647 25274 solver.cpp:403] Iteration 205000, lr = 0.00100155
I1027 03:47:56.468777 25274 solver.cpp:191] Iteration 206000, loss = 2.30265
I1027 03:47:56.469454 25274 solver.cpp:206]     Train net output #0: loss = 2.30265 (* 1 = 2.30265 loss)
I1027 03:47:56.469487 25274 solver.cpp:403] Iteration 206000, lr = 0.000998067
I1027 03:51:57.989833 25274 solver.cpp:191] Iteration 207000, loss = 2.44563
I1027 03:51:57.990381 25274 solver.cpp:206]     Train net output #0: loss = 2.44563 (* 1 = 2.44563 loss)
I1027 03:51:57.990419 25274 solver.cpp:403] Iteration 207000, lr = 0.000994615
I1027 03:55:59.520306 25274 solver.cpp:191] Iteration 208000, loss = 2.36126
I1027 03:55:59.520843 25274 solver.cpp:206]     Train net output #0: loss = 2.36126 (* 1 = 2.36126 loss)
I1027 03:55:59.520862 25274 solver.cpp:403] Iteration 208000, lr = 0.000991192
I1027 04:00:00.993350 25274 solver.cpp:191] Iteration 209000, loss = 2.25993
I1027 04:00:00.994020 25274 solver.cpp:206]     Train net output #0: loss = 2.25993 (* 1 = 2.25993 loss)
I1027 04:00:00.994057 25274 solver.cpp:403] Iteration 209000, lr = 0.000987795
I1027 04:04:02.786574 25274 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_210000.caffemodel
I1027 04:04:07.284247 25274 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_210000.solverstate
I1027 04:04:10.870024 25274 solver.cpp:191] Iteration 210000, loss = 2.45174
I1027 04:04:10.870570 25274 solver.cpp:206]     Train net output #0: loss = 2.45174 (* 1 = 2.45174 loss)
I1027 04:04:10.870602 25274 solver.cpp:403] Iteration 210000, lr = 0.000984426
I1027 04:08:12.195199 25274 solver.cpp:191] Iteration 211000, loss = 2.38334
I1027 04:08:12.195778 25274 solver.cpp:206]     Train net output #0: loss = 2.38334 (* 1 = 2.38334 loss)
I1027 04:08:12.195806 25274 solver.cpp:403] Iteration 211000, lr = 0.000981083
I1027 04:12:13.587062 25274 solver.cpp:191] Iteration 212000, loss = 2.26508
I1027 04:12:13.587682 25274 solver.cpp:206]     Train net output #0: loss = 2.26508 (* 1 = 2.26508 loss)
I1027 04:12:13.587714 25274 solver.cpp:403] Iteration 212000, lr = 0.000977767
I1027 04:16:14.954535 25274 solver.cpp:191] Iteration 213000, loss = 2.51105
I1027 04:16:14.955126 25274 solver.cpp:206]     Train net output #0: loss = 2.51105 (* 1 = 2.51105 loss)
I1027 04:16:14.955158 25274 solver.cpp:403] Iteration 213000, lr = 0.000974476
I1027 04:20:16.360254 25274 solver.cpp:191] Iteration 214000, loss = 2.27157
I1027 04:20:16.360878 25274 solver.cpp:206]     Train net output #0: loss = 2.27157 (* 1 = 2.27157 loss)
I1027 04:20:16.360913 25274 solver.cpp:403] Iteration 214000, lr = 0.000971212
I1027 04:24:18.069437 25274 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_215000.caffemodel
I1027 04:24:22.529876 25274 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_215000.solverstate
I1027 04:24:26.194903 25274 solver.cpp:228] Iteration 215000, loss = 2.44853
I1027 04:24:26.195422 25274 solver.cpp:233] Optimization Done.
I1027 04:24:26.195446 25274 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1027 04:46:32.041023 16255 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 04:46:32.041126 16255 net.cpp:358] Input 0 -> data
I1027 04:46:32.041157 16255 net.cpp:67] Creating Layer conv1
I1027 04:46:32.041162 16255 net.cpp:394] conv1 <- data
I1027 04:46:32.041168 16255 net.cpp:356] conv1 -> conv1
I1027 04:46:32.041179 16255 net.cpp:96] Setting up conv1
I1027 04:46:32.041501 16255 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 04:46:32.041525 16255 net.cpp:67] Creating Layer pool1
I1027 04:46:32.041532 16255 net.cpp:394] pool1 <- conv1
I1027 04:46:32.041538 16255 net.cpp:356] pool1 -> pool1
I1027 04:46:32.041545 16255 net.cpp:96] Setting up pool1
I1027 04:46:32.041558 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:46:32.041564 16255 net.cpp:67] Creating Layer relu1
I1027 04:46:32.041568 16255 net.cpp:394] relu1 <- pool1
I1027 04:46:32.041574 16255 net.cpp:345] relu1 -> pool1 (in-place)
I1027 04:46:32.041579 16255 net.cpp:96] Setting up relu1
I1027 04:46:32.041584 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:46:32.041590 16255 net.cpp:67] Creating Layer drop1
I1027 04:46:32.041595 16255 net.cpp:394] drop1 <- pool1
I1027 04:46:32.041600 16255 net.cpp:345] drop1 -> pool1 (in-place)
I1027 04:46:32.041606 16255 net.cpp:96] Setting up drop1
I1027 04:46:32.041611 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:46:32.041618 16255 net.cpp:67] Creating Layer conv2
I1027 04:46:32.041622 16255 net.cpp:394] conv2 <- pool1
I1027 04:46:32.041628 16255 net.cpp:356] conv2 -> conv2
I1027 04:46:32.041635 16255 net.cpp:96] Setting up conv2
I1027 04:46:32.042196 16255 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 04:46:32.042212 16255 net.cpp:67] Creating Layer pool2
I1027 04:46:32.042215 16255 net.cpp:394] pool2 <- conv2
I1027 04:46:32.042222 16255 net.cpp:356] pool2 -> pool2
I1027 04:46:32.042228 16255 net.cpp:96] Setting up pool2
I1027 04:46:32.042237 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:46:32.042244 16255 net.cpp:67] Creating Layer relu2
I1027 04:46:32.042249 16255 net.cpp:394] relu2 <- pool2
I1027 04:46:32.042254 16255 net.cpp:345] relu2 -> pool2 (in-place)
I1027 04:46:32.042260 16255 net.cpp:96] Setting up relu2
I1027 04:46:32.042264 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:46:32.042271 16255 net.cpp:67] Creating Layer drop2
I1027 04:46:32.042275 16255 net.cpp:394] drop2 <- pool2
I1027 04:46:32.042280 16255 net.cpp:345] drop2 -> pool2 (in-place)
I1027 04:46:32.042286 16255 net.cpp:96] Setting up drop2
I1027 04:46:32.042290 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:46:32.042297 16255 net.cpp:67] Creating Layer conv3
I1027 04:46:32.042301 16255 net.cpp:394] conv3 <- pool2
I1027 04:46:32.042309 16255 net.cpp:356] conv3 -> conv3
I1027 04:46:32.042316 16255 net.cpp:96] Setting up conv3
I1027 04:46:32.043797 16255 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 04:46:32.043814 16255 net.cpp:67] Creating Layer pool3
I1027 04:46:32.043819 16255 net.cpp:394] pool3 <- conv3
I1027 04:46:32.043825 16255 net.cpp:356] pool3 -> pool3
I1027 04:46:32.043831 16255 net.cpp:96] Setting up pool3
I1027 04:46:32.043836 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:46:32.043843 16255 net.cpp:67] Creating Layer relu3
I1027 04:46:32.043848 16255 net.cpp:394] relu3 <- pool3
I1027 04:46:32.043853 16255 net.cpp:345] relu3 -> pool3 (in-place)
I1027 04:46:32.043858 16255 net.cpp:96] Setting up relu3
I1027 04:46:32.043862 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:46:32.043867 16255 net.cpp:67] Creating Layer drop3
I1027 04:46:32.043871 16255 net.cpp:394] drop3 <- pool3
I1027 04:46:32.043876 16255 net.cpp:345] drop3 -> pool3 (in-place)
I1027 04:46:32.043881 16255 net.cpp:96] Setting up drop3
I1027 04:46:32.043885 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:46:32.043894 16255 net.cpp:67] Creating Layer ip1
I1027 04:46:32.043897 16255 net.cpp:394] ip1 <- pool3
I1027 04:46:32.043903 16255 net.cpp:356] ip1 -> ip1
I1027 04:46:32.043910 16255 net.cpp:96] Setting up ip1
I1027 04:46:32.535272 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:46:32.535333 16255 net.cpp:67] Creating Layer relu4
I1027 04:46:32.535341 16255 net.cpp:394] relu4 <- ip1
I1027 04:46:32.535348 16255 net.cpp:345] relu4 -> ip1 (in-place)
I1027 04:46:32.535358 16255 net.cpp:96] Setting up relu4
I1027 04:46:32.535363 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:46:32.535372 16255 net.cpp:67] Creating Layer drop4
I1027 04:46:32.535375 16255 net.cpp:394] drop4 <- ip1
I1027 04:46:32.535385 16255 net.cpp:345] drop4 -> ip1 (in-place)
I1027 04:46:32.535392 16255 net.cpp:96] Setting up drop4
I1027 04:46:32.535397 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:46:32.535404 16255 net.cpp:67] Creating Layer ip2
I1027 04:46:32.535408 16255 net.cpp:394] ip2 <- ip1
I1027 04:46:32.535416 16255 net.cpp:356] ip2 -> ip2
I1027 04:46:32.535429 16255 net.cpp:96] Setting up ip2
I1027 04:46:32.545430 16255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 04:46:32.545500 16255 net.cpp:67] Creating Layer prob
I1027 04:46:32.545508 16255 net.cpp:394] prob <- ip2
I1027 04:46:32.545516 16255 net.cpp:356] prob -> prob
I1027 04:46:32.545527 16255 net.cpp:96] Setting up prob
I1027 04:46:32.545533 16255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 04:46:32.545537 16255 net.cpp:172] prob does not need backward computation.
I1027 04:46:32.545542 16255 net.cpp:172] ip2 does not need backward computation.
I1027 04:46:32.545545 16255 net.cpp:172] drop4 does not need backward computation.
I1027 04:46:32.545548 16255 net.cpp:172] relu4 does not need backward computation.
I1027 04:46:32.545552 16255 net.cpp:172] ip1 does not need backward computation.
I1027 04:46:32.545555 16255 net.cpp:172] drop3 does not need backward computation.
I1027 04:46:32.545559 16255 net.cpp:172] relu3 does not need backward computation.
I1027 04:46:32.545562 16255 net.cpp:172] pool3 does not need backward computation.
I1027 04:46:32.545567 16255 net.cpp:172] conv3 does not need backward computation.
I1027 04:46:32.545577 16255 net.cpp:172] drop2 does not need backward computation.
I1027 04:46:32.545580 16255 net.cpp:172] relu2 does not need backward computation.
I1027 04:46:32.545584 16255 net.cpp:172] pool2 does not need backward computation.
I1027 04:46:32.545588 16255 net.cpp:172] conv2 does not need backward computation.
I1027 04:46:32.545591 16255 net.cpp:172] drop1 does not need backward computation.
I1027 04:46:32.545594 16255 net.cpp:172] relu1 does not need backward computation.
I1027 04:46:32.545598 16255 net.cpp:172] pool1 does not need backward computation.
I1027 04:46:32.545601 16255 net.cpp:172] conv1 does not need backward computation.
I1027 04:46:32.545605 16255 net.cpp:208] This network produces output prob
I1027 04:46:32.545619 16255 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 04:46:32.545626 16255 net.cpp:219] Network initialization done.
I1027 04:46:32.545630 16255 net.cpp:220] Memory required for data: 1837200
I1027 04:47:14.431795 16255 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 04:47:14.432267 16255 net.cpp:358] Input 0 -> data
I1027 04:47:14.432297 16255 net.cpp:67] Creating Layer conv1
I1027 04:47:14.432303 16255 net.cpp:394] conv1 <- data
I1027 04:47:14.432310 16255 net.cpp:356] conv1 -> conv1
I1027 04:47:14.432320 16255 net.cpp:96] Setting up conv1
I1027 04:47:14.432358 16255 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 04:47:14.432375 16255 net.cpp:67] Creating Layer pool1
I1027 04:47:14.432380 16255 net.cpp:394] pool1 <- conv1
I1027 04:47:14.432386 16255 net.cpp:356] pool1 -> pool1
I1027 04:47:14.432394 16255 net.cpp:96] Setting up pool1
I1027 04:47:14.432401 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:47:14.432409 16255 net.cpp:67] Creating Layer relu1
I1027 04:47:14.432412 16255 net.cpp:394] relu1 <- pool1
I1027 04:47:14.432432 16255 net.cpp:345] relu1 -> pool1 (in-place)
I1027 04:47:14.432447 16255 net.cpp:96] Setting up relu1
I1027 04:47:14.432452 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:47:14.432459 16255 net.cpp:67] Creating Layer drop1
I1027 04:47:14.432463 16255 net.cpp:394] drop1 <- pool1
I1027 04:47:14.432469 16255 net.cpp:345] drop1 -> pool1 (in-place)
I1027 04:47:14.432476 16255 net.cpp:96] Setting up drop1
I1027 04:47:14.432481 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:47:14.432488 16255 net.cpp:67] Creating Layer conv2
I1027 04:47:14.432492 16255 net.cpp:394] conv2 <- pool1
I1027 04:47:14.432498 16255 net.cpp:356] conv2 -> conv2
I1027 04:47:14.432505 16255 net.cpp:96] Setting up conv2
I1027 04:47:14.433007 16255 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 04:47:14.433022 16255 net.cpp:67] Creating Layer pool2
I1027 04:47:14.433027 16255 net.cpp:394] pool2 <- conv2
I1027 04:47:14.433033 16255 net.cpp:356] pool2 -> pool2
I1027 04:47:14.433040 16255 net.cpp:96] Setting up pool2
I1027 04:47:14.433046 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:47:14.433053 16255 net.cpp:67] Creating Layer relu2
I1027 04:47:14.433056 16255 net.cpp:394] relu2 <- pool2
I1027 04:47:14.433061 16255 net.cpp:345] relu2 -> pool2 (in-place)
I1027 04:47:14.433068 16255 net.cpp:96] Setting up relu2
I1027 04:47:14.433071 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:47:14.433076 16255 net.cpp:67] Creating Layer drop2
I1027 04:47:14.433080 16255 net.cpp:394] drop2 <- pool2
I1027 04:47:14.433085 16255 net.cpp:345] drop2 -> pool2 (in-place)
I1027 04:47:14.433091 16255 net.cpp:96] Setting up drop2
I1027 04:47:14.433095 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:47:14.433104 16255 net.cpp:67] Creating Layer conv3
I1027 04:47:14.433107 16255 net.cpp:394] conv3 <- pool2
I1027 04:47:14.433114 16255 net.cpp:356] conv3 -> conv3
I1027 04:47:14.433120 16255 net.cpp:96] Setting up conv3
I1027 04:47:14.434440 16255 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 04:47:14.434455 16255 net.cpp:67] Creating Layer pool3
I1027 04:47:14.434460 16255 net.cpp:394] pool3 <- conv3
I1027 04:47:14.434466 16255 net.cpp:356] pool3 -> pool3
I1027 04:47:14.434473 16255 net.cpp:96] Setting up pool3
I1027 04:47:14.434478 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:47:14.434484 16255 net.cpp:67] Creating Layer relu3
I1027 04:47:14.434489 16255 net.cpp:394] relu3 <- pool3
I1027 04:47:14.434494 16255 net.cpp:345] relu3 -> pool3 (in-place)
I1027 04:47:14.434499 16255 net.cpp:96] Setting up relu3
I1027 04:47:14.434502 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:47:14.434509 16255 net.cpp:67] Creating Layer drop3
I1027 04:47:14.434512 16255 net.cpp:394] drop3 <- pool3
I1027 04:47:14.434517 16255 net.cpp:345] drop3 -> pool3 (in-place)
I1027 04:47:14.434523 16255 net.cpp:96] Setting up drop3
I1027 04:47:14.434527 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:47:14.434535 16255 net.cpp:67] Creating Layer ip1
I1027 04:47:14.434538 16255 net.cpp:394] ip1 <- pool3
I1027 04:47:14.434545 16255 net.cpp:356] ip1 -> ip1
I1027 04:47:14.434551 16255 net.cpp:96] Setting up ip1
I1027 04:47:14.847955 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:47:14.848017 16255 net.cpp:67] Creating Layer relu4
I1027 04:47:14.848026 16255 net.cpp:394] relu4 <- ip1
I1027 04:47:14.848037 16255 net.cpp:345] relu4 -> ip1 (in-place)
I1027 04:47:14.848047 16255 net.cpp:96] Setting up relu4
I1027 04:47:14.848052 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:47:14.848060 16255 net.cpp:67] Creating Layer drop4
I1027 04:47:14.848078 16255 net.cpp:394] drop4 <- ip1
I1027 04:47:14.848085 16255 net.cpp:345] drop4 -> ip1 (in-place)
I1027 04:47:14.848093 16255 net.cpp:96] Setting up drop4
I1027 04:47:14.848098 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:47:14.848109 16255 net.cpp:67] Creating Layer ip2
I1027 04:47:14.848114 16255 net.cpp:394] ip2 <- ip1
I1027 04:47:14.848121 16255 net.cpp:356] ip2 -> ip2
I1027 04:47:14.848135 16255 net.cpp:96] Setting up ip2
I1027 04:47:14.855720 16255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 04:47:14.855780 16255 net.cpp:67] Creating Layer prob
I1027 04:47:14.855788 16255 net.cpp:394] prob <- ip2
I1027 04:47:14.855797 16255 net.cpp:356] prob -> prob
I1027 04:47:14.855808 16255 net.cpp:96] Setting up prob
I1027 04:47:14.855816 16255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 04:47:14.855820 16255 net.cpp:172] prob does not need backward computation.
I1027 04:47:14.855824 16255 net.cpp:172] ip2 does not need backward computation.
I1027 04:47:14.855829 16255 net.cpp:172] drop4 does not need backward computation.
I1027 04:47:14.855833 16255 net.cpp:172] relu4 does not need backward computation.
I1027 04:47:14.855836 16255 net.cpp:172] ip1 does not need backward computation.
I1027 04:47:14.855840 16255 net.cpp:172] drop3 does not need backward computation.
I1027 04:47:14.855844 16255 net.cpp:172] relu3 does not need backward computation.
I1027 04:47:14.855847 16255 net.cpp:172] pool3 does not need backward computation.
I1027 04:47:14.855851 16255 net.cpp:172] conv3 does not need backward computation.
I1027 04:47:14.855855 16255 net.cpp:172] drop2 does not need backward computation.
I1027 04:47:14.855859 16255 net.cpp:172] relu2 does not need backward computation.
I1027 04:47:14.855864 16255 net.cpp:172] pool2 does not need backward computation.
I1027 04:47:14.855867 16255 net.cpp:172] conv2 does not need backward computation.
I1027 04:47:14.855871 16255 net.cpp:172] drop1 does not need backward computation.
I1027 04:47:14.855875 16255 net.cpp:172] relu1 does not need backward computation.
I1027 04:47:14.855880 16255 net.cpp:172] pool1 does not need backward computation.
I1027 04:47:14.855882 16255 net.cpp:172] conv1 does not need backward computation.
I1027 04:47:14.855886 16255 net.cpp:208] This network produces output prob
I1027 04:47:14.855902 16255 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 04:47:14.855912 16255 net.cpp:219] Network initialization done.
I1027 04:47:14.855916 16255 net.cpp:220] Memory required for data: 1837200
I1027 04:47:51.906455 16255 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 04:47:51.906998 16255 net.cpp:358] Input 0 -> data
I1027 04:47:51.907028 16255 net.cpp:67] Creating Layer conv1
I1027 04:47:51.907034 16255 net.cpp:394] conv1 <- data
I1027 04:47:51.907042 16255 net.cpp:356] conv1 -> conv1
I1027 04:47:51.907052 16255 net.cpp:96] Setting up conv1
I1027 04:47:51.907081 16255 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 04:47:51.907097 16255 net.cpp:67] Creating Layer pool1
I1027 04:47:51.907102 16255 net.cpp:394] pool1 <- conv1
I1027 04:47:51.907109 16255 net.cpp:356] pool1 -> pool1
I1027 04:47:51.907115 16255 net.cpp:96] Setting up pool1
I1027 04:47:51.907124 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:47:51.907130 16255 net.cpp:67] Creating Layer relu1
I1027 04:47:51.907135 16255 net.cpp:394] relu1 <- pool1
I1027 04:47:51.907140 16255 net.cpp:345] relu1 -> pool1 (in-place)
I1027 04:47:51.907145 16255 net.cpp:96] Setting up relu1
I1027 04:47:51.907150 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:47:51.907156 16255 net.cpp:67] Creating Layer drop1
I1027 04:47:51.907160 16255 net.cpp:394] drop1 <- pool1
I1027 04:47:51.907166 16255 net.cpp:345] drop1 -> pool1 (in-place)
I1027 04:47:51.907171 16255 net.cpp:96] Setting up drop1
I1027 04:47:51.907176 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:47:51.907183 16255 net.cpp:67] Creating Layer conv2
I1027 04:47:51.907188 16255 net.cpp:394] conv2 <- pool1
I1027 04:47:51.907194 16255 net.cpp:356] conv2 -> conv2
I1027 04:47:51.907202 16255 net.cpp:96] Setting up conv2
I1027 04:47:51.907704 16255 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 04:47:51.907719 16255 net.cpp:67] Creating Layer pool2
I1027 04:47:51.907724 16255 net.cpp:394] pool2 <- conv2
I1027 04:47:51.907730 16255 net.cpp:356] pool2 -> pool2
I1027 04:47:51.907737 16255 net.cpp:96] Setting up pool2
I1027 04:47:51.907742 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:47:51.907748 16255 net.cpp:67] Creating Layer relu2
I1027 04:47:51.907752 16255 net.cpp:394] relu2 <- pool2
I1027 04:47:51.907757 16255 net.cpp:345] relu2 -> pool2 (in-place)
I1027 04:47:51.907763 16255 net.cpp:96] Setting up relu2
I1027 04:47:51.907768 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:47:51.907773 16255 net.cpp:67] Creating Layer drop2
I1027 04:47:51.907776 16255 net.cpp:394] drop2 <- pool2
I1027 04:47:51.907781 16255 net.cpp:345] drop2 -> pool2 (in-place)
I1027 04:47:51.907786 16255 net.cpp:96] Setting up drop2
I1027 04:47:51.907791 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:47:51.907799 16255 net.cpp:67] Creating Layer conv3
I1027 04:47:51.907802 16255 net.cpp:394] conv3 <- pool2
I1027 04:47:51.907809 16255 net.cpp:356] conv3 -> conv3
I1027 04:47:51.907815 16255 net.cpp:96] Setting up conv3
I1027 04:47:51.909167 16255 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 04:47:51.909188 16255 net.cpp:67] Creating Layer pool3
I1027 04:47:51.909193 16255 net.cpp:394] pool3 <- conv3
I1027 04:47:51.909199 16255 net.cpp:356] pool3 -> pool3
I1027 04:47:51.909206 16255 net.cpp:96] Setting up pool3
I1027 04:47:51.909212 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:47:51.909217 16255 net.cpp:67] Creating Layer relu3
I1027 04:47:51.909221 16255 net.cpp:394] relu3 <- pool3
I1027 04:47:51.909227 16255 net.cpp:345] relu3 -> pool3 (in-place)
I1027 04:47:51.909234 16255 net.cpp:96] Setting up relu3
I1027 04:47:51.909237 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:47:51.909242 16255 net.cpp:67] Creating Layer drop3
I1027 04:47:51.909246 16255 net.cpp:394] drop3 <- pool3
I1027 04:47:51.909251 16255 net.cpp:345] drop3 -> pool3 (in-place)
I1027 04:47:51.909257 16255 net.cpp:96] Setting up drop3
I1027 04:47:51.909262 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:47:51.909268 16255 net.cpp:67] Creating Layer ip1
I1027 04:47:51.909272 16255 net.cpp:394] ip1 <- pool3
I1027 04:47:51.909278 16255 net.cpp:356] ip1 -> ip1
I1027 04:47:51.909286 16255 net.cpp:96] Setting up ip1
I1027 04:47:52.315755 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:47:52.315820 16255 net.cpp:67] Creating Layer relu4
I1027 04:47:52.315827 16255 net.cpp:394] relu4 <- ip1
I1027 04:47:52.315837 16255 net.cpp:345] relu4 -> ip1 (in-place)
I1027 04:47:52.315847 16255 net.cpp:96] Setting up relu4
I1027 04:47:52.315853 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:47:52.315861 16255 net.cpp:67] Creating Layer drop4
I1027 04:47:52.315866 16255 net.cpp:394] drop4 <- ip1
I1027 04:47:52.315873 16255 net.cpp:345] drop4 -> ip1 (in-place)
I1027 04:47:52.315881 16255 net.cpp:96] Setting up drop4
I1027 04:47:52.315886 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:47:52.315897 16255 net.cpp:67] Creating Layer ip2
I1027 04:47:52.315902 16255 net.cpp:394] ip2 <- ip1
I1027 04:47:52.315910 16255 net.cpp:356] ip2 -> ip2
I1027 04:47:52.315924 16255 net.cpp:96] Setting up ip2
I1027 04:47:52.323550 16255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 04:47:52.323616 16255 net.cpp:67] Creating Layer prob
I1027 04:47:52.323624 16255 net.cpp:394] prob <- ip2
I1027 04:47:52.323633 16255 net.cpp:356] prob -> prob
I1027 04:47:52.323645 16255 net.cpp:96] Setting up prob
I1027 04:47:52.323653 16255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 04:47:52.323658 16255 net.cpp:172] prob does not need backward computation.
I1027 04:47:52.323663 16255 net.cpp:172] ip2 does not need backward computation.
I1027 04:47:52.323668 16255 net.cpp:172] drop4 does not need backward computation.
I1027 04:47:52.323671 16255 net.cpp:172] relu4 does not need backward computation.
I1027 04:47:52.323675 16255 net.cpp:172] ip1 does not need backward computation.
I1027 04:47:52.323679 16255 net.cpp:172] drop3 does not need backward computation.
I1027 04:47:52.323683 16255 net.cpp:172] relu3 does not need backward computation.
I1027 04:47:52.323688 16255 net.cpp:172] pool3 does not need backward computation.
I1027 04:47:52.323691 16255 net.cpp:172] conv3 does not need backward computation.
I1027 04:47:52.323695 16255 net.cpp:172] drop2 does not need backward computation.
I1027 04:47:52.323699 16255 net.cpp:172] relu2 does not need backward computation.
I1027 04:47:52.323704 16255 net.cpp:172] pool2 does not need backward computation.
I1027 04:47:52.323707 16255 net.cpp:172] conv2 does not need backward computation.
I1027 04:47:52.323710 16255 net.cpp:172] drop1 does not need backward computation.
I1027 04:47:52.323714 16255 net.cpp:172] relu1 does not need backward computation.
I1027 04:47:52.323719 16255 net.cpp:172] pool1 does not need backward computation.
I1027 04:47:52.323722 16255 net.cpp:172] conv1 does not need backward computation.
I1027 04:47:52.323726 16255 net.cpp:208] This network produces output prob
I1027 04:47:52.323740 16255 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 04:47:52.323750 16255 net.cpp:219] Network initialization done.
I1027 04:47:52.323755 16255 net.cpp:220] Memory required for data: 1837200
I1027 04:48:29.214408 16255 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 04:48:29.215041 16255 net.cpp:358] Input 0 -> data
I1027 04:48:29.215093 16255 net.cpp:67] Creating Layer conv1
I1027 04:48:29.215106 16255 net.cpp:394] conv1 <- data
I1027 04:48:29.215122 16255 net.cpp:356] conv1 -> conv1
I1027 04:48:29.215143 16255 net.cpp:96] Setting up conv1
I1027 04:48:29.215201 16255 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 04:48:29.215234 16255 net.cpp:67] Creating Layer pool1
I1027 04:48:29.215245 16255 net.cpp:394] pool1 <- conv1
I1027 04:48:29.215257 16255 net.cpp:356] pool1 -> pool1
I1027 04:48:29.215273 16255 net.cpp:96] Setting up pool1
I1027 04:48:29.215288 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:48:29.215304 16255 net.cpp:67] Creating Layer relu1
I1027 04:48:29.215313 16255 net.cpp:394] relu1 <- pool1
I1027 04:48:29.215325 16255 net.cpp:345] relu1 -> pool1 (in-place)
I1027 04:48:29.215338 16255 net.cpp:96] Setting up relu1
I1027 04:48:29.215348 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:48:29.215361 16255 net.cpp:67] Creating Layer drop1
I1027 04:48:29.215371 16255 net.cpp:394] drop1 <- pool1
I1027 04:48:29.215384 16255 net.cpp:345] drop1 -> pool1 (in-place)
I1027 04:48:29.215397 16255 net.cpp:96] Setting up drop1
I1027 04:48:29.215409 16255 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 04:48:29.215435 16255 net.cpp:67] Creating Layer conv2
I1027 04:48:29.215445 16255 net.cpp:394] conv2 <- pool1
I1027 04:48:29.215461 16255 net.cpp:356] conv2 -> conv2
I1027 04:48:29.215476 16255 net.cpp:96] Setting up conv2
I1027 04:48:29.216657 16255 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 04:48:29.216688 16255 net.cpp:67] Creating Layer pool2
I1027 04:48:29.216699 16255 net.cpp:394] pool2 <- conv2
I1027 04:48:29.216713 16255 net.cpp:356] pool2 -> pool2
I1027 04:48:29.216729 16255 net.cpp:96] Setting up pool2
I1027 04:48:29.216742 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:48:29.216755 16255 net.cpp:67] Creating Layer relu2
I1027 04:48:29.216764 16255 net.cpp:394] relu2 <- pool2
I1027 04:48:29.216776 16255 net.cpp:345] relu2 -> pool2 (in-place)
I1027 04:48:29.216795 16255 net.cpp:96] Setting up relu2
I1027 04:48:29.216809 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:48:29.216823 16255 net.cpp:67] Creating Layer drop2
I1027 04:48:29.216835 16255 net.cpp:394] drop2 <- pool2
I1027 04:48:29.216850 16255 net.cpp:345] drop2 -> pool2 (in-place)
I1027 04:48:29.216866 16255 net.cpp:96] Setting up drop2
I1027 04:48:29.216878 16255 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 04:48:29.216898 16255 net.cpp:67] Creating Layer conv3
I1027 04:48:29.216909 16255 net.cpp:394] conv3 <- pool2
I1027 04:48:29.216927 16255 net.cpp:356] conv3 -> conv3
I1027 04:48:29.216944 16255 net.cpp:96] Setting up conv3
I1027 04:48:29.220566 16255 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 04:48:29.220587 16255 net.cpp:67] Creating Layer pool3
I1027 04:48:29.220592 16255 net.cpp:394] pool3 <- conv3
I1027 04:48:29.220599 16255 net.cpp:356] pool3 -> pool3
I1027 04:48:29.220605 16255 net.cpp:96] Setting up pool3
I1027 04:48:29.220612 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:48:29.220618 16255 net.cpp:67] Creating Layer relu3
I1027 04:48:29.220621 16255 net.cpp:394] relu3 <- pool3
I1027 04:48:29.220628 16255 net.cpp:345] relu3 -> pool3 (in-place)
I1027 04:48:29.220633 16255 net.cpp:96] Setting up relu3
I1027 04:48:29.220636 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:48:29.220643 16255 net.cpp:67] Creating Layer drop3
I1027 04:48:29.220646 16255 net.cpp:394] drop3 <- pool3
I1027 04:48:29.220652 16255 net.cpp:345] drop3 -> pool3 (in-place)
I1027 04:48:29.220657 16255 net.cpp:96] Setting up drop3
I1027 04:48:29.220662 16255 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 04:48:29.220669 16255 net.cpp:67] Creating Layer ip1
I1027 04:48:29.220672 16255 net.cpp:394] ip1 <- pool3
I1027 04:48:29.220679 16255 net.cpp:356] ip1 -> ip1
I1027 04:48:29.220686 16255 net.cpp:96] Setting up ip1
I1027 04:48:29.624008 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:48:29.624073 16255 net.cpp:67] Creating Layer relu4
I1027 04:48:29.624081 16255 net.cpp:394] relu4 <- ip1
I1027 04:48:29.624092 16255 net.cpp:345] relu4 -> ip1 (in-place)
I1027 04:48:29.624102 16255 net.cpp:96] Setting up relu4
I1027 04:48:29.624107 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:48:29.624116 16255 net.cpp:67] Creating Layer drop4
I1027 04:48:29.624120 16255 net.cpp:394] drop4 <- ip1
I1027 04:48:29.624127 16255 net.cpp:345] drop4 -> ip1 (in-place)
I1027 04:48:29.624135 16255 net.cpp:96] Setting up drop4
I1027 04:48:29.624140 16255 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 04:48:29.624151 16255 net.cpp:67] Creating Layer ip2
I1027 04:48:29.624155 16255 net.cpp:394] ip2 <- ip1
I1027 04:48:29.624163 16255 net.cpp:356] ip2 -> ip2
I1027 04:48:29.624178 16255 net.cpp:96] Setting up ip2
I1027 04:48:29.631781 16255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 04:48:29.631845 16255 net.cpp:67] Creating Layer prob
I1027 04:48:29.631852 16255 net.cpp:394] prob <- ip2
I1027 04:48:29.631863 16255 net.cpp:356] prob -> prob
I1027 04:48:29.631875 16255 net.cpp:96] Setting up prob
I1027 04:48:29.631883 16255 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 04:48:29.631888 16255 net.cpp:172] prob does not need backward computation.
I1027 04:48:29.631892 16255 net.cpp:172] ip2 does not need backward computation.
I1027 04:48:29.631907 16255 net.cpp:172] drop4 does not need backward computation.
I1027 04:48:29.631911 16255 net.cpp:172] relu4 does not need backward computation.
I1027 04:48:29.631916 16255 net.cpp:172] ip1 does not need backward computation.
I1027 04:48:29.631919 16255 net.cpp:172] drop3 does not need backward computation.
I1027 04:48:29.631923 16255 net.cpp:172] relu3 does not need backward computation.
I1027 04:48:29.631927 16255 net.cpp:172] pool3 does not need backward computation.
I1027 04:48:29.631932 16255 net.cpp:172] conv3 does not need backward computation.
I1027 04:48:29.631935 16255 net.cpp:172] drop2 does not need backward computation.
I1027 04:48:29.631938 16255 net.cpp:172] relu2 does not need backward computation.
I1027 04:48:29.631942 16255 net.cpp:172] pool2 does not need backward computation.
I1027 04:48:29.631947 16255 net.cpp:172] conv2 does not need backward computation.
I1027 04:48:29.631950 16255 net.cpp:172] drop1 does not need backward computation.
I1027 04:48:29.631954 16255 net.cpp:172] relu1 does not need backward computation.
I1027 04:48:29.631958 16255 net.cpp:172] pool1 does not need backward computation.
I1027 04:48:29.631961 16255 net.cpp:172] conv1 does not need backward computation.
I1027 04:48:29.631965 16255 net.cpp:208] This network produces output prob
I1027 04:48:29.631980 16255 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 04:48:29.631989 16255 net.cpp:219] Network initialization done.
I1027 04:48:29.631994 16255 net.cpp:220] Memory required for data: 1837200
I1027 05:22:38.156875 22950 convert_imageset.cpp:70] Shuffling data
I1027 05:22:38.725926 22950 convert_imageset.cpp:73] A total of 60000 images.
I1027 05:22:38.726003 22950 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1027 05:22:40.952808 22950 convert_imageset.cpp:177] Processed 1000 files.
E1027 05:22:43.214571 22950 convert_imageset.cpp:177] Processed 2000 files.
E1027 05:22:45.114358 22950 convert_imageset.cpp:177] Processed 3000 files.
E1027 05:22:47.225798 22950 convert_imageset.cpp:177] Processed 4000 files.
E1027 05:22:49.345477 22950 convert_imageset.cpp:177] Processed 5000 files.
E1027 05:22:51.168926 22950 convert_imageset.cpp:177] Processed 6000 files.
E1027 05:22:53.110745 22950 convert_imageset.cpp:177] Processed 7000 files.
E1027 05:22:54.946949 22950 convert_imageset.cpp:177] Processed 8000 files.
E1027 05:22:56.847338 22950 convert_imageset.cpp:177] Processed 9000 files.
E1027 05:22:58.601678 22950 convert_imageset.cpp:177] Processed 10000 files.
E1027 05:23:00.499445 22950 convert_imageset.cpp:177] Processed 11000 files.
E1027 05:23:02.503129 22950 convert_imageset.cpp:177] Processed 12000 files.
E1027 05:23:04.323773 22950 convert_imageset.cpp:177] Processed 13000 files.
E1027 05:23:05.974176 22950 convert_imageset.cpp:177] Processed 14000 files.
E1027 05:23:07.592725 22950 convert_imageset.cpp:177] Processed 15000 files.
E1027 05:23:09.251085 22950 convert_imageset.cpp:177] Processed 16000 files.
E1027 05:23:11.103796 22950 convert_imageset.cpp:177] Processed 17000 files.
E1027 05:23:12.765830 22950 convert_imageset.cpp:177] Processed 18000 files.
E1027 05:23:14.331506 22950 convert_imageset.cpp:177] Processed 19000 files.
E1027 05:23:16.196755 22950 convert_imageset.cpp:177] Processed 20000 files.
E1027 05:23:18.279134 22950 convert_imageset.cpp:177] Processed 21000 files.
E1027 05:23:20.029886 22950 convert_imageset.cpp:177] Processed 22000 files.
E1027 05:23:21.668998 22950 convert_imageset.cpp:177] Processed 23000 files.
E1027 05:23:23.351660 22950 convert_imageset.cpp:177] Processed 24000 files.
E1027 05:23:24.927837 22950 convert_imageset.cpp:177] Processed 25000 files.
E1027 05:23:26.632336 22950 convert_imageset.cpp:177] Processed 26000 files.
E1027 05:23:28.269877 22950 convert_imageset.cpp:177] Processed 27000 files.
E1027 05:23:29.957643 22950 convert_imageset.cpp:177] Processed 28000 files.
E1027 05:23:31.521561 22950 convert_imageset.cpp:177] Processed 29000 files.
E1027 05:23:33.197924 22950 convert_imageset.cpp:177] Processed 30000 files.
E1027 05:23:34.899672 22950 convert_imageset.cpp:177] Processed 31000 files.
E1027 05:23:36.459202 22950 convert_imageset.cpp:177] Processed 32000 files.
E1027 05:23:38.053573 22950 convert_imageset.cpp:177] Processed 33000 files.
E1027 05:23:39.618947 22950 convert_imageset.cpp:177] Processed 34000 files.
E1027 05:23:41.241477 22950 convert_imageset.cpp:177] Processed 35000 files.
E1027 05:23:42.872712 22950 convert_imageset.cpp:177] Processed 36000 files.
E1027 05:23:44.506976 22950 convert_imageset.cpp:177] Processed 37000 files.
E1027 05:23:45.985250 22950 convert_imageset.cpp:177] Processed 38000 files.
E1027 05:23:47.605844 22950 convert_imageset.cpp:177] Processed 39000 files.
E1027 05:23:49.140837 22950 convert_imageset.cpp:177] Processed 40000 files.
E1027 05:23:50.679579 22950 convert_imageset.cpp:177] Processed 41000 files.
E1027 05:23:52.301939 22950 convert_imageset.cpp:177] Processed 42000 files.
E1027 05:23:53.791343 22950 convert_imageset.cpp:177] Processed 43000 files.
E1027 05:23:55.263772 22950 convert_imageset.cpp:177] Processed 44000 files.
E1027 05:23:56.804306 22950 convert_imageset.cpp:177] Processed 45000 files.
E1027 05:23:58.263197 22950 convert_imageset.cpp:177] Processed 46000 files.
E1027 05:23:59.802376 22950 convert_imageset.cpp:177] Processed 47000 files.
E1027 05:24:01.250856 22950 convert_imageset.cpp:177] Processed 48000 files.
E1027 05:24:02.835448 22950 convert_imageset.cpp:177] Processed 49000 files.
E1027 05:24:04.361860 22950 convert_imageset.cpp:177] Processed 50000 files.
E1027 05:24:05.797701 22950 convert_imageset.cpp:177] Processed 51000 files.
E1027 05:24:07.297642 22950 convert_imageset.cpp:177] Processed 52000 files.
E1027 05:24:08.773529 22950 convert_imageset.cpp:177] Processed 53000 files.
E1027 05:24:10.338779 22950 convert_imageset.cpp:177] Processed 54000 files.
E1027 05:24:11.841804 22950 convert_imageset.cpp:177] Processed 55000 files.
E1027 05:24:13.265900 22950 convert_imageset.cpp:177] Processed 56000 files.
E1027 05:24:15.160117 22950 convert_imageset.cpp:177] Processed 57000 files.
E1027 05:24:16.611222 22950 convert_imageset.cpp:177] Processed 58000 files.
E1027 05:24:18.080706 22950 convert_imageset.cpp:177] Processed 59000 files.
E1027 05:24:19.684944 22950 convert_imageset.cpp:177] Processed 60000 files.
I1027 05:24:19.871855 23014 caffe.cpp:99] Use GPU with device ID 0
I1027 05:24:20.214815 23014 caffe.cpp:107] Starting Optimization
I1027 05:24:20.214925 23014 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 235000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1027 05:24:20.214948 23014 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1027 05:24:20.227897 23014 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1027 05:24:20.227994 23014 net.cpp:67] Creating Layer mnist
I1027 05:24:20.228005 23014 net.cpp:356] mnist -> data
I1027 05:24:20.228021 23014 net.cpp:356] mnist -> label
I1027 05:24:20.228035 23014 net.cpp:96] Setting up mnist
I1027 05:24:20.235067 23014 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1027 05:24:20.235196 23014 data_layer.cpp:128] output data size: 64,1,50,180
I1027 05:24:20.236867 23014 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1027 05:24:20.236906 23014 net.cpp:103] Top shape: 64 1 1 1 (64)
I1027 05:24:20.236933 23014 net.cpp:67] Creating Layer conv1
I1027 05:24:20.236948 23014 net.cpp:394] conv1 <- data
I1027 05:24:20.236982 23014 net.cpp:356] conv1 -> conv1
I1027 05:24:20.237011 23014 net.cpp:96] Setting up conv1
I1027 05:24:20.237936 23014 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1027 05:24:20.237999 23014 net.cpp:67] Creating Layer pool1
I1027 05:24:20.238014 23014 net.cpp:394] pool1 <- conv1
I1027 05:24:20.238037 23014 net.cpp:356] pool1 -> pool1
I1027 05:24:20.238059 23014 net.cpp:96] Setting up pool1
I1027 05:24:20.238090 23014 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 05:24:20.238111 23014 net.cpp:67] Creating Layer relu1
I1027 05:24:20.238123 23014 net.cpp:394] relu1 <- pool1
I1027 05:24:20.238140 23014 net.cpp:345] relu1 -> pool1 (in-place)
I1027 05:24:20.238157 23014 net.cpp:96] Setting up relu1
I1027 05:24:20.238170 23014 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 05:24:20.238194 23014 net.cpp:67] Creating Layer drop1
I1027 05:24:20.238209 23014 net.cpp:394] drop1 <- pool1
I1027 05:24:20.238226 23014 net.cpp:345] drop1 -> pool1 (in-place)
I1027 05:24:20.238243 23014 net.cpp:96] Setting up drop1
I1027 05:24:20.238258 23014 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 05:24:20.238277 23014 net.cpp:67] Creating Layer conv2
I1027 05:24:20.238291 23014 net.cpp:394] conv2 <- pool1
I1027 05:24:20.238312 23014 net.cpp:356] conv2 -> conv2
I1027 05:24:20.238343 23014 net.cpp:96] Setting up conv2
I1027 05:24:20.239881 23014 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1027 05:24:20.239918 23014 net.cpp:67] Creating Layer pool2
I1027 05:24:20.239933 23014 net.cpp:394] pool2 <- conv2
I1027 05:24:20.239951 23014 net.cpp:356] pool2 -> pool2
I1027 05:24:20.239970 23014 net.cpp:96] Setting up pool2
I1027 05:24:20.239986 23014 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 05:24:20.240007 23014 net.cpp:67] Creating Layer relu2
I1027 05:24:20.240021 23014 net.cpp:394] relu2 <- pool2
I1027 05:24:20.240037 23014 net.cpp:345] relu2 -> pool2 (in-place)
I1027 05:24:20.240054 23014 net.cpp:96] Setting up relu2
I1027 05:24:20.240067 23014 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 05:24:20.240087 23014 net.cpp:67] Creating Layer drop2
I1027 05:24:20.240105 23014 net.cpp:394] drop2 <- pool2
I1027 05:24:20.240123 23014 net.cpp:345] drop2 -> pool2 (in-place)
I1027 05:24:20.240139 23014 net.cpp:96] Setting up drop2
I1027 05:24:20.240154 23014 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 05:24:20.240173 23014 net.cpp:67] Creating Layer conv3
I1027 05:24:20.240186 23014 net.cpp:394] conv3 <- pool2
I1027 05:24:20.240208 23014 net.cpp:356] conv3 -> conv3
I1027 05:24:20.240229 23014 net.cpp:96] Setting up conv3
I1027 05:24:20.244279 23014 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1027 05:24:20.244334 23014 net.cpp:67] Creating Layer pool3
I1027 05:24:20.244350 23014 net.cpp:394] pool3 <- conv3
I1027 05:24:20.244369 23014 net.cpp:356] pool3 -> pool3
I1027 05:24:20.244388 23014 net.cpp:96] Setting up pool3
I1027 05:24:20.244405 23014 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 05:24:20.244433 23014 net.cpp:67] Creating Layer relu3
I1027 05:24:20.244452 23014 net.cpp:394] relu3 <- pool3
I1027 05:24:20.244469 23014 net.cpp:345] relu3 -> pool3 (in-place)
I1027 05:24:20.244488 23014 net.cpp:96] Setting up relu3
I1027 05:24:20.244500 23014 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 05:24:20.244518 23014 net.cpp:67] Creating Layer drop3
I1027 05:24:20.244530 23014 net.cpp:394] drop3 <- pool3
I1027 05:24:20.244546 23014 net.cpp:345] drop3 -> pool3 (in-place)
I1027 05:24:20.244563 23014 net.cpp:96] Setting up drop3
I1027 05:24:20.244577 23014 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 05:24:20.244601 23014 net.cpp:67] Creating Layer ip1
I1027 05:24:20.244613 23014 net.cpp:394] ip1 <- pool3
I1027 05:24:20.244632 23014 net.cpp:356] ip1 -> ip1
I1027 05:24:20.244694 23014 net.cpp:96] Setting up ip1
I1027 05:24:20.680127 23014 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 05:24:20.680186 23014 net.cpp:67] Creating Layer relu4
I1027 05:24:20.680194 23014 net.cpp:394] relu4 <- ip1
I1027 05:24:20.680203 23014 net.cpp:345] relu4 -> ip1 (in-place)
I1027 05:24:20.680212 23014 net.cpp:96] Setting up relu4
I1027 05:24:20.680217 23014 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 05:24:20.680225 23014 net.cpp:67] Creating Layer drop4
I1027 05:24:20.680229 23014 net.cpp:394] drop4 <- ip1
I1027 05:24:20.680238 23014 net.cpp:345] drop4 -> ip1 (in-place)
I1027 05:24:20.680245 23014 net.cpp:96] Setting up drop4
I1027 05:24:20.680250 23014 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 05:24:20.680260 23014 net.cpp:67] Creating Layer ip2
I1027 05:24:20.680265 23014 net.cpp:394] ip2 <- ip1
I1027 05:24:20.680274 23014 net.cpp:356] ip2 -> ip2
I1027 05:24:20.680282 23014 net.cpp:96] Setting up ip2
I1027 05:24:20.689757 23014 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1027 05:24:20.689815 23014 net.cpp:67] Creating Layer loss
I1027 05:24:20.689822 23014 net.cpp:394] loss <- ip2
I1027 05:24:20.689831 23014 net.cpp:394] loss <- label
I1027 05:24:20.689837 23014 net.cpp:356] loss -> loss
I1027 05:24:20.689847 23014 net.cpp:96] Setting up loss
I1027 05:24:20.689859 23014 net.cpp:103] Top shape: 1 1 1 1 (1)
I1027 05:24:20.689865 23014 net.cpp:109]     with loss weight 1
I1027 05:24:20.689899 23014 net.cpp:170] loss needs backward computation.
I1027 05:24:20.689905 23014 net.cpp:170] ip2 needs backward computation.
I1027 05:24:20.689909 23014 net.cpp:170] drop4 needs backward computation.
I1027 05:24:20.689920 23014 net.cpp:170] relu4 needs backward computation.
I1027 05:24:20.689925 23014 net.cpp:170] ip1 needs backward computation.
I1027 05:24:20.689930 23014 net.cpp:170] drop3 needs backward computation.
I1027 05:24:20.689935 23014 net.cpp:170] relu3 needs backward computation.
I1027 05:24:20.689939 23014 net.cpp:170] pool3 needs backward computation.
I1027 05:24:20.689944 23014 net.cpp:170] conv3 needs backward computation.
I1027 05:24:20.689949 23014 net.cpp:170] drop2 needs backward computation.
I1027 05:24:20.689954 23014 net.cpp:170] relu2 needs backward computation.
I1027 05:24:20.689959 23014 net.cpp:170] pool2 needs backward computation.
I1027 05:24:20.689963 23014 net.cpp:170] conv2 needs backward computation.
I1027 05:24:20.689967 23014 net.cpp:170] drop1 needs backward computation.
I1027 05:24:20.689972 23014 net.cpp:170] relu1 needs backward computation.
I1027 05:24:20.689976 23014 net.cpp:170] pool1 needs backward computation.
I1027 05:24:20.689981 23014 net.cpp:170] conv1 needs backward computation.
I1027 05:24:20.689985 23014 net.cpp:172] mnist does not need backward computation.
I1027 05:24:20.689990 23014 net.cpp:208] This network produces output loss
I1027 05:24:20.690001 23014 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 05:24:20.690008 23014 net.cpp:219] Network initialization done.
I1027 05:24:20.690012 23014 net.cpp:220] Memory required for data: 119788292
I1027 05:24:20.690073 23014 solver.cpp:41] Solver scaffolding done.
I1027 05:24:20.690080 23014 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_215000.solverstate
I1027 05:24:20.690084 23014 solver.cpp:160] Solving Captcha
I1027 05:24:20.690104 23014 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_215000.solverstate
I1027 05:24:25.814404 23014 solver.cpp:502] SGDSolver: restoring history
I1027 05:24:26.606631 23014 solver.cpp:191] Iteration 215000, loss = 2.75466
I1027 05:24:26.606688 23014 solver.cpp:206]     Train net output #0: loss = 2.75466 (* 1 = 2.75466 loss)
I1027 05:24:26.606703 23014 solver.cpp:403] Iteration 215000, lr = 0.000967973
I1027 05:28:28.637913 23014 solver.cpp:191] Iteration 216000, loss = 2.66552
I1027 05:28:28.638646 23014 solver.cpp:206]     Train net output #0: loss = 2.66552 (* 1 = 2.66552 loss)
I1027 05:28:28.638679 23014 solver.cpp:403] Iteration 216000, lr = 0.000964759
I1027 05:32:30.179775 23014 solver.cpp:191] Iteration 217000, loss = 2.63695
I1027 05:32:30.180356 23014 solver.cpp:206]     Train net output #0: loss = 2.63695 (* 1 = 2.63695 loss)
I1027 05:32:30.180392 23014 solver.cpp:403] Iteration 217000, lr = 0.000961569
I1027 05:36:31.702864 23014 solver.cpp:191] Iteration 218000, loss = 2.57882
I1027 05:36:31.703655 23014 solver.cpp:206]     Train net output #0: loss = 2.57882 (* 1 = 2.57882 loss)
I1027 05:36:31.703690 23014 solver.cpp:403] Iteration 218000, lr = 0.000958405
I1027 05:40:33.159090 23014 solver.cpp:191] Iteration 219000, loss = 2.48443
I1027 05:40:33.159626 23014 solver.cpp:206]     Train net output #0: loss = 2.48443 (* 1 = 2.48443 loss)
I1027 05:40:33.159659 23014 solver.cpp:403] Iteration 219000, lr = 0.000955264
I1027 05:44:35.482540 23014 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_220000.caffemodel
I1027 05:44:39.820906 23014 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_220000.solverstate
I1027 05:44:44.199146 23014 solver.cpp:191] Iteration 220000, loss = 2.32671
I1027 05:44:44.199817 23014 solver.cpp:206]     Train net output #0: loss = 2.32671 (* 1 = 2.32671 loss)
I1027 05:44:44.199856 23014 solver.cpp:403] Iteration 220000, lr = 0.000952147
I1027 05:48:45.612596 23014 solver.cpp:191] Iteration 221000, loss = 2.59685
I1027 05:48:45.613359 23014 solver.cpp:206]     Train net output #0: loss = 2.59685 (* 1 = 2.59685 loss)
I1027 05:48:45.613394 23014 solver.cpp:403] Iteration 221000, lr = 0.000949054
I1027 05:52:46.924743 23014 solver.cpp:191] Iteration 222000, loss = 2.42041
I1027 05:52:46.925467 23014 solver.cpp:206]     Train net output #0: loss = 2.42041 (* 1 = 2.42041 loss)
I1027 05:52:46.925503 23014 solver.cpp:403] Iteration 222000, lr = 0.000945985
I1027 05:56:48.264329 23014 solver.cpp:191] Iteration 223000, loss = 2.44688
I1027 05:56:48.264938 23014 solver.cpp:206]     Train net output #0: loss = 2.44688 (* 1 = 2.44688 loss)
I1027 05:56:48.264971 23014 solver.cpp:403] Iteration 223000, lr = 0.000942938
I1027 06:00:49.742715 23014 solver.cpp:191] Iteration 224000, loss = 2.27681
I1027 06:00:49.743347 23014 solver.cpp:206]     Train net output #0: loss = 2.27681 (* 1 = 2.27681 loss)
I1027 06:00:49.743381 23014 solver.cpp:403] Iteration 224000, lr = 0.000939914
I1027 06:04:51.564772 23014 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_225000.caffemodel
I1027 06:04:55.791424 23014 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_225000.solverstate
I1027 06:04:59.532858 23014 solver.cpp:191] Iteration 225000, loss = 2.57629
I1027 06:04:59.533409 23014 solver.cpp:206]     Train net output #0: loss = 2.57629 (* 1 = 2.57629 loss)
I1027 06:04:59.533442 23014 solver.cpp:403] Iteration 225000, lr = 0.000936913
I1027 06:09:00.901309 23014 solver.cpp:191] Iteration 226000, loss = 2.56437
I1027 06:09:00.901940 23014 solver.cpp:206]     Train net output #0: loss = 2.56437 (* 1 = 2.56437 loss)
I1027 06:09:00.901974 23014 solver.cpp:403] Iteration 226000, lr = 0.000933934
I1027 06:13:02.317699 23014 solver.cpp:191] Iteration 227000, loss = 2.36885
I1027 06:13:02.318464 23014 solver.cpp:206]     Train net output #0: loss = 2.36885 (* 1 = 2.36885 loss)
I1027 06:13:02.318500 23014 solver.cpp:403] Iteration 227000, lr = 0.000930977
I1027 06:17:03.751647 23014 solver.cpp:191] Iteration 228000, loss = 2.61222
I1027 06:17:03.752241 23014 solver.cpp:206]     Train net output #0: loss = 2.61222 (* 1 = 2.61222 loss)
I1027 06:17:03.752279 23014 solver.cpp:403] Iteration 228000, lr = 0.000928041
I1027 06:21:05.184864 23014 solver.cpp:191] Iteration 229000, loss = 2.31078
I1027 06:21:05.185535 23014 solver.cpp:206]     Train net output #0: loss = 2.31078 (* 1 = 2.31078 loss)
I1027 06:21:05.185570 23014 solver.cpp:403] Iteration 229000, lr = 0.000925127
I1027 06:25:07.091161 23014 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_230000.caffemodel
I1027 06:25:11.532248 23014 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_230000.solverstate
I1027 06:25:14.898301 23014 solver.cpp:191] Iteration 230000, loss = 2.32145
I1027 06:25:14.898787 23014 solver.cpp:206]     Train net output #0: loss = 2.32145 (* 1 = 2.32145 loss)
I1027 06:25:14.898824 23014 solver.cpp:403] Iteration 230000, lr = 0.000922235
I1027 06:29:16.374251 23014 solver.cpp:191] Iteration 231000, loss = 2.31942
I1027 06:29:16.374836 23014 solver.cpp:206]     Train net output #0: loss = 2.31942 (* 1 = 2.31942 loss)
I1027 06:29:16.374866 23014 solver.cpp:403] Iteration 231000, lr = 0.000919363
I1027 06:33:17.838332 23014 solver.cpp:191] Iteration 232000, loss = 2.27472
I1027 06:33:17.838893 23014 solver.cpp:206]     Train net output #0: loss = 2.27472 (* 1 = 2.27472 loss)
I1027 06:33:17.838927 23014 solver.cpp:403] Iteration 232000, lr = 0.000916513
I1027 06:37:19.346606 23014 solver.cpp:191] Iteration 233000, loss = 2.40217
I1027 06:37:19.347230 23014 solver.cpp:206]     Train net output #0: loss = 2.40217 (* 1 = 2.40217 loss)
I1027 06:37:19.347265 23014 solver.cpp:403] Iteration 233000, lr = 0.000913682
I1027 06:41:20.906414 23014 solver.cpp:191] Iteration 234000, loss = 2.3355
I1027 06:41:20.907068 23014 solver.cpp:206]     Train net output #0: loss = 2.3355 (* 1 = 2.3355 loss)
I1027 06:41:20.907102 23014 solver.cpp:403] Iteration 234000, lr = 0.000910873
I1027 06:45:22.866461 23014 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_235000.caffemodel
I1027 06:45:27.250078 23014 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_235000.solverstate
I1027 06:45:30.898804 23014 solver.cpp:228] Iteration 235000, loss = 2.24423
I1027 06:45:30.899305 23014 solver.cpp:233] Optimization Done.
I1027 06:45:30.899329 23014 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1027 07:07:30.528859 13879 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 07:07:30.529449 13879 net.cpp:358] Input 0 -> data
I1027 07:07:30.529489 13879 net.cpp:67] Creating Layer conv1
I1027 07:07:30.529495 13879 net.cpp:394] conv1 <- data
I1027 07:07:30.529505 13879 net.cpp:356] conv1 -> conv1
I1027 07:07:30.529517 13879 net.cpp:96] Setting up conv1
I1027 07:07:30.529953 13879 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 07:07:30.529978 13879 net.cpp:67] Creating Layer pool1
I1027 07:07:30.529984 13879 net.cpp:394] pool1 <- conv1
I1027 07:07:30.529992 13879 net.cpp:356] pool1 -> pool1
I1027 07:07:30.530002 13879 net.cpp:96] Setting up pool1
I1027 07:07:30.530019 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:07:30.530027 13879 net.cpp:67] Creating Layer relu1
I1027 07:07:30.530037 13879 net.cpp:394] relu1 <- pool1
I1027 07:07:30.530050 13879 net.cpp:345] relu1 -> pool1 (in-place)
I1027 07:07:30.530057 13879 net.cpp:96] Setting up relu1
I1027 07:07:30.530063 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:07:30.530071 13879 net.cpp:67] Creating Layer drop1
I1027 07:07:30.530076 13879 net.cpp:394] drop1 <- pool1
I1027 07:07:30.530083 13879 net.cpp:345] drop1 -> pool1 (in-place)
I1027 07:07:30.530092 13879 net.cpp:96] Setting up drop1
I1027 07:07:30.530098 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:07:30.530107 13879 net.cpp:67] Creating Layer conv2
I1027 07:07:30.530112 13879 net.cpp:394] conv2 <- pool1
I1027 07:07:30.530120 13879 net.cpp:356] conv2 -> conv2
I1027 07:07:30.530129 13879 net.cpp:96] Setting up conv2
I1027 07:07:30.530877 13879 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 07:07:30.530895 13879 net.cpp:67] Creating Layer pool2
I1027 07:07:30.530902 13879 net.cpp:394] pool2 <- conv2
I1027 07:07:30.530912 13879 net.cpp:356] pool2 -> pool2
I1027 07:07:30.530922 13879 net.cpp:96] Setting up pool2
I1027 07:07:30.530930 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:07:30.530936 13879 net.cpp:67] Creating Layer relu2
I1027 07:07:30.530942 13879 net.cpp:394] relu2 <- pool2
I1027 07:07:30.530949 13879 net.cpp:345] relu2 -> pool2 (in-place)
I1027 07:07:30.530958 13879 net.cpp:96] Setting up relu2
I1027 07:07:30.530964 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:07:30.530971 13879 net.cpp:67] Creating Layer drop2
I1027 07:07:30.530977 13879 net.cpp:394] drop2 <- pool2
I1027 07:07:30.530983 13879 net.cpp:345] drop2 -> pool2 (in-place)
I1027 07:07:30.530990 13879 net.cpp:96] Setting up drop2
I1027 07:07:30.530997 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:07:30.531005 13879 net.cpp:67] Creating Layer conv3
I1027 07:07:30.531010 13879 net.cpp:394] conv3 <- pool2
I1027 07:07:30.531020 13879 net.cpp:356] conv3 -> conv3
I1027 07:07:30.531030 13879 net.cpp:96] Setting up conv3
I1027 07:07:30.533610 13879 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 07:07:30.533655 13879 net.cpp:67] Creating Layer pool3
I1027 07:07:30.533669 13879 net.cpp:394] pool3 <- conv3
I1027 07:07:30.533689 13879 net.cpp:356] pool3 -> pool3
I1027 07:07:30.533707 13879 net.cpp:96] Setting up pool3
I1027 07:07:30.533721 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:07:30.533736 13879 net.cpp:67] Creating Layer relu3
I1027 07:07:30.533748 13879 net.cpp:394] relu3 <- pool3
I1027 07:07:30.533762 13879 net.cpp:345] relu3 -> pool3 (in-place)
I1027 07:07:30.533777 13879 net.cpp:96] Setting up relu3
I1027 07:07:30.533788 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:07:30.533803 13879 net.cpp:67] Creating Layer drop3
I1027 07:07:30.533814 13879 net.cpp:394] drop3 <- pool3
I1027 07:07:30.533833 13879 net.cpp:345] drop3 -> pool3 (in-place)
I1027 07:07:30.533850 13879 net.cpp:96] Setting up drop3
I1027 07:07:30.533862 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:07:30.533879 13879 net.cpp:67] Creating Layer ip1
I1027 07:07:30.533890 13879 net.cpp:394] ip1 <- pool3
I1027 07:07:30.533910 13879 net.cpp:356] ip1 -> ip1
I1027 07:07:30.533928 13879 net.cpp:96] Setting up ip1
I1027 07:07:31.048684 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:07:31.048745 13879 net.cpp:67] Creating Layer relu4
I1027 07:07:31.048753 13879 net.cpp:394] relu4 <- ip1
I1027 07:07:31.048761 13879 net.cpp:345] relu4 -> ip1 (in-place)
I1027 07:07:31.048771 13879 net.cpp:96] Setting up relu4
I1027 07:07:31.048776 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:07:31.048785 13879 net.cpp:67] Creating Layer drop4
I1027 07:07:31.048790 13879 net.cpp:394] drop4 <- ip1
I1027 07:07:31.048796 13879 net.cpp:345] drop4 -> ip1 (in-place)
I1027 07:07:31.048802 13879 net.cpp:96] Setting up drop4
I1027 07:07:31.048807 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:07:31.048815 13879 net.cpp:67] Creating Layer ip2
I1027 07:07:31.048818 13879 net.cpp:394] ip2 <- ip1
I1027 07:07:31.048828 13879 net.cpp:356] ip2 -> ip2
I1027 07:07:31.048847 13879 net.cpp:96] Setting up ip2
I1027 07:07:31.058955 13879 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 07:07:31.059018 13879 net.cpp:67] Creating Layer prob
I1027 07:07:31.059026 13879 net.cpp:394] prob <- ip2
I1027 07:07:31.059033 13879 net.cpp:356] prob -> prob
I1027 07:07:31.059044 13879 net.cpp:96] Setting up prob
I1027 07:07:31.059052 13879 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 07:07:31.059057 13879 net.cpp:172] prob does not need backward computation.
I1027 07:07:31.059062 13879 net.cpp:172] ip2 does not need backward computation.
I1027 07:07:31.059064 13879 net.cpp:172] drop4 does not need backward computation.
I1027 07:07:31.059068 13879 net.cpp:172] relu4 does not need backward computation.
I1027 07:07:31.059072 13879 net.cpp:172] ip1 does not need backward computation.
I1027 07:07:31.059075 13879 net.cpp:172] drop3 does not need backward computation.
I1027 07:07:31.059079 13879 net.cpp:172] relu3 does not need backward computation.
I1027 07:07:31.059082 13879 net.cpp:172] pool3 does not need backward computation.
I1027 07:07:31.059087 13879 net.cpp:172] conv3 does not need backward computation.
I1027 07:07:31.059090 13879 net.cpp:172] drop2 does not need backward computation.
I1027 07:07:31.059093 13879 net.cpp:172] relu2 does not need backward computation.
I1027 07:07:31.059098 13879 net.cpp:172] pool2 does not need backward computation.
I1027 07:07:31.059100 13879 net.cpp:172] conv2 does not need backward computation.
I1027 07:07:31.059104 13879 net.cpp:172] drop1 does not need backward computation.
I1027 07:07:31.059108 13879 net.cpp:172] relu1 does not need backward computation.
I1027 07:07:31.059111 13879 net.cpp:172] pool1 does not need backward computation.
I1027 07:07:31.059115 13879 net.cpp:172] conv1 does not need backward computation.
I1027 07:07:31.059118 13879 net.cpp:208] This network produces output prob
I1027 07:07:31.059130 13879 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 07:07:31.059137 13879 net.cpp:219] Network initialization done.
I1027 07:07:31.059141 13879 net.cpp:220] Memory required for data: 1837200
I1027 07:08:13.974130 13879 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 07:08:13.974661 13879 net.cpp:358] Input 0 -> data
I1027 07:08:13.974710 13879 net.cpp:67] Creating Layer conv1
I1027 07:08:13.974722 13879 net.cpp:394] conv1 <- data
I1027 07:08:13.974738 13879 net.cpp:356] conv1 -> conv1
I1027 07:08:13.974758 13879 net.cpp:96] Setting up conv1
I1027 07:08:13.974814 13879 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 07:08:13.974844 13879 net.cpp:67] Creating Layer pool1
I1027 07:08:13.974855 13879 net.cpp:394] pool1 <- conv1
I1027 07:08:13.974869 13879 net.cpp:356] pool1 -> pool1
I1027 07:08:13.974884 13879 net.cpp:96] Setting up pool1
I1027 07:08:13.974899 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:08:13.974913 13879 net.cpp:67] Creating Layer relu1
I1027 07:08:13.974923 13879 net.cpp:394] relu1 <- pool1
I1027 07:08:13.974936 13879 net.cpp:345] relu1 -> pool1 (in-place)
I1027 07:08:13.974948 13879 net.cpp:96] Setting up relu1
I1027 07:08:13.974958 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:08:13.974970 13879 net.cpp:67] Creating Layer drop1
I1027 07:08:13.974979 13879 net.cpp:394] drop1 <- pool1
I1027 07:08:13.974992 13879 net.cpp:345] drop1 -> pool1 (in-place)
I1027 07:08:13.975005 13879 net.cpp:96] Setting up drop1
I1027 07:08:13.975016 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:08:13.975031 13879 net.cpp:67] Creating Layer conv2
I1027 07:08:13.975041 13879 net.cpp:394] conv2 <- pool1
I1027 07:08:13.975054 13879 net.cpp:356] conv2 -> conv2
I1027 07:08:13.975069 13879 net.cpp:96] Setting up conv2
I1027 07:08:13.976207 13879 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 07:08:13.976234 13879 net.cpp:67] Creating Layer pool2
I1027 07:08:13.976245 13879 net.cpp:394] pool2 <- conv2
I1027 07:08:13.976258 13879 net.cpp:356] pool2 -> pool2
I1027 07:08:13.976274 13879 net.cpp:96] Setting up pool2
I1027 07:08:13.976287 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:08:13.976300 13879 net.cpp:67] Creating Layer relu2
I1027 07:08:13.976310 13879 net.cpp:394] relu2 <- pool2
I1027 07:08:13.976321 13879 net.cpp:345] relu2 -> pool2 (in-place)
I1027 07:08:13.976335 13879 net.cpp:96] Setting up relu2
I1027 07:08:13.976344 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:08:13.976363 13879 net.cpp:67] Creating Layer drop2
I1027 07:08:13.976377 13879 net.cpp:394] drop2 <- pool2
I1027 07:08:13.976392 13879 net.cpp:345] drop2 -> pool2 (in-place)
I1027 07:08:13.976408 13879 net.cpp:96] Setting up drop2
I1027 07:08:13.976466 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:08:13.976501 13879 net.cpp:67] Creating Layer conv3
I1027 07:08:13.976514 13879 net.cpp:394] conv3 <- pool2
I1027 07:08:13.976533 13879 net.cpp:356] conv3 -> conv3
I1027 07:08:13.976553 13879 net.cpp:96] Setting up conv3
I1027 07:08:13.980178 13879 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 07:08:13.980216 13879 net.cpp:67] Creating Layer pool3
I1027 07:08:13.980228 13879 net.cpp:394] pool3 <- conv3
I1027 07:08:13.980245 13879 net.cpp:356] pool3 -> pool3
I1027 07:08:13.980263 13879 net.cpp:96] Setting up pool3
I1027 07:08:13.980278 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:08:13.980293 13879 net.cpp:67] Creating Layer relu3
I1027 07:08:13.980305 13879 net.cpp:394] relu3 <- pool3
I1027 07:08:13.980319 13879 net.cpp:345] relu3 -> pool3 (in-place)
I1027 07:08:13.980342 13879 net.cpp:96] Setting up relu3
I1027 07:08:13.980355 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:08:13.980370 13879 net.cpp:67] Creating Layer drop3
I1027 07:08:13.980381 13879 net.cpp:394] drop3 <- pool3
I1027 07:08:13.980396 13879 net.cpp:345] drop3 -> pool3 (in-place)
I1027 07:08:13.980412 13879 net.cpp:96] Setting up drop3
I1027 07:08:13.980491 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:08:13.980518 13879 net.cpp:67] Creating Layer ip1
I1027 07:08:13.980531 13879 net.cpp:394] ip1 <- pool3
I1027 07:08:13.980551 13879 net.cpp:356] ip1 -> ip1
I1027 07:08:13.980571 13879 net.cpp:96] Setting up ip1
I1027 07:08:14.417919 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:08:14.417982 13879 net.cpp:67] Creating Layer relu4
I1027 07:08:14.417990 13879 net.cpp:394] relu4 <- ip1
I1027 07:08:14.418001 13879 net.cpp:345] relu4 -> ip1 (in-place)
I1027 07:08:14.418012 13879 net.cpp:96] Setting up relu4
I1027 07:08:14.418017 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:08:14.418025 13879 net.cpp:67] Creating Layer drop4
I1027 07:08:14.418030 13879 net.cpp:394] drop4 <- ip1
I1027 07:08:14.418037 13879 net.cpp:345] drop4 -> ip1 (in-place)
I1027 07:08:14.418045 13879 net.cpp:96] Setting up drop4
I1027 07:08:14.418051 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:08:14.418061 13879 net.cpp:67] Creating Layer ip2
I1027 07:08:14.418064 13879 net.cpp:394] ip2 <- ip1
I1027 07:08:14.418072 13879 net.cpp:356] ip2 -> ip2
I1027 07:08:14.418087 13879 net.cpp:96] Setting up ip2
I1027 07:08:14.425694 13879 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 07:08:14.425755 13879 net.cpp:67] Creating Layer prob
I1027 07:08:14.425765 13879 net.cpp:394] prob <- ip2
I1027 07:08:14.425773 13879 net.cpp:356] prob -> prob
I1027 07:08:14.425784 13879 net.cpp:96] Setting up prob
I1027 07:08:14.425792 13879 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 07:08:14.425797 13879 net.cpp:172] prob does not need backward computation.
I1027 07:08:14.425801 13879 net.cpp:172] ip2 does not need backward computation.
I1027 07:08:14.425806 13879 net.cpp:172] drop4 does not need backward computation.
I1027 07:08:14.425809 13879 net.cpp:172] relu4 does not need backward computation.
I1027 07:08:14.425813 13879 net.cpp:172] ip1 does not need backward computation.
I1027 07:08:14.425817 13879 net.cpp:172] drop3 does not need backward computation.
I1027 07:08:14.425822 13879 net.cpp:172] relu3 does not need backward computation.
I1027 07:08:14.425825 13879 net.cpp:172] pool3 does not need backward computation.
I1027 07:08:14.425829 13879 net.cpp:172] conv3 does not need backward computation.
I1027 07:08:14.425833 13879 net.cpp:172] drop2 does not need backward computation.
I1027 07:08:14.425837 13879 net.cpp:172] relu2 does not need backward computation.
I1027 07:08:14.425842 13879 net.cpp:172] pool2 does not need backward computation.
I1027 07:08:14.425845 13879 net.cpp:172] conv2 does not need backward computation.
I1027 07:08:14.425849 13879 net.cpp:172] drop1 does not need backward computation.
I1027 07:08:14.425853 13879 net.cpp:172] relu1 does not need backward computation.
I1027 07:08:14.425858 13879 net.cpp:172] pool1 does not need backward computation.
I1027 07:08:14.425861 13879 net.cpp:172] conv1 does not need backward computation.
I1027 07:08:14.425864 13879 net.cpp:208] This network produces output prob
I1027 07:08:14.425880 13879 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 07:08:14.425890 13879 net.cpp:219] Network initialization done.
I1027 07:08:14.425894 13879 net.cpp:220] Memory required for data: 1837200
I1027 07:08:50.694526 13879 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 07:08:50.695080 13879 net.cpp:358] Input 0 -> data
I1027 07:08:50.695111 13879 net.cpp:67] Creating Layer conv1
I1027 07:08:50.695117 13879 net.cpp:394] conv1 <- data
I1027 07:08:50.695124 13879 net.cpp:356] conv1 -> conv1
I1027 07:08:50.695134 13879 net.cpp:96] Setting up conv1
I1027 07:08:50.695165 13879 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 07:08:50.695180 13879 net.cpp:67] Creating Layer pool1
I1027 07:08:50.695185 13879 net.cpp:394] pool1 <- conv1
I1027 07:08:50.695191 13879 net.cpp:356] pool1 -> pool1
I1027 07:08:50.695199 13879 net.cpp:96] Setting up pool1
I1027 07:08:50.695206 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:08:50.695214 13879 net.cpp:67] Creating Layer relu1
I1027 07:08:50.695217 13879 net.cpp:394] relu1 <- pool1
I1027 07:08:50.695222 13879 net.cpp:345] relu1 -> pool1 (in-place)
I1027 07:08:50.695229 13879 net.cpp:96] Setting up relu1
I1027 07:08:50.695233 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:08:50.695240 13879 net.cpp:67] Creating Layer drop1
I1027 07:08:50.695243 13879 net.cpp:394] drop1 <- pool1
I1027 07:08:50.695248 13879 net.cpp:345] drop1 -> pool1 (in-place)
I1027 07:08:50.695255 13879 net.cpp:96] Setting up drop1
I1027 07:08:50.695260 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:08:50.695266 13879 net.cpp:67] Creating Layer conv2
I1027 07:08:50.695271 13879 net.cpp:394] conv2 <- pool1
I1027 07:08:50.695276 13879 net.cpp:356] conv2 -> conv2
I1027 07:08:50.695284 13879 net.cpp:96] Setting up conv2
I1027 07:08:50.695785 13879 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 07:08:50.695799 13879 net.cpp:67] Creating Layer pool2
I1027 07:08:50.695804 13879 net.cpp:394] pool2 <- conv2
I1027 07:08:50.695811 13879 net.cpp:356] pool2 -> pool2
I1027 07:08:50.695822 13879 net.cpp:96] Setting up pool2
I1027 07:08:50.695828 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:08:50.695834 13879 net.cpp:67] Creating Layer relu2
I1027 07:08:50.695838 13879 net.cpp:394] relu2 <- pool2
I1027 07:08:50.695843 13879 net.cpp:345] relu2 -> pool2 (in-place)
I1027 07:08:50.695849 13879 net.cpp:96] Setting up relu2
I1027 07:08:50.695853 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:08:50.695859 13879 net.cpp:67] Creating Layer drop2
I1027 07:08:50.695863 13879 net.cpp:394] drop2 <- pool2
I1027 07:08:50.695868 13879 net.cpp:345] drop2 -> pool2 (in-place)
I1027 07:08:50.695873 13879 net.cpp:96] Setting up drop2
I1027 07:08:50.695878 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:08:50.695885 13879 net.cpp:67] Creating Layer conv3
I1027 07:08:50.695889 13879 net.cpp:394] conv3 <- pool2
I1027 07:08:50.695895 13879 net.cpp:356] conv3 -> conv3
I1027 07:08:50.695902 13879 net.cpp:96] Setting up conv3
I1027 07:08:50.697248 13879 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 07:08:50.697268 13879 net.cpp:67] Creating Layer pool3
I1027 07:08:50.697273 13879 net.cpp:394] pool3 <- conv3
I1027 07:08:50.697278 13879 net.cpp:356] pool3 -> pool3
I1027 07:08:50.697286 13879 net.cpp:96] Setting up pool3
I1027 07:08:50.697293 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:08:50.697299 13879 net.cpp:67] Creating Layer relu3
I1027 07:08:50.697302 13879 net.cpp:394] relu3 <- pool3
I1027 07:08:50.697307 13879 net.cpp:345] relu3 -> pool3 (in-place)
I1027 07:08:50.697314 13879 net.cpp:96] Setting up relu3
I1027 07:08:50.697317 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:08:50.697324 13879 net.cpp:67] Creating Layer drop3
I1027 07:08:50.697327 13879 net.cpp:394] drop3 <- pool3
I1027 07:08:50.697332 13879 net.cpp:345] drop3 -> pool3 (in-place)
I1027 07:08:50.697337 13879 net.cpp:96] Setting up drop3
I1027 07:08:50.697342 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:08:50.697348 13879 net.cpp:67] Creating Layer ip1
I1027 07:08:50.697352 13879 net.cpp:394] ip1 <- pool3
I1027 07:08:50.697358 13879 net.cpp:356] ip1 -> ip1
I1027 07:08:50.697365 13879 net.cpp:96] Setting up ip1
I1027 07:08:51.104527 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:08:51.104593 13879 net.cpp:67] Creating Layer relu4
I1027 07:08:51.104600 13879 net.cpp:394] relu4 <- ip1
I1027 07:08:51.104611 13879 net.cpp:345] relu4 -> ip1 (in-place)
I1027 07:08:51.104621 13879 net.cpp:96] Setting up relu4
I1027 07:08:51.104627 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:08:51.104635 13879 net.cpp:67] Creating Layer drop4
I1027 07:08:51.104640 13879 net.cpp:394] drop4 <- ip1
I1027 07:08:51.104646 13879 net.cpp:345] drop4 -> ip1 (in-place)
I1027 07:08:51.104655 13879 net.cpp:96] Setting up drop4
I1027 07:08:51.104660 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:08:51.104671 13879 net.cpp:67] Creating Layer ip2
I1027 07:08:51.104674 13879 net.cpp:394] ip2 <- ip1
I1027 07:08:51.104682 13879 net.cpp:356] ip2 -> ip2
I1027 07:08:51.104696 13879 net.cpp:96] Setting up ip2
I1027 07:08:51.112246 13879 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 07:08:51.112308 13879 net.cpp:67] Creating Layer prob
I1027 07:08:51.112315 13879 net.cpp:394] prob <- ip2
I1027 07:08:51.112325 13879 net.cpp:356] prob -> prob
I1027 07:08:51.112336 13879 net.cpp:96] Setting up prob
I1027 07:08:51.112344 13879 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 07:08:51.112349 13879 net.cpp:172] prob does not need backward computation.
I1027 07:08:51.112354 13879 net.cpp:172] ip2 does not need backward computation.
I1027 07:08:51.112357 13879 net.cpp:172] drop4 does not need backward computation.
I1027 07:08:51.112362 13879 net.cpp:172] relu4 does not need backward computation.
I1027 07:08:51.112366 13879 net.cpp:172] ip1 does not need backward computation.
I1027 07:08:51.112370 13879 net.cpp:172] drop3 does not need backward computation.
I1027 07:08:51.112375 13879 net.cpp:172] relu3 does not need backward computation.
I1027 07:08:51.112378 13879 net.cpp:172] pool3 does not need backward computation.
I1027 07:08:51.112393 13879 net.cpp:172] conv3 does not need backward computation.
I1027 07:08:51.112397 13879 net.cpp:172] drop2 does not need backward computation.
I1027 07:08:51.112401 13879 net.cpp:172] relu2 does not need backward computation.
I1027 07:08:51.112406 13879 net.cpp:172] pool2 does not need backward computation.
I1027 07:08:51.112411 13879 net.cpp:172] conv2 does not need backward computation.
I1027 07:08:51.112413 13879 net.cpp:172] drop1 does not need backward computation.
I1027 07:08:51.112423 13879 net.cpp:172] relu1 does not need backward computation.
I1027 07:08:51.112428 13879 net.cpp:172] pool1 does not need backward computation.
I1027 07:08:51.112432 13879 net.cpp:172] conv1 does not need backward computation.
I1027 07:08:51.112437 13879 net.cpp:208] This network produces output prob
I1027 07:08:51.112452 13879 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 07:08:51.112462 13879 net.cpp:219] Network initialization done.
I1027 07:08:51.112465 13879 net.cpp:220] Memory required for data: 1837200
I1027 07:09:27.466004 13879 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 07:09:27.466529 13879 net.cpp:358] Input 0 -> data
I1027 07:09:27.466560 13879 net.cpp:67] Creating Layer conv1
I1027 07:09:27.466567 13879 net.cpp:394] conv1 <- data
I1027 07:09:27.466581 13879 net.cpp:356] conv1 -> conv1
I1027 07:09:27.466593 13879 net.cpp:96] Setting up conv1
I1027 07:09:27.466624 13879 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 07:09:27.466639 13879 net.cpp:67] Creating Layer pool1
I1027 07:09:27.466645 13879 net.cpp:394] pool1 <- conv1
I1027 07:09:27.466650 13879 net.cpp:356] pool1 -> pool1
I1027 07:09:27.466657 13879 net.cpp:96] Setting up pool1
I1027 07:09:27.466665 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:09:27.466672 13879 net.cpp:67] Creating Layer relu1
I1027 07:09:27.466676 13879 net.cpp:394] relu1 <- pool1
I1027 07:09:27.466681 13879 net.cpp:345] relu1 -> pool1 (in-place)
I1027 07:09:27.466687 13879 net.cpp:96] Setting up relu1
I1027 07:09:27.466692 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:09:27.466698 13879 net.cpp:67] Creating Layer drop1
I1027 07:09:27.466702 13879 net.cpp:394] drop1 <- pool1
I1027 07:09:27.466707 13879 net.cpp:345] drop1 -> pool1 (in-place)
I1027 07:09:27.466713 13879 net.cpp:96] Setting up drop1
I1027 07:09:27.466718 13879 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 07:09:27.466725 13879 net.cpp:67] Creating Layer conv2
I1027 07:09:27.466729 13879 net.cpp:394] conv2 <- pool1
I1027 07:09:27.466735 13879 net.cpp:356] conv2 -> conv2
I1027 07:09:27.466742 13879 net.cpp:96] Setting up conv2
I1027 07:09:27.467243 13879 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 07:09:27.467258 13879 net.cpp:67] Creating Layer pool2
I1027 07:09:27.467263 13879 net.cpp:394] pool2 <- conv2
I1027 07:09:27.467269 13879 net.cpp:356] pool2 -> pool2
I1027 07:09:27.467277 13879 net.cpp:96] Setting up pool2
I1027 07:09:27.467283 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:09:27.467288 13879 net.cpp:67] Creating Layer relu2
I1027 07:09:27.467293 13879 net.cpp:394] relu2 <- pool2
I1027 07:09:27.467298 13879 net.cpp:345] relu2 -> pool2 (in-place)
I1027 07:09:27.467303 13879 net.cpp:96] Setting up relu2
I1027 07:09:27.467306 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:09:27.467313 13879 net.cpp:67] Creating Layer drop2
I1027 07:09:27.467316 13879 net.cpp:394] drop2 <- pool2
I1027 07:09:27.467321 13879 net.cpp:345] drop2 -> pool2 (in-place)
I1027 07:09:27.467327 13879 net.cpp:96] Setting up drop2
I1027 07:09:27.467331 13879 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 07:09:27.467339 13879 net.cpp:67] Creating Layer conv3
I1027 07:09:27.467344 13879 net.cpp:394] conv3 <- pool2
I1027 07:09:27.467350 13879 net.cpp:356] conv3 -> conv3
I1027 07:09:27.467357 13879 net.cpp:96] Setting up conv3
I1027 07:09:27.469252 13879 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 07:09:27.469293 13879 net.cpp:67] Creating Layer pool3
I1027 07:09:27.469308 13879 net.cpp:394] pool3 <- conv3
I1027 07:09:27.469324 13879 net.cpp:356] pool3 -> pool3
I1027 07:09:27.469342 13879 net.cpp:96] Setting up pool3
I1027 07:09:27.469357 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:09:27.469372 13879 net.cpp:67] Creating Layer relu3
I1027 07:09:27.469384 13879 net.cpp:394] relu3 <- pool3
I1027 07:09:27.469398 13879 net.cpp:345] relu3 -> pool3 (in-place)
I1027 07:09:27.469413 13879 net.cpp:96] Setting up relu3
I1027 07:09:27.469425 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:09:27.469440 13879 net.cpp:67] Creating Layer drop3
I1027 07:09:27.469452 13879 net.cpp:394] drop3 <- pool3
I1027 07:09:27.469466 13879 net.cpp:345] drop3 -> pool3 (in-place)
I1027 07:09:27.469482 13879 net.cpp:96] Setting up drop3
I1027 07:09:27.469496 13879 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 07:09:27.469512 13879 net.cpp:67] Creating Layer ip1
I1027 07:09:27.469524 13879 net.cpp:394] ip1 <- pool3
I1027 07:09:27.469542 13879 net.cpp:356] ip1 -> ip1
I1027 07:09:27.469560 13879 net.cpp:96] Setting up ip1
I1027 07:09:27.889724 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:09:27.889785 13879 net.cpp:67] Creating Layer relu4
I1027 07:09:27.889793 13879 net.cpp:394] relu4 <- ip1
I1027 07:09:27.889803 13879 net.cpp:345] relu4 -> ip1 (in-place)
I1027 07:09:27.889813 13879 net.cpp:96] Setting up relu4
I1027 07:09:27.889827 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:09:27.889835 13879 net.cpp:67] Creating Layer drop4
I1027 07:09:27.889840 13879 net.cpp:394] drop4 <- ip1
I1027 07:09:27.889847 13879 net.cpp:345] drop4 -> ip1 (in-place)
I1027 07:09:27.889855 13879 net.cpp:96] Setting up drop4
I1027 07:09:27.889861 13879 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 07:09:27.889871 13879 net.cpp:67] Creating Layer ip2
I1027 07:09:27.889876 13879 net.cpp:394] ip2 <- ip1
I1027 07:09:27.889884 13879 net.cpp:356] ip2 -> ip2
I1027 07:09:27.889897 13879 net.cpp:96] Setting up ip2
I1027 07:09:27.897496 13879 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 07:09:27.897563 13879 net.cpp:67] Creating Layer prob
I1027 07:09:27.897572 13879 net.cpp:394] prob <- ip2
I1027 07:09:27.897583 13879 net.cpp:356] prob -> prob
I1027 07:09:27.897593 13879 net.cpp:96] Setting up prob
I1027 07:09:27.897601 13879 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 07:09:27.897606 13879 net.cpp:172] prob does not need backward computation.
I1027 07:09:27.897611 13879 net.cpp:172] ip2 does not need backward computation.
I1027 07:09:27.897615 13879 net.cpp:172] drop4 does not need backward computation.
I1027 07:09:27.897619 13879 net.cpp:172] relu4 does not need backward computation.
I1027 07:09:27.897624 13879 net.cpp:172] ip1 does not need backward computation.
I1027 07:09:27.897627 13879 net.cpp:172] drop3 does not need backward computation.
I1027 07:09:27.897631 13879 net.cpp:172] relu3 does not need backward computation.
I1027 07:09:27.897635 13879 net.cpp:172] pool3 does not need backward computation.
I1027 07:09:27.897639 13879 net.cpp:172] conv3 does not need backward computation.
I1027 07:09:27.897644 13879 net.cpp:172] drop2 does not need backward computation.
I1027 07:09:27.897647 13879 net.cpp:172] relu2 does not need backward computation.
I1027 07:09:27.897651 13879 net.cpp:172] pool2 does not need backward computation.
I1027 07:09:27.897655 13879 net.cpp:172] conv2 does not need backward computation.
I1027 07:09:27.897660 13879 net.cpp:172] drop1 does not need backward computation.
I1027 07:09:27.897663 13879 net.cpp:172] relu1 does not need backward computation.
I1027 07:09:27.897667 13879 net.cpp:172] pool1 does not need backward computation.
I1027 07:09:27.897672 13879 net.cpp:172] conv1 does not need backward computation.
I1027 07:09:27.897676 13879 net.cpp:208] This network produces output prob
I1027 07:09:27.897691 13879 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 07:09:27.897701 13879 net.cpp:219] Network initialization done.
I1027 07:09:27.897706 13879 net.cpp:220] Memory required for data: 1837200
I1027 07:43:30.370421 20628 convert_imageset.cpp:70] Shuffling data
I1027 07:43:30.874069 20628 convert_imageset.cpp:73] A total of 60000 images.
I1027 07:43:30.874150 20628 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1027 07:43:32.981562 20628 convert_imageset.cpp:177] Processed 1000 files.
E1027 07:43:34.973556 20628 convert_imageset.cpp:177] Processed 2000 files.
E1027 07:43:36.920197 20628 convert_imageset.cpp:177] Processed 3000 files.
E1027 07:43:39.111888 20628 convert_imageset.cpp:177] Processed 4000 files.
E1027 07:43:40.821955 20628 convert_imageset.cpp:177] Processed 5000 files.
E1027 07:43:42.588948 20628 convert_imageset.cpp:177] Processed 6000 files.
E1027 07:43:44.486251 20628 convert_imageset.cpp:177] Processed 7000 files.
E1027 07:43:46.185216 20628 convert_imageset.cpp:177] Processed 8000 files.
E1027 07:43:47.999228 20628 convert_imageset.cpp:177] Processed 9000 files.
E1027 07:43:49.717783 20628 convert_imageset.cpp:177] Processed 10000 files.
E1027 07:43:51.574450 20628 convert_imageset.cpp:177] Processed 11000 files.
E1027 07:43:53.426741 20628 convert_imageset.cpp:177] Processed 12000 files.
E1027 07:43:55.099272 20628 convert_imageset.cpp:177] Processed 13000 files.
E1027 07:43:56.883601 20628 convert_imageset.cpp:177] Processed 14000 files.
E1027 07:43:58.516535 20628 convert_imageset.cpp:177] Processed 15000 files.
E1027 07:44:00.172808 20628 convert_imageset.cpp:177] Processed 16000 files.
E1027 07:44:01.981539 20628 convert_imageset.cpp:177] Processed 17000 files.
E1027 07:44:03.582041 20628 convert_imageset.cpp:177] Processed 18000 files.
E1027 07:44:05.216486 20628 convert_imageset.cpp:177] Processed 19000 files.
E1027 07:44:06.907157 20628 convert_imageset.cpp:177] Processed 20000 files.
E1027 07:44:08.509866 20628 convert_imageset.cpp:177] Processed 21000 files.
E1027 07:44:10.291846 20628 convert_imageset.cpp:177] Processed 22000 files.
E1027 07:44:11.899621 20628 convert_imageset.cpp:177] Processed 23000 files.
E1027 07:44:13.505048 20628 convert_imageset.cpp:177] Processed 24000 files.
E1027 07:44:15.574347 20628 convert_imageset.cpp:177] Processed 25000 files.
E1027 07:44:17.205143 20628 convert_imageset.cpp:177] Processed 26000 files.
E1027 07:44:18.760251 20628 convert_imageset.cpp:177] Processed 27000 files.
E1027 07:44:20.279954 20628 convert_imageset.cpp:177] Processed 28000 files.
E1027 07:44:21.848809 20628 convert_imageset.cpp:177] Processed 29000 files.
E1027 07:44:23.470979 20628 convert_imageset.cpp:177] Processed 30000 files.
E1027 07:44:25.103942 20628 convert_imageset.cpp:177] Processed 31000 files.
E1027 07:44:26.745512 20628 convert_imageset.cpp:177] Processed 32000 files.
E1027 07:44:28.444658 20628 convert_imageset.cpp:177] Processed 33000 files.
E1027 07:44:30.070962 20628 convert_imageset.cpp:177] Processed 34000 files.
E1027 07:44:31.630746 20628 convert_imageset.cpp:177] Processed 35000 files.
E1027 07:44:33.175057 20628 convert_imageset.cpp:177] Processed 36000 files.
E1027 07:44:34.685190 20628 convert_imageset.cpp:177] Processed 37000 files.
E1027 07:44:36.229693 20628 convert_imageset.cpp:177] Processed 38000 files.
E1027 07:44:37.860553 20628 convert_imageset.cpp:177] Processed 39000 files.
E1027 07:44:39.475594 20628 convert_imageset.cpp:177] Processed 40000 files.
E1027 07:44:40.940897 20628 convert_imageset.cpp:177] Processed 41000 files.
E1027 07:44:42.581435 20628 convert_imageset.cpp:177] Processed 42000 files.
E1027 07:44:44.071898 20628 convert_imageset.cpp:177] Processed 43000 files.
E1027 07:44:45.542377 20628 convert_imageset.cpp:177] Processed 44000 files.
E1027 07:44:46.994201 20628 convert_imageset.cpp:177] Processed 45000 files.
E1027 07:44:48.510833 20628 convert_imageset.cpp:177] Processed 46000 files.
E1027 07:44:50.053336 20628 convert_imageset.cpp:177] Processed 47000 files.
E1027 07:44:51.547818 20628 convert_imageset.cpp:177] Processed 48000 files.
E1027 07:44:53.041548 20628 convert_imageset.cpp:177] Processed 49000 files.
E1027 07:44:54.549952 20628 convert_imageset.cpp:177] Processed 50000 files.
E1027 07:44:56.112351 20628 convert_imageset.cpp:177] Processed 51000 files.
E1027 07:44:57.596520 20628 convert_imageset.cpp:177] Processed 52000 files.
E1027 07:44:59.015810 20628 convert_imageset.cpp:177] Processed 53000 files.
E1027 07:45:00.463785 20628 convert_imageset.cpp:177] Processed 54000 files.
E1027 07:45:01.899541 20628 convert_imageset.cpp:177] Processed 55000 files.
E1027 07:45:03.569895 20628 convert_imageset.cpp:177] Processed 56000 files.
E1027 07:45:05.230074 20628 convert_imageset.cpp:177] Processed 57000 files.
E1027 07:45:06.801311 20628 convert_imageset.cpp:177] Processed 58000 files.
E1027 07:45:08.372583 20628 convert_imageset.cpp:177] Processed 59000 files.
E1027 07:45:09.873162 20628 convert_imageset.cpp:177] Processed 60000 files.
I1027 07:45:10.077636 20759 caffe.cpp:99] Use GPU with device ID 0
I1027 07:45:10.412731 20759 caffe.cpp:107] Starting Optimization
I1027 07:45:10.412880 20759 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 255000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1027 07:45:10.412905 20759 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1027 07:45:10.436692 20759 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1027 07:45:10.436910 20759 net.cpp:67] Creating Layer mnist
I1027 07:45:10.436936 20759 net.cpp:356] mnist -> data
I1027 07:45:10.436971 20759 net.cpp:356] mnist -> label
I1027 07:45:10.437003 20759 net.cpp:96] Setting up mnist
I1027 07:45:10.444955 20759 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1027 07:45:10.445046 20759 data_layer.cpp:128] output data size: 64,1,50,180
I1027 07:45:10.445863 20759 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1027 07:45:10.445886 20759 net.cpp:103] Top shape: 64 1 1 1 (64)
I1027 07:45:10.445900 20759 net.cpp:67] Creating Layer conv1
I1027 07:45:10.445905 20759 net.cpp:394] conv1 <- data
I1027 07:45:10.445919 20759 net.cpp:356] conv1 -> conv1
I1027 07:45:10.445930 20759 net.cpp:96] Setting up conv1
I1027 07:45:10.446288 20759 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1027 07:45:10.446326 20759 net.cpp:67] Creating Layer pool1
I1027 07:45:10.446331 20759 net.cpp:394] pool1 <- conv1
I1027 07:45:10.446338 20759 net.cpp:356] pool1 -> pool1
I1027 07:45:10.446347 20759 net.cpp:96] Setting up pool1
I1027 07:45:10.446363 20759 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 07:45:10.446370 20759 net.cpp:67] Creating Layer relu1
I1027 07:45:10.446374 20759 net.cpp:394] relu1 <- pool1
I1027 07:45:10.446380 20759 net.cpp:345] relu1 -> pool1 (in-place)
I1027 07:45:10.446387 20759 net.cpp:96] Setting up relu1
I1027 07:45:10.446391 20759 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 07:45:10.446398 20759 net.cpp:67] Creating Layer drop1
I1027 07:45:10.446403 20759 net.cpp:394] drop1 <- pool1
I1027 07:45:10.446411 20759 net.cpp:345] drop1 -> pool1 (in-place)
I1027 07:45:10.446418 20759 net.cpp:96] Setting up drop1
I1027 07:45:10.446424 20759 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 07:45:10.446429 20759 net.cpp:67] Creating Layer conv2
I1027 07:45:10.446434 20759 net.cpp:394] conv2 <- pool1
I1027 07:45:10.446442 20759 net.cpp:356] conv2 -> conv2
I1027 07:45:10.446450 20759 net.cpp:96] Setting up conv2
I1027 07:45:10.447033 20759 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1027 07:45:10.447052 20759 net.cpp:67] Creating Layer pool2
I1027 07:45:10.447058 20759 net.cpp:394] pool2 <- conv2
I1027 07:45:10.447064 20759 net.cpp:356] pool2 -> pool2
I1027 07:45:10.447072 20759 net.cpp:96] Setting up pool2
I1027 07:45:10.447077 20759 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 07:45:10.447083 20759 net.cpp:67] Creating Layer relu2
I1027 07:45:10.447088 20759 net.cpp:394] relu2 <- pool2
I1027 07:45:10.447095 20759 net.cpp:345] relu2 -> pool2 (in-place)
I1027 07:45:10.447101 20759 net.cpp:96] Setting up relu2
I1027 07:45:10.447105 20759 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 07:45:10.447113 20759 net.cpp:67] Creating Layer drop2
I1027 07:45:10.447118 20759 net.cpp:394] drop2 <- pool2
I1027 07:45:10.447123 20759 net.cpp:345] drop2 -> pool2 (in-place)
I1027 07:45:10.447129 20759 net.cpp:96] Setting up drop2
I1027 07:45:10.447134 20759 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 07:45:10.447142 20759 net.cpp:67] Creating Layer conv3
I1027 07:45:10.447147 20759 net.cpp:394] conv3 <- pool2
I1027 07:45:10.447154 20759 net.cpp:356] conv3 -> conv3
I1027 07:45:10.447160 20759 net.cpp:96] Setting up conv3
I1027 07:45:10.449236 20759 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1027 07:45:10.449285 20759 net.cpp:67] Creating Layer pool3
I1027 07:45:10.449301 20759 net.cpp:394] pool3 <- conv3
I1027 07:45:10.449321 20759 net.cpp:356] pool3 -> pool3
I1027 07:45:10.449342 20759 net.cpp:96] Setting up pool3
I1027 07:45:10.449357 20759 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 07:45:10.449373 20759 net.cpp:67] Creating Layer relu3
I1027 07:45:10.449385 20759 net.cpp:394] relu3 <- pool3
I1027 07:45:10.449404 20759 net.cpp:345] relu3 -> pool3 (in-place)
I1027 07:45:10.449421 20759 net.cpp:96] Setting up relu3
I1027 07:45:10.449434 20759 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 07:45:10.449450 20759 net.cpp:67] Creating Layer drop3
I1027 07:45:10.449461 20759 net.cpp:394] drop3 <- pool3
I1027 07:45:10.449477 20759 net.cpp:345] drop3 -> pool3 (in-place)
I1027 07:45:10.449494 20759 net.cpp:96] Setting up drop3
I1027 07:45:10.449507 20759 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 07:45:10.449525 20759 net.cpp:67] Creating Layer ip1
I1027 07:45:10.449537 20759 net.cpp:394] ip1 <- pool3
I1027 07:45:10.449558 20759 net.cpp:356] ip1 -> ip1
I1027 07:45:10.449621 20759 net.cpp:96] Setting up ip1
I1027 07:45:10.844177 20759 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 07:45:10.844235 20759 net.cpp:67] Creating Layer relu4
I1027 07:45:10.844243 20759 net.cpp:394] relu4 <- ip1
I1027 07:45:10.844251 20759 net.cpp:345] relu4 -> ip1 (in-place)
I1027 07:45:10.844261 20759 net.cpp:96] Setting up relu4
I1027 07:45:10.844266 20759 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 07:45:10.844272 20759 net.cpp:67] Creating Layer drop4
I1027 07:45:10.844276 20759 net.cpp:394] drop4 <- ip1
I1027 07:45:10.844290 20759 net.cpp:345] drop4 -> ip1 (in-place)
I1027 07:45:10.844296 20759 net.cpp:96] Setting up drop4
I1027 07:45:10.844301 20759 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 07:45:10.844315 20759 net.cpp:67] Creating Layer ip2
I1027 07:45:10.844318 20759 net.cpp:394] ip2 <- ip1
I1027 07:45:10.844327 20759 net.cpp:356] ip2 -> ip2
I1027 07:45:10.844336 20759 net.cpp:96] Setting up ip2
I1027 07:45:10.855888 20759 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1027 07:45:10.855960 20759 net.cpp:67] Creating Layer loss
I1027 07:45:10.855967 20759 net.cpp:394] loss <- ip2
I1027 07:45:10.855975 20759 net.cpp:394] loss <- label
I1027 07:45:10.855983 20759 net.cpp:356] loss -> loss
I1027 07:45:10.855991 20759 net.cpp:96] Setting up loss
I1027 07:45:10.856006 20759 net.cpp:103] Top shape: 1 1 1 1 (1)
I1027 07:45:10.856011 20759 net.cpp:109]     with loss weight 1
I1027 07:45:10.856046 20759 net.cpp:170] loss needs backward computation.
I1027 07:45:10.856051 20759 net.cpp:170] ip2 needs backward computation.
I1027 07:45:10.856055 20759 net.cpp:170] drop4 needs backward computation.
I1027 07:45:10.856060 20759 net.cpp:170] relu4 needs backward computation.
I1027 07:45:10.856065 20759 net.cpp:170] ip1 needs backward computation.
I1027 07:45:10.856068 20759 net.cpp:170] drop3 needs backward computation.
I1027 07:45:10.856072 20759 net.cpp:170] relu3 needs backward computation.
I1027 07:45:10.856076 20759 net.cpp:170] pool3 needs backward computation.
I1027 07:45:10.856081 20759 net.cpp:170] conv3 needs backward computation.
I1027 07:45:10.856086 20759 net.cpp:170] drop2 needs backward computation.
I1027 07:45:10.856089 20759 net.cpp:170] relu2 needs backward computation.
I1027 07:45:10.856093 20759 net.cpp:170] pool2 needs backward computation.
I1027 07:45:10.856098 20759 net.cpp:170] conv2 needs backward computation.
I1027 07:45:10.856102 20759 net.cpp:170] drop1 needs backward computation.
I1027 07:45:10.856106 20759 net.cpp:170] relu1 needs backward computation.
I1027 07:45:10.856111 20759 net.cpp:170] pool1 needs backward computation.
I1027 07:45:10.856115 20759 net.cpp:170] conv1 needs backward computation.
I1027 07:45:10.856119 20759 net.cpp:172] mnist does not need backward computation.
I1027 07:45:10.856123 20759 net.cpp:208] This network produces output loss
I1027 07:45:10.856133 20759 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 07:45:10.856140 20759 net.cpp:219] Network initialization done.
I1027 07:45:10.856144 20759 net.cpp:220] Memory required for data: 119788292
I1027 07:45:10.856204 20759 solver.cpp:41] Solver scaffolding done.
I1027 07:45:10.856210 20759 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_235000.solverstate
I1027 07:45:10.856215 20759 solver.cpp:160] Solving Captcha
I1027 07:45:10.856233 20759 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_235000.solverstate
I1027 07:45:16.382987 20759 solver.cpp:502] SGDSolver: restoring history
I1027 07:45:17.221035 20759 solver.cpp:191] Iteration 235000, loss = 2.28578
I1027 07:45:17.221096 20759 solver.cpp:206]     Train net output #0: loss = 2.28578 (* 1 = 2.28578 loss)
I1027 07:45:17.221110 20759 solver.cpp:403] Iteration 235000, lr = 0.000908083
I1027 07:49:18.884218 20759 solver.cpp:191] Iteration 236000, loss = 2.51534
I1027 07:49:18.885196 20759 solver.cpp:206]     Train net output #0: loss = 2.51534 (* 1 = 2.51534 loss)
I1027 07:49:18.885228 20759 solver.cpp:403] Iteration 236000, lr = 0.000905313
I1027 07:53:20.157466 20759 solver.cpp:191] Iteration 237000, loss = 2.47677
I1027 07:53:20.158254 20759 solver.cpp:206]     Train net output #0: loss = 2.47677 (* 1 = 2.47677 loss)
I1027 07:53:20.158287 20759 solver.cpp:403] Iteration 237000, lr = 0.000902563
I1027 07:57:21.503037 20759 solver.cpp:191] Iteration 238000, loss = 2.74424
I1027 07:57:21.503854 20759 solver.cpp:206]     Train net output #0: loss = 2.74424 (* 1 = 2.74424 loss)
I1027 07:57:21.503886 20759 solver.cpp:403] Iteration 238000, lr = 0.000899832
I1027 08:01:22.924176 20759 solver.cpp:191] Iteration 239000, loss = 2.56594
I1027 08:01:22.924811 20759 solver.cpp:206]     Train net output #0: loss = 2.56594 (* 1 = 2.56594 loss)
I1027 08:01:22.924844 20759 solver.cpp:403] Iteration 239000, lr = 0.00089712
I1027 08:05:24.964942 20759 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_240000.caffemodel
I1027 08:05:29.682081 20759 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_240000.solverstate
I1027 08:05:33.611980 20759 solver.cpp:191] Iteration 240000, loss = 2.5815
I1027 08:05:33.612551 20759 solver.cpp:206]     Train net output #0: loss = 2.5815 (* 1 = 2.5815 loss)
I1027 08:05:33.612572 20759 solver.cpp:403] Iteration 240000, lr = 0.000894427
I1027 08:09:36.062432 20759 solver.cpp:191] Iteration 241000, loss = 2.36939
I1027 08:09:36.062988 20759 solver.cpp:206]     Train net output #0: loss = 2.36939 (* 1 = 2.36939 loss)
I1027 08:09:36.063021 20759 solver.cpp:403] Iteration 241000, lr = 0.000891753
I1027 08:13:37.394733 20759 solver.cpp:191] Iteration 242000, loss = 2.56059
I1027 08:13:37.395462 20759 solver.cpp:206]     Train net output #0: loss = 2.56059 (* 1 = 2.56059 loss)
I1027 08:13:37.395498 20759 solver.cpp:403] Iteration 242000, lr = 0.000889098
I1027 08:17:38.680459 20759 solver.cpp:191] Iteration 243000, loss = 2.23588
I1027 08:17:38.689146 20759 solver.cpp:206]     Train net output #0: loss = 2.23588 (* 1 = 2.23588 loss)
I1027 08:17:38.689177 20759 solver.cpp:403] Iteration 243000, lr = 0.000886461
I1027 08:21:40.006350 20759 solver.cpp:191] Iteration 244000, loss = 2.31869
I1027 08:21:40.006960 20759 solver.cpp:206]     Train net output #0: loss = 2.31869 (* 1 = 2.31869 loss)
I1027 08:21:40.006996 20759 solver.cpp:403] Iteration 244000, lr = 0.000883842
I1027 08:25:41.820094 20759 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_245000.caffemodel
I1027 08:25:46.236632 20759 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_245000.solverstate
I1027 08:25:49.826257 20759 solver.cpp:191] Iteration 245000, loss = 2.37654
I1027 08:25:49.826789 20759 solver.cpp:206]     Train net output #0: loss = 2.37654 (* 1 = 2.37654 loss)
I1027 08:25:49.826817 20759 solver.cpp:403] Iteration 245000, lr = 0.000881241
I1027 08:29:51.125254 20759 solver.cpp:191] Iteration 246000, loss = 2.29614
I1027 08:29:51.125821 20759 solver.cpp:206]     Train net output #0: loss = 2.29614 (* 1 = 2.29614 loss)
I1027 08:29:51.125857 20759 solver.cpp:403] Iteration 246000, lr = 0.000878658
I1027 08:33:52.433156 20759 solver.cpp:191] Iteration 247000, loss = 2.33817
I1027 08:33:52.433904 20759 solver.cpp:206]     Train net output #0: loss = 2.33817 (* 1 = 2.33817 loss)
I1027 08:33:52.433936 20759 solver.cpp:403] Iteration 247000, lr = 0.000876093
I1027 08:37:54.013727 20759 solver.cpp:191] Iteration 248000, loss = 2.3289
I1027 08:37:54.014303 20759 solver.cpp:206]     Train net output #0: loss = 2.3289 (* 1 = 2.3289 loss)
I1027 08:37:54.014340 20759 solver.cpp:403] Iteration 248000, lr = 0.000873545
I1027 08:41:55.727857 20759 solver.cpp:191] Iteration 249000, loss = 2.5827
I1027 08:41:55.728409 20759 solver.cpp:206]     Train net output #0: loss = 2.5827 (* 1 = 2.5827 loss)
I1027 08:41:55.728482 20759 solver.cpp:403] Iteration 249000, lr = 0.000871014
I1027 08:45:57.866417 20759 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_250000.caffemodel
I1027 08:46:02.512248 20759 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_250000.solverstate
I1027 08:46:06.278291 20759 solver.cpp:191] Iteration 250000, loss = 2.27816
I1027 08:46:06.278939 20759 solver.cpp:206]     Train net output #0: loss = 2.27816 (* 1 = 2.27816 loss)
I1027 08:46:06.278972 20759 solver.cpp:403] Iteration 250000, lr = 0.0008685
I1027 08:50:08.037081 20759 solver.cpp:191] Iteration 251000, loss = 2.58981
I1027 08:50:08.037802 20759 solver.cpp:206]     Train net output #0: loss = 2.58981 (* 1 = 2.58981 loss)
I1027 08:50:08.037837 20759 solver.cpp:403] Iteration 251000, lr = 0.000866003
I1027 08:54:09.337290 20759 solver.cpp:191] Iteration 252000, loss = 2.41982
I1027 08:54:09.338026 20759 solver.cpp:206]     Train net output #0: loss = 2.41982 (* 1 = 2.41982 loss)
I1027 08:54:09.338057 20759 solver.cpp:403] Iteration 252000, lr = 0.000863523
I1027 08:58:10.647558 20759 solver.cpp:191] Iteration 253000, loss = 2.34393
I1027 08:58:10.653841 20759 solver.cpp:206]     Train net output #0: loss = 2.34393 (* 1 = 2.34393 loss)
I1027 08:58:10.653872 20759 solver.cpp:403] Iteration 253000, lr = 0.00086106
I1027 09:02:12.088414 20759 solver.cpp:191] Iteration 254000, loss = 2.46726
I1027 09:02:12.089223 20759 solver.cpp:206]     Train net output #0: loss = 2.46726 (* 1 = 2.46726 loss)
I1027 09:02:12.089257 20759 solver.cpp:403] Iteration 254000, lr = 0.000858612
I1027 09:06:13.861213 20759 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_255000.caffemodel
I1027 09:06:18.332392 20759 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_255000.solverstate
I1027 09:06:22.338755 20759 solver.cpp:228] Iteration 255000, loss = 2.37683
I1027 09:06:22.339303 20759 solver.cpp:233] Optimization Done.
I1027 09:06:22.347606 20759 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1027 09:28:43.623953  8417 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 09:28:43.624058  8417 net.cpp:358] Input 0 -> data
I1027 09:28:43.624088  8417 net.cpp:67] Creating Layer conv1
I1027 09:28:43.624094  8417 net.cpp:394] conv1 <- data
I1027 09:28:43.624100  8417 net.cpp:356] conv1 -> conv1
I1027 09:28:43.624110  8417 net.cpp:96] Setting up conv1
I1027 09:28:43.624446  8417 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 09:28:43.624472  8417 net.cpp:67] Creating Layer pool1
I1027 09:28:43.624478  8417 net.cpp:394] pool1 <- conv1
I1027 09:28:43.624485  8417 net.cpp:356] pool1 -> pool1
I1027 09:28:43.624495  8417 net.cpp:96] Setting up pool1
I1027 09:28:43.624510  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:28:43.624521  8417 net.cpp:67] Creating Layer relu1
I1027 09:28:43.624526  8417 net.cpp:394] relu1 <- pool1
I1027 09:28:43.624531  8417 net.cpp:345] relu1 -> pool1 (in-place)
I1027 09:28:43.624541  8417 net.cpp:96] Setting up relu1
I1027 09:28:43.624546  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:28:43.624552  8417 net.cpp:67] Creating Layer drop1
I1027 09:28:43.624555  8417 net.cpp:394] drop1 <- pool1
I1027 09:28:43.624560  8417 net.cpp:345] drop1 -> pool1 (in-place)
I1027 09:28:43.624567  8417 net.cpp:96] Setting up drop1
I1027 09:28:43.624572  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:28:43.624578  8417 net.cpp:67] Creating Layer conv2
I1027 09:28:43.624583  8417 net.cpp:394] conv2 <- pool1
I1027 09:28:43.624588  8417 net.cpp:356] conv2 -> conv2
I1027 09:28:43.624594  8417 net.cpp:96] Setting up conv2
I1027 09:28:43.625151  8417 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 09:28:43.625166  8417 net.cpp:67] Creating Layer pool2
I1027 09:28:43.625170  8417 net.cpp:394] pool2 <- conv2
I1027 09:28:43.625179  8417 net.cpp:356] pool2 -> pool2
I1027 09:28:43.625185  8417 net.cpp:96] Setting up pool2
I1027 09:28:43.625191  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:28:43.625197  8417 net.cpp:67] Creating Layer relu2
I1027 09:28:43.625201  8417 net.cpp:394] relu2 <- pool2
I1027 09:28:43.625207  8417 net.cpp:345] relu2 -> pool2 (in-place)
I1027 09:28:43.625213  8417 net.cpp:96] Setting up relu2
I1027 09:28:43.625217  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:28:43.625222  8417 net.cpp:67] Creating Layer drop2
I1027 09:28:43.625226  8417 net.cpp:394] drop2 <- pool2
I1027 09:28:43.625232  8417 net.cpp:345] drop2 -> pool2 (in-place)
I1027 09:28:43.625237  8417 net.cpp:96] Setting up drop2
I1027 09:28:43.625241  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:28:43.625250  8417 net.cpp:67] Creating Layer conv3
I1027 09:28:43.625255  8417 net.cpp:394] conv3 <- pool2
I1027 09:28:43.625262  8417 net.cpp:356] conv3 -> conv3
I1027 09:28:43.625267  8417 net.cpp:96] Setting up conv3
I1027 09:28:43.626723  8417 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 09:28:43.626737  8417 net.cpp:67] Creating Layer pool3
I1027 09:28:43.626742  8417 net.cpp:394] pool3 <- conv3
I1027 09:28:43.626750  8417 net.cpp:356] pool3 -> pool3
I1027 09:28:43.626757  8417 net.cpp:96] Setting up pool3
I1027 09:28:43.626762  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:28:43.626768  8417 net.cpp:67] Creating Layer relu3
I1027 09:28:43.626772  8417 net.cpp:394] relu3 <- pool3
I1027 09:28:43.626777  8417 net.cpp:345] relu3 -> pool3 (in-place)
I1027 09:28:43.626782  8417 net.cpp:96] Setting up relu3
I1027 09:28:43.626786  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:28:43.626796  8417 net.cpp:67] Creating Layer drop3
I1027 09:28:43.626801  8417 net.cpp:394] drop3 <- pool3
I1027 09:28:43.626806  8417 net.cpp:345] drop3 -> pool3 (in-place)
I1027 09:28:43.626811  8417 net.cpp:96] Setting up drop3
I1027 09:28:43.626816  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:28:43.626824  8417 net.cpp:67] Creating Layer ip1
I1027 09:28:43.626828  8417 net.cpp:394] ip1 <- pool3
I1027 09:28:43.626837  8417 net.cpp:356] ip1 -> ip1
I1027 09:28:43.626843  8417 net.cpp:96] Setting up ip1
I1027 09:28:44.166645  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:28:44.166698  8417 net.cpp:67] Creating Layer relu4
I1027 09:28:44.166705  8417 net.cpp:394] relu4 <- ip1
I1027 09:28:44.166717  8417 net.cpp:345] relu4 -> ip1 (in-place)
I1027 09:28:44.166726  8417 net.cpp:96] Setting up relu4
I1027 09:28:44.166731  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:28:44.166739  8417 net.cpp:67] Creating Layer drop4
I1027 09:28:44.166743  8417 net.cpp:394] drop4 <- ip1
I1027 09:28:44.166749  8417 net.cpp:345] drop4 -> ip1 (in-place)
I1027 09:28:44.166754  8417 net.cpp:96] Setting up drop4
I1027 09:28:44.166760  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:28:44.166769  8417 net.cpp:67] Creating Layer ip2
I1027 09:28:44.166774  8417 net.cpp:394] ip2 <- ip1
I1027 09:28:44.166779  8417 net.cpp:356] ip2 -> ip2
I1027 09:28:44.166791  8417 net.cpp:96] Setting up ip2
I1027 09:28:44.178319  8417 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 09:28:44.178385  8417 net.cpp:67] Creating Layer prob
I1027 09:28:44.178392  8417 net.cpp:394] prob <- ip2
I1027 09:28:44.178401  8417 net.cpp:356] prob -> prob
I1027 09:28:44.178411  8417 net.cpp:96] Setting up prob
I1027 09:28:44.178419  8417 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 09:28:44.178424  8417 net.cpp:172] prob does not need backward computation.
I1027 09:28:44.178428  8417 net.cpp:172] ip2 does not need backward computation.
I1027 09:28:44.178432  8417 net.cpp:172] drop4 does not need backward computation.
I1027 09:28:44.178436  8417 net.cpp:172] relu4 does not need backward computation.
I1027 09:28:44.178439  8417 net.cpp:172] ip1 does not need backward computation.
I1027 09:28:44.178443  8417 net.cpp:172] drop3 does not need backward computation.
I1027 09:28:44.178447  8417 net.cpp:172] relu3 does not need backward computation.
I1027 09:28:44.178450  8417 net.cpp:172] pool3 does not need backward computation.
I1027 09:28:44.178453  8417 net.cpp:172] conv3 does not need backward computation.
I1027 09:28:44.178457  8417 net.cpp:172] drop2 does not need backward computation.
I1027 09:28:44.178460  8417 net.cpp:172] relu2 does not need backward computation.
I1027 09:28:44.178464  8417 net.cpp:172] pool2 does not need backward computation.
I1027 09:28:44.178467  8417 net.cpp:172] conv2 does not need backward computation.
I1027 09:28:44.178472  8417 net.cpp:172] drop1 does not need backward computation.
I1027 09:28:44.178475  8417 net.cpp:172] relu1 does not need backward computation.
I1027 09:28:44.178478  8417 net.cpp:172] pool1 does not need backward computation.
I1027 09:28:44.178483  8417 net.cpp:172] conv1 does not need backward computation.
I1027 09:28:44.178485  8417 net.cpp:208] This network produces output prob
I1027 09:28:44.178498  8417 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 09:28:44.178506  8417 net.cpp:219] Network initialization done.
I1027 09:28:44.178510  8417 net.cpp:220] Memory required for data: 1837200
I1027 09:29:25.534953  8417 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 09:29:25.535583  8417 net.cpp:358] Input 0 -> data
I1027 09:29:25.535645  8417 net.cpp:67] Creating Layer conv1
I1027 09:29:25.535661  8417 net.cpp:394] conv1 <- data
I1027 09:29:25.535681  8417 net.cpp:356] conv1 -> conv1
I1027 09:29:25.535706  8417 net.cpp:96] Setting up conv1
I1027 09:29:25.535775  8417 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 09:29:25.535814  8417 net.cpp:67] Creating Layer pool1
I1027 09:29:25.535826  8417 net.cpp:394] pool1 <- conv1
I1027 09:29:25.535843  8417 net.cpp:356] pool1 -> pool1
I1027 09:29:25.535863  8417 net.cpp:96] Setting up pool1
I1027 09:29:25.535882  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:29:25.535900  8417 net.cpp:67] Creating Layer relu1
I1027 09:29:25.535912  8417 net.cpp:394] relu1 <- pool1
I1027 09:29:25.535926  8417 net.cpp:345] relu1 -> pool1 (in-place)
I1027 09:29:25.535943  8417 net.cpp:96] Setting up relu1
I1027 09:29:25.535955  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:29:25.535971  8417 net.cpp:67] Creating Layer drop1
I1027 09:29:25.535982  8417 net.cpp:394] drop1 <- pool1
I1027 09:29:25.535997  8417 net.cpp:345] drop1 -> pool1 (in-place)
I1027 09:29:25.536015  8417 net.cpp:96] Setting up drop1
I1027 09:29:25.536027  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:29:25.536046  8417 net.cpp:67] Creating Layer conv2
I1027 09:29:25.536058  8417 net.cpp:394] conv2 <- pool1
I1027 09:29:25.536075  8417 net.cpp:356] conv2 -> conv2
I1027 09:29:25.536093  8417 net.cpp:96] Setting up conv2
I1027 09:29:25.537515  8417 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 09:29:25.537554  8417 net.cpp:67] Creating Layer pool2
I1027 09:29:25.537566  8417 net.cpp:394] pool2 <- conv2
I1027 09:29:25.537583  8417 net.cpp:356] pool2 -> pool2
I1027 09:29:25.537603  8417 net.cpp:96] Setting up pool2
I1027 09:29:25.537619  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:29:25.537636  8417 net.cpp:67] Creating Layer relu2
I1027 09:29:25.537647  8417 net.cpp:394] relu2 <- pool2
I1027 09:29:25.537662  8417 net.cpp:345] relu2 -> pool2 (in-place)
I1027 09:29:25.537678  8417 net.cpp:96] Setting up relu2
I1027 09:29:25.537689  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:29:25.537711  8417 net.cpp:67] Creating Layer drop2
I1027 09:29:25.537724  8417 net.cpp:394] drop2 <- pool2
I1027 09:29:25.537739  8417 net.cpp:345] drop2 -> pool2 (in-place)
I1027 09:29:25.537755  8417 net.cpp:96] Setting up drop2
I1027 09:29:25.537768  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:29:25.537788  8417 net.cpp:67] Creating Layer conv3
I1027 09:29:25.537801  8417 net.cpp:394] conv3 <- pool2
I1027 09:29:25.537817  8417 net.cpp:356] conv3 -> conv3
I1027 09:29:25.537837  8417 net.cpp:96] Setting up conv3
I1027 09:29:25.541496  8417 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 09:29:25.541538  8417 net.cpp:67] Creating Layer pool3
I1027 09:29:25.541553  8417 net.cpp:394] pool3 <- conv3
I1027 09:29:25.541568  8417 net.cpp:356] pool3 -> pool3
I1027 09:29:25.541587  8417 net.cpp:96] Setting up pool3
I1027 09:29:25.541602  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:29:25.541617  8417 net.cpp:67] Creating Layer relu3
I1027 09:29:25.541630  8417 net.cpp:394] relu3 <- pool3
I1027 09:29:25.541643  8417 net.cpp:345] relu3 -> pool3 (in-place)
I1027 09:29:25.541659  8417 net.cpp:96] Setting up relu3
I1027 09:29:25.541671  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:29:25.541687  8417 net.cpp:67] Creating Layer drop3
I1027 09:29:25.541697  8417 net.cpp:394] drop3 <- pool3
I1027 09:29:25.541713  8417 net.cpp:345] drop3 -> pool3 (in-place)
I1027 09:29:25.541729  8417 net.cpp:96] Setting up drop3
I1027 09:29:25.541743  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:29:25.541759  8417 net.cpp:67] Creating Layer ip1
I1027 09:29:25.541770  8417 net.cpp:394] ip1 <- pool3
I1027 09:29:25.541787  8417 net.cpp:356] ip1 -> ip1
I1027 09:29:25.541807  8417 net.cpp:96] Setting up ip1
I1027 09:29:25.997282  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:29:25.997352  8417 net.cpp:67] Creating Layer relu4
I1027 09:29:25.997371  8417 net.cpp:394] relu4 <- ip1
I1027 09:29:25.997383  8417 net.cpp:345] relu4 -> ip1 (in-place)
I1027 09:29:25.997395  8417 net.cpp:96] Setting up relu4
I1027 09:29:25.997400  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:29:25.997408  8417 net.cpp:67] Creating Layer drop4
I1027 09:29:25.997412  8417 net.cpp:394] drop4 <- ip1
I1027 09:29:25.997419  8417 net.cpp:345] drop4 -> ip1 (in-place)
I1027 09:29:25.997427  8417 net.cpp:96] Setting up drop4
I1027 09:29:25.997433  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:29:25.997444  8417 net.cpp:67] Creating Layer ip2
I1027 09:29:25.997450  8417 net.cpp:394] ip2 <- ip1
I1027 09:29:25.997463  8417 net.cpp:356] ip2 -> ip2
I1027 09:29:25.997483  8417 net.cpp:96] Setting up ip2
I1027 09:29:26.005182  8417 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 09:29:26.005246  8417 net.cpp:67] Creating Layer prob
I1027 09:29:26.005254  8417 net.cpp:394] prob <- ip2
I1027 09:29:26.005264  8417 net.cpp:356] prob -> prob
I1027 09:29:26.005275  8417 net.cpp:96] Setting up prob
I1027 09:29:26.005282  8417 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 09:29:26.005287  8417 net.cpp:172] prob does not need backward computation.
I1027 09:29:26.005291  8417 net.cpp:172] ip2 does not need backward computation.
I1027 09:29:26.005295  8417 net.cpp:172] drop4 does not need backward computation.
I1027 09:29:26.005300  8417 net.cpp:172] relu4 does not need backward computation.
I1027 09:29:26.005303  8417 net.cpp:172] ip1 does not need backward computation.
I1027 09:29:26.005307  8417 net.cpp:172] drop3 does not need backward computation.
I1027 09:29:26.005311  8417 net.cpp:172] relu3 does not need backward computation.
I1027 09:29:26.005314  8417 net.cpp:172] pool3 does not need backward computation.
I1027 09:29:26.005318  8417 net.cpp:172] conv3 does not need backward computation.
I1027 09:29:26.005322  8417 net.cpp:172] drop2 does not need backward computation.
I1027 09:29:26.005326  8417 net.cpp:172] relu2 does not need backward computation.
I1027 09:29:26.005331  8417 net.cpp:172] pool2 does not need backward computation.
I1027 09:29:26.005334  8417 net.cpp:172] conv2 does not need backward computation.
I1027 09:29:26.005347  8417 net.cpp:172] drop1 does not need backward computation.
I1027 09:29:26.005350  8417 net.cpp:172] relu1 does not need backward computation.
I1027 09:29:26.005354  8417 net.cpp:172] pool1 does not need backward computation.
I1027 09:29:26.005358  8417 net.cpp:172] conv1 does not need backward computation.
I1027 09:29:26.005362  8417 net.cpp:208] This network produces output prob
I1027 09:29:26.005378  8417 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 09:29:26.005388  8417 net.cpp:219] Network initialization done.
I1027 09:29:26.005391  8417 net.cpp:220] Memory required for data: 1837200
I1027 09:30:02.467979  8417 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 09:30:02.468616  8417 net.cpp:358] Input 0 -> data
I1027 09:30:02.468647  8417 net.cpp:67] Creating Layer conv1
I1027 09:30:02.468653  8417 net.cpp:394] conv1 <- data
I1027 09:30:02.468662  8417 net.cpp:356] conv1 -> conv1
I1027 09:30:02.468672  8417 net.cpp:96] Setting up conv1
I1027 09:30:02.468703  8417 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 09:30:02.468720  8417 net.cpp:67] Creating Layer pool1
I1027 09:30:02.468725  8417 net.cpp:394] pool1 <- conv1
I1027 09:30:02.468731  8417 net.cpp:356] pool1 -> pool1
I1027 09:30:02.468739  8417 net.cpp:96] Setting up pool1
I1027 09:30:02.468747  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:30:02.468762  8417 net.cpp:67] Creating Layer relu1
I1027 09:30:02.468767  8417 net.cpp:394] relu1 <- pool1
I1027 09:30:02.468772  8417 net.cpp:345] relu1 -> pool1 (in-place)
I1027 09:30:02.468778  8417 net.cpp:96] Setting up relu1
I1027 09:30:02.468783  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:30:02.468789  8417 net.cpp:67] Creating Layer drop1
I1027 09:30:02.468793  8417 net.cpp:394] drop1 <- pool1
I1027 09:30:02.468799  8417 net.cpp:345] drop1 -> pool1 (in-place)
I1027 09:30:02.468806  8417 net.cpp:96] Setting up drop1
I1027 09:30:02.468811  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:30:02.468818  8417 net.cpp:67] Creating Layer conv2
I1027 09:30:02.468822  8417 net.cpp:394] conv2 <- pool1
I1027 09:30:02.468829  8417 net.cpp:356] conv2 -> conv2
I1027 09:30:02.468837  8417 net.cpp:96] Setting up conv2
I1027 09:30:02.469338  8417 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 09:30:02.469353  8417 net.cpp:67] Creating Layer pool2
I1027 09:30:02.469358  8417 net.cpp:394] pool2 <- conv2
I1027 09:30:02.469364  8417 net.cpp:356] pool2 -> pool2
I1027 09:30:02.469372  8417 net.cpp:96] Setting up pool2
I1027 09:30:02.469378  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:30:02.469383  8417 net.cpp:67] Creating Layer relu2
I1027 09:30:02.469388  8417 net.cpp:394] relu2 <- pool2
I1027 09:30:02.469393  8417 net.cpp:345] relu2 -> pool2 (in-place)
I1027 09:30:02.469399  8417 net.cpp:96] Setting up relu2
I1027 09:30:02.469404  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:30:02.469409  8417 net.cpp:67] Creating Layer drop2
I1027 09:30:02.469414  8417 net.cpp:394] drop2 <- pool2
I1027 09:30:02.469419  8417 net.cpp:345] drop2 -> pool2 (in-place)
I1027 09:30:02.469425  8417 net.cpp:96] Setting up drop2
I1027 09:30:02.469430  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:30:02.469437  8417 net.cpp:67] Creating Layer conv3
I1027 09:30:02.469442  8417 net.cpp:394] conv3 <- pool2
I1027 09:30:02.469449  8417 net.cpp:356] conv3 -> conv3
I1027 09:30:02.469456  8417 net.cpp:96] Setting up conv3
I1027 09:30:02.470904  8417 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 09:30:02.470922  8417 net.cpp:67] Creating Layer pool3
I1027 09:30:02.470928  8417 net.cpp:394] pool3 <- conv3
I1027 09:30:02.470934  8417 net.cpp:356] pool3 -> pool3
I1027 09:30:02.470942  8417 net.cpp:96] Setting up pool3
I1027 09:30:02.470947  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:30:02.470953  8417 net.cpp:67] Creating Layer relu3
I1027 09:30:02.470957  8417 net.cpp:394] relu3 <- pool3
I1027 09:30:02.470963  8417 net.cpp:345] relu3 -> pool3 (in-place)
I1027 09:30:02.470968  8417 net.cpp:96] Setting up relu3
I1027 09:30:02.470973  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:30:02.470978  8417 net.cpp:67] Creating Layer drop3
I1027 09:30:02.470983  8417 net.cpp:394] drop3 <- pool3
I1027 09:30:02.470988  8417 net.cpp:345] drop3 -> pool3 (in-place)
I1027 09:30:02.470994  8417 net.cpp:96] Setting up drop3
I1027 09:30:02.470999  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:30:02.471005  8417 net.cpp:67] Creating Layer ip1
I1027 09:30:02.471010  8417 net.cpp:394] ip1 <- pool3
I1027 09:30:02.471016  8417 net.cpp:356] ip1 -> ip1
I1027 09:30:02.471024  8417 net.cpp:96] Setting up ip1
I1027 09:30:02.872795  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:30:02.872860  8417 net.cpp:67] Creating Layer relu4
I1027 09:30:02.872869  8417 net.cpp:394] relu4 <- ip1
I1027 09:30:02.872880  8417 net.cpp:345] relu4 -> ip1 (in-place)
I1027 09:30:02.872890  8417 net.cpp:96] Setting up relu4
I1027 09:30:02.872896  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:30:02.872905  8417 net.cpp:67] Creating Layer drop4
I1027 09:30:02.872910  8417 net.cpp:394] drop4 <- ip1
I1027 09:30:02.872917  8417 net.cpp:345] drop4 -> ip1 (in-place)
I1027 09:30:02.872925  8417 net.cpp:96] Setting up drop4
I1027 09:30:02.872931  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:30:02.872941  8417 net.cpp:67] Creating Layer ip2
I1027 09:30:02.872946  8417 net.cpp:394] ip2 <- ip1
I1027 09:30:02.872967  8417 net.cpp:356] ip2 -> ip2
I1027 09:30:02.872982  8417 net.cpp:96] Setting up ip2
I1027 09:30:02.880584  8417 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 09:30:02.880648  8417 net.cpp:67] Creating Layer prob
I1027 09:30:02.880656  8417 net.cpp:394] prob <- ip2
I1027 09:30:02.880664  8417 net.cpp:356] prob -> prob
I1027 09:30:02.880676  8417 net.cpp:96] Setting up prob
I1027 09:30:02.880684  8417 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 09:30:02.880689  8417 net.cpp:172] prob does not need backward computation.
I1027 09:30:02.880693  8417 net.cpp:172] ip2 does not need backward computation.
I1027 09:30:02.880697  8417 net.cpp:172] drop4 does not need backward computation.
I1027 09:30:02.880702  8417 net.cpp:172] relu4 does not need backward computation.
I1027 09:30:02.880705  8417 net.cpp:172] ip1 does not need backward computation.
I1027 09:30:02.880709  8417 net.cpp:172] drop3 does not need backward computation.
I1027 09:30:02.880713  8417 net.cpp:172] relu3 does not need backward computation.
I1027 09:30:02.880717  8417 net.cpp:172] pool3 does not need backward computation.
I1027 09:30:02.880722  8417 net.cpp:172] conv3 does not need backward computation.
I1027 09:30:02.880725  8417 net.cpp:172] drop2 does not need backward computation.
I1027 09:30:02.880730  8417 net.cpp:172] relu2 does not need backward computation.
I1027 09:30:02.880734  8417 net.cpp:172] pool2 does not need backward computation.
I1027 09:30:02.880738  8417 net.cpp:172] conv2 does not need backward computation.
I1027 09:30:02.880743  8417 net.cpp:172] drop1 does not need backward computation.
I1027 09:30:02.880746  8417 net.cpp:172] relu1 does not need backward computation.
I1027 09:30:02.880750  8417 net.cpp:172] pool1 does not need backward computation.
I1027 09:30:02.880754  8417 net.cpp:172] conv1 does not need backward computation.
I1027 09:30:02.880759  8417 net.cpp:208] This network produces output prob
I1027 09:30:02.880772  8417 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 09:30:02.880782  8417 net.cpp:219] Network initialization done.
I1027 09:30:02.880786  8417 net.cpp:220] Memory required for data: 1837200
I1027 09:30:39.709964  8417 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 09:30:39.710491  8417 net.cpp:358] Input 0 -> data
I1027 09:30:39.710522  8417 net.cpp:67] Creating Layer conv1
I1027 09:30:39.710528  8417 net.cpp:394] conv1 <- data
I1027 09:30:39.710536  8417 net.cpp:356] conv1 -> conv1
I1027 09:30:39.710546  8417 net.cpp:96] Setting up conv1
I1027 09:30:39.710577  8417 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 09:30:39.710593  8417 net.cpp:67] Creating Layer pool1
I1027 09:30:39.710598  8417 net.cpp:394] pool1 <- conv1
I1027 09:30:39.710604  8417 net.cpp:356] pool1 -> pool1
I1027 09:30:39.710613  8417 net.cpp:96] Setting up pool1
I1027 09:30:39.710619  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:30:39.710626  8417 net.cpp:67] Creating Layer relu1
I1027 09:30:39.710630  8417 net.cpp:394] relu1 <- pool1
I1027 09:30:39.710636  8417 net.cpp:345] relu1 -> pool1 (in-place)
I1027 09:30:39.710643  8417 net.cpp:96] Setting up relu1
I1027 09:30:39.710647  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:30:39.710652  8417 net.cpp:67] Creating Layer drop1
I1027 09:30:39.710657  8417 net.cpp:394] drop1 <- pool1
I1027 09:30:39.710664  8417 net.cpp:345] drop1 -> pool1 (in-place)
I1027 09:30:39.710669  8417 net.cpp:96] Setting up drop1
I1027 09:30:39.710675  8417 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 09:30:39.710682  8417 net.cpp:67] Creating Layer conv2
I1027 09:30:39.710686  8417 net.cpp:394] conv2 <- pool1
I1027 09:30:39.710692  8417 net.cpp:356] conv2 -> conv2
I1027 09:30:39.710700  8417 net.cpp:96] Setting up conv2
I1027 09:30:39.711201  8417 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 09:30:39.711216  8417 net.cpp:67] Creating Layer pool2
I1027 09:30:39.711221  8417 net.cpp:394] pool2 <- conv2
I1027 09:30:39.711227  8417 net.cpp:356] pool2 -> pool2
I1027 09:30:39.711235  8417 net.cpp:96] Setting up pool2
I1027 09:30:39.711241  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:30:39.711246  8417 net.cpp:67] Creating Layer relu2
I1027 09:30:39.711251  8417 net.cpp:394] relu2 <- pool2
I1027 09:30:39.711256  8417 net.cpp:345] relu2 -> pool2 (in-place)
I1027 09:30:39.711261  8417 net.cpp:96] Setting up relu2
I1027 09:30:39.711266  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:30:39.711271  8417 net.cpp:67] Creating Layer drop2
I1027 09:30:39.711274  8417 net.cpp:394] drop2 <- pool2
I1027 09:30:39.711280  8417 net.cpp:345] drop2 -> pool2 (in-place)
I1027 09:30:39.711285  8417 net.cpp:96] Setting up drop2
I1027 09:30:39.711290  8417 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 09:30:39.711297  8417 net.cpp:67] Creating Layer conv3
I1027 09:30:39.711302  8417 net.cpp:394] conv3 <- pool2
I1027 09:30:39.711308  8417 net.cpp:356] conv3 -> conv3
I1027 09:30:39.711315  8417 net.cpp:96] Setting up conv3
I1027 09:30:39.713088  8417 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 09:30:39.713131  8417 net.cpp:67] Creating Layer pool3
I1027 09:30:39.713146  8417 net.cpp:394] pool3 <- conv3
I1027 09:30:39.713163  8417 net.cpp:356] pool3 -> pool3
I1027 09:30:39.713182  8417 net.cpp:96] Setting up pool3
I1027 09:30:39.713197  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:30:39.713213  8417 net.cpp:67] Creating Layer relu3
I1027 09:30:39.713233  8417 net.cpp:394] relu3 <- pool3
I1027 09:30:39.713249  8417 net.cpp:345] relu3 -> pool3 (in-place)
I1027 09:30:39.713265  8417 net.cpp:96] Setting up relu3
I1027 09:30:39.713277  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:30:39.713294  8417 net.cpp:67] Creating Layer drop3
I1027 09:30:39.713304  8417 net.cpp:394] drop3 <- pool3
I1027 09:30:39.713320  8417 net.cpp:345] drop3 -> pool3 (in-place)
I1027 09:30:39.713335  8417 net.cpp:96] Setting up drop3
I1027 09:30:39.713349  8417 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 09:30:39.713366  8417 net.cpp:67] Creating Layer ip1
I1027 09:30:39.713378  8417 net.cpp:394] ip1 <- pool3
I1027 09:30:39.713395  8417 net.cpp:356] ip1 -> ip1
I1027 09:30:39.713415  8417 net.cpp:96] Setting up ip1
I1027 09:30:40.132737  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:30:40.132797  8417 net.cpp:67] Creating Layer relu4
I1027 09:30:40.132805  8417 net.cpp:394] relu4 <- ip1
I1027 09:30:40.132817  8417 net.cpp:345] relu4 -> ip1 (in-place)
I1027 09:30:40.132827  8417 net.cpp:96] Setting up relu4
I1027 09:30:40.132833  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:30:40.132841  8417 net.cpp:67] Creating Layer drop4
I1027 09:30:40.132846  8417 net.cpp:394] drop4 <- ip1
I1027 09:30:40.132853  8417 net.cpp:345] drop4 -> ip1 (in-place)
I1027 09:30:40.132861  8417 net.cpp:96] Setting up drop4
I1027 09:30:40.132868  8417 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 09:30:40.132877  8417 net.cpp:67] Creating Layer ip2
I1027 09:30:40.132882  8417 net.cpp:394] ip2 <- ip1
I1027 09:30:40.132890  8417 net.cpp:356] ip2 -> ip2
I1027 09:30:40.132906  8417 net.cpp:96] Setting up ip2
I1027 09:30:40.140586  8417 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 09:30:40.140648  8417 net.cpp:67] Creating Layer prob
I1027 09:30:40.140656  8417 net.cpp:394] prob <- ip2
I1027 09:30:40.140666  8417 net.cpp:356] prob -> prob
I1027 09:30:40.140677  8417 net.cpp:96] Setting up prob
I1027 09:30:40.140686  8417 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 09:30:40.140691  8417 net.cpp:172] prob does not need backward computation.
I1027 09:30:40.140696  8417 net.cpp:172] ip2 does not need backward computation.
I1027 09:30:40.140699  8417 net.cpp:172] drop4 does not need backward computation.
I1027 09:30:40.140703  8417 net.cpp:172] relu4 does not need backward computation.
I1027 09:30:40.140707  8417 net.cpp:172] ip1 does not need backward computation.
I1027 09:30:40.140712  8417 net.cpp:172] drop3 does not need backward computation.
I1027 09:30:40.140717  8417 net.cpp:172] relu3 does not need backward computation.
I1027 09:30:40.140719  8417 net.cpp:172] pool3 does not need backward computation.
I1027 09:30:40.140724  8417 net.cpp:172] conv3 does not need backward computation.
I1027 09:30:40.140727  8417 net.cpp:172] drop2 does not need backward computation.
I1027 09:30:40.140732  8417 net.cpp:172] relu2 does not need backward computation.
I1027 09:30:40.140735  8417 net.cpp:172] pool2 does not need backward computation.
I1027 09:30:40.140739  8417 net.cpp:172] conv2 does not need backward computation.
I1027 09:30:40.140743  8417 net.cpp:172] drop1 does not need backward computation.
I1027 09:30:40.140748  8417 net.cpp:172] relu1 does not need backward computation.
I1027 09:30:40.140751  8417 net.cpp:172] pool1 does not need backward computation.
I1027 09:30:40.140755  8417 net.cpp:172] conv1 does not need backward computation.
I1027 09:30:40.140759  8417 net.cpp:208] This network produces output prob
I1027 09:30:40.140774  8417 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 09:30:40.140784  8417 net.cpp:219] Network initialization done.
I1027 09:30:40.140789  8417 net.cpp:220] Memory required for data: 1837200
I1027 10:06:14.703634 15606 convert_imageset.cpp:70] Shuffling data
I1027 10:06:15.288349 15606 convert_imageset.cpp:73] A total of 60000 images.
I1027 10:06:15.288466 15606 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1027 10:06:17.683519 15606 convert_imageset.cpp:177] Processed 1000 files.
E1027 10:06:19.960305 15606 convert_imageset.cpp:177] Processed 2000 files.
E1027 10:06:22.156857 15606 convert_imageset.cpp:177] Processed 3000 files.
E1027 10:06:24.233111 15606 convert_imageset.cpp:177] Processed 4000 files.
E1027 10:06:26.262907 15606 convert_imageset.cpp:177] Processed 5000 files.
E1027 10:06:28.231647 15606 convert_imageset.cpp:177] Processed 6000 files.
E1027 10:06:30.184939 15606 convert_imageset.cpp:177] Processed 7000 files.
E1027 10:06:32.180543 15606 convert_imageset.cpp:177] Processed 8000 files.
E1027 10:06:33.988144 15606 convert_imageset.cpp:177] Processed 9000 files.
E1027 10:06:35.909030 15606 convert_imageset.cpp:177] Processed 10000 files.
E1027 10:06:37.708400 15606 convert_imageset.cpp:177] Processed 11000 files.
E1027 10:06:39.565017 15606 convert_imageset.cpp:177] Processed 12000 files.
E1027 10:06:41.429013 15606 convert_imageset.cpp:177] Processed 13000 files.
E1027 10:06:43.274247 15606 convert_imageset.cpp:177] Processed 14000 files.
E1027 10:06:45.020695 15606 convert_imageset.cpp:177] Processed 15000 files.
E1027 10:06:46.808230 15606 convert_imageset.cpp:177] Processed 16000 files.
E1027 10:06:48.674088 15606 convert_imageset.cpp:177] Processed 17000 files.
E1027 10:06:50.500007 15606 convert_imageset.cpp:177] Processed 18000 files.
E1027 10:06:52.686638 15606 convert_imageset.cpp:177] Processed 19000 files.
E1027 10:06:54.329517 15606 convert_imageset.cpp:177] Processed 20000 files.
E1027 10:06:56.074182 15606 convert_imageset.cpp:177] Processed 21000 files.
E1027 10:06:57.691617 15606 convert_imageset.cpp:177] Processed 22000 files.
E1027 10:06:59.318780 15606 convert_imageset.cpp:177] Processed 23000 files.
E1027 10:07:00.901096 15606 convert_imageset.cpp:177] Processed 24000 files.
E1027 10:07:02.473363 15606 convert_imageset.cpp:177] Processed 25000 files.
E1027 10:07:04.083189 15606 convert_imageset.cpp:177] Processed 26000 files.
E1027 10:07:05.724170 15606 convert_imageset.cpp:177] Processed 27000 files.
E1027 10:07:07.402371 15606 convert_imageset.cpp:177] Processed 28000 files.
E1027 10:07:09.085558 15606 convert_imageset.cpp:177] Processed 29000 files.
E1027 10:07:10.735968 15606 convert_imageset.cpp:177] Processed 30000 files.
E1027 10:07:12.484978 15606 convert_imageset.cpp:177] Processed 31000 files.
E1027 10:07:14.204967 15606 convert_imageset.cpp:177] Processed 32000 files.
E1027 10:07:15.874955 15606 convert_imageset.cpp:177] Processed 33000 files.
E1027 10:07:17.529685 15606 convert_imageset.cpp:177] Processed 34000 files.
E1027 10:07:19.161615 15606 convert_imageset.cpp:177] Processed 35000 files.
E1027 10:07:20.816329 15606 convert_imageset.cpp:177] Processed 36000 files.
E1027 10:07:22.454927 15606 convert_imageset.cpp:177] Processed 37000 files.
E1027 10:07:23.952769 15606 convert_imageset.cpp:177] Processed 38000 files.
E1027 10:07:25.562371 15606 convert_imageset.cpp:177] Processed 39000 files.
E1027 10:07:27.258047 15606 convert_imageset.cpp:177] Processed 40000 files.
E1027 10:07:28.925422 15606 convert_imageset.cpp:177] Processed 41000 files.
E1027 10:07:30.517755 15606 convert_imageset.cpp:177] Processed 42000 files.
E1027 10:07:32.137842 15606 convert_imageset.cpp:177] Processed 43000 files.
E1027 10:07:33.742118 15606 convert_imageset.cpp:177] Processed 44000 files.
E1027 10:07:35.289932 15606 convert_imageset.cpp:177] Processed 45000 files.
E1027 10:07:36.889552 15606 convert_imageset.cpp:177] Processed 46000 files.
E1027 10:07:38.380903 15606 convert_imageset.cpp:177] Processed 47000 files.
E1027 10:07:40.002611 15606 convert_imageset.cpp:177] Processed 48000 files.
E1027 10:07:41.683326 15606 convert_imageset.cpp:177] Processed 49000 files.
E1027 10:07:43.283365 15606 convert_imageset.cpp:177] Processed 50000 files.
E1027 10:07:44.878108 15606 convert_imageset.cpp:177] Processed 51000 files.
E1027 10:07:46.448056 15606 convert_imageset.cpp:177] Processed 52000 files.
E1027 10:07:48.041082 15606 convert_imageset.cpp:177] Processed 53000 files.
E1027 10:07:49.613593 15606 convert_imageset.cpp:177] Processed 54000 files.
E1027 10:07:51.245316 15606 convert_imageset.cpp:177] Processed 55000 files.
E1027 10:07:52.777901 15606 convert_imageset.cpp:177] Processed 56000 files.
E1027 10:07:54.505535 15606 convert_imageset.cpp:177] Processed 57000 files.
E1027 10:07:56.073580 15606 convert_imageset.cpp:177] Processed 58000 files.
E1027 10:07:57.529858 15606 convert_imageset.cpp:177] Processed 59000 files.
E1027 10:07:59.088454 15606 convert_imageset.cpp:177] Processed 60000 files.
I1027 10:07:59.614940 15659 caffe.cpp:99] Use GPU with device ID 0
I1027 10:07:59.976620 15659 caffe.cpp:107] Starting Optimization
I1027 10:07:59.976744 15659 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 275000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1027 10:07:59.976768 15659 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1027 10:07:59.985239 15659 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1027 10:07:59.985337 15659 net.cpp:67] Creating Layer mnist
I1027 10:07:59.985347 15659 net.cpp:356] mnist -> data
I1027 10:07:59.985365 15659 net.cpp:356] mnist -> label
I1027 10:07:59.985379 15659 net.cpp:96] Setting up mnist
I1027 10:07:59.993099 15659 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1027 10:07:59.993196 15659 data_layer.cpp:128] output data size: 64,1,50,180
I1027 10:07:59.994164 15659 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1027 10:07:59.994191 15659 net.cpp:103] Top shape: 64 1 1 1 (64)
I1027 10:07:59.994207 15659 net.cpp:67] Creating Layer conv1
I1027 10:07:59.994215 15659 net.cpp:394] conv1 <- data
I1027 10:07:59.994235 15659 net.cpp:356] conv1 -> conv1
I1027 10:07:59.994251 15659 net.cpp:96] Setting up conv1
I1027 10:07:59.994776 15659 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1027 10:07:59.994819 15659 net.cpp:67] Creating Layer pool1
I1027 10:07:59.994828 15659 net.cpp:394] pool1 <- conv1
I1027 10:07:59.994837 15659 net.cpp:356] pool1 -> pool1
I1027 10:07:59.994850 15659 net.cpp:96] Setting up pool1
I1027 10:07:59.994870 15659 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 10:07:59.994880 15659 net.cpp:67] Creating Layer relu1
I1027 10:07:59.994887 15659 net.cpp:394] relu1 <- pool1
I1027 10:07:59.994896 15659 net.cpp:345] relu1 -> pool1 (in-place)
I1027 10:07:59.994905 15659 net.cpp:96] Setting up relu1
I1027 10:07:59.994912 15659 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 10:07:59.994923 15659 net.cpp:67] Creating Layer drop1
I1027 10:07:59.994930 15659 net.cpp:394] drop1 <- pool1
I1027 10:07:59.994941 15659 net.cpp:345] drop1 -> pool1 (in-place)
I1027 10:07:59.994951 15659 net.cpp:96] Setting up drop1
I1027 10:07:59.994958 15659 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 10:07:59.994969 15659 net.cpp:67] Creating Layer conv2
I1027 10:07:59.994976 15659 net.cpp:394] conv2 <- pool1
I1027 10:07:59.994987 15659 net.cpp:356] conv2 -> conv2
I1027 10:07:59.994998 15659 net.cpp:96] Setting up conv2
I1027 10:07:59.995822 15659 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1027 10:07:59.995846 15659 net.cpp:67] Creating Layer pool2
I1027 10:07:59.995854 15659 net.cpp:394] pool2 <- conv2
I1027 10:07:59.995863 15659 net.cpp:356] pool2 -> pool2
I1027 10:07:59.995874 15659 net.cpp:96] Setting up pool2
I1027 10:07:59.995882 15659 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 10:07:59.995892 15659 net.cpp:67] Creating Layer relu2
I1027 10:07:59.995898 15659 net.cpp:394] relu2 <- pool2
I1027 10:07:59.995908 15659 net.cpp:345] relu2 -> pool2 (in-place)
I1027 10:07:59.995918 15659 net.cpp:96] Setting up relu2
I1027 10:07:59.995925 15659 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 10:07:59.995935 15659 net.cpp:67] Creating Layer drop2
I1027 10:07:59.995941 15659 net.cpp:394] drop2 <- pool2
I1027 10:07:59.995950 15659 net.cpp:345] drop2 -> pool2 (in-place)
I1027 10:07:59.995959 15659 net.cpp:96] Setting up drop2
I1027 10:07:59.995966 15659 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 10:07:59.995978 15659 net.cpp:67] Creating Layer conv3
I1027 10:07:59.995985 15659 net.cpp:394] conv3 <- pool2
I1027 10:07:59.995995 15659 net.cpp:356] conv3 -> conv3
I1027 10:07:59.996006 15659 net.cpp:96] Setting up conv3
I1027 10:07:59.999780 15659 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1027 10:07:59.999836 15659 net.cpp:67] Creating Layer pool3
I1027 10:07:59.999851 15659 net.cpp:394] pool3 <- conv3
I1027 10:07:59.999874 15659 net.cpp:356] pool3 -> pool3
I1027 10:07:59.999896 15659 net.cpp:96] Setting up pool3
I1027 10:07:59.999912 15659 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 10:07:59.999928 15659 net.cpp:67] Creating Layer relu3
I1027 10:07:59.999940 15659 net.cpp:394] relu3 <- pool3
I1027 10:07:59.999959 15659 net.cpp:345] relu3 -> pool3 (in-place)
I1027 10:07:59.999977 15659 net.cpp:96] Setting up relu3
I1027 10:08:00.000000 15659 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 10:08:00.000017 15659 net.cpp:67] Creating Layer drop3
I1027 10:08:00.000030 15659 net.cpp:394] drop3 <- pool3
I1027 10:08:00.000046 15659 net.cpp:345] drop3 -> pool3 (in-place)
I1027 10:08:00.000063 15659 net.cpp:96] Setting up drop3
I1027 10:08:00.000077 15659 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 10:08:00.000095 15659 net.cpp:67] Creating Layer ip1
I1027 10:08:00.000108 15659 net.cpp:394] ip1 <- pool3
I1027 10:08:00.000129 15659 net.cpp:356] ip1 -> ip1
I1027 10:08:00.000195 15659 net.cpp:96] Setting up ip1
I1027 10:08:00.447726 15659 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 10:08:00.447788 15659 net.cpp:67] Creating Layer relu4
I1027 10:08:00.447795 15659 net.cpp:394] relu4 <- ip1
I1027 10:08:00.447805 15659 net.cpp:345] relu4 -> ip1 (in-place)
I1027 10:08:00.447815 15659 net.cpp:96] Setting up relu4
I1027 10:08:00.447820 15659 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 10:08:00.447827 15659 net.cpp:67] Creating Layer drop4
I1027 10:08:00.447832 15659 net.cpp:394] drop4 <- ip1
I1027 10:08:00.447837 15659 net.cpp:345] drop4 -> ip1 (in-place)
I1027 10:08:00.447844 15659 net.cpp:96] Setting up drop4
I1027 10:08:00.447849 15659 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 10:08:00.447863 15659 net.cpp:67] Creating Layer ip2
I1027 10:08:00.447867 15659 net.cpp:394] ip2 <- ip1
I1027 10:08:00.447876 15659 net.cpp:356] ip2 -> ip2
I1027 10:08:00.447885 15659 net.cpp:96] Setting up ip2
I1027 10:08:00.464670 15659 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1027 10:08:00.464751 15659 net.cpp:67] Creating Layer loss
I1027 10:08:00.464761 15659 net.cpp:394] loss <- ip2
I1027 10:08:00.464768 15659 net.cpp:394] loss <- label
I1027 10:08:00.464776 15659 net.cpp:356] loss -> loss
I1027 10:08:00.464787 15659 net.cpp:96] Setting up loss
I1027 10:08:00.464799 15659 net.cpp:103] Top shape: 1 1 1 1 (1)
I1027 10:08:00.464804 15659 net.cpp:109]     with loss weight 1
I1027 10:08:00.464855 15659 net.cpp:170] loss needs backward computation.
I1027 10:08:00.464861 15659 net.cpp:170] ip2 needs backward computation.
I1027 10:08:00.464866 15659 net.cpp:170] drop4 needs backward computation.
I1027 10:08:00.464870 15659 net.cpp:170] relu4 needs backward computation.
I1027 10:08:00.464874 15659 net.cpp:170] ip1 needs backward computation.
I1027 10:08:00.464879 15659 net.cpp:170] drop3 needs backward computation.
I1027 10:08:00.464884 15659 net.cpp:170] relu3 needs backward computation.
I1027 10:08:00.464889 15659 net.cpp:170] pool3 needs backward computation.
I1027 10:08:00.464892 15659 net.cpp:170] conv3 needs backward computation.
I1027 10:08:00.464897 15659 net.cpp:170] drop2 needs backward computation.
I1027 10:08:00.464902 15659 net.cpp:170] relu2 needs backward computation.
I1027 10:08:00.464906 15659 net.cpp:170] pool2 needs backward computation.
I1027 10:08:00.464911 15659 net.cpp:170] conv2 needs backward computation.
I1027 10:08:00.464915 15659 net.cpp:170] drop1 needs backward computation.
I1027 10:08:00.464920 15659 net.cpp:170] relu1 needs backward computation.
I1027 10:08:00.464925 15659 net.cpp:170] pool1 needs backward computation.
I1027 10:08:00.464929 15659 net.cpp:170] conv1 needs backward computation.
I1027 10:08:00.464968 15659 net.cpp:172] mnist does not need backward computation.
I1027 10:08:00.464973 15659 net.cpp:208] This network produces output loss
I1027 10:08:00.464985 15659 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 10:08:00.464993 15659 net.cpp:219] Network initialization done.
I1027 10:08:00.464998 15659 net.cpp:220] Memory required for data: 119788292
I1027 10:08:00.465062 15659 solver.cpp:41] Solver scaffolding done.
I1027 10:08:00.465068 15659 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_255000.solverstate
I1027 10:08:00.465073 15659 solver.cpp:160] Solving Captcha
I1027 10:08:00.465092 15659 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_255000.solverstate
I1027 10:08:07.685457 15659 solver.cpp:502] SGDSolver: restoring history
I1027 10:08:08.465946 15659 solver.cpp:191] Iteration 255000, loss = 2.41802
I1027 10:08:08.466003 15659 solver.cpp:206]     Train net output #0: loss = 2.41802 (* 1 = 2.41802 loss)
I1027 10:08:08.466018 15659 solver.cpp:403] Iteration 255000, lr = 0.000856181
I1027 10:12:10.288955 15659 solver.cpp:191] Iteration 256000, loss = 2.62089
I1027 10:12:10.289700 15659 solver.cpp:206]     Train net output #0: loss = 2.62089 (* 1 = 2.62089 loss)
I1027 10:12:10.289737 15659 solver.cpp:403] Iteration 256000, lr = 0.000853766
I1027 10:16:11.460558 15659 solver.cpp:191] Iteration 257000, loss = 2.45004
I1027 10:16:11.461089 15659 solver.cpp:206]     Train net output #0: loss = 2.45004 (* 1 = 2.45004 loss)
I1027 10:16:11.461125 15659 solver.cpp:403] Iteration 257000, lr = 0.000851366
I1027 10:20:12.632287 15659 solver.cpp:191] Iteration 258000, loss = 2.37328
I1027 10:20:12.632971 15659 solver.cpp:206]     Train net output #0: loss = 2.37328 (* 1 = 2.37328 loss)
I1027 10:20:12.633002 15659 solver.cpp:403] Iteration 258000, lr = 0.000848983
I1027 10:24:13.845911 15659 solver.cpp:191] Iteration 259000, loss = 2.74563
I1027 10:24:13.846492 15659 solver.cpp:206]     Train net output #0: loss = 2.74563 (* 1 = 2.74563 loss)
I1027 10:24:13.846524 15659 solver.cpp:403] Iteration 259000, lr = 0.000846615
I1027 10:28:15.615113 15659 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_260000.caffemodel
I1027 10:28:20.184075 15659 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_260000.solverstate
I1027 10:28:23.854400 15659 solver.cpp:191] Iteration 260000, loss = 2.35121
I1027 10:28:23.854897 15659 solver.cpp:206]     Train net output #0: loss = 2.35121 (* 1 = 2.35121 loss)
I1027 10:28:23.854935 15659 solver.cpp:403] Iteration 260000, lr = 0.000844262
I1027 10:32:25.099726 15659 solver.cpp:191] Iteration 261000, loss = 2.66523
I1027 10:32:25.100380 15659 solver.cpp:206]     Train net output #0: loss = 2.66523 (* 1 = 2.66523 loss)
I1027 10:32:25.100414 15659 solver.cpp:403] Iteration 261000, lr = 0.000841924
I1027 10:36:26.227154 15659 solver.cpp:191] Iteration 262000, loss = 2.66624
I1027 10:36:26.227773 15659 solver.cpp:206]     Train net output #0: loss = 2.66624 (* 1 = 2.66624 loss)
I1027 10:36:26.227805 15659 solver.cpp:403] Iteration 262000, lr = 0.000839602
I1027 10:40:27.540930 15659 solver.cpp:191] Iteration 263000, loss = 2.29083
I1027 10:40:27.541523 15659 solver.cpp:206]     Train net output #0: loss = 2.29083 (* 1 = 2.29083 loss)
I1027 10:40:27.541556 15659 solver.cpp:403] Iteration 263000, lr = 0.000837294
I1027 10:44:28.922719 15659 solver.cpp:191] Iteration 264000, loss = 2.43207
I1027 10:44:28.923261 15659 solver.cpp:206]     Train net output #0: loss = 2.43207 (* 1 = 2.43207 loss)
I1027 10:44:28.923297 15659 solver.cpp:403] Iteration 264000, lr = 0.000835001
I1027 10:48:30.599993 15659 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_265000.caffemodel
I1027 10:48:34.496780 15659 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_265000.solverstate
I1027 10:48:37.808266 15659 solver.cpp:191] Iteration 265000, loss = 2.39309
I1027 10:48:37.808894 15659 solver.cpp:206]     Train net output #0: loss = 2.39309 (* 1 = 2.39309 loss)
I1027 10:48:37.808924 15659 solver.cpp:403] Iteration 265000, lr = 0.000832723
I1027 10:52:38.850605 15659 solver.cpp:191] Iteration 266000, loss = 2.46564
I1027 10:52:38.851169 15659 solver.cpp:206]     Train net output #0: loss = 2.46564 (* 1 = 2.46564 loss)
I1027 10:52:38.851204 15659 solver.cpp:403] Iteration 266000, lr = 0.000830459
I1027 10:56:40.141654 15659 solver.cpp:191] Iteration 267000, loss = 2.25034
I1027 10:56:40.142220 15659 solver.cpp:206]     Train net output #0: loss = 2.25034 (* 1 = 2.25034 loss)
I1027 10:56:40.142256 15659 solver.cpp:403] Iteration 267000, lr = 0.000828209
I1027 11:00:41.478863 15659 solver.cpp:191] Iteration 268000, loss = 2.34817
I1027 11:00:41.479547 15659 solver.cpp:206]     Train net output #0: loss = 2.34817 (* 1 = 2.34817 loss)
I1027 11:00:41.479580 15659 solver.cpp:403] Iteration 268000, lr = 0.000825974
I1027 11:04:42.836506 15659 solver.cpp:191] Iteration 269000, loss = 2.36478
I1027 11:04:42.837080 15659 solver.cpp:206]     Train net output #0: loss = 2.36478 (* 1 = 2.36478 loss)
I1027 11:04:42.837119 15659 solver.cpp:403] Iteration 269000, lr = 0.000823753
I1027 11:08:44.690667 15659 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_270000.caffemodel
I1027 11:08:48.915129 15659 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_270000.solverstate
I1027 11:08:52.522112 15659 solver.cpp:191] Iteration 270000, loss = 2.60192
I1027 11:08:52.522866 15659 solver.cpp:206]     Train net output #0: loss = 2.60192 (* 1 = 2.60192 loss)
I1027 11:08:52.522900 15659 solver.cpp:403] Iteration 270000, lr = 0.000821545
I1027 11:12:53.839476 15659 solver.cpp:191] Iteration 271000, loss = 2.43645
I1027 11:12:53.840257 15659 solver.cpp:206]     Train net output #0: loss = 2.43645 (* 1 = 2.43645 loss)
I1027 11:12:53.840289 15659 solver.cpp:403] Iteration 271000, lr = 0.000819352
I1027 11:16:55.196478 15659 solver.cpp:191] Iteration 272000, loss = 2.25685
I1027 11:16:55.197090 15659 solver.cpp:206]     Train net output #0: loss = 2.25685 (* 1 = 2.25685 loss)
I1027 11:16:55.197123 15659 solver.cpp:403] Iteration 272000, lr = 0.000817171
I1027 11:20:56.536952 15659 solver.cpp:191] Iteration 273000, loss = 2.45675
I1027 11:20:56.537492 15659 solver.cpp:206]     Train net output #0: loss = 2.45675 (* 1 = 2.45675 loss)
I1027 11:20:56.537524 15659 solver.cpp:403] Iteration 273000, lr = 0.000815005
I1027 11:24:57.876657 15659 solver.cpp:191] Iteration 274000, loss = 2.39679
I1027 11:24:57.877367 15659 solver.cpp:206]     Train net output #0: loss = 2.39679 (* 1 = 2.39679 loss)
I1027 11:24:57.877400 15659 solver.cpp:403] Iteration 274000, lr = 0.000812852
I1027 11:28:59.693073 15659 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_275000.caffemodel
I1027 11:29:03.783444 15659 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_275000.solverstate
I1027 11:29:07.631083 15659 solver.cpp:228] Iteration 275000, loss = 2.35591
I1027 11:29:07.631546 15659 solver.cpp:233] Optimization Done.
I1027 11:29:07.631570 15659 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1027 11:52:04.966578 14455 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 11:52:04.966697 14455 net.cpp:358] Input 0 -> data
I1027 11:52:04.966727 14455 net.cpp:67] Creating Layer conv1
I1027 11:52:04.966733 14455 net.cpp:394] conv1 <- data
I1027 11:52:04.966742 14455 net.cpp:356] conv1 -> conv1
I1027 11:52:04.966753 14455 net.cpp:96] Setting up conv1
I1027 11:52:04.967114 14455 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 11:52:04.967135 14455 net.cpp:67] Creating Layer pool1
I1027 11:52:04.967140 14455 net.cpp:394] pool1 <- conv1
I1027 11:52:04.967146 14455 net.cpp:356] pool1 -> pool1
I1027 11:52:04.967154 14455 net.cpp:96] Setting up pool1
I1027 11:52:04.967167 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:52:04.967175 14455 net.cpp:67] Creating Layer relu1
I1027 11:52:04.967180 14455 net.cpp:394] relu1 <- pool1
I1027 11:52:04.967185 14455 net.cpp:345] relu1 -> pool1 (in-place)
I1027 11:52:04.967191 14455 net.cpp:96] Setting up relu1
I1027 11:52:04.967196 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:52:04.967202 14455 net.cpp:67] Creating Layer drop1
I1027 11:52:04.967207 14455 net.cpp:394] drop1 <- pool1
I1027 11:52:04.967216 14455 net.cpp:345] drop1 -> pool1 (in-place)
I1027 11:52:04.967222 14455 net.cpp:96] Setting up drop1
I1027 11:52:04.967227 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:52:04.967236 14455 net.cpp:67] Creating Layer conv2
I1027 11:52:04.967239 14455 net.cpp:394] conv2 <- pool1
I1027 11:52:04.967248 14455 net.cpp:356] conv2 -> conv2
I1027 11:52:04.967255 14455 net.cpp:96] Setting up conv2
I1027 11:52:04.967882 14455 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 11:52:04.967900 14455 net.cpp:67] Creating Layer pool2
I1027 11:52:04.967905 14455 net.cpp:394] pool2 <- conv2
I1027 11:52:04.967912 14455 net.cpp:356] pool2 -> pool2
I1027 11:52:04.967919 14455 net.cpp:96] Setting up pool2
I1027 11:52:04.967926 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:52:04.967931 14455 net.cpp:67] Creating Layer relu2
I1027 11:52:04.967936 14455 net.cpp:394] relu2 <- pool2
I1027 11:52:04.967944 14455 net.cpp:345] relu2 -> pool2 (in-place)
I1027 11:52:04.967950 14455 net.cpp:96] Setting up relu2
I1027 11:52:04.967954 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:52:04.967960 14455 net.cpp:67] Creating Layer drop2
I1027 11:52:04.967965 14455 net.cpp:394] drop2 <- pool2
I1027 11:52:04.967972 14455 net.cpp:345] drop2 -> pool2 (in-place)
I1027 11:52:04.967978 14455 net.cpp:96] Setting up drop2
I1027 11:52:04.967983 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:52:04.967991 14455 net.cpp:67] Creating Layer conv3
I1027 11:52:04.967995 14455 net.cpp:394] conv3 <- pool2
I1027 11:52:04.968004 14455 net.cpp:356] conv3 -> conv3
I1027 11:52:04.968013 14455 net.cpp:96] Setting up conv3
I1027 11:52:04.969583 14455 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 11:52:04.969600 14455 net.cpp:67] Creating Layer pool3
I1027 11:52:04.969605 14455 net.cpp:394] pool3 <- conv3
I1027 11:52:04.969614 14455 net.cpp:356] pool3 -> pool3
I1027 11:52:04.969620 14455 net.cpp:96] Setting up pool3
I1027 11:52:04.969626 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:52:04.969631 14455 net.cpp:67] Creating Layer relu3
I1027 11:52:04.969635 14455 net.cpp:394] relu3 <- pool3
I1027 11:52:04.969642 14455 net.cpp:345] relu3 -> pool3 (in-place)
I1027 11:52:04.969647 14455 net.cpp:96] Setting up relu3
I1027 11:52:04.969652 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:52:04.969657 14455 net.cpp:67] Creating Layer drop3
I1027 11:52:04.969661 14455 net.cpp:394] drop3 <- pool3
I1027 11:52:04.969666 14455 net.cpp:345] drop3 -> pool3 (in-place)
I1027 11:52:04.969671 14455 net.cpp:96] Setting up drop3
I1027 11:52:04.969676 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:52:04.969681 14455 net.cpp:67] Creating Layer ip1
I1027 11:52:04.969686 14455 net.cpp:394] ip1 <- pool3
I1027 11:52:04.969693 14455 net.cpp:356] ip1 -> ip1
I1027 11:52:04.969701 14455 net.cpp:96] Setting up ip1
I1027 11:52:05.522675 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:52:05.522737 14455 net.cpp:67] Creating Layer relu4
I1027 11:52:05.522745 14455 net.cpp:394] relu4 <- ip1
I1027 11:52:05.522753 14455 net.cpp:345] relu4 -> ip1 (in-place)
I1027 11:52:05.522763 14455 net.cpp:96] Setting up relu4
I1027 11:52:05.522768 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:52:05.522775 14455 net.cpp:67] Creating Layer drop4
I1027 11:52:05.522779 14455 net.cpp:394] drop4 <- ip1
I1027 11:52:05.522785 14455 net.cpp:345] drop4 -> ip1 (in-place)
I1027 11:52:05.522791 14455 net.cpp:96] Setting up drop4
I1027 11:52:05.522796 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:52:05.522806 14455 net.cpp:67] Creating Layer ip2
I1027 11:52:05.522810 14455 net.cpp:394] ip2 <- ip1
I1027 11:52:05.522816 14455 net.cpp:356] ip2 -> ip2
I1027 11:52:05.522830 14455 net.cpp:96] Setting up ip2
I1027 11:52:05.532598 14455 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 11:52:05.532673 14455 net.cpp:67] Creating Layer prob
I1027 11:52:05.532680 14455 net.cpp:394] prob <- ip2
I1027 11:52:05.532688 14455 net.cpp:356] prob -> prob
I1027 11:52:05.532698 14455 net.cpp:96] Setting up prob
I1027 11:52:05.532704 14455 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 11:52:05.532709 14455 net.cpp:172] prob does not need backward computation.
I1027 11:52:05.532712 14455 net.cpp:172] ip2 does not need backward computation.
I1027 11:52:05.532716 14455 net.cpp:172] drop4 does not need backward computation.
I1027 11:52:05.532721 14455 net.cpp:172] relu4 does not need backward computation.
I1027 11:52:05.532723 14455 net.cpp:172] ip1 does not need backward computation.
I1027 11:52:05.532727 14455 net.cpp:172] drop3 does not need backward computation.
I1027 11:52:05.532730 14455 net.cpp:172] relu3 does not need backward computation.
I1027 11:52:05.532734 14455 net.cpp:172] pool3 does not need backward computation.
I1027 11:52:05.532737 14455 net.cpp:172] conv3 does not need backward computation.
I1027 11:52:05.532742 14455 net.cpp:172] drop2 does not need backward computation.
I1027 11:52:05.532744 14455 net.cpp:172] relu2 does not need backward computation.
I1027 11:52:05.532748 14455 net.cpp:172] pool2 does not need backward computation.
I1027 11:52:05.532752 14455 net.cpp:172] conv2 does not need backward computation.
I1027 11:52:05.532755 14455 net.cpp:172] drop1 does not need backward computation.
I1027 11:52:05.532759 14455 net.cpp:172] relu1 does not need backward computation.
I1027 11:52:05.532763 14455 net.cpp:172] pool1 does not need backward computation.
I1027 11:52:05.532766 14455 net.cpp:172] conv1 does not need backward computation.
I1027 11:52:05.532769 14455 net.cpp:208] This network produces output prob
I1027 11:52:05.532788 14455 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 11:52:05.532798 14455 net.cpp:219] Network initialization done.
I1027 11:52:05.532801 14455 net.cpp:220] Memory required for data: 1837200
I1027 11:53:16.748937 14455 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 11:53:16.749517 14455 net.cpp:358] Input 0 -> data
I1027 11:53:16.749573 14455 net.cpp:67] Creating Layer conv1
I1027 11:53:16.749588 14455 net.cpp:394] conv1 <- data
I1027 11:53:16.749608 14455 net.cpp:356] conv1 -> conv1
I1027 11:53:16.749631 14455 net.cpp:96] Setting up conv1
I1027 11:53:16.749696 14455 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 11:53:16.749732 14455 net.cpp:67] Creating Layer pool1
I1027 11:53:16.749747 14455 net.cpp:394] pool1 <- conv1
I1027 11:53:16.749763 14455 net.cpp:356] pool1 -> pool1
I1027 11:53:16.749781 14455 net.cpp:96] Setting up pool1
I1027 11:53:16.749799 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:53:16.749819 14455 net.cpp:67] Creating Layer relu1
I1027 11:53:16.749830 14455 net.cpp:394] relu1 <- pool1
I1027 11:53:16.749845 14455 net.cpp:345] relu1 -> pool1 (in-place)
I1027 11:53:16.749861 14455 net.cpp:96] Setting up relu1
I1027 11:53:16.749873 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:53:16.749888 14455 net.cpp:67] Creating Layer drop1
I1027 11:53:16.749900 14455 net.cpp:394] drop1 <- pool1
I1027 11:53:16.749928 14455 net.cpp:345] drop1 -> pool1 (in-place)
I1027 11:53:16.749946 14455 net.cpp:96] Setting up drop1
I1027 11:53:16.749959 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:53:16.749979 14455 net.cpp:67] Creating Layer conv2
I1027 11:53:16.749991 14455 net.cpp:394] conv2 <- pool1
I1027 11:53:16.750008 14455 net.cpp:356] conv2 -> conv2
I1027 11:53:16.750027 14455 net.cpp:96] Setting up conv2
I1027 11:53:16.751401 14455 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 11:53:16.751436 14455 net.cpp:67] Creating Layer pool2
I1027 11:53:16.751449 14455 net.cpp:394] pool2 <- conv2
I1027 11:53:16.751466 14455 net.cpp:356] pool2 -> pool2
I1027 11:53:16.751484 14455 net.cpp:96] Setting up pool2
I1027 11:53:16.751502 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:53:16.751516 14455 net.cpp:67] Creating Layer relu2
I1027 11:53:16.751528 14455 net.cpp:394] relu2 <- pool2
I1027 11:53:16.751543 14455 net.cpp:345] relu2 -> pool2 (in-place)
I1027 11:53:16.751559 14455 net.cpp:96] Setting up relu2
I1027 11:53:16.751570 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:53:16.751586 14455 net.cpp:67] Creating Layer drop2
I1027 11:53:16.751597 14455 net.cpp:394] drop2 <- pool2
I1027 11:53:16.751612 14455 net.cpp:345] drop2 -> pool2 (in-place)
I1027 11:53:16.751628 14455 net.cpp:96] Setting up drop2
I1027 11:53:16.751641 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:53:16.751660 14455 net.cpp:67] Creating Layer conv3
I1027 11:53:16.751672 14455 net.cpp:394] conv3 <- pool2
I1027 11:53:16.751689 14455 net.cpp:356] conv3 -> conv3
I1027 11:53:16.751708 14455 net.cpp:96] Setting up conv3
I1027 11:53:16.755372 14455 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 11:53:16.755412 14455 net.cpp:67] Creating Layer pool3
I1027 11:53:16.755426 14455 net.cpp:394] pool3 <- conv3
I1027 11:53:16.755444 14455 net.cpp:356] pool3 -> pool3
I1027 11:53:16.755461 14455 net.cpp:96] Setting up pool3
I1027 11:53:16.755476 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:53:16.755492 14455 net.cpp:67] Creating Layer relu3
I1027 11:53:16.755503 14455 net.cpp:394] relu3 <- pool3
I1027 11:53:16.755518 14455 net.cpp:345] relu3 -> pool3 (in-place)
I1027 11:53:16.755534 14455 net.cpp:96] Setting up relu3
I1027 11:53:16.755547 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:53:16.755561 14455 net.cpp:67] Creating Layer drop3
I1027 11:53:16.755573 14455 net.cpp:394] drop3 <- pool3
I1027 11:53:16.755587 14455 net.cpp:345] drop3 -> pool3 (in-place)
I1027 11:53:16.755604 14455 net.cpp:96] Setting up drop3
I1027 11:53:16.755616 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:53:16.755633 14455 net.cpp:67] Creating Layer ip1
I1027 11:53:16.755645 14455 net.cpp:394] ip1 <- pool3
I1027 11:53:16.755661 14455 net.cpp:356] ip1 -> ip1
I1027 11:53:16.755681 14455 net.cpp:96] Setting up ip1
I1027 11:53:17.202299 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:53:17.202358 14455 net.cpp:67] Creating Layer relu4
I1027 11:53:17.202365 14455 net.cpp:394] relu4 <- ip1
I1027 11:53:17.202376 14455 net.cpp:345] relu4 -> ip1 (in-place)
I1027 11:53:17.202386 14455 net.cpp:96] Setting up relu4
I1027 11:53:17.202391 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:53:17.202399 14455 net.cpp:67] Creating Layer drop4
I1027 11:53:17.202404 14455 net.cpp:394] drop4 <- ip1
I1027 11:53:17.202410 14455 net.cpp:345] drop4 -> ip1 (in-place)
I1027 11:53:17.202417 14455 net.cpp:96] Setting up drop4
I1027 11:53:17.202424 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:53:17.202433 14455 net.cpp:67] Creating Layer ip2
I1027 11:53:17.202437 14455 net.cpp:394] ip2 <- ip1
I1027 11:53:17.202445 14455 net.cpp:356] ip2 -> ip2
I1027 11:53:17.202457 14455 net.cpp:96] Setting up ip2
I1027 11:53:17.210067 14455 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 11:53:17.210129 14455 net.cpp:67] Creating Layer prob
I1027 11:53:17.210135 14455 net.cpp:394] prob <- ip2
I1027 11:53:17.210144 14455 net.cpp:356] prob -> prob
I1027 11:53:17.210155 14455 net.cpp:96] Setting up prob
I1027 11:53:17.210173 14455 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 11:53:17.210178 14455 net.cpp:172] prob does not need backward computation.
I1027 11:53:17.210183 14455 net.cpp:172] ip2 does not need backward computation.
I1027 11:53:17.210186 14455 net.cpp:172] drop4 does not need backward computation.
I1027 11:53:17.210191 14455 net.cpp:172] relu4 does not need backward computation.
I1027 11:53:17.210194 14455 net.cpp:172] ip1 does not need backward computation.
I1027 11:53:17.210198 14455 net.cpp:172] drop3 does not need backward computation.
I1027 11:53:17.210202 14455 net.cpp:172] relu3 does not need backward computation.
I1027 11:53:17.210206 14455 net.cpp:172] pool3 does not need backward computation.
I1027 11:53:17.210211 14455 net.cpp:172] conv3 does not need backward computation.
I1027 11:53:17.210214 14455 net.cpp:172] drop2 does not need backward computation.
I1027 11:53:17.210217 14455 net.cpp:172] relu2 does not need backward computation.
I1027 11:53:17.210222 14455 net.cpp:172] pool2 does not need backward computation.
I1027 11:53:17.210225 14455 net.cpp:172] conv2 does not need backward computation.
I1027 11:53:17.210228 14455 net.cpp:172] drop1 does not need backward computation.
I1027 11:53:17.210232 14455 net.cpp:172] relu1 does not need backward computation.
I1027 11:53:17.210237 14455 net.cpp:172] pool1 does not need backward computation.
I1027 11:53:17.210240 14455 net.cpp:172] conv1 does not need backward computation.
I1027 11:53:17.210244 14455 net.cpp:208] This network produces output prob
I1027 11:53:17.210260 14455 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 11:53:17.210270 14455 net.cpp:219] Network initialization done.
I1027 11:53:17.210274 14455 net.cpp:220] Memory required for data: 1837200
I1027 11:54:16.286769 14455 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 11:54:16.287369 14455 net.cpp:358] Input 0 -> data
I1027 11:54:16.287418 14455 net.cpp:67] Creating Layer conv1
I1027 11:54:16.287431 14455 net.cpp:394] conv1 <- data
I1027 11:54:16.287446 14455 net.cpp:356] conv1 -> conv1
I1027 11:54:16.287467 14455 net.cpp:96] Setting up conv1
I1027 11:54:16.287521 14455 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 11:54:16.287551 14455 net.cpp:67] Creating Layer pool1
I1027 11:54:16.287562 14455 net.cpp:394] pool1 <- conv1
I1027 11:54:16.287575 14455 net.cpp:356] pool1 -> pool1
I1027 11:54:16.287591 14455 net.cpp:96] Setting up pool1
I1027 11:54:16.287606 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:54:16.287621 14455 net.cpp:67] Creating Layer relu1
I1027 11:54:16.287631 14455 net.cpp:394] relu1 <- pool1
I1027 11:54:16.287643 14455 net.cpp:345] relu1 -> pool1 (in-place)
I1027 11:54:16.287657 14455 net.cpp:96] Setting up relu1
I1027 11:54:16.287667 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:54:16.287679 14455 net.cpp:67] Creating Layer drop1
I1027 11:54:16.287688 14455 net.cpp:394] drop1 <- pool1
I1027 11:54:16.287700 14455 net.cpp:345] drop1 -> pool1 (in-place)
I1027 11:54:16.287714 14455 net.cpp:96] Setting up drop1
I1027 11:54:16.287724 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:54:16.287740 14455 net.cpp:67] Creating Layer conv2
I1027 11:54:16.287750 14455 net.cpp:394] conv2 <- pool1
I1027 11:54:16.287763 14455 net.cpp:356] conv2 -> conv2
I1027 11:54:16.287778 14455 net.cpp:96] Setting up conv2
I1027 11:54:16.288966 14455 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 11:54:16.289000 14455 net.cpp:67] Creating Layer pool2
I1027 11:54:16.289011 14455 net.cpp:394] pool2 <- conv2
I1027 11:54:16.289026 14455 net.cpp:356] pool2 -> pool2
I1027 11:54:16.289042 14455 net.cpp:96] Setting up pool2
I1027 11:54:16.289057 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:54:16.289069 14455 net.cpp:67] Creating Layer relu2
I1027 11:54:16.289078 14455 net.cpp:394] relu2 <- pool2
I1027 11:54:16.289091 14455 net.cpp:345] relu2 -> pool2 (in-place)
I1027 11:54:16.289103 14455 net.cpp:96] Setting up relu2
I1027 11:54:16.289113 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:54:16.289135 14455 net.cpp:67] Creating Layer drop2
I1027 11:54:16.289146 14455 net.cpp:394] drop2 <- pool2
I1027 11:54:16.289161 14455 net.cpp:345] drop2 -> pool2 (in-place)
I1027 11:54:16.289177 14455 net.cpp:96] Setting up drop2
I1027 11:54:16.289191 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:54:16.289211 14455 net.cpp:67] Creating Layer conv3
I1027 11:54:16.289222 14455 net.cpp:394] conv3 <- pool2
I1027 11:54:16.289239 14455 net.cpp:356] conv3 -> conv3
I1027 11:54:16.289258 14455 net.cpp:96] Setting up conv3
I1027 11:54:16.292784 14455 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 11:54:16.292807 14455 net.cpp:67] Creating Layer pool3
I1027 11:54:16.292814 14455 net.cpp:394] pool3 <- conv3
I1027 11:54:16.292822 14455 net.cpp:356] pool3 -> pool3
I1027 11:54:16.292832 14455 net.cpp:96] Setting up pool3
I1027 11:54:16.292840 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:54:16.292846 14455 net.cpp:67] Creating Layer relu3
I1027 11:54:16.292852 14455 net.cpp:394] relu3 <- pool3
I1027 11:54:16.292860 14455 net.cpp:345] relu3 -> pool3 (in-place)
I1027 11:54:16.292866 14455 net.cpp:96] Setting up relu3
I1027 11:54:16.292872 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:54:16.292879 14455 net.cpp:67] Creating Layer drop3
I1027 11:54:16.292886 14455 net.cpp:394] drop3 <- pool3
I1027 11:54:16.292892 14455 net.cpp:345] drop3 -> pool3 (in-place)
I1027 11:54:16.292906 14455 net.cpp:96] Setting up drop3
I1027 11:54:16.292912 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:54:16.292922 14455 net.cpp:67] Creating Layer ip1
I1027 11:54:16.292927 14455 net.cpp:394] ip1 <- pool3
I1027 11:54:16.292935 14455 net.cpp:356] ip1 -> ip1
I1027 11:54:16.292945 14455 net.cpp:96] Setting up ip1
I1027 11:54:16.702992 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:54:16.703058 14455 net.cpp:67] Creating Layer relu4
I1027 11:54:16.703064 14455 net.cpp:394] relu4 <- ip1
I1027 11:54:16.703075 14455 net.cpp:345] relu4 -> ip1 (in-place)
I1027 11:54:16.703085 14455 net.cpp:96] Setting up relu4
I1027 11:54:16.703090 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:54:16.703099 14455 net.cpp:67] Creating Layer drop4
I1027 11:54:16.703104 14455 net.cpp:394] drop4 <- ip1
I1027 11:54:16.703110 14455 net.cpp:345] drop4 -> ip1 (in-place)
I1027 11:54:16.703117 14455 net.cpp:96] Setting up drop4
I1027 11:54:16.703124 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:54:16.703135 14455 net.cpp:67] Creating Layer ip2
I1027 11:54:16.703138 14455 net.cpp:394] ip2 <- ip1
I1027 11:54:16.703146 14455 net.cpp:356] ip2 -> ip2
I1027 11:54:16.703161 14455 net.cpp:96] Setting up ip2
I1027 11:54:16.710749 14455 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 11:54:16.710813 14455 net.cpp:67] Creating Layer prob
I1027 11:54:16.710820 14455 net.cpp:394] prob <- ip2
I1027 11:54:16.710830 14455 net.cpp:356] prob -> prob
I1027 11:54:16.710840 14455 net.cpp:96] Setting up prob
I1027 11:54:16.710850 14455 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 11:54:16.710855 14455 net.cpp:172] prob does not need backward computation.
I1027 11:54:16.710858 14455 net.cpp:172] ip2 does not need backward computation.
I1027 11:54:16.710862 14455 net.cpp:172] drop4 does not need backward computation.
I1027 11:54:16.710866 14455 net.cpp:172] relu4 does not need backward computation.
I1027 11:54:16.710870 14455 net.cpp:172] ip1 does not need backward computation.
I1027 11:54:16.710875 14455 net.cpp:172] drop3 does not need backward computation.
I1027 11:54:16.710878 14455 net.cpp:172] relu3 does not need backward computation.
I1027 11:54:16.710882 14455 net.cpp:172] pool3 does not need backward computation.
I1027 11:54:16.710886 14455 net.cpp:172] conv3 does not need backward computation.
I1027 11:54:16.710891 14455 net.cpp:172] drop2 does not need backward computation.
I1027 11:54:16.710894 14455 net.cpp:172] relu2 does not need backward computation.
I1027 11:54:16.710898 14455 net.cpp:172] pool2 does not need backward computation.
I1027 11:54:16.710902 14455 net.cpp:172] conv2 does not need backward computation.
I1027 11:54:16.710906 14455 net.cpp:172] drop1 does not need backward computation.
I1027 11:54:16.710911 14455 net.cpp:172] relu1 does not need backward computation.
I1027 11:54:16.710914 14455 net.cpp:172] pool1 does not need backward computation.
I1027 11:54:16.710918 14455 net.cpp:172] conv1 does not need backward computation.
I1027 11:54:16.710922 14455 net.cpp:208] This network produces output prob
I1027 11:54:16.710937 14455 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 11:54:16.710945 14455 net.cpp:219] Network initialization done.
I1027 11:54:16.710950 14455 net.cpp:220] Memory required for data: 1837200
I1027 11:55:15.294944 14455 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 11:55:15.295502 14455 net.cpp:358] Input 0 -> data
I1027 11:55:15.295558 14455 net.cpp:67] Creating Layer conv1
I1027 11:55:15.295573 14455 net.cpp:394] conv1 <- data
I1027 11:55:15.295593 14455 net.cpp:356] conv1 -> conv1
I1027 11:55:15.295616 14455 net.cpp:96] Setting up conv1
I1027 11:55:15.295682 14455 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 11:55:15.295718 14455 net.cpp:67] Creating Layer pool1
I1027 11:55:15.295732 14455 net.cpp:394] pool1 <- conv1
I1027 11:55:15.295748 14455 net.cpp:356] pool1 -> pool1
I1027 11:55:15.295768 14455 net.cpp:96] Setting up pool1
I1027 11:55:15.295785 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:55:15.295804 14455 net.cpp:67] Creating Layer relu1
I1027 11:55:15.295815 14455 net.cpp:394] relu1 <- pool1
I1027 11:55:15.295830 14455 net.cpp:345] relu1 -> pool1 (in-place)
I1027 11:55:15.295846 14455 net.cpp:96] Setting up relu1
I1027 11:55:15.295860 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:55:15.295876 14455 net.cpp:67] Creating Layer drop1
I1027 11:55:15.295886 14455 net.cpp:394] drop1 <- pool1
I1027 11:55:15.295902 14455 net.cpp:345] drop1 -> pool1 (in-place)
I1027 11:55:15.295918 14455 net.cpp:96] Setting up drop1
I1027 11:55:15.295933 14455 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 11:55:15.295951 14455 net.cpp:67] Creating Layer conv2
I1027 11:55:15.295964 14455 net.cpp:394] conv2 <- pool1
I1027 11:55:15.295980 14455 net.cpp:356] conv2 -> conv2
I1027 11:55:15.296000 14455 net.cpp:96] Setting up conv2
I1027 11:55:15.297456 14455 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 11:55:15.297495 14455 net.cpp:67] Creating Layer pool2
I1027 11:55:15.297509 14455 net.cpp:394] pool2 <- conv2
I1027 11:55:15.297525 14455 net.cpp:356] pool2 -> pool2
I1027 11:55:15.297545 14455 net.cpp:96] Setting up pool2
I1027 11:55:15.297562 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:55:15.297577 14455 net.cpp:67] Creating Layer relu2
I1027 11:55:15.297590 14455 net.cpp:394] relu2 <- pool2
I1027 11:55:15.297605 14455 net.cpp:345] relu2 -> pool2 (in-place)
I1027 11:55:15.297627 14455 net.cpp:96] Setting up relu2
I1027 11:55:15.297641 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:55:15.297655 14455 net.cpp:67] Creating Layer drop2
I1027 11:55:15.297667 14455 net.cpp:394] drop2 <- pool2
I1027 11:55:15.297683 14455 net.cpp:345] drop2 -> pool2 (in-place)
I1027 11:55:15.297698 14455 net.cpp:96] Setting up drop2
I1027 11:55:15.297711 14455 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 11:55:15.297730 14455 net.cpp:67] Creating Layer conv3
I1027 11:55:15.297742 14455 net.cpp:394] conv3 <- pool2
I1027 11:55:15.297760 14455 net.cpp:356] conv3 -> conv3
I1027 11:55:15.297778 14455 net.cpp:96] Setting up conv3
I1027 11:55:15.300865 14455 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 11:55:15.300884 14455 net.cpp:67] Creating Layer pool3
I1027 11:55:15.300889 14455 net.cpp:394] pool3 <- conv3
I1027 11:55:15.300894 14455 net.cpp:356] pool3 -> pool3
I1027 11:55:15.300900 14455 net.cpp:96] Setting up pool3
I1027 11:55:15.300906 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:55:15.300912 14455 net.cpp:67] Creating Layer relu3
I1027 11:55:15.300916 14455 net.cpp:394] relu3 <- pool3
I1027 11:55:15.300921 14455 net.cpp:345] relu3 -> pool3 (in-place)
I1027 11:55:15.300926 14455 net.cpp:96] Setting up relu3
I1027 11:55:15.300931 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:55:15.300936 14455 net.cpp:67] Creating Layer drop3
I1027 11:55:15.300940 14455 net.cpp:394] drop3 <- pool3
I1027 11:55:15.300946 14455 net.cpp:345] drop3 -> pool3 (in-place)
I1027 11:55:15.300952 14455 net.cpp:96] Setting up drop3
I1027 11:55:15.300956 14455 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 11:55:15.300963 14455 net.cpp:67] Creating Layer ip1
I1027 11:55:15.300967 14455 net.cpp:394] ip1 <- pool3
I1027 11:55:15.300973 14455 net.cpp:356] ip1 -> ip1
I1027 11:55:15.300981 14455 net.cpp:96] Setting up ip1
I1027 11:55:15.716226 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:55:15.716289 14455 net.cpp:67] Creating Layer relu4
I1027 11:55:15.716297 14455 net.cpp:394] relu4 <- ip1
I1027 11:55:15.716307 14455 net.cpp:345] relu4 -> ip1 (in-place)
I1027 11:55:15.716318 14455 net.cpp:96] Setting up relu4
I1027 11:55:15.716323 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:55:15.716332 14455 net.cpp:67] Creating Layer drop4
I1027 11:55:15.716336 14455 net.cpp:394] drop4 <- ip1
I1027 11:55:15.716343 14455 net.cpp:345] drop4 -> ip1 (in-place)
I1027 11:55:15.716351 14455 net.cpp:96] Setting up drop4
I1027 11:55:15.716356 14455 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 11:55:15.716367 14455 net.cpp:67] Creating Layer ip2
I1027 11:55:15.716372 14455 net.cpp:394] ip2 <- ip1
I1027 11:55:15.716380 14455 net.cpp:356] ip2 -> ip2
I1027 11:55:15.716395 14455 net.cpp:96] Setting up ip2
I1027 11:55:15.723917 14455 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 11:55:15.723978 14455 net.cpp:67] Creating Layer prob
I1027 11:55:15.723985 14455 net.cpp:394] prob <- ip2
I1027 11:55:15.723994 14455 net.cpp:356] prob -> prob
I1027 11:55:15.724004 14455 net.cpp:96] Setting up prob
I1027 11:55:15.724012 14455 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 11:55:15.724017 14455 net.cpp:172] prob does not need backward computation.
I1027 11:55:15.724022 14455 net.cpp:172] ip2 does not need backward computation.
I1027 11:55:15.724026 14455 net.cpp:172] drop4 does not need backward computation.
I1027 11:55:15.724030 14455 net.cpp:172] relu4 does not need backward computation.
I1027 11:55:15.724035 14455 net.cpp:172] ip1 does not need backward computation.
I1027 11:55:15.724037 14455 net.cpp:172] drop3 does not need backward computation.
I1027 11:55:15.724041 14455 net.cpp:172] relu3 does not need backward computation.
I1027 11:55:15.724045 14455 net.cpp:172] pool3 does not need backward computation.
I1027 11:55:15.724050 14455 net.cpp:172] conv3 does not need backward computation.
I1027 11:55:15.724053 14455 net.cpp:172] drop2 does not need backward computation.
I1027 11:55:15.724057 14455 net.cpp:172] relu2 does not need backward computation.
I1027 11:55:15.724061 14455 net.cpp:172] pool2 does not need backward computation.
I1027 11:55:15.724076 14455 net.cpp:172] conv2 does not need backward computation.
I1027 11:55:15.724081 14455 net.cpp:172] drop1 does not need backward computation.
I1027 11:55:15.724084 14455 net.cpp:172] relu1 does not need backward computation.
I1027 11:55:15.724088 14455 net.cpp:172] pool1 does not need backward computation.
I1027 11:55:15.724092 14455 net.cpp:172] conv1 does not need backward computation.
I1027 11:55:15.724095 14455 net.cpp:208] This network produces output prob
I1027 11:55:15.724112 14455 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 11:55:15.724122 14455 net.cpp:219] Network initialization done.
I1027 11:55:15.724125 14455 net.cpp:220] Memory required for data: 1837200
I1027 12:48:37.864285 10267 convert_imageset.cpp:70] Shuffling data
I1027 12:48:38.539978 10267 convert_imageset.cpp:73] A total of 60000 images.
I1027 12:48:38.540061 10267 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1027 12:48:40.572408 10267 convert_imageset.cpp:177] Processed 1000 files.
E1027 12:48:42.538262 10267 convert_imageset.cpp:177] Processed 2000 files.
E1027 12:48:44.467617 10267 convert_imageset.cpp:177] Processed 3000 files.
E1027 12:48:46.528344 10267 convert_imageset.cpp:177] Processed 4000 files.
E1027 12:48:48.363708 10267 convert_imageset.cpp:177] Processed 5000 files.
E1027 12:48:50.205176 10267 convert_imageset.cpp:177] Processed 6000 files.
E1027 12:48:51.901557 10267 convert_imageset.cpp:177] Processed 7000 files.
E1027 12:48:53.823360 10267 convert_imageset.cpp:177] Processed 8000 files.
E1027 12:48:55.670303 10267 convert_imageset.cpp:177] Processed 9000 files.
E1027 12:48:57.448135 10267 convert_imageset.cpp:177] Processed 10000 files.
E1027 12:48:59.131764 10267 convert_imageset.cpp:177] Processed 11000 files.
E1027 12:49:01.039134 10267 convert_imageset.cpp:177] Processed 12000 files.
E1027 12:49:02.768614 10267 convert_imageset.cpp:177] Processed 13000 files.
E1027 12:49:04.494411 10267 convert_imageset.cpp:177] Processed 14000 files.
E1027 12:49:06.132649 10267 convert_imageset.cpp:177] Processed 15000 files.
E1027 12:49:07.877876 10267 convert_imageset.cpp:177] Processed 16000 files.
E1027 12:49:09.592918 10267 convert_imageset.cpp:177] Processed 17000 files.
E1027 12:49:11.130272 10267 convert_imageset.cpp:177] Processed 18000 files.
E1027 12:49:12.800137 10267 convert_imageset.cpp:177] Processed 19000 files.
E1027 12:49:14.688719 10267 convert_imageset.cpp:177] Processed 20000 files.
E1027 12:49:16.336472 10267 convert_imageset.cpp:177] Processed 21000 files.
E1027 12:49:18.008047 10267 convert_imageset.cpp:177] Processed 22000 files.
E1027 12:49:19.670121 10267 convert_imageset.cpp:177] Processed 23000 files.
E1027 12:49:21.156698 10267 convert_imageset.cpp:177] Processed 24000 files.
E1027 12:49:22.794961 10267 convert_imageset.cpp:177] Processed 25000 files.
E1027 12:49:24.477596 10267 convert_imageset.cpp:177] Processed 26000 files.
E1027 12:49:26.012533 10267 convert_imageset.cpp:177] Processed 27000 files.
E1027 12:49:27.610054 10267 convert_imageset.cpp:177] Processed 28000 files.
E1027 12:49:29.082799 10267 convert_imageset.cpp:177] Processed 29000 files.
E1027 12:49:30.802233 10267 convert_imageset.cpp:177] Processed 30000 files.
E1027 12:49:32.298112 10267 convert_imageset.cpp:177] Processed 31000 files.
E1027 12:49:33.911176 10267 convert_imageset.cpp:177] Processed 32000 files.
E1027 12:49:35.436033 10267 convert_imageset.cpp:177] Processed 33000 files.
E1027 12:49:37.061334 10267 convert_imageset.cpp:177] Processed 34000 files.
E1027 12:49:38.541465 10267 convert_imageset.cpp:177] Processed 35000 files.
E1027 12:49:40.135269 10267 convert_imageset.cpp:177] Processed 36000 files.
E1027 12:49:41.723486 10267 convert_imageset.cpp:177] Processed 37000 files.
E1027 12:49:43.290261 10267 convert_imageset.cpp:177] Processed 38000 files.
E1027 12:49:45.000988 10267 convert_imageset.cpp:177] Processed 39000 files.
E1027 12:49:46.633904 10267 convert_imageset.cpp:177] Processed 40000 files.
E1027 12:49:48.250861 10267 convert_imageset.cpp:177] Processed 41000 files.
E1027 12:49:49.787658 10267 convert_imageset.cpp:177] Processed 42000 files.
E1027 12:49:51.176522 10267 convert_imageset.cpp:177] Processed 43000 files.
E1027 12:49:52.687515 10267 convert_imageset.cpp:177] Processed 44000 files.
E1027 12:49:54.154714 10267 convert_imageset.cpp:177] Processed 45000 files.
E1027 12:49:55.610462 10267 convert_imageset.cpp:177] Processed 46000 files.
E1027 12:49:57.164753 10267 convert_imageset.cpp:177] Processed 47000 files.
E1027 12:49:58.722100 10267 convert_imageset.cpp:177] Processed 48000 files.
E1027 12:50:00.347146 10267 convert_imageset.cpp:177] Processed 49000 files.
E1027 12:50:01.903650 10267 convert_imageset.cpp:177] Processed 50000 files.
E1027 12:50:03.526418 10267 convert_imageset.cpp:177] Processed 51000 files.
E1027 12:50:05.163053 10267 convert_imageset.cpp:177] Processed 52000 files.
E1027 12:50:06.775115 10267 convert_imageset.cpp:177] Processed 53000 files.
E1027 12:50:08.280086 10267 convert_imageset.cpp:177] Processed 54000 files.
E1027 12:50:09.829164 10267 convert_imageset.cpp:177] Processed 55000 files.
E1027 12:50:11.304473 10267 convert_imageset.cpp:177] Processed 56000 files.
E1027 12:50:12.747903 10267 convert_imageset.cpp:177] Processed 57000 files.
E1027 12:50:14.306635 10267 convert_imageset.cpp:177] Processed 58000 files.
E1027 12:50:15.733675 10267 convert_imageset.cpp:177] Processed 59000 files.
E1027 12:50:17.270865 10267 convert_imageset.cpp:177] Processed 60000 files.
I1027 12:50:17.642256 10983 caffe.cpp:99] Use GPU with device ID 0
I1027 12:50:18.097215 10983 caffe.cpp:107] Starting Optimization
I1027 12:50:18.097328 10983 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 290000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1027 12:50:18.097352 10983 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1027 12:50:18.106969 10983 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1027 12:50:18.107193 10983 net.cpp:67] Creating Layer mnist
I1027 12:50:18.107219 10983 net.cpp:356] mnist -> data
I1027 12:50:18.107254 10983 net.cpp:356] mnist -> label
I1027 12:50:18.107286 10983 net.cpp:96] Setting up mnist
I1027 12:50:18.114835 10983 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1027 12:50:18.114959 10983 data_layer.cpp:128] output data size: 64,1,50,180
I1027 12:50:18.116727 10983 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1027 12:50:18.116766 10983 net.cpp:103] Top shape: 64 1 1 1 (64)
I1027 12:50:18.116794 10983 net.cpp:67] Creating Layer conv1
I1027 12:50:18.116807 10983 net.cpp:394] conv1 <- data
I1027 12:50:18.116840 10983 net.cpp:356] conv1 -> conv1
I1027 12:50:18.116866 10983 net.cpp:96] Setting up conv1
I1027 12:50:18.117226 10983 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1027 12:50:18.117257 10983 net.cpp:67] Creating Layer pool1
I1027 12:50:18.117264 10983 net.cpp:394] pool1 <- conv1
I1027 12:50:18.117274 10983 net.cpp:356] pool1 -> pool1
I1027 12:50:18.117282 10983 net.cpp:96] Setting up pool1
I1027 12:50:18.117298 10983 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 12:50:18.117305 10983 net.cpp:67] Creating Layer relu1
I1027 12:50:18.117311 10983 net.cpp:394] relu1 <- pool1
I1027 12:50:18.117316 10983 net.cpp:345] relu1 -> pool1 (in-place)
I1027 12:50:18.117322 10983 net.cpp:96] Setting up relu1
I1027 12:50:18.117326 10983 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 12:50:18.117336 10983 net.cpp:67] Creating Layer drop1
I1027 12:50:18.117341 10983 net.cpp:394] drop1 <- pool1
I1027 12:50:18.117347 10983 net.cpp:345] drop1 -> pool1 (in-place)
I1027 12:50:18.117353 10983 net.cpp:96] Setting up drop1
I1027 12:50:18.117358 10983 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 12:50:18.117365 10983 net.cpp:67] Creating Layer conv2
I1027 12:50:18.117369 10983 net.cpp:394] conv2 <- pool1
I1027 12:50:18.117377 10983 net.cpp:356] conv2 -> conv2
I1027 12:50:18.117385 10983 net.cpp:96] Setting up conv2
I1027 12:50:18.117979 10983 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1027 12:50:18.117996 10983 net.cpp:67] Creating Layer pool2
I1027 12:50:18.118001 10983 net.cpp:394] pool2 <- conv2
I1027 12:50:18.118007 10983 net.cpp:356] pool2 -> pool2
I1027 12:50:18.118015 10983 net.cpp:96] Setting up pool2
I1027 12:50:18.118021 10983 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 12:50:18.118028 10983 net.cpp:67] Creating Layer relu2
I1027 12:50:18.118033 10983 net.cpp:394] relu2 <- pool2
I1027 12:50:18.118039 10983 net.cpp:345] relu2 -> pool2 (in-place)
I1027 12:50:18.118046 10983 net.cpp:96] Setting up relu2
I1027 12:50:18.118049 10983 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 12:50:18.118057 10983 net.cpp:67] Creating Layer drop2
I1027 12:50:18.118067 10983 net.cpp:394] drop2 <- pool2
I1027 12:50:18.118075 10983 net.cpp:345] drop2 -> pool2 (in-place)
I1027 12:50:18.118083 10983 net.cpp:96] Setting up drop2
I1027 12:50:18.118088 10983 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 12:50:18.118094 10983 net.cpp:67] Creating Layer conv3
I1027 12:50:18.118098 10983 net.cpp:394] conv3 <- pool2
I1027 12:50:18.118106 10983 net.cpp:356] conv3 -> conv3
I1027 12:50:18.118114 10983 net.cpp:96] Setting up conv3
I1027 12:50:18.120769 10983 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1027 12:50:18.120796 10983 net.cpp:67] Creating Layer pool3
I1027 12:50:18.120802 10983 net.cpp:394] pool3 <- conv3
I1027 12:50:18.120810 10983 net.cpp:356] pool3 -> pool3
I1027 12:50:18.120817 10983 net.cpp:96] Setting up pool3
I1027 12:50:18.120823 10983 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 12:50:18.120829 10983 net.cpp:67] Creating Layer relu3
I1027 12:50:18.120836 10983 net.cpp:394] relu3 <- pool3
I1027 12:50:18.120841 10983 net.cpp:345] relu3 -> pool3 (in-place)
I1027 12:50:18.120847 10983 net.cpp:96] Setting up relu3
I1027 12:50:18.120851 10983 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 12:50:18.120858 10983 net.cpp:67] Creating Layer drop3
I1027 12:50:18.120862 10983 net.cpp:394] drop3 <- pool3
I1027 12:50:18.120868 10983 net.cpp:345] drop3 -> pool3 (in-place)
I1027 12:50:18.120874 10983 net.cpp:96] Setting up drop3
I1027 12:50:18.120878 10983 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 12:50:18.120888 10983 net.cpp:67] Creating Layer ip1
I1027 12:50:18.120893 10983 net.cpp:394] ip1 <- pool3
I1027 12:50:18.120898 10983 net.cpp:356] ip1 -> ip1
I1027 12:50:18.120934 10983 net.cpp:96] Setting up ip1
I1027 12:50:18.632764 10983 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 12:50:18.632822 10983 net.cpp:67] Creating Layer relu4
I1027 12:50:18.632829 10983 net.cpp:394] relu4 <- ip1
I1027 12:50:18.632838 10983 net.cpp:345] relu4 -> ip1 (in-place)
I1027 12:50:18.632848 10983 net.cpp:96] Setting up relu4
I1027 12:50:18.632853 10983 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 12:50:18.632859 10983 net.cpp:67] Creating Layer drop4
I1027 12:50:18.632863 10983 net.cpp:394] drop4 <- ip1
I1027 12:50:18.632871 10983 net.cpp:345] drop4 -> ip1 (in-place)
I1027 12:50:18.632879 10983 net.cpp:96] Setting up drop4
I1027 12:50:18.632884 10983 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 12:50:18.632894 10983 net.cpp:67] Creating Layer ip2
I1027 12:50:18.632897 10983 net.cpp:394] ip2 <- ip1
I1027 12:50:18.632905 10983 net.cpp:356] ip2 -> ip2
I1027 12:50:18.632913 10983 net.cpp:96] Setting up ip2
I1027 12:50:18.641021 10983 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1027 12:50:18.641067 10983 net.cpp:67] Creating Layer loss
I1027 12:50:18.641072 10983 net.cpp:394] loss <- ip2
I1027 12:50:18.641079 10983 net.cpp:394] loss <- label
I1027 12:50:18.641086 10983 net.cpp:356] loss -> loss
I1027 12:50:18.641094 10983 net.cpp:96] Setting up loss
I1027 12:50:18.641106 10983 net.cpp:103] Top shape: 1 1 1 1 (1)
I1027 12:50:18.641111 10983 net.cpp:109]     with loss weight 1
I1027 12:50:18.641149 10983 net.cpp:170] loss needs backward computation.
I1027 12:50:18.641154 10983 net.cpp:170] ip2 needs backward computation.
I1027 12:50:18.641157 10983 net.cpp:170] drop4 needs backward computation.
I1027 12:50:18.641162 10983 net.cpp:170] relu4 needs backward computation.
I1027 12:50:18.641166 10983 net.cpp:170] ip1 needs backward computation.
I1027 12:50:18.641170 10983 net.cpp:170] drop3 needs backward computation.
I1027 12:50:18.641175 10983 net.cpp:170] relu3 needs backward computation.
I1027 12:50:18.641180 10983 net.cpp:170] pool3 needs backward computation.
I1027 12:50:18.641185 10983 net.cpp:170] conv3 needs backward computation.
I1027 12:50:18.641188 10983 net.cpp:170] drop2 needs backward computation.
I1027 12:50:18.641193 10983 net.cpp:170] relu2 needs backward computation.
I1027 12:50:18.641197 10983 net.cpp:170] pool2 needs backward computation.
I1027 12:50:18.641202 10983 net.cpp:170] conv2 needs backward computation.
I1027 12:50:18.641217 10983 net.cpp:170] drop1 needs backward computation.
I1027 12:50:18.641222 10983 net.cpp:170] relu1 needs backward computation.
I1027 12:50:18.641227 10983 net.cpp:170] pool1 needs backward computation.
I1027 12:50:18.641232 10983 net.cpp:170] conv1 needs backward computation.
I1027 12:50:18.641235 10983 net.cpp:172] mnist does not need backward computation.
I1027 12:50:18.641239 10983 net.cpp:208] This network produces output loss
I1027 12:50:18.641249 10983 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 12:50:18.641257 10983 net.cpp:219] Network initialization done.
I1027 12:50:18.641260 10983 net.cpp:220] Memory required for data: 119788292
I1027 12:50:18.641320 10983 solver.cpp:41] Solver scaffolding done.
I1027 12:50:18.641327 10983 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_275000.solverstate
I1027 12:50:18.641331 10983 solver.cpp:160] Solving Captcha
I1027 12:50:18.641350 10983 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_275000.solverstate
I1027 12:50:25.414865 10983 solver.cpp:502] SGDSolver: restoring history
I1027 12:50:26.510186 10983 solver.cpp:191] Iteration 275000, loss = 2.70395
I1027 12:50:26.510241 10983 solver.cpp:206]     Train net output #0: loss = 2.70395 (* 1 = 2.70395 loss)
I1027 12:50:26.510257 10983 solver.cpp:403] Iteration 275000, lr = 0.000810712
I1027 12:58:53.120614 10983 solver.cpp:191] Iteration 276000, loss = 2.59462
I1027 12:58:53.121278 10983 solver.cpp:206]     Train net output #0: loss = 2.59462 (* 1 = 2.59462 loss)
I1027 12:58:53.121315 10983 solver.cpp:403] Iteration 276000, lr = 0.000808585
I1027 13:07:18.851945 10983 solver.cpp:191] Iteration 277000, loss = 2.39828
I1027 13:07:18.852741 10983 solver.cpp:206]     Train net output #0: loss = 2.39828 (* 1 = 2.39828 loss)
I1027 13:07:18.852774 10983 solver.cpp:403] Iteration 277000, lr = 0.000806471
I1027 13:15:43.622387 10983 solver.cpp:191] Iteration 278000, loss = 2.4989
I1027 13:15:43.623612 10983 solver.cpp:206]     Train net output #0: loss = 2.4989 (* 1 = 2.4989 loss)
I1027 13:15:43.623644 10983 solver.cpp:403] Iteration 278000, lr = 0.00080437
I1027 13:24:09.701987 10983 solver.cpp:191] Iteration 279000, loss = 2.6441
I1027 13:24:09.702649 10983 solver.cpp:206]     Train net output #0: loss = 2.6441 (* 1 = 2.6441 loss)
I1027 13:24:09.702687 10983 solver.cpp:403] Iteration 279000, lr = 0.000802281
I1027 13:32:35.321619 10983 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_280000.caffemodel
I1027 13:32:39.933676 10983 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_280000.solverstate
I1027 13:32:43.653909 10983 solver.cpp:191] Iteration 280000, loss = 2.4655
I1027 13:32:43.654513 10983 solver.cpp:206]     Train net output #0: loss = 2.4655 (* 1 = 2.4655 loss)
I1027 13:32:43.654546 10983 solver.cpp:403] Iteration 280000, lr = 0.000800205
I1027 13:41:11.806957 10983 solver.cpp:191] Iteration 281000, loss = 2.35829
I1027 13:41:11.807529 10983 solver.cpp:206]     Train net output #0: loss = 2.35829 (* 1 = 2.35829 loss)
I1027 13:41:11.807560 10983 solver.cpp:403] Iteration 281000, lr = 0.000798142
I1027 13:49:38.587723 10983 solver.cpp:191] Iteration 282000, loss = 2.54423
I1027 13:49:38.588270 10983 solver.cpp:206]     Train net output #0: loss = 2.54423 (* 1 = 2.54423 loss)
I1027 13:49:38.588304 10983 solver.cpp:403] Iteration 282000, lr = 0.000796091
I1027 13:58:04.511334 10983 solver.cpp:191] Iteration 283000, loss = 2.24724
I1027 13:58:04.512040 10983 solver.cpp:206]     Train net output #0: loss = 2.24724 (* 1 = 2.24724 loss)
I1027 13:58:04.512086 10983 solver.cpp:403] Iteration 283000, lr = 0.000794053
I1027 14:06:30.700275 10983 solver.cpp:191] Iteration 284000, loss = 2.3911
I1027 14:06:30.700891 10983 solver.cpp:206]     Train net output #0: loss = 2.3911 (* 1 = 2.3911 loss)
I1027 14:06:30.700924 10983 solver.cpp:403] Iteration 284000, lr = 0.000792026
I1027 14:14:57.612568 10983 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_285000.caffemodel
I1027 14:15:02.262686 10983 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_285000.solverstate
I1027 14:15:06.463959 10983 solver.cpp:191] Iteration 285000, loss = 2.37941
I1027 14:15:06.464484 10983 solver.cpp:206]     Train net output #0: loss = 2.37941 (* 1 = 2.37941 loss)
I1027 14:15:06.464499 10983 solver.cpp:403] Iteration 285000, lr = 0.000790012
I1027 14:23:34.230288 10983 solver.cpp:191] Iteration 286000, loss = 2.56724
I1027 14:23:34.231050 10983 solver.cpp:206]     Train net output #0: loss = 2.56724 (* 1 = 2.56724 loss)
I1027 14:23:34.231084 10983 solver.cpp:403] Iteration 286000, lr = 0.000788009
I1027 14:31:59.591560 10983 solver.cpp:191] Iteration 287000, loss = 2.4607
I1027 14:31:59.592211 10983 solver.cpp:206]     Train net output #0: loss = 2.4607 (* 1 = 2.4607 loss)
I1027 14:31:59.592247 10983 solver.cpp:403] Iteration 287000, lr = 0.000786018
I1027 14:40:24.782783 10983 solver.cpp:191] Iteration 288000, loss = 2.43894
I1027 14:40:24.783392 10983 solver.cpp:206]     Train net output #0: loss = 2.43894 (* 1 = 2.43894 loss)
I1027 14:40:24.783429 10983 solver.cpp:403] Iteration 288000, lr = 0.000784039
I1027 14:48:50.811727 10983 solver.cpp:191] Iteration 289000, loss = 2.2715
I1027 14:48:50.812773 10983 solver.cpp:206]     Train net output #0: loss = 2.2715 (* 1 = 2.2715 loss)
I1027 14:48:50.812813 10983 solver.cpp:403] Iteration 289000, lr = 0.000782072
I1027 14:57:17.012684 10983 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_290000.caffemodel
I1027 14:57:22.849836 10983 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_290000.solverstate
I1027 14:57:26.440464 10983 solver.cpp:228] Iteration 290000, loss = 2.25626
I1027 14:57:26.441033 10983 solver.cpp:233] Optimization Done.
I1027 14:57:26.441061 10983 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1027 15:19:45.424495 20921 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 15:19:45.424597 20921 net.cpp:358] Input 0 -> data
I1027 15:19:45.424621 20921 net.cpp:67] Creating Layer conv1
I1027 15:19:45.424628 20921 net.cpp:394] conv1 <- data
I1027 15:19:45.424634 20921 net.cpp:356] conv1 -> conv1
I1027 15:19:45.424643 20921 net.cpp:96] Setting up conv1
I1027 15:19:45.424960 20921 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 15:19:45.424979 20921 net.cpp:67] Creating Layer pool1
I1027 15:19:45.424984 20921 net.cpp:394] pool1 <- conv1
I1027 15:19:45.424990 20921 net.cpp:356] pool1 -> pool1
I1027 15:19:45.424998 20921 net.cpp:96] Setting up pool1
I1027 15:19:45.425010 20921 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 15:19:45.425017 20921 net.cpp:67] Creating Layer relu1
I1027 15:19:45.425021 20921 net.cpp:394] relu1 <- pool1
I1027 15:19:45.425026 20921 net.cpp:345] relu1 -> pool1 (in-place)
I1027 15:19:45.425032 20921 net.cpp:96] Setting up relu1
I1027 15:19:45.425037 20921 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 15:19:45.425042 20921 net.cpp:67] Creating Layer drop1
I1027 15:19:45.425046 20921 net.cpp:394] drop1 <- pool1
I1027 15:19:45.425051 20921 net.cpp:345] drop1 -> pool1 (in-place)
I1027 15:19:45.425057 20921 net.cpp:96] Setting up drop1
I1027 15:19:45.425062 20921 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 15:19:45.425071 20921 net.cpp:67] Creating Layer conv2
I1027 15:19:45.425076 20921 net.cpp:394] conv2 <- pool1
I1027 15:19:45.425082 20921 net.cpp:356] conv2 -> conv2
I1027 15:19:45.425088 20921 net.cpp:96] Setting up conv2
I1027 15:19:45.425640 20921 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 15:19:45.425654 20921 net.cpp:67] Creating Layer pool2
I1027 15:19:45.425659 20921 net.cpp:394] pool2 <- conv2
I1027 15:19:45.425667 20921 net.cpp:356] pool2 -> pool2
I1027 15:19:45.425673 20921 net.cpp:96] Setting up pool2
I1027 15:19:45.425679 20921 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 15:19:45.425684 20921 net.cpp:67] Creating Layer relu2
I1027 15:19:45.425688 20921 net.cpp:394] relu2 <- pool2
I1027 15:19:45.425693 20921 net.cpp:345] relu2 -> pool2 (in-place)
I1027 15:19:45.425699 20921 net.cpp:96] Setting up relu2
I1027 15:19:45.425704 20921 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 15:19:45.425710 20921 net.cpp:67] Creating Layer drop2
I1027 15:19:45.425714 20921 net.cpp:394] drop2 <- pool2
I1027 15:19:45.425720 20921 net.cpp:345] drop2 -> pool2 (in-place)
I1027 15:19:45.425725 20921 net.cpp:96] Setting up drop2
I1027 15:19:45.425729 20921 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 15:19:45.425736 20921 net.cpp:67] Creating Layer conv3
I1027 15:19:45.425740 20921 net.cpp:394] conv3 <- pool2
I1027 15:19:45.425748 20921 net.cpp:356] conv3 -> conv3
I1027 15:19:45.425755 20921 net.cpp:96] Setting up conv3
I1027 15:19:45.427214 20921 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 15:19:45.427230 20921 net.cpp:67] Creating Layer pool3
I1027 15:19:45.427235 20921 net.cpp:394] pool3 <- conv3
I1027 15:19:45.427242 20921 net.cpp:356] pool3 -> pool3
I1027 15:19:45.427248 20921 net.cpp:96] Setting up pool3
I1027 15:19:45.427253 20921 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 15:19:45.427259 20921 net.cpp:67] Creating Layer relu3
I1027 15:19:45.427266 20921 net.cpp:394] relu3 <- pool3
I1027 15:19:45.427271 20921 net.cpp:345] relu3 -> pool3 (in-place)
I1027 15:19:45.427278 20921 net.cpp:96] Setting up relu3
I1027 15:19:45.427281 20921 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 15:19:45.427286 20921 net.cpp:67] Creating Layer drop3
I1027 15:19:45.427290 20921 net.cpp:394] drop3 <- pool3
I1027 15:19:45.427297 20921 net.cpp:345] drop3 -> pool3 (in-place)
I1027 15:19:45.427304 20921 net.cpp:96] Setting up drop3
I1027 15:19:45.427307 20921 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 15:19:45.427314 20921 net.cpp:67] Creating Layer ip1
I1027 15:19:45.427317 20921 net.cpp:394] ip1 <- pool3
I1027 15:19:45.427323 20921 net.cpp:356] ip1 -> ip1
I1027 15:19:45.427330 20921 net.cpp:96] Setting up ip1
I1027 15:19:45.927196 20921 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 15:19:45.927263 20921 net.cpp:67] Creating Layer relu4
I1027 15:19:45.927271 20921 net.cpp:394] relu4 <- ip1
I1027 15:19:45.927281 20921 net.cpp:345] relu4 -> ip1 (in-place)
I1027 15:19:45.927290 20921 net.cpp:96] Setting up relu4
I1027 15:19:45.927296 20921 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 15:19:45.927305 20921 net.cpp:67] Creating Layer drop4
I1027 15:19:45.927310 20921 net.cpp:394] drop4 <- ip1
I1027 15:19:45.927315 20921 net.cpp:345] drop4 -> ip1 (in-place)
I1027 15:19:45.927321 20921 net.cpp:96] Setting up drop4
I1027 15:19:45.927327 20921 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 15:19:45.927335 20921 net.cpp:67] Creating Layer ip2
I1027 15:19:45.927340 20921 net.cpp:394] ip2 <- ip1
I1027 15:19:45.927347 20921 net.cpp:356] ip2 -> ip2
I1027 15:19:45.927359 20921 net.cpp:96] Setting up ip2
I1027 15:19:45.936657 20921 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 15:19:45.936722 20921 net.cpp:67] Creating Layer prob
I1027 15:19:45.936728 20921 net.cpp:394] prob <- ip2
I1027 15:19:45.936740 20921 net.cpp:356] prob -> prob
I1027 15:19:45.936751 20921 net.cpp:96] Setting up prob
I1027 15:19:45.936758 20921 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 15:19:45.936763 20921 net.cpp:172] prob does not need backward computation.
I1027 15:19:45.936766 20921 net.cpp:172] ip2 does not need backward computation.
I1027 15:19:45.936770 20921 net.cpp:172] drop4 does not need backward computation.
I1027 15:19:45.936774 20921 net.cpp:172] relu4 does not need backward computation.
I1027 15:19:45.936777 20921 net.cpp:172] ip1 does not need backward computation.
I1027 15:19:45.936781 20921 net.cpp:172] drop3 does not need backward computation.
I1027 15:19:45.936784 20921 net.cpp:172] relu3 does not need backward computation.
I1027 15:19:45.936789 20921 net.cpp:172] pool3 does not need backward computation.
I1027 15:19:45.936791 20921 net.cpp:172] conv3 does not need backward computation.
I1027 15:19:45.936795 20921 net.cpp:172] drop2 does not need backward computation.
I1027 15:19:45.936799 20921 net.cpp:172] relu2 does not need backward computation.
I1027 15:19:45.936802 20921 net.cpp:172] pool2 does not need backward computation.
I1027 15:19:45.936806 20921 net.cpp:172] conv2 does not need backward computation.
I1027 15:19:45.936810 20921 net.cpp:172] drop1 does not need backward computation.
I1027 15:19:45.936813 20921 net.cpp:172] relu1 does not need backward computation.
I1027 15:19:45.936816 20921 net.cpp:172] pool1 does not need backward computation.
I1027 15:19:45.936820 20921 net.cpp:172] conv1 does not need backward computation.
I1027 15:19:45.936823 20921 net.cpp:208] This network produces output prob
I1027 15:19:45.936835 20921 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 15:19:45.936844 20921 net.cpp:219] Network initialization done.
I1027 15:19:45.936848 20921 net.cpp:220] Memory required for data: 1837200
I1027 15:20:53.302240 20921 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 15:20:53.302848 20921 net.cpp:358] Input 0 -> data
I1027 15:20:53.302903 20921 net.cpp:67] Creating Layer conv1
I1027 15:20:53.302918 20921 net.cpp:394] conv1 <- data
I1027 15:20:53.302937 20921 net.cpp:356] conv1 -> conv1
I1027 15:20:53.302959 20921 net.cpp:96] Setting up conv1
I1027 15:20:53.303024 20921 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 15:20:53.303058 20921 net.cpp:67] Creating Layer pool1
I1027 15:20:53.303071 20921 net.cpp:394] pool1 <- conv1
I1027 15:20:53.303087 20921 net.cpp:356] pool1 -> pool1
I1027 15:20:53.303107 20921 net.cpp:96] Setting up pool1
I1027 15:20:53.303124 20921 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 15:20:53.303143 20921 net.cpp:67] Creating Layer relu1
I1027 15:20:53.303154 20921 net.cpp:394] relu1 <- pool1
I1027 15:20:53.303169 20921 net.cpp:345] relu1 -> pool1 (in-place)
I1027 15:20:53.303185 20921 net.cpp:96] Setting up relu1
I1027 15:20:53.303197 20921 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 15:20:53.303213 20921 net.cpp:67] Creating Layer drop1
I1027 15:20:53.303225 20921 net.cpp:394] drop1 <- pool1
I1027 15:20:53.303239 20921 net.cpp:345] drop1 -> pool1 (in-place)
I1027 15:20:53.303256 20921 net.cpp:96] Setting up drop1
I1027 15:20:53.303268 20921 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 15:20:53.303288 20921 net.cpp:67] Creating Layer conv2
I1027 15:20:53.303300 20921 net.cpp:394] conv2 <- pool1
I1027 15:20:53.303316 20921 net.cpp:356] conv2 -> conv2
I1027 15:20:53.303335 20921 net.cpp:96] Setting up conv2
I1027 15:20:53.304754 20921 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 15:20:53.304797 20921 net.cpp:67] Creating Layer pool2
I1027 15:20:53.304811 20921 net.cpp:394] pool2 <- conv2
I1027 15:20:53.304827 20921 net.cpp:356] pool2 -> pool2
I1027 15:20:53.304847 20921 net.cpp:96] Setting up pool2
I1027 15:20:53.304862 20921 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 15:20:53.304878 20921 net.cpp:67] Creating Layer relu2
I1027 15:20:53.304889 20921 net.cpp:394] relu2 <- pool2
I1027 15:20:53.304904 20921 net.cpp:345] relu2 -> pool2 (in-place)
I1027 15:20:53.304920 20921 net.cpp:96] Setting up relu2
I1027 15:20:53.304931 20921 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 15:20:53.304946 20921 net.cpp:67] Creating Layer drop2
I1027 15:20:53.304958 20921 net.cpp:394] drop2 <- pool2
I1027 15:20:53.304973 20921 net.cpp:345] drop2 -> pool2 (in-place)
I1027 15:20:53.304988 20921 net.cpp:96] Setting up drop2
I1027 15:20:53.305001 20921 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 15:20:53.305022 20921 net.cpp:67] Creating Layer conv3
I1027 15:20:53.305032 20921 net.cpp:394] conv3 <- pool2
I1027 15:20:53.305049 20921 net.cpp:356] conv3 -> conv3
I1027 15:20:53.305068 20921 net.cpp:96] Setting up conv3
I1027 15:20:53.308713 20921 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 15:20:53.308749 20921 net.cpp:67] Creating Layer pool3
I1027 15:20:53.308763 20921 net.cpp:394] pool3 <- conv3
I1027 15:20:53.308779 20921 net.cpp:356] pool3 -> pool3
I1027 15:20:53.308796 20921 net.cpp:96] Setting up pool3
I1027 15:20:53.308810 20921 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 15:20:53.308825 20921 net.cpp:67] Creating Layer relu3
I1027 15:20:53.308837 20921 net.cpp:394] relu3 <- pool3
I1027 15:20:53.308852 20921 net.cpp:345] relu3 -> pool3 (in-place)
I1027 15:20:53.308867 20921 net.cpp:96] Setting up relu3
I1027 15:20:53.308878 20921 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 15:20:53.308893 20921 net.cpp:67] Creating Layer drop3
I1027 15:20:53.308905 20921 net.cpp:394] drop3 <- pool3
I1027 15:20:53.308920 20921 net.cpp:345] drop3 -> pool3 (in-place)
I1027 15:20:53.308935 20921 net.cpp:96] Setting up drop3
I1027 15:20:53.308948 20921 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 15:20:53.308965 20921 net.cpp:67] Creating Layer ip1
I1027 15:20:53.308976 20921 net.cpp:394] ip1 <- pool3
I1027 15:20:53.308993 20921 net.cpp:356] ip1 -> ip1
I1027 15:20:53.309012 20921 net.cpp:96] Setting up ip1
I1027 15:20:53.751719 20921 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 15:20:53.751782 20921 net.cpp:67] Creating Layer relu4
I1027 15:20:53.751791 20921 net.cpp:394] relu4 <- ip1
I1027 15:20:53.751801 20921 net.cpp:345] relu4 -> ip1 (in-place)
I1027 15:20:53.751811 20921 net.cpp:96] Setting up relu4
I1027 15:20:53.751816 20921 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 15:20:53.751823 20921 net.cpp:67] Creating Layer drop4
I1027 15:20:53.751828 20921 net.cpp:394] drop4 <- ip1
I1027 15:20:53.751834 20921 net.cpp:345] drop4 -> ip1 (in-place)
I1027 15:20:53.751842 20921 net.cpp:96] Setting up drop4
I1027 15:20:53.751847 20921 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 15:20:53.751857 20921 net.cpp:67] Creating Layer ip2
I1027 15:20:53.751862 20921 net.cpp:394] ip2 <- ip1
I1027 15:20:53.751868 20921 net.cpp:356] ip2 -> ip2
I1027 15:20:53.751881 20921 net.cpp:96] Setting up ip2
I1027 15:20:53.759627 20921 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 15:20:53.759687 20921 net.cpp:67] Creating Layer prob
I1027 15:20:53.759695 20921 net.cpp:394] prob <- ip2
I1027 15:20:53.759703 20921 net.cpp:356] prob -> prob
I1027 15:20:53.759714 20921 net.cpp:96] Setting up prob
I1027 15:20:53.759721 20921 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 15:20:53.759727 20921 net.cpp:172] prob does not need backward computation.
I1027 15:20:53.759730 20921 net.cpp:172] ip2 does not need backward computation.
I1027 15:20:53.759734 20921 net.cpp:172] drop4 does not need backward computation.
I1027 15:20:53.759738 20921 net.cpp:172] relu4 does not need backward computation.
I1027 15:20:53.759743 20921 net.cpp:172] ip1 does not need backward computation.
I1027 15:20:53.759747 20921 net.cpp:172] drop3 does not need backward computation.
I1027 15:20:53.759762 20921 net.cpp:172] relu3 does not need backward computation.
I1027 15:20:53.759765 20921 net.cpp:172] pool3 does not need backward computation.
I1027 15:20:53.759770 20921 net.cpp:172] conv3 does not need backward computation.
I1027 15:20:53.759773 20921 net.cpp:172] drop2 does not need backward computation.
I1027 15:20:53.759778 20921 net.cpp:172] relu2 does not need backward computation.
I1027 15:20:53.759781 20921 net.cpp:172] pool2 does not need backward computation.
I1027 15:20:53.759785 20921 net.cpp:172] conv2 does not need backward computation.
I1027 15:20:53.759789 20921 net.cpp:172] drop1 does not need backward computation.
I1027 15:20:53.759793 20921 net.cpp:172] relu1 does not need backward computation.
I1027 15:20:53.759798 20921 net.cpp:172] pool1 does not need backward computation.
I1027 15:20:53.759801 20921 net.cpp:172] conv1 does not need backward computation.
I1027 15:20:53.759804 20921 net.cpp:208] This network produces output prob
I1027 15:20:53.759820 20921 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 15:20:53.759829 20921 net.cpp:219] Network initialization done.
I1027 15:20:53.759834 20921 net.cpp:220] Memory required for data: 1837200
I1027 15:21:55.121039 20921 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 15:21:55.121595 20921 net.cpp:358] Input 0 -> data
I1027 15:21:55.121654 20921 net.cpp:67] Creating Layer conv1
I1027 15:21:55.121667 20921 net.cpp:394] conv1 <- data
I1027 15:21:55.121683 20921 net.cpp:356] conv1 -> conv1
I1027 15:21:55.121702 20921 net.cpp:96] Setting up conv1
I1027 15:21:55.121757 20921 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 15:21:55.121788 20921 net.cpp:67] Creating Layer pool1
I1027 15:21:55.121798 20921 net.cpp:394] pool1 <- conv1
I1027 15:21:55.121811 20921 net.cpp:356] pool1 -> pool1
I1027 15:21:55.121827 20921 net.cpp:96] Setting up pool1
I1027 15:21:55.121841 20921 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 15:21:55.121856 20921 net.cpp:67] Creating Layer relu1
I1027 15:21:55.121865 20921 net.cpp:394] relu1 <- pool1
I1027 15:21:55.121877 20921 net.cpp:345] relu1 -> pool1 (in-place)
I1027 15:21:55.121891 20921 net.cpp:96] Setting up relu1
I1027 15:21:55.121901 20921 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 15:21:55.121912 20921 net.cpp:67] Creating Layer drop1
I1027 15:21:55.121922 20921 net.cpp:394] drop1 <- pool1
I1027 15:21:55.121934 20921 net.cpp:345] drop1 -> pool1 (in-place)
I1027 15:21:55.121947 20921 net.cpp:96] Setting up drop1
I1027 15:21:55.121958 20921 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 15:21:55.121973 20921 net.cpp:67] Creating Layer conv2
I1027 15:21:55.121983 20921 net.cpp:394] conv2 <- pool1
I1027 15:21:55.121995 20921 net.cpp:356] conv2 -> conv2
I1027 15:21:55.122011 20921 net.cpp:96] Setting up conv2
I1027 15:21:55.123147 20921 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 15:21:55.123174 20921 net.cpp:67] Creating Layer pool2
I1027 15:21:55.123184 20921 net.cpp:394] pool2 <- conv2
I1027 15:21:55.123198 20921 net.cpp:356] pool2 -> pool2
I1027 15:21:55.123214 20921 net.cpp:96] Setting up pool2
I1027 15:21:55.123226 20921 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 15:21:55.123239 20921 net.cpp:67] Creating Layer relu2
I1027 15:21:55.123248 20921 net.cpp:394] relu2 <- pool2
I1027 15:21:55.123260 20921 net.cpp:345] relu2 -> pool2 (in-place)
I1027 15:21:55.123273 20921 net.cpp:96] Setting up relu2
I1027 15:21:55.123282 20921 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 15:21:55.123294 20921 net.cpp:67] Creating Layer drop2
I1027 15:21:55.123303 20921 net.cpp:394] drop2 <- pool2
I1027 15:21:55.123316 20921 net.cpp:345] drop2 -> pool2 (in-place)
I1027 15:21:55.123337 20921 net.cpp:96] Setting up drop2
I1027 15:21:55.123349 20921 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 15:21:55.123369 20921 net.cpp:67] Creating Layer conv3
I1027 15:21:55.123380 20921 net.cpp:394] conv3 <- pool2
I1027 15:21:55.123397 20921 net.cpp:356] conv3 -> conv3
I1027 15:21:55.123415 20921 net.cpp:96] Setting up conv3
I1027 15:21:55.127112 20921 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 15:21:55.127151 20921 net.cpp:67] Creating Layer pool3
I1027 15:21:55.127164 20921 net.cpp:394] pool3 <- conv3
I1027 15:21:55.127181 20921 net.cpp:356] pool3 -> pool3
I1027 15:21:55.127198 20921 net.cpp:96] Setting up pool3
I1027 15:21:55.127213 20921 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 15:21:55.127228 20921 net.cpp:67] Creating Layer relu3
I1027 15:21:55.127238 20921 net.cpp:394] relu3 <- pool3
I1027 15:21:55.127254 20921 net.cpp:345] relu3 -> pool3 (in-place)
I1027 15:21:55.127269 20921 net.cpp:96] Setting up relu3
I1027 15:21:55.127280 20921 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 15:21:55.127295 20921 net.cpp:67] Creating Layer drop3
I1027 15:21:55.127306 20921 net.cpp:394] drop3 <- pool3
I1027 15:21:55.127321 20921 net.cpp:345] drop3 -> pool3 (in-place)
I1027 15:21:55.127336 20921 net.cpp:96] Setting up drop3
I1027 15:21:55.127348 20921 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 15:21:55.127365 20921 net.cpp:67] Creating Layer ip1
I1027 15:21:55.127377 20921 net.cpp:394] ip1 <- pool3
I1027 15:21:55.127393 20921 net.cpp:356] ip1 -> ip1
I1027 15:21:55.127411 20921 net.cpp:96] Setting up ip1
I1027 15:21:55.546993 20921 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 15:21:55.547058 20921 net.cpp:67] Creating Layer relu4
I1027 15:21:55.547066 20921 net.cpp:394] relu4 <- ip1
I1027 15:21:55.547087 20921 net.cpp:345] relu4 -> ip1 (in-place)
I1027 15:21:55.547098 20921 net.cpp:96] Setting up relu4
I1027 15:21:55.547103 20921 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 15:21:55.547112 20921 net.cpp:67] Creating Layer drop4
I1027 15:21:55.547116 20921 net.cpp:394] drop4 <- ip1
I1027 15:21:55.547123 20921 net.cpp:345] drop4 -> ip1 (in-place)
I1027 15:21:55.547130 20921 net.cpp:96] Setting up drop4
I1027 15:21:55.547137 20921 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 15:21:55.547147 20921 net.cpp:67] Creating Layer ip2
I1027 15:21:55.547152 20921 net.cpp:394] ip2 <- ip1
I1027 15:21:55.547159 20921 net.cpp:356] ip2 -> ip2
I1027 15:21:55.547173 20921 net.cpp:96] Setting up ip2
I1027 15:21:55.554782 20921 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 15:21:55.554844 20921 net.cpp:67] Creating Layer prob
I1027 15:21:55.554852 20921 net.cpp:394] prob <- ip2
I1027 15:21:55.554862 20921 net.cpp:356] prob -> prob
I1027 15:21:55.554872 20921 net.cpp:96] Setting up prob
I1027 15:21:55.554880 20921 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 15:21:55.554885 20921 net.cpp:172] prob does not need backward computation.
I1027 15:21:55.554889 20921 net.cpp:172] ip2 does not need backward computation.
I1027 15:21:55.554893 20921 net.cpp:172] drop4 does not need backward computation.
I1027 15:21:55.554898 20921 net.cpp:172] relu4 does not need backward computation.
I1027 15:21:55.554901 20921 net.cpp:172] ip1 does not need backward computation.
I1027 15:21:55.554905 20921 net.cpp:172] drop3 does not need backward computation.
I1027 15:21:55.554909 20921 net.cpp:172] relu3 does not need backward computation.
I1027 15:21:55.554913 20921 net.cpp:172] pool3 does not need backward computation.
I1027 15:21:55.554918 20921 net.cpp:172] conv3 does not need backward computation.
I1027 15:21:55.554921 20921 net.cpp:172] drop2 does not need backward computation.
I1027 15:21:55.554925 20921 net.cpp:172] relu2 does not need backward computation.
I1027 15:21:55.554929 20921 net.cpp:172] pool2 does not need backward computation.
I1027 15:21:55.554932 20921 net.cpp:172] conv2 does not need backward computation.
I1027 15:21:55.554936 20921 net.cpp:172] drop1 does not need backward computation.
I1027 15:21:55.554940 20921 net.cpp:172] relu1 does not need backward computation.
I1027 15:21:55.554944 20921 net.cpp:172] pool1 does not need backward computation.
I1027 15:21:55.554949 20921 net.cpp:172] conv1 does not need backward computation.
I1027 15:21:55.554951 20921 net.cpp:208] This network produces output prob
I1027 15:21:55.554965 20921 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 15:21:55.554976 20921 net.cpp:219] Network initialization done.
I1027 15:21:55.554980 20921 net.cpp:220] Memory required for data: 1837200
I1027 16:16:32.731577 16832 convert_imageset.cpp:70] Shuffling data
I1027 16:16:33.341519 16832 convert_imageset.cpp:73] A total of 60000 images.
I1027 16:16:33.356076 16832 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1027 16:16:35.951057 16832 convert_imageset.cpp:177] Processed 1000 files.
E1027 16:16:38.344108 16832 convert_imageset.cpp:177] Processed 2000 files.
E1027 16:16:40.749538 16832 convert_imageset.cpp:177] Processed 3000 files.
E1027 16:16:42.917490 16832 convert_imageset.cpp:177] Processed 4000 files.
E1027 16:16:45.039198 16832 convert_imageset.cpp:177] Processed 5000 files.
E1027 16:16:47.095201 16832 convert_imageset.cpp:177] Processed 6000 files.
E1027 16:16:49.629122 16832 convert_imageset.cpp:177] Processed 7000 files.
E1027 16:16:51.770833 16832 convert_imageset.cpp:177] Processed 8000 files.
E1027 16:16:53.767957 16832 convert_imageset.cpp:177] Processed 9000 files.
E1027 16:16:55.689533 16832 convert_imageset.cpp:177] Processed 10000 files.
E1027 16:16:57.591768 16832 convert_imageset.cpp:177] Processed 11000 files.
E1027 16:16:59.578965 16832 convert_imageset.cpp:177] Processed 12000 files.
E1027 16:17:01.394703 16832 convert_imageset.cpp:177] Processed 13000 files.
E1027 16:17:03.288744 16832 convert_imageset.cpp:177] Processed 14000 files.
E1027 16:17:05.106904 16832 convert_imageset.cpp:177] Processed 15000 files.
E1027 16:17:06.914330 16832 convert_imageset.cpp:177] Processed 16000 files.
E1027 16:17:08.644562 16832 convert_imageset.cpp:177] Processed 17000 files.
E1027 16:17:10.457523 16832 convert_imageset.cpp:177] Processed 18000 files.
E1027 16:17:12.333319 16832 convert_imageset.cpp:177] Processed 19000 files.
E1027 16:17:14.735059 16832 convert_imageset.cpp:177] Processed 20000 files.
E1027 16:17:16.398540 16832 convert_imageset.cpp:177] Processed 21000 files.
E1027 16:17:18.138041 16832 convert_imageset.cpp:177] Processed 22000 files.
E1027 16:17:20.191145 16832 convert_imageset.cpp:177] Processed 23000 files.
E1027 16:17:21.948261 16832 convert_imageset.cpp:177] Processed 24000 files.
E1027 16:17:23.450301 16832 convert_imageset.cpp:177] Processed 25000 files.
E1027 16:17:25.126888 16832 convert_imageset.cpp:177] Processed 26000 files.
E1027 16:17:26.700810 16832 convert_imageset.cpp:177] Processed 27000 files.
E1027 16:17:28.372942 16832 convert_imageset.cpp:177] Processed 28000 files.
E1027 16:17:29.926893 16832 convert_imageset.cpp:177] Processed 29000 files.
E1027 16:17:31.542958 16832 convert_imageset.cpp:177] Processed 30000 files.
E1027 16:17:33.149719 16832 convert_imageset.cpp:177] Processed 31000 files.
E1027 16:17:34.863487 16832 convert_imageset.cpp:177] Processed 32000 files.
E1027 16:17:36.642477 16832 convert_imageset.cpp:177] Processed 33000 files.
E1027 16:17:38.133450 16832 convert_imageset.cpp:177] Processed 34000 files.
E1027 16:17:39.782169 16832 convert_imageset.cpp:177] Processed 35000 files.
E1027 16:17:41.314148 16832 convert_imageset.cpp:177] Processed 36000 files.
E1027 16:17:42.908259 16832 convert_imageset.cpp:177] Processed 37000 files.
E1027 16:17:44.434765 16832 convert_imageset.cpp:177] Processed 38000 files.
E1027 16:17:45.940278 16832 convert_imageset.cpp:177] Processed 39000 files.
E1027 16:17:47.478049 16832 convert_imageset.cpp:177] Processed 40000 files.
E1027 16:17:49.017704 16832 convert_imageset.cpp:177] Processed 41000 files.
E1027 16:17:50.588065 16832 convert_imageset.cpp:177] Processed 42000 files.
E1027 16:17:52.370887 16832 convert_imageset.cpp:177] Processed 43000 files.
E1027 16:17:53.812942 16832 convert_imageset.cpp:177] Processed 44000 files.
E1027 16:17:55.278767 16832 convert_imageset.cpp:177] Processed 45000 files.
E1027 16:17:56.852501 16832 convert_imageset.cpp:177] Processed 46000 files.
E1027 16:17:58.849792 16832 convert_imageset.cpp:177] Processed 47000 files.
E1027 16:18:00.281509 16832 convert_imageset.cpp:177] Processed 48000 files.
E1027 16:18:01.989935 16832 convert_imageset.cpp:177] Processed 49000 files.
E1027 16:18:03.477848 16832 convert_imageset.cpp:177] Processed 50000 files.
E1027 16:18:05.018223 16832 convert_imageset.cpp:177] Processed 51000 files.
E1027 16:18:06.577744 16832 convert_imageset.cpp:177] Processed 52000 files.
E1027 16:18:07.946280 16832 convert_imageset.cpp:177] Processed 53000 files.
E1027 16:18:09.424769 16832 convert_imageset.cpp:177] Processed 54000 files.
E1027 16:18:10.883435 16832 convert_imageset.cpp:177] Processed 55000 files.
E1027 16:18:12.452263 16832 convert_imageset.cpp:177] Processed 56000 files.
E1027 16:18:13.903518 16832 convert_imageset.cpp:177] Processed 57000 files.
E1027 16:18:15.367482 16832 convert_imageset.cpp:177] Processed 58000 files.
E1027 16:18:16.793176 16832 convert_imageset.cpp:177] Processed 59000 files.
E1027 16:18:18.220526 16832 convert_imageset.cpp:177] Processed 60000 files.
I1027 16:18:18.451993 17475 caffe.cpp:99] Use GPU with device ID 0
I1027 16:18:18.897316 17475 caffe.cpp:107] Starting Optimization
I1027 16:18:18.897436 17475 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 305000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1027 16:18:18.897467 17475 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1027 16:18:18.905427 17475 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1027 16:18:18.905645 17475 net.cpp:67] Creating Layer mnist
I1027 16:18:18.905671 17475 net.cpp:356] mnist -> data
I1027 16:18:18.905706 17475 net.cpp:356] mnist -> label
I1027 16:18:18.905737 17475 net.cpp:96] Setting up mnist
I1027 16:18:18.913099 17475 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1027 16:18:18.913193 17475 data_layer.cpp:128] output data size: 64,1,50,180
I1027 16:18:18.914228 17475 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1027 16:18:18.914250 17475 net.cpp:103] Top shape: 64 1 1 1 (64)
I1027 16:18:18.914265 17475 net.cpp:67] Creating Layer conv1
I1027 16:18:18.914273 17475 net.cpp:394] conv1 <- data
I1027 16:18:18.914289 17475 net.cpp:356] conv1 -> conv1
I1027 16:18:18.914309 17475 net.cpp:96] Setting up conv1
I1027 16:18:18.914779 17475 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1027 16:18:18.914819 17475 net.cpp:67] Creating Layer pool1
I1027 16:18:18.914827 17475 net.cpp:394] pool1 <- conv1
I1027 16:18:18.914835 17475 net.cpp:356] pool1 -> pool1
I1027 16:18:18.914844 17475 net.cpp:96] Setting up pool1
I1027 16:18:18.914863 17475 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 16:18:18.914875 17475 net.cpp:67] Creating Layer relu1
I1027 16:18:18.914881 17475 net.cpp:394] relu1 <- pool1
I1027 16:18:18.914890 17475 net.cpp:345] relu1 -> pool1 (in-place)
I1027 16:18:18.914897 17475 net.cpp:96] Setting up relu1
I1027 16:18:18.914903 17475 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 16:18:18.914912 17475 net.cpp:67] Creating Layer drop1
I1027 16:18:18.914918 17475 net.cpp:394] drop1 <- pool1
I1027 16:18:18.914928 17475 net.cpp:345] drop1 -> pool1 (in-place)
I1027 16:18:18.914937 17475 net.cpp:96] Setting up drop1
I1027 16:18:18.914944 17475 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 16:18:18.914953 17475 net.cpp:67] Creating Layer conv2
I1027 16:18:18.914959 17475 net.cpp:394] conv2 <- pool1
I1027 16:18:18.914969 17475 net.cpp:356] conv2 -> conv2
I1027 16:18:18.914980 17475 net.cpp:96] Setting up conv2
I1027 16:18:18.915745 17475 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1027 16:18:18.915767 17475 net.cpp:67] Creating Layer pool2
I1027 16:18:18.915774 17475 net.cpp:394] pool2 <- conv2
I1027 16:18:18.915783 17475 net.cpp:356] pool2 -> pool2
I1027 16:18:18.915792 17475 net.cpp:96] Setting up pool2
I1027 16:18:18.915799 17475 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 16:18:18.915807 17475 net.cpp:67] Creating Layer relu2
I1027 16:18:18.915813 17475 net.cpp:394] relu2 <- pool2
I1027 16:18:18.915823 17475 net.cpp:345] relu2 -> pool2 (in-place)
I1027 16:18:18.915832 17475 net.cpp:96] Setting up relu2
I1027 16:18:18.915838 17475 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 16:18:18.915846 17475 net.cpp:67] Creating Layer drop2
I1027 16:18:18.915853 17475 net.cpp:394] drop2 <- pool2
I1027 16:18:18.915860 17475 net.cpp:345] drop2 -> pool2 (in-place)
I1027 16:18:18.915868 17475 net.cpp:96] Setting up drop2
I1027 16:18:18.915874 17475 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 16:18:18.915885 17475 net.cpp:67] Creating Layer conv3
I1027 16:18:18.915891 17475 net.cpp:394] conv3 <- pool2
I1027 16:18:18.915899 17475 net.cpp:356] conv3 -> conv3
I1027 16:18:18.915909 17475 net.cpp:96] Setting up conv3
I1027 16:18:18.919632 17475 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1027 16:18:18.919685 17475 net.cpp:67] Creating Layer pool3
I1027 16:18:18.919700 17475 net.cpp:394] pool3 <- conv3
I1027 16:18:18.919723 17475 net.cpp:356] pool3 -> pool3
I1027 16:18:18.919742 17475 net.cpp:96] Setting up pool3
I1027 16:18:18.919759 17475 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 16:18:18.919775 17475 net.cpp:67] Creating Layer relu3
I1027 16:18:18.919788 17475 net.cpp:394] relu3 <- pool3
I1027 16:18:18.919803 17475 net.cpp:345] relu3 -> pool3 (in-place)
I1027 16:18:18.919824 17475 net.cpp:96] Setting up relu3
I1027 16:18:18.919837 17475 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 16:18:18.919853 17475 net.cpp:67] Creating Layer drop3
I1027 16:18:18.919865 17475 net.cpp:394] drop3 <- pool3
I1027 16:18:18.919881 17475 net.cpp:345] drop3 -> pool3 (in-place)
I1027 16:18:18.919898 17475 net.cpp:96] Setting up drop3
I1027 16:18:18.919913 17475 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 16:18:18.919930 17475 net.cpp:67] Creating Layer ip1
I1027 16:18:18.919942 17475 net.cpp:394] ip1 <- pool3
I1027 16:18:18.919965 17475 net.cpp:356] ip1 -> ip1
I1027 16:18:18.920029 17475 net.cpp:96] Setting up ip1
I1027 16:18:19.458364 17475 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 16:18:19.458420 17475 net.cpp:67] Creating Layer relu4
I1027 16:18:19.458427 17475 net.cpp:394] relu4 <- ip1
I1027 16:18:19.458436 17475 net.cpp:345] relu4 -> ip1 (in-place)
I1027 16:18:19.458446 17475 net.cpp:96] Setting up relu4
I1027 16:18:19.458451 17475 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 16:18:19.458464 17475 net.cpp:67] Creating Layer drop4
I1027 16:18:19.458469 17475 net.cpp:394] drop4 <- ip1
I1027 16:18:19.458475 17475 net.cpp:345] drop4 -> ip1 (in-place)
I1027 16:18:19.458482 17475 net.cpp:96] Setting up drop4
I1027 16:18:19.458487 17475 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 16:18:19.458498 17475 net.cpp:67] Creating Layer ip2
I1027 16:18:19.458503 17475 net.cpp:394] ip2 <- ip1
I1027 16:18:19.458511 17475 net.cpp:356] ip2 -> ip2
I1027 16:18:19.458520 17475 net.cpp:96] Setting up ip2
I1027 16:18:19.467396 17475 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1027 16:18:19.467466 17475 net.cpp:67] Creating Layer loss
I1027 16:18:19.467474 17475 net.cpp:394] loss <- ip2
I1027 16:18:19.467483 17475 net.cpp:394] loss <- label
I1027 16:18:19.467489 17475 net.cpp:356] loss -> loss
I1027 16:18:19.467499 17475 net.cpp:96] Setting up loss
I1027 16:18:19.467510 17475 net.cpp:103] Top shape: 1 1 1 1 (1)
I1027 16:18:19.467515 17475 net.cpp:109]     with loss weight 1
I1027 16:18:19.467556 17475 net.cpp:170] loss needs backward computation.
I1027 16:18:19.467561 17475 net.cpp:170] ip2 needs backward computation.
I1027 16:18:19.467566 17475 net.cpp:170] drop4 needs backward computation.
I1027 16:18:19.467571 17475 net.cpp:170] relu4 needs backward computation.
I1027 16:18:19.467574 17475 net.cpp:170] ip1 needs backward computation.
I1027 16:18:19.467579 17475 net.cpp:170] drop3 needs backward computation.
I1027 16:18:19.467583 17475 net.cpp:170] relu3 needs backward computation.
I1027 16:18:19.467588 17475 net.cpp:170] pool3 needs backward computation.
I1027 16:18:19.467592 17475 net.cpp:170] conv3 needs backward computation.
I1027 16:18:19.467597 17475 net.cpp:170] drop2 needs backward computation.
I1027 16:18:19.467602 17475 net.cpp:170] relu2 needs backward computation.
I1027 16:18:19.467607 17475 net.cpp:170] pool2 needs backward computation.
I1027 16:18:19.467610 17475 net.cpp:170] conv2 needs backward computation.
I1027 16:18:19.467615 17475 net.cpp:170] drop1 needs backward computation.
I1027 16:18:19.467619 17475 net.cpp:170] relu1 needs backward computation.
I1027 16:18:19.467623 17475 net.cpp:170] pool1 needs backward computation.
I1027 16:18:19.467628 17475 net.cpp:170] conv1 needs backward computation.
I1027 16:18:19.467633 17475 net.cpp:172] mnist does not need backward computation.
I1027 16:18:19.467638 17475 net.cpp:208] This network produces output loss
I1027 16:18:19.467648 17475 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 16:18:19.467654 17475 net.cpp:219] Network initialization done.
I1027 16:18:19.467658 17475 net.cpp:220] Memory required for data: 119788292
I1027 16:18:19.467718 17475 solver.cpp:41] Solver scaffolding done.
I1027 16:18:19.467725 17475 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_290000.solverstate
I1027 16:18:19.467730 17475 solver.cpp:160] Solving Captcha
I1027 16:18:19.467748 17475 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_290000.solverstate
I1027 16:18:25.529026 17475 solver.cpp:502] SGDSolver: restoring history
I1027 16:18:26.494045 17475 solver.cpp:191] Iteration 290000, loss = 2.40219
I1027 16:18:26.494665 17475 solver.cpp:206]     Train net output #0: loss = 2.40219 (* 1 = 2.40219 loss)
I1027 16:18:26.494707 17475 solver.cpp:403] Iteration 290000, lr = 0.000780116
I1027 16:26:55.114706 17475 solver.cpp:191] Iteration 291000, loss = 2.28009
I1027 16:26:55.115533 17475 solver.cpp:206]     Train net output #0: loss = 2.28009 (* 1 = 2.28009 loss)
I1027 16:26:55.115566 17475 solver.cpp:403] Iteration 291000, lr = 0.000778171
I1027 16:35:21.182919 17475 solver.cpp:191] Iteration 292000, loss = 2.59874
I1027 16:35:21.183696 17475 solver.cpp:206]     Train net output #0: loss = 2.59874 (* 1 = 2.59874 loss)
I1027 16:35:21.183727 17475 solver.cpp:403] Iteration 292000, lr = 0.000776238
I1027 16:43:48.066697 17475 solver.cpp:191] Iteration 293000, loss = 2.50957
I1027 16:43:48.067229 17475 solver.cpp:206]     Train net output #0: loss = 2.50957 (* 1 = 2.50957 loss)
I1027 16:43:48.067261 17475 solver.cpp:403] Iteration 293000, lr = 0.000774316
I1027 16:52:14.595978 17475 solver.cpp:191] Iteration 294000, loss = 2.59351
I1027 16:52:14.596761 17475 solver.cpp:206]     Train net output #0: loss = 2.59351 (* 1 = 2.59351 loss)
I1027 16:52:14.596794 17475 solver.cpp:403] Iteration 294000, lr = 0.000772405
I1027 17:00:41.252634 17475 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_295000.caffemodel
I1027 17:00:45.687417 17475 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_295000.solverstate
I1027 17:00:49.666623 17475 solver.cpp:191] Iteration 295000, loss = 2.47004
I1027 17:00:49.667126 17475 solver.cpp:206]     Train net output #0: loss = 2.47004 (* 1 = 2.47004 loss)
I1027 17:00:49.667160 17475 solver.cpp:403] Iteration 295000, lr = 0.000770504
I1027 17:09:17.866508 17475 solver.cpp:191] Iteration 296000, loss = 2.56443
I1027 17:09:17.867024 17475 solver.cpp:206]     Train net output #0: loss = 2.56443 (* 1 = 2.56443 loss)
I1027 17:09:17.867061 17475 solver.cpp:403] Iteration 296000, lr = 0.000768615
I1027 17:17:44.295105 17475 solver.cpp:191] Iteration 297000, loss = 2.22778
I1027 17:17:44.295825 17475 solver.cpp:206]     Train net output #0: loss = 2.22778 (* 1 = 2.22778 loss)
I1027 17:17:44.295864 17475 solver.cpp:403] Iteration 297000, lr = 0.000766737
I1027 17:26:12.353929 17475 solver.cpp:191] Iteration 298000, loss = 2.55525
I1027 17:26:12.354509 17475 solver.cpp:206]     Train net output #0: loss = 2.55525 (* 1 = 2.55525 loss)
I1027 17:26:12.354547 17475 solver.cpp:403] Iteration 298000, lr = 0.000764869
I1027 17:34:38.335541 17475 solver.cpp:191] Iteration 299000, loss = 2.22394
I1027 17:34:38.336230 17475 solver.cpp:206]     Train net output #0: loss = 2.22394 (* 1 = 2.22394 loss)
I1027 17:34:38.336262 17475 solver.cpp:403] Iteration 299000, lr = 0.000763012
I1027 17:43:06.729219 17475 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_300000.caffemodel
I1027 17:43:11.760789 17475 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_300000.solverstate
I1027 17:43:15.574089 17475 solver.cpp:191] Iteration 300000, loss = 2.37746
I1027 17:43:15.574574 17475 solver.cpp:206]     Train net output #0: loss = 2.37746 (* 1 = 2.37746 loss)
I1027 17:43:15.574605 17475 solver.cpp:403] Iteration 300000, lr = 0.000761165
I1027 17:51:42.065918 17475 solver.cpp:191] Iteration 301000, loss = 2.417
I1027 17:51:42.066623 17475 solver.cpp:206]     Train net output #0: loss = 2.417 (* 1 = 2.417 loss)
I1027 17:51:42.066661 17475 solver.cpp:403] Iteration 301000, lr = 0.000759329
I1027 18:00:09.554217 17475 solver.cpp:191] Iteration 302000, loss = 2.49019
I1027 18:00:09.554882 17475 solver.cpp:206]     Train net output #0: loss = 2.49019 (* 1 = 2.49019 loss)
I1027 18:00:09.554914 17475 solver.cpp:403] Iteration 302000, lr = 0.000757502
I1027 18:08:35.790125 17475 solver.cpp:191] Iteration 303000, loss = 2.31573
I1027 18:08:35.790763 17475 solver.cpp:206]     Train net output #0: loss = 2.31573 (* 1 = 2.31573 loss)
I1027 18:08:35.790801 17475 solver.cpp:403] Iteration 303000, lr = 0.000755687
I1027 18:17:01.634659 17475 solver.cpp:191] Iteration 304000, loss = 2.40819
I1027 18:17:01.635452 17475 solver.cpp:206]     Train net output #0: loss = 2.40819 (* 1 = 2.40819 loss)
I1027 18:17:01.635484 17475 solver.cpp:403] Iteration 304000, lr = 0.000753881
I1027 18:25:28.149907 17475 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_305000.caffemodel
I1027 18:25:32.746834 17475 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_305000.solverstate
I1027 18:25:36.364337 17475 solver.cpp:228] Iteration 305000, loss = 2.24109
I1027 18:25:36.364895 17475 solver.cpp:233] Optimization Done.
I1027 18:25:36.364917 17475 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1027 18:48:12.959987 27131 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 18:48:12.960083 27131 net.cpp:358] Input 0 -> data
I1027 18:48:12.960106 27131 net.cpp:67] Creating Layer conv1
I1027 18:48:12.960111 27131 net.cpp:394] conv1 <- data
I1027 18:48:12.960119 27131 net.cpp:356] conv1 -> conv1
I1027 18:48:12.960127 27131 net.cpp:96] Setting up conv1
I1027 18:48:12.960469 27131 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 18:48:12.960490 27131 net.cpp:67] Creating Layer pool1
I1027 18:48:12.960500 27131 net.cpp:394] pool1 <- conv1
I1027 18:48:12.960506 27131 net.cpp:356] pool1 -> pool1
I1027 18:48:12.960515 27131 net.cpp:96] Setting up pool1
I1027 18:48:12.960526 27131 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 18:48:12.960535 27131 net.cpp:67] Creating Layer relu1
I1027 18:48:12.960541 27131 net.cpp:394] relu1 <- pool1
I1027 18:48:12.960547 27131 net.cpp:345] relu1 -> pool1 (in-place)
I1027 18:48:12.960552 27131 net.cpp:96] Setting up relu1
I1027 18:48:12.960557 27131 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 18:48:12.960568 27131 net.cpp:67] Creating Layer drop1
I1027 18:48:12.960573 27131 net.cpp:394] drop1 <- pool1
I1027 18:48:12.960583 27131 net.cpp:345] drop1 -> pool1 (in-place)
I1027 18:48:12.960589 27131 net.cpp:96] Setting up drop1
I1027 18:48:12.960594 27131 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 18:48:12.960602 27131 net.cpp:67] Creating Layer conv2
I1027 18:48:12.960605 27131 net.cpp:394] conv2 <- pool1
I1027 18:48:12.960611 27131 net.cpp:356] conv2 -> conv2
I1027 18:48:12.960618 27131 net.cpp:96] Setting up conv2
I1027 18:48:12.961184 27131 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 18:48:12.961199 27131 net.cpp:67] Creating Layer pool2
I1027 18:48:12.961202 27131 net.cpp:394] pool2 <- conv2
I1027 18:48:12.961208 27131 net.cpp:356] pool2 -> pool2
I1027 18:48:12.961215 27131 net.cpp:96] Setting up pool2
I1027 18:48:12.961220 27131 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 18:48:12.961228 27131 net.cpp:67] Creating Layer relu2
I1027 18:48:12.961233 27131 net.cpp:394] relu2 <- pool2
I1027 18:48:12.961238 27131 net.cpp:345] relu2 -> pool2 (in-place)
I1027 18:48:12.961243 27131 net.cpp:96] Setting up relu2
I1027 18:48:12.961247 27131 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 18:48:12.961252 27131 net.cpp:67] Creating Layer drop2
I1027 18:48:12.961256 27131 net.cpp:394] drop2 <- pool2
I1027 18:48:12.961263 27131 net.cpp:345] drop2 -> pool2 (in-place)
I1027 18:48:12.961269 27131 net.cpp:96] Setting up drop2
I1027 18:48:12.961273 27131 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 18:48:12.961280 27131 net.cpp:67] Creating Layer conv3
I1027 18:48:12.961284 27131 net.cpp:394] conv3 <- pool2
I1027 18:48:12.961292 27131 net.cpp:356] conv3 -> conv3
I1027 18:48:12.961299 27131 net.cpp:96] Setting up conv3
I1027 18:48:12.962787 27131 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 18:48:12.962805 27131 net.cpp:67] Creating Layer pool3
I1027 18:48:12.962810 27131 net.cpp:394] pool3 <- conv3
I1027 18:48:12.962815 27131 net.cpp:356] pool3 -> pool3
I1027 18:48:12.962821 27131 net.cpp:96] Setting up pool3
I1027 18:48:12.962827 27131 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 18:48:12.962833 27131 net.cpp:67] Creating Layer relu3
I1027 18:48:12.962838 27131 net.cpp:394] relu3 <- pool3
I1027 18:48:12.962843 27131 net.cpp:345] relu3 -> pool3 (in-place)
I1027 18:48:12.962848 27131 net.cpp:96] Setting up relu3
I1027 18:48:12.962852 27131 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 18:48:12.962858 27131 net.cpp:67] Creating Layer drop3
I1027 18:48:12.962862 27131 net.cpp:394] drop3 <- pool3
I1027 18:48:12.962867 27131 net.cpp:345] drop3 -> pool3 (in-place)
I1027 18:48:12.962872 27131 net.cpp:96] Setting up drop3
I1027 18:48:12.962877 27131 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 18:48:12.962884 27131 net.cpp:67] Creating Layer ip1
I1027 18:48:12.962888 27131 net.cpp:394] ip1 <- pool3
I1027 18:48:12.962894 27131 net.cpp:356] ip1 -> ip1
I1027 18:48:12.962901 27131 net.cpp:96] Setting up ip1
I1027 18:48:13.502360 27131 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 18:48:13.502418 27131 net.cpp:67] Creating Layer relu4
I1027 18:48:13.502426 27131 net.cpp:394] relu4 <- ip1
I1027 18:48:13.502435 27131 net.cpp:345] relu4 -> ip1 (in-place)
I1027 18:48:13.502445 27131 net.cpp:96] Setting up relu4
I1027 18:48:13.502450 27131 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 18:48:13.502459 27131 net.cpp:67] Creating Layer drop4
I1027 18:48:13.502462 27131 net.cpp:394] drop4 <- ip1
I1027 18:48:13.502471 27131 net.cpp:345] drop4 -> ip1 (in-place)
I1027 18:48:13.502478 27131 net.cpp:96] Setting up drop4
I1027 18:48:13.502483 27131 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 18:48:13.502492 27131 net.cpp:67] Creating Layer ip2
I1027 18:48:13.502496 27131 net.cpp:394] ip2 <- ip1
I1027 18:48:13.502504 27131 net.cpp:356] ip2 -> ip2
I1027 18:48:13.502517 27131 net.cpp:96] Setting up ip2
I1027 18:48:13.513545 27131 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 18:48:13.513618 27131 net.cpp:67] Creating Layer prob
I1027 18:48:13.513625 27131 net.cpp:394] prob <- ip2
I1027 18:48:13.513634 27131 net.cpp:356] prob -> prob
I1027 18:48:13.513644 27131 net.cpp:96] Setting up prob
I1027 18:48:13.513650 27131 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 18:48:13.513661 27131 net.cpp:172] prob does not need backward computation.
I1027 18:48:13.513665 27131 net.cpp:172] ip2 does not need backward computation.
I1027 18:48:13.513669 27131 net.cpp:172] drop4 does not need backward computation.
I1027 18:48:13.513674 27131 net.cpp:172] relu4 does not need backward computation.
I1027 18:48:13.513676 27131 net.cpp:172] ip1 does not need backward computation.
I1027 18:48:13.513680 27131 net.cpp:172] drop3 does not need backward computation.
I1027 18:48:13.513684 27131 net.cpp:172] relu3 does not need backward computation.
I1027 18:48:13.513687 27131 net.cpp:172] pool3 does not need backward computation.
I1027 18:48:13.513691 27131 net.cpp:172] conv3 does not need backward computation.
I1027 18:48:13.513695 27131 net.cpp:172] drop2 does not need backward computation.
I1027 18:48:13.513698 27131 net.cpp:172] relu2 does not need backward computation.
I1027 18:48:13.513701 27131 net.cpp:172] pool2 does not need backward computation.
I1027 18:48:13.513705 27131 net.cpp:172] conv2 does not need backward computation.
I1027 18:48:13.513710 27131 net.cpp:172] drop1 does not need backward computation.
I1027 18:48:13.513712 27131 net.cpp:172] relu1 does not need backward computation.
I1027 18:48:13.513716 27131 net.cpp:172] pool1 does not need backward computation.
I1027 18:48:13.513720 27131 net.cpp:172] conv1 does not need backward computation.
I1027 18:48:13.513723 27131 net.cpp:208] This network produces output prob
I1027 18:48:13.513736 27131 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 18:48:13.513744 27131 net.cpp:219] Network initialization done.
I1027 18:48:13.513747 27131 net.cpp:220] Memory required for data: 1837200
I1027 18:49:19.664619 27131 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 18:49:19.665225 27131 net.cpp:358] Input 0 -> data
I1027 18:49:19.665276 27131 net.cpp:67] Creating Layer conv1
I1027 18:49:19.665288 27131 net.cpp:394] conv1 <- data
I1027 18:49:19.665303 27131 net.cpp:356] conv1 -> conv1
I1027 18:49:19.665323 27131 net.cpp:96] Setting up conv1
I1027 18:49:19.665379 27131 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 18:49:19.665408 27131 net.cpp:67] Creating Layer pool1
I1027 18:49:19.665419 27131 net.cpp:394] pool1 <- conv1
I1027 18:49:19.665432 27131 net.cpp:356] pool1 -> pool1
I1027 18:49:19.665448 27131 net.cpp:96] Setting up pool1
I1027 18:49:19.665463 27131 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 18:49:19.665478 27131 net.cpp:67] Creating Layer relu1
I1027 18:49:19.665488 27131 net.cpp:394] relu1 <- pool1
I1027 18:49:19.665500 27131 net.cpp:345] relu1 -> pool1 (in-place)
I1027 18:49:19.665513 27131 net.cpp:96] Setting up relu1
I1027 18:49:19.665524 27131 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 18:49:19.665536 27131 net.cpp:67] Creating Layer drop1
I1027 18:49:19.665545 27131 net.cpp:394] drop1 <- pool1
I1027 18:49:19.665559 27131 net.cpp:345] drop1 -> pool1 (in-place)
I1027 18:49:19.665571 27131 net.cpp:96] Setting up drop1
I1027 18:49:19.665582 27131 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 18:49:19.665598 27131 net.cpp:67] Creating Layer conv2
I1027 18:49:19.665607 27131 net.cpp:394] conv2 <- pool1
I1027 18:49:19.665621 27131 net.cpp:356] conv2 -> conv2
I1027 18:49:19.665637 27131 net.cpp:96] Setting up conv2
I1027 18:49:19.666771 27131 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 18:49:19.666800 27131 net.cpp:67] Creating Layer pool2
I1027 18:49:19.666810 27131 net.cpp:394] pool2 <- conv2
I1027 18:49:19.666824 27131 net.cpp:356] pool2 -> pool2
I1027 18:49:19.666841 27131 net.cpp:96] Setting up pool2
I1027 18:49:19.666853 27131 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 18:49:19.666867 27131 net.cpp:67] Creating Layer relu2
I1027 18:49:19.666875 27131 net.cpp:394] relu2 <- pool2
I1027 18:49:19.666887 27131 net.cpp:345] relu2 -> pool2 (in-place)
I1027 18:49:19.666900 27131 net.cpp:96] Setting up relu2
I1027 18:49:19.666909 27131 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 18:49:19.666923 27131 net.cpp:67] Creating Layer drop2
I1027 18:49:19.666931 27131 net.cpp:394] drop2 <- pool2
I1027 18:49:19.666944 27131 net.cpp:345] drop2 -> pool2 (in-place)
I1027 18:49:19.666965 27131 net.cpp:96] Setting up drop2
I1027 18:49:19.666978 27131 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 18:49:19.666997 27131 net.cpp:67] Creating Layer conv3
I1027 18:49:19.667009 27131 net.cpp:394] conv3 <- pool2
I1027 18:49:19.667026 27131 net.cpp:356] conv3 -> conv3
I1027 18:49:19.667044 27131 net.cpp:96] Setting up conv3
I1027 18:49:19.670727 27131 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 18:49:19.670771 27131 net.cpp:67] Creating Layer pool3
I1027 18:49:19.670785 27131 net.cpp:394] pool3 <- conv3
I1027 18:49:19.670802 27131 net.cpp:356] pool3 -> pool3
I1027 18:49:19.670821 27131 net.cpp:96] Setting up pool3
I1027 18:49:19.670836 27131 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 18:49:19.670852 27131 net.cpp:67] Creating Layer relu3
I1027 18:49:19.670863 27131 net.cpp:394] relu3 <- pool3
I1027 18:49:19.670878 27131 net.cpp:345] relu3 -> pool3 (in-place)
I1027 18:49:19.670893 27131 net.cpp:96] Setting up relu3
I1027 18:49:19.670905 27131 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 18:49:19.670920 27131 net.cpp:67] Creating Layer drop3
I1027 18:49:19.670931 27131 net.cpp:394] drop3 <- pool3
I1027 18:49:19.670946 27131 net.cpp:345] drop3 -> pool3 (in-place)
I1027 18:49:19.670963 27131 net.cpp:96] Setting up drop3
I1027 18:49:19.670982 27131 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 18:49:19.671001 27131 net.cpp:67] Creating Layer ip1
I1027 18:49:19.671013 27131 net.cpp:394] ip1 <- pool3
I1027 18:49:19.671030 27131 net.cpp:356] ip1 -> ip1
I1027 18:49:19.671049 27131 net.cpp:96] Setting up ip1
I1027 18:49:20.109580 27131 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 18:49:20.109649 27131 net.cpp:67] Creating Layer relu4
I1027 18:49:20.109658 27131 net.cpp:394] relu4 <- ip1
I1027 18:49:20.109671 27131 net.cpp:345] relu4 -> ip1 (in-place)
I1027 18:49:20.109683 27131 net.cpp:96] Setting up relu4
I1027 18:49:20.109688 27131 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 18:49:20.109697 27131 net.cpp:67] Creating Layer drop4
I1027 18:49:20.109702 27131 net.cpp:394] drop4 <- ip1
I1027 18:49:20.109709 27131 net.cpp:345] drop4 -> ip1 (in-place)
I1027 18:49:20.109717 27131 net.cpp:96] Setting up drop4
I1027 18:49:20.109724 27131 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 18:49:20.109735 27131 net.cpp:67] Creating Layer ip2
I1027 18:49:20.109740 27131 net.cpp:394] ip2 <- ip1
I1027 18:49:20.109748 27131 net.cpp:356] ip2 -> ip2
I1027 18:49:20.109765 27131 net.cpp:96] Setting up ip2
I1027 18:49:20.117390 27131 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 18:49:20.117457 27131 net.cpp:67] Creating Layer prob
I1027 18:49:20.117465 27131 net.cpp:394] prob <- ip2
I1027 18:49:20.117475 27131 net.cpp:356] prob -> prob
I1027 18:49:20.117486 27131 net.cpp:96] Setting up prob
I1027 18:49:20.117496 27131 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 18:49:20.117501 27131 net.cpp:172] prob does not need backward computation.
I1027 18:49:20.117506 27131 net.cpp:172] ip2 does not need backward computation.
I1027 18:49:20.117509 27131 net.cpp:172] drop4 does not need backward computation.
I1027 18:49:20.117513 27131 net.cpp:172] relu4 does not need backward computation.
I1027 18:49:20.117517 27131 net.cpp:172] ip1 does not need backward computation.
I1027 18:49:20.117522 27131 net.cpp:172] drop3 does not need backward computation.
I1027 18:49:20.117525 27131 net.cpp:172] relu3 does not need backward computation.
I1027 18:49:20.117529 27131 net.cpp:172] pool3 does not need backward computation.
I1027 18:49:20.117533 27131 net.cpp:172] conv3 does not need backward computation.
I1027 18:49:20.117537 27131 net.cpp:172] drop2 does not need backward computation.
I1027 18:49:20.117542 27131 net.cpp:172] relu2 does not need backward computation.
I1027 18:49:20.117545 27131 net.cpp:172] pool2 does not need backward computation.
I1027 18:49:20.117549 27131 net.cpp:172] conv2 does not need backward computation.
I1027 18:49:20.117553 27131 net.cpp:172] drop1 does not need backward computation.
I1027 18:49:20.117558 27131 net.cpp:172] relu1 does not need backward computation.
I1027 18:49:20.117561 27131 net.cpp:172] pool1 does not need backward computation.
I1027 18:49:20.117565 27131 net.cpp:172] conv1 does not need backward computation.
I1027 18:49:20.117569 27131 net.cpp:208] This network produces output prob
I1027 18:49:20.117585 27131 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 18:49:20.117595 27131 net.cpp:219] Network initialization done.
I1027 18:49:20.117599 27131 net.cpp:220] Memory required for data: 1837200
I1027 18:50:18.650926 27131 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 18:50:18.651518 27131 net.cpp:358] Input 0 -> data
I1027 18:50:18.651547 27131 net.cpp:67] Creating Layer conv1
I1027 18:50:18.651553 27131 net.cpp:394] conv1 <- data
I1027 18:50:18.651561 27131 net.cpp:356] conv1 -> conv1
I1027 18:50:18.651571 27131 net.cpp:96] Setting up conv1
I1027 18:50:18.651602 27131 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 18:50:18.651618 27131 net.cpp:67] Creating Layer pool1
I1027 18:50:18.651623 27131 net.cpp:394] pool1 <- conv1
I1027 18:50:18.651628 27131 net.cpp:356] pool1 -> pool1
I1027 18:50:18.651636 27131 net.cpp:96] Setting up pool1
I1027 18:50:18.651643 27131 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 18:50:18.651650 27131 net.cpp:67] Creating Layer relu1
I1027 18:50:18.651654 27131 net.cpp:394] relu1 <- pool1
I1027 18:50:18.651660 27131 net.cpp:345] relu1 -> pool1 (in-place)
I1027 18:50:18.651666 27131 net.cpp:96] Setting up relu1
I1027 18:50:18.651670 27131 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 18:50:18.651676 27131 net.cpp:67] Creating Layer drop1
I1027 18:50:18.651680 27131 net.cpp:394] drop1 <- pool1
I1027 18:50:18.651686 27131 net.cpp:345] drop1 -> pool1 (in-place)
I1027 18:50:18.651692 27131 net.cpp:96] Setting up drop1
I1027 18:50:18.651697 27131 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 18:50:18.651705 27131 net.cpp:67] Creating Layer conv2
I1027 18:50:18.651708 27131 net.cpp:394] conv2 <- pool1
I1027 18:50:18.651715 27131 net.cpp:356] conv2 -> conv2
I1027 18:50:18.651721 27131 net.cpp:96] Setting up conv2
I1027 18:50:18.652222 27131 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 18:50:18.652237 27131 net.cpp:67] Creating Layer pool2
I1027 18:50:18.652242 27131 net.cpp:394] pool2 <- conv2
I1027 18:50:18.652248 27131 net.cpp:356] pool2 -> pool2
I1027 18:50:18.652256 27131 net.cpp:96] Setting up pool2
I1027 18:50:18.652262 27131 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 18:50:18.652268 27131 net.cpp:67] Creating Layer relu2
I1027 18:50:18.652272 27131 net.cpp:394] relu2 <- pool2
I1027 18:50:18.652277 27131 net.cpp:345] relu2 -> pool2 (in-place)
I1027 18:50:18.652283 27131 net.cpp:96] Setting up relu2
I1027 18:50:18.652290 27131 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 18:50:18.652297 27131 net.cpp:67] Creating Layer drop2
I1027 18:50:18.652302 27131 net.cpp:394] drop2 <- pool2
I1027 18:50:18.652307 27131 net.cpp:345] drop2 -> pool2 (in-place)
I1027 18:50:18.652312 27131 net.cpp:96] Setting up drop2
I1027 18:50:18.652317 27131 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 18:50:18.652324 27131 net.cpp:67] Creating Layer conv3
I1027 18:50:18.652328 27131 net.cpp:394] conv3 <- pool2
I1027 18:50:18.652334 27131 net.cpp:356] conv3 -> conv3
I1027 18:50:18.652341 27131 net.cpp:96] Setting up conv3
I1027 18:50:18.654067 27131 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 18:50:18.654088 27131 net.cpp:67] Creating Layer pool3
I1027 18:50:18.654095 27131 net.cpp:394] pool3 <- conv3
I1027 18:50:18.654103 27131 net.cpp:356] pool3 -> pool3
I1027 18:50:18.654111 27131 net.cpp:96] Setting up pool3
I1027 18:50:18.654119 27131 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 18:50:18.654126 27131 net.cpp:67] Creating Layer relu3
I1027 18:50:18.654131 27131 net.cpp:394] relu3 <- pool3
I1027 18:50:18.654139 27131 net.cpp:345] relu3 -> pool3 (in-place)
I1027 18:50:18.654145 27131 net.cpp:96] Setting up relu3
I1027 18:50:18.654150 27131 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 18:50:18.654158 27131 net.cpp:67] Creating Layer drop3
I1027 18:50:18.654163 27131 net.cpp:394] drop3 <- pool3
I1027 18:50:18.654170 27131 net.cpp:345] drop3 -> pool3 (in-place)
I1027 18:50:18.654178 27131 net.cpp:96] Setting up drop3
I1027 18:50:18.654184 27131 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 18:50:18.654192 27131 net.cpp:67] Creating Layer ip1
I1027 18:50:18.654197 27131 net.cpp:394] ip1 <- pool3
I1027 18:50:18.654206 27131 net.cpp:356] ip1 -> ip1
I1027 18:50:18.654214 27131 net.cpp:96] Setting up ip1
I1027 18:50:19.063005 27131 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 18:50:19.063071 27131 net.cpp:67] Creating Layer relu4
I1027 18:50:19.063079 27131 net.cpp:394] relu4 <- ip1
I1027 18:50:19.063091 27131 net.cpp:345] relu4 -> ip1 (in-place)
I1027 18:50:19.063101 27131 net.cpp:96] Setting up relu4
I1027 18:50:19.063107 27131 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 18:50:19.063115 27131 net.cpp:67] Creating Layer drop4
I1027 18:50:19.063119 27131 net.cpp:394] drop4 <- ip1
I1027 18:50:19.063127 27131 net.cpp:345] drop4 -> ip1 (in-place)
I1027 18:50:19.063133 27131 net.cpp:96] Setting up drop4
I1027 18:50:19.063139 27131 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 18:50:19.063150 27131 net.cpp:67] Creating Layer ip2
I1027 18:50:19.063155 27131 net.cpp:394] ip2 <- ip1
I1027 18:50:19.063163 27131 net.cpp:356] ip2 -> ip2
I1027 18:50:19.063177 27131 net.cpp:96] Setting up ip2
I1027 18:50:19.070735 27131 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 18:50:19.070799 27131 net.cpp:67] Creating Layer prob
I1027 18:50:19.070807 27131 net.cpp:394] prob <- ip2
I1027 18:50:19.070817 27131 net.cpp:356] prob -> prob
I1027 18:50:19.070829 27131 net.cpp:96] Setting up prob
I1027 18:50:19.070838 27131 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 18:50:19.070843 27131 net.cpp:172] prob does not need backward computation.
I1027 18:50:19.070847 27131 net.cpp:172] ip2 does not need backward computation.
I1027 18:50:19.070852 27131 net.cpp:172] drop4 does not need backward computation.
I1027 18:50:19.070855 27131 net.cpp:172] relu4 does not need backward computation.
I1027 18:50:19.070859 27131 net.cpp:172] ip1 does not need backward computation.
I1027 18:50:19.070863 27131 net.cpp:172] drop3 does not need backward computation.
I1027 18:50:19.070868 27131 net.cpp:172] relu3 does not need backward computation.
I1027 18:50:19.070871 27131 net.cpp:172] pool3 does not need backward computation.
I1027 18:50:19.070874 27131 net.cpp:172] conv3 does not need backward computation.
I1027 18:50:19.070878 27131 net.cpp:172] drop2 does not need backward computation.
I1027 18:50:19.070883 27131 net.cpp:172] relu2 does not need backward computation.
I1027 18:50:19.070886 27131 net.cpp:172] pool2 does not need backward computation.
I1027 18:50:19.070900 27131 net.cpp:172] conv2 does not need backward computation.
I1027 18:50:19.070905 27131 net.cpp:172] drop1 does not need backward computation.
I1027 18:50:19.070909 27131 net.cpp:172] relu1 does not need backward computation.
I1027 18:50:19.070914 27131 net.cpp:172] pool1 does not need backward computation.
I1027 18:50:19.070917 27131 net.cpp:172] conv1 does not need backward computation.
I1027 18:50:19.070921 27131 net.cpp:208] This network produces output prob
I1027 18:50:19.070935 27131 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 18:50:19.070945 27131 net.cpp:219] Network initialization done.
I1027 18:50:19.070950 27131 net.cpp:220] Memory required for data: 1837200
I1027 19:45:06.618705 23122 convert_imageset.cpp:70] Shuffling data
I1027 19:45:07.358952 23122 convert_imageset.cpp:73] A total of 60000 images.
I1027 19:45:07.359033 23122 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1027 19:45:09.460032 23122 convert_imageset.cpp:177] Processed 1000 files.
E1027 19:45:11.661025 23122 convert_imageset.cpp:177] Processed 2000 files.
E1027 19:45:13.571817 23122 convert_imageset.cpp:177] Processed 3000 files.
E1027 19:45:15.472954 23122 convert_imageset.cpp:177] Processed 4000 files.
E1027 19:45:17.308310 23122 convert_imageset.cpp:177] Processed 5000 files.
E1027 19:45:19.303182 23122 convert_imageset.cpp:177] Processed 6000 files.
E1027 19:45:21.085259 23122 convert_imageset.cpp:177] Processed 7000 files.
E1027 19:45:22.848975 23122 convert_imageset.cpp:177] Processed 8000 files.
E1027 19:45:24.843561 23122 convert_imageset.cpp:177] Processed 9000 files.
E1027 19:45:26.795145 23122 convert_imageset.cpp:177] Processed 10000 files.
E1027 19:45:28.456018 23122 convert_imageset.cpp:177] Processed 11000 files.
E1027 19:45:30.277040 23122 convert_imageset.cpp:177] Processed 12000 files.
E1027 19:45:31.954876 23122 convert_imageset.cpp:177] Processed 13000 files.
E1027 19:45:33.738241 23122 convert_imageset.cpp:177] Processed 14000 files.
E1027 19:45:35.301503 23122 convert_imageset.cpp:177] Processed 15000 files.
E1027 19:45:36.974911 23122 convert_imageset.cpp:177] Processed 16000 files.
E1027 19:45:38.585280 23122 convert_imageset.cpp:177] Processed 17000 files.
E1027 19:45:40.272233 23122 convert_imageset.cpp:177] Processed 18000 files.
E1027 19:45:41.858990 23122 convert_imageset.cpp:177] Processed 19000 files.
E1027 19:45:43.421713 23122 convert_imageset.cpp:177] Processed 20000 files.
E1027 19:45:45.086112 23122 convert_imageset.cpp:177] Processed 21000 files.
E1027 19:45:46.716892 23122 convert_imageset.cpp:177] Processed 22000 files.
E1027 19:45:48.364575 23122 convert_imageset.cpp:177] Processed 23000 files.
E1027 19:45:49.868312 23122 convert_imageset.cpp:177] Processed 24000 files.
E1027 19:45:51.518754 23122 convert_imageset.cpp:177] Processed 25000 files.
E1027 19:45:53.034914 23122 convert_imageset.cpp:177] Processed 26000 files.
E1027 19:45:54.607460 23122 convert_imageset.cpp:177] Processed 27000 files.
E1027 19:45:56.260387 23122 convert_imageset.cpp:177] Processed 28000 files.
E1027 19:45:57.778362 23122 convert_imageset.cpp:177] Processed 29000 files.
E1027 19:45:59.267910 23122 convert_imageset.cpp:177] Processed 30000 files.
E1027 19:46:00.779464 23122 convert_imageset.cpp:177] Processed 31000 files.
E1027 19:46:02.296977 23122 convert_imageset.cpp:177] Processed 32000 files.
E1027 19:46:03.886421 23122 convert_imageset.cpp:177] Processed 33000 files.
E1027 19:46:05.394588 23122 convert_imageset.cpp:177] Processed 34000 files.
E1027 19:46:06.898319 23122 convert_imageset.cpp:177] Processed 35000 files.
E1027 19:46:08.425051 23122 convert_imageset.cpp:177] Processed 36000 files.
E1027 19:46:10.008950 23122 convert_imageset.cpp:177] Processed 37000 files.
E1027 19:46:11.477854 23122 convert_imageset.cpp:177] Processed 38000 files.
E1027 19:46:12.996598 23122 convert_imageset.cpp:177] Processed 39000 files.
E1027 19:46:14.497851 23122 convert_imageset.cpp:177] Processed 40000 files.
E1027 19:46:15.959722 23122 convert_imageset.cpp:177] Processed 41000 files.
E1027 19:46:17.548213 23122 convert_imageset.cpp:177] Processed 42000 files.
E1027 19:46:19.041895 23122 convert_imageset.cpp:177] Processed 43000 files.
E1027 19:46:20.591897 23122 convert_imageset.cpp:177] Processed 44000 files.
E1027 19:46:22.184763 23122 convert_imageset.cpp:177] Processed 45000 files.
E1027 19:46:23.791843 23122 convert_imageset.cpp:177] Processed 46000 files.
E1027 19:46:25.292415 23122 convert_imageset.cpp:177] Processed 47000 files.
E1027 19:46:26.683537 23122 convert_imageset.cpp:177] Processed 48000 files.
E1027 19:46:28.131343 23122 convert_imageset.cpp:177] Processed 49000 files.
E1027 19:46:29.695170 23122 convert_imageset.cpp:177] Processed 50000 files.
E1027 19:46:31.401041 23122 convert_imageset.cpp:177] Processed 51000 files.
E1027 19:46:33.016149 23122 convert_imageset.cpp:177] Processed 52000 files.
E1027 19:46:34.560477 23122 convert_imageset.cpp:177] Processed 53000 files.
E1027 19:46:36.228039 23122 convert_imageset.cpp:177] Processed 54000 files.
E1027 19:46:37.781829 23122 convert_imageset.cpp:177] Processed 55000 files.
E1027 19:46:39.412019 23122 convert_imageset.cpp:177] Processed 56000 files.
E1027 19:46:40.916980 23122 convert_imageset.cpp:177] Processed 57000 files.
E1027 19:46:42.382941 23122 convert_imageset.cpp:177] Processed 58000 files.
E1027 19:46:43.864506 23122 convert_imageset.cpp:177] Processed 59000 files.
E1027 19:46:45.367671 23122 convert_imageset.cpp:177] Processed 60000 files.
I1027 19:46:45.611824 23769 caffe.cpp:99] Use GPU with device ID 0
I1027 19:46:46.078572 23769 caffe.cpp:107] Starting Optimization
I1027 19:46:46.078694 23769 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 320000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1027 19:46:46.078719 23769 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1027 19:46:46.119464 23769 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1027 19:46:46.119571 23769 net.cpp:67] Creating Layer mnist
I1027 19:46:46.119582 23769 net.cpp:356] mnist -> data
I1027 19:46:46.119601 23769 net.cpp:356] mnist -> label
I1027 19:46:46.119614 23769 net.cpp:96] Setting up mnist
I1027 19:46:46.126679 23769 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1027 19:46:46.126823 23769 data_layer.cpp:128] output data size: 64,1,50,180
I1027 19:46:46.128597 23769 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1027 19:46:46.128633 23769 net.cpp:103] Top shape: 64 1 1 1 (64)
I1027 19:46:46.128665 23769 net.cpp:67] Creating Layer conv1
I1027 19:46:46.128679 23769 net.cpp:394] conv1 <- data
I1027 19:46:46.128713 23769 net.cpp:356] conv1 -> conv1
I1027 19:46:46.128737 23769 net.cpp:96] Setting up conv1
I1027 19:46:46.129117 23769 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1027 19:46:46.129150 23769 net.cpp:67] Creating Layer pool1
I1027 19:46:46.129156 23769 net.cpp:394] pool1 <- conv1
I1027 19:46:46.129168 23769 net.cpp:356] pool1 -> pool1
I1027 19:46:46.129175 23769 net.cpp:96] Setting up pool1
I1027 19:46:46.129191 23769 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 19:46:46.129199 23769 net.cpp:67] Creating Layer relu1
I1027 19:46:46.129202 23769 net.cpp:394] relu1 <- pool1
I1027 19:46:46.129209 23769 net.cpp:345] relu1 -> pool1 (in-place)
I1027 19:46:46.129215 23769 net.cpp:96] Setting up relu1
I1027 19:46:46.129220 23769 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 19:46:46.129226 23769 net.cpp:67] Creating Layer drop1
I1027 19:46:46.129231 23769 net.cpp:394] drop1 <- pool1
I1027 19:46:46.129240 23769 net.cpp:345] drop1 -> pool1 (in-place)
I1027 19:46:46.129246 23769 net.cpp:96] Setting up drop1
I1027 19:46:46.129252 23769 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 19:46:46.129259 23769 net.cpp:67] Creating Layer conv2
I1027 19:46:46.129263 23769 net.cpp:394] conv2 <- pool1
I1027 19:46:46.129271 23769 net.cpp:356] conv2 -> conv2
I1027 19:46:46.129279 23769 net.cpp:96] Setting up conv2
I1027 19:46:46.129871 23769 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1027 19:46:46.129890 23769 net.cpp:67] Creating Layer pool2
I1027 19:46:46.129896 23769 net.cpp:394] pool2 <- conv2
I1027 19:46:46.129902 23769 net.cpp:356] pool2 -> pool2
I1027 19:46:46.129909 23769 net.cpp:96] Setting up pool2
I1027 19:46:46.129915 23769 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 19:46:46.129921 23769 net.cpp:67] Creating Layer relu2
I1027 19:46:46.129925 23769 net.cpp:394] relu2 <- pool2
I1027 19:46:46.129933 23769 net.cpp:345] relu2 -> pool2 (in-place)
I1027 19:46:46.129940 23769 net.cpp:96] Setting up relu2
I1027 19:46:46.129945 23769 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 19:46:46.129951 23769 net.cpp:67] Creating Layer drop2
I1027 19:46:46.129956 23769 net.cpp:394] drop2 <- pool2
I1027 19:46:46.129968 23769 net.cpp:345] drop2 -> pool2 (in-place)
I1027 19:46:46.129977 23769 net.cpp:96] Setting up drop2
I1027 19:46:46.129982 23769 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 19:46:46.129990 23769 net.cpp:67] Creating Layer conv3
I1027 19:46:46.129994 23769 net.cpp:394] conv3 <- pool2
I1027 19:46:46.130002 23769 net.cpp:356] conv3 -> conv3
I1027 19:46:46.130007 23769 net.cpp:96] Setting up conv3
I1027 19:46:46.133067 23769 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1027 19:46:46.133117 23769 net.cpp:67] Creating Layer pool3
I1027 19:46:46.133131 23769 net.cpp:394] pool3 <- conv3
I1027 19:46:46.133154 23769 net.cpp:356] pool3 -> pool3
I1027 19:46:46.133175 23769 net.cpp:96] Setting up pool3
I1027 19:46:46.133191 23769 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 19:46:46.133208 23769 net.cpp:67] Creating Layer relu3
I1027 19:46:46.133220 23769 net.cpp:394] relu3 <- pool3
I1027 19:46:46.133239 23769 net.cpp:345] relu3 -> pool3 (in-place)
I1027 19:46:46.133257 23769 net.cpp:96] Setting up relu3
I1027 19:46:46.133270 23769 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 19:46:46.133286 23769 net.cpp:67] Creating Layer drop3
I1027 19:46:46.133298 23769 net.cpp:394] drop3 <- pool3
I1027 19:46:46.133314 23769 net.cpp:345] drop3 -> pool3 (in-place)
I1027 19:46:46.133332 23769 net.cpp:96] Setting up drop3
I1027 19:46:46.133344 23769 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 19:46:46.133363 23769 net.cpp:67] Creating Layer ip1
I1027 19:46:46.133375 23769 net.cpp:394] ip1 <- pool3
I1027 19:46:46.133396 23769 net.cpp:356] ip1 -> ip1
I1027 19:46:46.133465 23769 net.cpp:96] Setting up ip1
I1027 19:46:46.677583 23769 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 19:46:46.677642 23769 net.cpp:67] Creating Layer relu4
I1027 19:46:46.677650 23769 net.cpp:394] relu4 <- ip1
I1027 19:46:46.677659 23769 net.cpp:345] relu4 -> ip1 (in-place)
I1027 19:46:46.677669 23769 net.cpp:96] Setting up relu4
I1027 19:46:46.677673 23769 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 19:46:46.677680 23769 net.cpp:67] Creating Layer drop4
I1027 19:46:46.677685 23769 net.cpp:394] drop4 <- ip1
I1027 19:46:46.677692 23769 net.cpp:345] drop4 -> ip1 (in-place)
I1027 19:46:46.677700 23769 net.cpp:96] Setting up drop4
I1027 19:46:46.677705 23769 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 19:46:46.677714 23769 net.cpp:67] Creating Layer ip2
I1027 19:46:46.677719 23769 net.cpp:394] ip2 <- ip1
I1027 19:46:46.677727 23769 net.cpp:356] ip2 -> ip2
I1027 19:46:46.677736 23769 net.cpp:96] Setting up ip2
I1027 19:46:46.688676 23769 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1027 19:46:46.688745 23769 net.cpp:67] Creating Layer loss
I1027 19:46:46.688752 23769 net.cpp:394] loss <- ip2
I1027 19:46:46.688760 23769 net.cpp:394] loss <- label
I1027 19:46:46.688766 23769 net.cpp:356] loss -> loss
I1027 19:46:46.688776 23769 net.cpp:96] Setting up loss
I1027 19:46:46.688791 23769 net.cpp:103] Top shape: 1 1 1 1 (1)
I1027 19:46:46.688796 23769 net.cpp:109]     with loss weight 1
I1027 19:46:46.688832 23769 net.cpp:170] loss needs backward computation.
I1027 19:46:46.688837 23769 net.cpp:170] ip2 needs backward computation.
I1027 19:46:46.688840 23769 net.cpp:170] drop4 needs backward computation.
I1027 19:46:46.688845 23769 net.cpp:170] relu4 needs backward computation.
I1027 19:46:46.688849 23769 net.cpp:170] ip1 needs backward computation.
I1027 19:46:46.688853 23769 net.cpp:170] drop3 needs backward computation.
I1027 19:46:46.688858 23769 net.cpp:170] relu3 needs backward computation.
I1027 19:46:46.688863 23769 net.cpp:170] pool3 needs backward computation.
I1027 19:46:46.688868 23769 net.cpp:170] conv3 needs backward computation.
I1027 19:46:46.688871 23769 net.cpp:170] drop2 needs backward computation.
I1027 19:46:46.688876 23769 net.cpp:170] relu2 needs backward computation.
I1027 19:46:46.688880 23769 net.cpp:170] pool2 needs backward computation.
I1027 19:46:46.688885 23769 net.cpp:170] conv2 needs backward computation.
I1027 19:46:46.688889 23769 net.cpp:170] drop1 needs backward computation.
I1027 19:46:46.688901 23769 net.cpp:170] relu1 needs backward computation.
I1027 19:46:46.688906 23769 net.cpp:170] pool1 needs backward computation.
I1027 19:46:46.688911 23769 net.cpp:170] conv1 needs backward computation.
I1027 19:46:46.688916 23769 net.cpp:172] mnist does not need backward computation.
I1027 19:46:46.688920 23769 net.cpp:208] This network produces output loss
I1027 19:46:46.688931 23769 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 19:46:46.688938 23769 net.cpp:219] Network initialization done.
I1027 19:46:46.688942 23769 net.cpp:220] Memory required for data: 119788292
I1027 19:46:46.689003 23769 solver.cpp:41] Solver scaffolding done.
I1027 19:46:46.689009 23769 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_305000.solverstate
I1027 19:46:46.689013 23769 solver.cpp:160] Solving Captcha
I1027 19:46:46.689031 23769 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_305000.solverstate
I1027 19:46:53.031119 23769 solver.cpp:502] SGDSolver: restoring history
I1027 19:46:54.030418 23769 solver.cpp:191] Iteration 305000, loss = 2.465
I1027 19:46:54.030474 23769 solver.cpp:206]     Train net output #0: loss = 2.465 (* 1 = 2.465 loss)
I1027 19:46:54.030489 23769 solver.cpp:403] Iteration 305000, lr = 0.000752085
I1027 19:55:20.832065 23769 solver.cpp:191] Iteration 306000, loss = 2.43834
I1027 19:55:20.832713 23769 solver.cpp:206]     Train net output #0: loss = 2.43834 (* 1 = 2.43834 loss)
I1027 19:55:20.832751 23769 solver.cpp:403] Iteration 306000, lr = 0.0007503
I1027 20:01:56.286288 23769 solver.cpp:191] Iteration 307000, loss = 2.27293
I1027 20:01:56.287134 23769 solver.cpp:206]     Train net output #0: loss = 2.27293 (* 1 = 2.27293 loss)
I1027 20:01:56.287168 23769 solver.cpp:403] Iteration 307000, lr = 0.000748524
I1027 20:09:11.352278 23769 solver.cpp:191] Iteration 308000, loss = 2.5707
I1027 20:09:11.352918 23769 solver.cpp:206]     Train net output #0: loss = 2.5707 (* 1 = 2.5707 loss)
I1027 20:09:11.352952 23769 solver.cpp:403] Iteration 308000, lr = 0.000746758
I1027 20:17:37.066270 23769 solver.cpp:191] Iteration 309000, loss = 2.49864
I1027 20:17:37.066903 23769 solver.cpp:206]     Train net output #0: loss = 2.49864 (* 1 = 2.49864 loss)
I1027 20:17:37.066941 23769 solver.cpp:403] Iteration 309000, lr = 0.000745001
I1027 20:26:05.853327 23769 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_310000.caffemodel
I1027 20:26:10.365175 23769 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_310000.solverstate
I1027 20:26:14.170843 23769 solver.cpp:191] Iteration 310000, loss = 2.29598
I1027 20:26:14.171341 23769 solver.cpp:206]     Train net output #0: loss = 2.29598 (* 1 = 2.29598 loss)
I1027 20:26:14.171380 23769 solver.cpp:403] Iteration 310000, lr = 0.000743254
I1027 20:34:40.303719 23769 solver.cpp:191] Iteration 311000, loss = 2.3919
I1027 20:34:40.304468 23769 solver.cpp:206]     Train net output #0: loss = 2.3919 (* 1 = 2.3919 loss)
I1027 20:34:40.304503 23769 solver.cpp:403] Iteration 311000, lr = 0.000741517
I1027 20:43:06.163382 23769 solver.cpp:191] Iteration 312000, loss = 2.42294
I1027 20:43:06.163913 23769 solver.cpp:206]     Train net output #0: loss = 2.42294 (* 1 = 2.42294 loss)
I1027 20:43:06.163950 23769 solver.cpp:403] Iteration 312000, lr = 0.000739789
I1027 20:51:32.455024 23769 solver.cpp:191] Iteration 313000, loss = 2.53736
I1027 20:51:32.455595 23769 solver.cpp:206]     Train net output #0: loss = 2.53736 (* 1 = 2.53736 loss)
I1027 20:51:32.455633 23769 solver.cpp:403] Iteration 313000, lr = 0.000738071
I1027 20:59:58.031997 23769 solver.cpp:191] Iteration 314000, loss = 2.48127
I1027 20:59:58.032677 23769 solver.cpp:206]     Train net output #0: loss = 2.48127 (* 1 = 2.48127 loss)
I1027 20:59:58.032698 23769 solver.cpp:403] Iteration 314000, lr = 0.000736362
I1027 21:08:24.290921 23769 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_315000.caffemodel
I1027 21:08:29.111923 23769 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_315000.solverstate
I1027 21:08:33.427579 23769 solver.cpp:191] Iteration 315000, loss = 2.38304
I1027 21:08:33.428263 23769 solver.cpp:206]     Train net output #0: loss = 2.38304 (* 1 = 2.38304 loss)
I1027 21:08:33.428298 23769 solver.cpp:403] Iteration 315000, lr = 0.000734662
I1027 21:17:00.605840 23769 solver.cpp:191] Iteration 316000, loss = 2.48078
I1027 21:17:00.606453 23769 solver.cpp:206]     Train net output #0: loss = 2.48078 (* 1 = 2.48078 loss)
I1027 21:17:00.606487 23769 solver.cpp:403] Iteration 316000, lr = 0.000732971
I1027 21:25:27.432374 23769 solver.cpp:191] Iteration 317000, loss = 2.59466
I1027 21:25:27.432955 23769 solver.cpp:206]     Train net output #0: loss = 2.59466 (* 1 = 2.59466 loss)
I1027 21:25:27.432986 23769 solver.cpp:403] Iteration 317000, lr = 0.000731289
I1027 21:33:18.028033 23769 solver.cpp:191] Iteration 318000, loss = 2.28449
I1027 21:33:18.029323 23769 solver.cpp:206]     Train net output #0: loss = 2.28449 (* 1 = 2.28449 loss)
I1027 21:33:18.029356 23769 solver.cpp:403] Iteration 318000, lr = 0.000729616
I1027 21:37:19.376991 23769 solver.cpp:191] Iteration 319000, loss = 2.44174
I1027 21:37:19.377601 23769 solver.cpp:206]     Train net output #0: loss = 2.44174 (* 1 = 2.44174 loss)
I1027 21:37:19.377638 23769 solver.cpp:403] Iteration 319000, lr = 0.000727953
I1027 21:41:21.159662 23769 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_320000.caffemodel
I1027 21:41:25.503269 23769 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_320000.solverstate
I1027 21:41:29.162881 23769 solver.cpp:228] Iteration 320000, loss = 2.3089
I1027 21:41:29.163413 23769 solver.cpp:233] Optimization Done.
I1027 21:41:29.163435 23769 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1027 22:04:03.477190 25546 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 22:04:03.477725 25546 net.cpp:358] Input 0 -> data
I1027 22:04:03.477756 25546 net.cpp:67] Creating Layer conv1
I1027 22:04:03.477761 25546 net.cpp:394] conv1 <- data
I1027 22:04:03.477768 25546 net.cpp:356] conv1 -> conv1
I1027 22:04:03.477778 25546 net.cpp:96] Setting up conv1
I1027 22:04:03.478102 25546 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 22:04:03.478121 25546 net.cpp:67] Creating Layer pool1
I1027 22:04:03.478127 25546 net.cpp:394] pool1 <- conv1
I1027 22:04:03.478133 25546 net.cpp:356] pool1 -> pool1
I1027 22:04:03.478142 25546 net.cpp:96] Setting up pool1
I1027 22:04:03.478159 25546 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 22:04:03.478166 25546 net.cpp:67] Creating Layer relu1
I1027 22:04:03.478170 25546 net.cpp:394] relu1 <- pool1
I1027 22:04:03.478175 25546 net.cpp:345] relu1 -> pool1 (in-place)
I1027 22:04:03.478181 25546 net.cpp:96] Setting up relu1
I1027 22:04:03.478185 25546 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 22:04:03.478193 25546 net.cpp:67] Creating Layer drop1
I1027 22:04:03.478199 25546 net.cpp:394] drop1 <- pool1
I1027 22:04:03.478204 25546 net.cpp:345] drop1 -> pool1 (in-place)
I1027 22:04:03.478209 25546 net.cpp:96] Setting up drop1
I1027 22:04:03.478214 25546 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 22:04:03.478221 25546 net.cpp:67] Creating Layer conv2
I1027 22:04:03.478225 25546 net.cpp:394] conv2 <- pool1
I1027 22:04:03.478231 25546 net.cpp:356] conv2 -> conv2
I1027 22:04:03.478237 25546 net.cpp:96] Setting up conv2
I1027 22:04:03.478792 25546 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 22:04:03.478807 25546 net.cpp:67] Creating Layer pool2
I1027 22:04:03.478811 25546 net.cpp:394] pool2 <- conv2
I1027 22:04:03.478817 25546 net.cpp:356] pool2 -> pool2
I1027 22:04:03.478824 25546 net.cpp:96] Setting up pool2
I1027 22:04:03.478831 25546 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 22:04:03.478837 25546 net.cpp:67] Creating Layer relu2
I1027 22:04:03.478842 25546 net.cpp:394] relu2 <- pool2
I1027 22:04:03.478847 25546 net.cpp:345] relu2 -> pool2 (in-place)
I1027 22:04:03.478852 25546 net.cpp:96] Setting up relu2
I1027 22:04:03.478857 25546 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 22:04:03.478863 25546 net.cpp:67] Creating Layer drop2
I1027 22:04:03.478868 25546 net.cpp:394] drop2 <- pool2
I1027 22:04:03.478873 25546 net.cpp:345] drop2 -> pool2 (in-place)
I1027 22:04:03.478878 25546 net.cpp:96] Setting up drop2
I1027 22:04:03.478883 25546 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 22:04:03.478889 25546 net.cpp:67] Creating Layer conv3
I1027 22:04:03.478894 25546 net.cpp:394] conv3 <- pool2
I1027 22:04:03.478901 25546 net.cpp:356] conv3 -> conv3
I1027 22:04:03.478909 25546 net.cpp:96] Setting up conv3
I1027 22:04:03.480367 25546 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 22:04:03.480384 25546 net.cpp:67] Creating Layer pool3
I1027 22:04:03.480389 25546 net.cpp:394] pool3 <- conv3
I1027 22:04:03.480396 25546 net.cpp:356] pool3 -> pool3
I1027 22:04:03.480401 25546 net.cpp:96] Setting up pool3
I1027 22:04:03.480407 25546 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 22:04:03.480414 25546 net.cpp:67] Creating Layer relu3
I1027 22:04:03.480453 25546 net.cpp:394] relu3 <- pool3
I1027 22:04:03.480460 25546 net.cpp:345] relu3 -> pool3 (in-place)
I1027 22:04:03.480468 25546 net.cpp:96] Setting up relu3
I1027 22:04:03.480471 25546 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 22:04:03.480481 25546 net.cpp:67] Creating Layer drop3
I1027 22:04:03.480486 25546 net.cpp:394] drop3 <- pool3
I1027 22:04:03.480491 25546 net.cpp:345] drop3 -> pool3 (in-place)
I1027 22:04:03.480497 25546 net.cpp:96] Setting up drop3
I1027 22:04:03.480502 25546 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 22:04:03.480511 25546 net.cpp:67] Creating Layer ip1
I1027 22:04:03.480516 25546 net.cpp:394] ip1 <- pool3
I1027 22:04:03.480522 25546 net.cpp:356] ip1 -> ip1
I1027 22:04:03.480530 25546 net.cpp:96] Setting up ip1
I1027 22:04:04.024740 25546 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 22:04:04.024799 25546 net.cpp:67] Creating Layer relu4
I1027 22:04:04.024806 25546 net.cpp:394] relu4 <- ip1
I1027 22:04:04.024816 25546 net.cpp:345] relu4 -> ip1 (in-place)
I1027 22:04:04.024824 25546 net.cpp:96] Setting up relu4
I1027 22:04:04.024830 25546 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 22:04:04.024837 25546 net.cpp:67] Creating Layer drop4
I1027 22:04:04.024842 25546 net.cpp:394] drop4 <- ip1
I1027 22:04:04.024849 25546 net.cpp:345] drop4 -> ip1 (in-place)
I1027 22:04:04.024855 25546 net.cpp:96] Setting up drop4
I1027 22:04:04.024862 25546 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 22:04:04.024869 25546 net.cpp:67] Creating Layer ip2
I1027 22:04:04.024873 25546 net.cpp:394] ip2 <- ip1
I1027 22:04:04.024881 25546 net.cpp:356] ip2 -> ip2
I1027 22:04:04.024893 25546 net.cpp:96] Setting up ip2
I1027 22:04:04.034767 25546 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 22:04:04.034837 25546 net.cpp:67] Creating Layer prob
I1027 22:04:04.034845 25546 net.cpp:394] prob <- ip2
I1027 22:04:04.034853 25546 net.cpp:356] prob -> prob
I1027 22:04:04.034864 25546 net.cpp:96] Setting up prob
I1027 22:04:04.034870 25546 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 22:04:04.034874 25546 net.cpp:172] prob does not need backward computation.
I1027 22:04:04.034878 25546 net.cpp:172] ip2 does not need backward computation.
I1027 22:04:04.034883 25546 net.cpp:172] drop4 does not need backward computation.
I1027 22:04:04.034886 25546 net.cpp:172] relu4 does not need backward computation.
I1027 22:04:04.034889 25546 net.cpp:172] ip1 does not need backward computation.
I1027 22:04:04.034893 25546 net.cpp:172] drop3 does not need backward computation.
I1027 22:04:04.034898 25546 net.cpp:172] relu3 does not need backward computation.
I1027 22:04:04.034900 25546 net.cpp:172] pool3 does not need backward computation.
I1027 22:04:04.034904 25546 net.cpp:172] conv3 does not need backward computation.
I1027 22:04:04.034909 25546 net.cpp:172] drop2 does not need backward computation.
I1027 22:04:04.034911 25546 net.cpp:172] relu2 does not need backward computation.
I1027 22:04:04.034915 25546 net.cpp:172] pool2 does not need backward computation.
I1027 22:04:04.034919 25546 net.cpp:172] conv2 does not need backward computation.
I1027 22:04:04.034922 25546 net.cpp:172] drop1 does not need backward computation.
I1027 22:04:04.034926 25546 net.cpp:172] relu1 does not need backward computation.
I1027 22:04:04.034930 25546 net.cpp:172] pool1 does not need backward computation.
I1027 22:04:04.034934 25546 net.cpp:172] conv1 does not need backward computation.
I1027 22:04:04.034937 25546 net.cpp:208] This network produces output prob
I1027 22:04:04.034950 25546 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 22:04:04.034958 25546 net.cpp:219] Network initialization done.
I1027 22:04:04.034961 25546 net.cpp:220] Memory required for data: 1837200
I1027 22:04:48.863580 25546 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 22:04:48.864140 25546 net.cpp:358] Input 0 -> data
I1027 22:04:48.864197 25546 net.cpp:67] Creating Layer conv1
I1027 22:04:48.864212 25546 net.cpp:394] conv1 <- data
I1027 22:04:48.864229 25546 net.cpp:356] conv1 -> conv1
I1027 22:04:48.864253 25546 net.cpp:96] Setting up conv1
I1027 22:04:48.864318 25546 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 22:04:48.864354 25546 net.cpp:67] Creating Layer pool1
I1027 22:04:48.864367 25546 net.cpp:394] pool1 <- conv1
I1027 22:04:48.864383 25546 net.cpp:356] pool1 -> pool1
I1027 22:04:48.864403 25546 net.cpp:96] Setting up pool1
I1027 22:04:48.864464 25546 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 22:04:48.864500 25546 net.cpp:67] Creating Layer relu1
I1027 22:04:48.864513 25546 net.cpp:394] relu1 <- pool1
I1027 22:04:48.864528 25546 net.cpp:345] relu1 -> pool1 (in-place)
I1027 22:04:48.864547 25546 net.cpp:96] Setting up relu1
I1027 22:04:48.864559 25546 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 22:04:48.864574 25546 net.cpp:67] Creating Layer drop1
I1027 22:04:48.864585 25546 net.cpp:394] drop1 <- pool1
I1027 22:04:48.864600 25546 net.cpp:345] drop1 -> pool1 (in-place)
I1027 22:04:48.864616 25546 net.cpp:96] Setting up drop1
I1027 22:04:48.864630 25546 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 22:04:48.864650 25546 net.cpp:67] Creating Layer conv2
I1027 22:04:48.864660 25546 net.cpp:394] conv2 <- pool1
I1027 22:04:48.864678 25546 net.cpp:356] conv2 -> conv2
I1027 22:04:48.864697 25546 net.cpp:96] Setting up conv2
I1027 22:04:48.866088 25546 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 22:04:48.866124 25546 net.cpp:67] Creating Layer pool2
I1027 22:04:48.866145 25546 net.cpp:394] pool2 <- conv2
I1027 22:04:48.866163 25546 net.cpp:356] pool2 -> pool2
I1027 22:04:48.866183 25546 net.cpp:96] Setting up pool2
I1027 22:04:48.866199 25546 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 22:04:48.866214 25546 net.cpp:67] Creating Layer relu2
I1027 22:04:48.866226 25546 net.cpp:394] relu2 <- pool2
I1027 22:04:48.866240 25546 net.cpp:345] relu2 -> pool2 (in-place)
I1027 22:04:48.866256 25546 net.cpp:96] Setting up relu2
I1027 22:04:48.866268 25546 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 22:04:48.866283 25546 net.cpp:67] Creating Layer drop2
I1027 22:04:48.866294 25546 net.cpp:394] drop2 <- pool2
I1027 22:04:48.866309 25546 net.cpp:345] drop2 -> pool2 (in-place)
I1027 22:04:48.866324 25546 net.cpp:96] Setting up drop2
I1027 22:04:48.866338 25546 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 22:04:48.866358 25546 net.cpp:67] Creating Layer conv3
I1027 22:04:48.866369 25546 net.cpp:394] conv3 <- pool2
I1027 22:04:48.866386 25546 net.cpp:356] conv3 -> conv3
I1027 22:04:48.866405 25546 net.cpp:96] Setting up conv3
I1027 22:04:48.870093 25546 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 22:04:48.870136 25546 net.cpp:67] Creating Layer pool3
I1027 22:04:48.870149 25546 net.cpp:394] pool3 <- conv3
I1027 22:04:48.870167 25546 net.cpp:356] pool3 -> pool3
I1027 22:04:48.870184 25546 net.cpp:96] Setting up pool3
I1027 22:04:48.870200 25546 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 22:04:48.870215 25546 net.cpp:67] Creating Layer relu3
I1027 22:04:48.870226 25546 net.cpp:394] relu3 <- pool3
I1027 22:04:48.870241 25546 net.cpp:345] relu3 -> pool3 (in-place)
I1027 22:04:48.870257 25546 net.cpp:96] Setting up relu3
I1027 22:04:48.870270 25546 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 22:04:48.870285 25546 net.cpp:67] Creating Layer drop3
I1027 22:04:48.870296 25546 net.cpp:394] drop3 <- pool3
I1027 22:04:48.870311 25546 net.cpp:345] drop3 -> pool3 (in-place)
I1027 22:04:48.870326 25546 net.cpp:96] Setting up drop3
I1027 22:04:48.870339 25546 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 22:04:48.870357 25546 net.cpp:67] Creating Layer ip1
I1027 22:04:48.870368 25546 net.cpp:394] ip1 <- pool3
I1027 22:04:48.870384 25546 net.cpp:356] ip1 -> ip1
I1027 22:04:48.870404 25546 net.cpp:96] Setting up ip1
I1027 22:04:49.280383 25546 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 22:04:49.280455 25546 net.cpp:67] Creating Layer relu4
I1027 22:04:49.280464 25546 net.cpp:394] relu4 <- ip1
I1027 22:04:49.280475 25546 net.cpp:345] relu4 -> ip1 (in-place)
I1027 22:04:49.280485 25546 net.cpp:96] Setting up relu4
I1027 22:04:49.280490 25546 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 22:04:49.280498 25546 net.cpp:67] Creating Layer drop4
I1027 22:04:49.280503 25546 net.cpp:394] drop4 <- ip1
I1027 22:04:49.280509 25546 net.cpp:345] drop4 -> ip1 (in-place)
I1027 22:04:49.280516 25546 net.cpp:96] Setting up drop4
I1027 22:04:49.280521 25546 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 22:04:49.280532 25546 net.cpp:67] Creating Layer ip2
I1027 22:04:49.280536 25546 net.cpp:394] ip2 <- ip1
I1027 22:04:49.280544 25546 net.cpp:356] ip2 -> ip2
I1027 22:04:49.280558 25546 net.cpp:96] Setting up ip2
I1027 22:04:49.288288 25546 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 22:04:49.288352 25546 net.cpp:67] Creating Layer prob
I1027 22:04:49.288362 25546 net.cpp:394] prob <- ip2
I1027 22:04:49.288370 25546 net.cpp:356] prob -> prob
I1027 22:04:49.288382 25546 net.cpp:96] Setting up prob
I1027 22:04:49.288390 25546 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 22:04:49.288394 25546 net.cpp:172] prob does not need backward computation.
I1027 22:04:49.288399 25546 net.cpp:172] ip2 does not need backward computation.
I1027 22:04:49.288403 25546 net.cpp:172] drop4 does not need backward computation.
I1027 22:04:49.288408 25546 net.cpp:172] relu4 does not need backward computation.
I1027 22:04:49.288411 25546 net.cpp:172] ip1 does not need backward computation.
I1027 22:04:49.288415 25546 net.cpp:172] drop3 does not need backward computation.
I1027 22:04:49.288437 25546 net.cpp:172] relu3 does not need backward computation.
I1027 22:04:49.288455 25546 net.cpp:172] pool3 does not need backward computation.
I1027 22:04:49.288460 25546 net.cpp:172] conv3 does not need backward computation.
I1027 22:04:49.288465 25546 net.cpp:172] drop2 does not need backward computation.
I1027 22:04:49.288468 25546 net.cpp:172] relu2 does not need backward computation.
I1027 22:04:49.288472 25546 net.cpp:172] pool2 does not need backward computation.
I1027 22:04:49.288476 25546 net.cpp:172] conv2 does not need backward computation.
I1027 22:04:49.288480 25546 net.cpp:172] drop1 does not need backward computation.
I1027 22:04:49.288485 25546 net.cpp:172] relu1 does not need backward computation.
I1027 22:04:49.288488 25546 net.cpp:172] pool1 does not need backward computation.
I1027 22:04:49.288493 25546 net.cpp:172] conv1 does not need backward computation.
I1027 22:04:49.288497 25546 net.cpp:208] This network produces output prob
I1027 22:04:49.288513 25546 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 22:04:49.288523 25546 net.cpp:219] Network initialization done.
I1027 22:04:49.288527 25546 net.cpp:220] Memory required for data: 1837200
I1027 22:05:26.193027 25546 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1027 22:05:26.193558 25546 net.cpp:358] Input 0 -> data
I1027 22:05:26.193588 25546 net.cpp:67] Creating Layer conv1
I1027 22:05:26.193598 25546 net.cpp:394] conv1 <- data
I1027 22:05:26.193605 25546 net.cpp:356] conv1 -> conv1
I1027 22:05:26.193616 25546 net.cpp:96] Setting up conv1
I1027 22:05:26.193647 25546 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1027 22:05:26.193663 25546 net.cpp:67] Creating Layer pool1
I1027 22:05:26.193670 25546 net.cpp:394] pool1 <- conv1
I1027 22:05:26.193675 25546 net.cpp:356] pool1 -> pool1
I1027 22:05:26.193682 25546 net.cpp:96] Setting up pool1
I1027 22:05:26.193691 25546 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 22:05:26.193696 25546 net.cpp:67] Creating Layer relu1
I1027 22:05:26.193701 25546 net.cpp:394] relu1 <- pool1
I1027 22:05:26.193706 25546 net.cpp:345] relu1 -> pool1 (in-place)
I1027 22:05:26.193712 25546 net.cpp:96] Setting up relu1
I1027 22:05:26.193717 25546 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 22:05:26.193723 25546 net.cpp:67] Creating Layer drop1
I1027 22:05:26.193727 25546 net.cpp:394] drop1 <- pool1
I1027 22:05:26.193732 25546 net.cpp:345] drop1 -> pool1 (in-place)
I1027 22:05:26.193738 25546 net.cpp:96] Setting up drop1
I1027 22:05:26.193743 25546 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1027 22:05:26.193750 25546 net.cpp:67] Creating Layer conv2
I1027 22:05:26.193755 25546 net.cpp:394] conv2 <- pool1
I1027 22:05:26.193761 25546 net.cpp:356] conv2 -> conv2
I1027 22:05:26.193768 25546 net.cpp:96] Setting up conv2
I1027 22:05:26.194272 25546 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1027 22:05:26.194286 25546 net.cpp:67] Creating Layer pool2
I1027 22:05:26.194291 25546 net.cpp:394] pool2 <- conv2
I1027 22:05:26.194298 25546 net.cpp:356] pool2 -> pool2
I1027 22:05:26.194305 25546 net.cpp:96] Setting up pool2
I1027 22:05:26.194311 25546 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 22:05:26.194316 25546 net.cpp:67] Creating Layer relu2
I1027 22:05:26.194320 25546 net.cpp:394] relu2 <- pool2
I1027 22:05:26.194326 25546 net.cpp:345] relu2 -> pool2 (in-place)
I1027 22:05:26.194332 25546 net.cpp:96] Setting up relu2
I1027 22:05:26.194336 25546 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 22:05:26.194341 25546 net.cpp:67] Creating Layer drop2
I1027 22:05:26.194345 25546 net.cpp:394] drop2 <- pool2
I1027 22:05:26.194350 25546 net.cpp:345] drop2 -> pool2 (in-place)
I1027 22:05:26.194356 25546 net.cpp:96] Setting up drop2
I1027 22:05:26.194361 25546 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1027 22:05:26.194368 25546 net.cpp:67] Creating Layer conv3
I1027 22:05:26.194373 25546 net.cpp:394] conv3 <- pool2
I1027 22:05:26.194380 25546 net.cpp:356] conv3 -> conv3
I1027 22:05:26.194386 25546 net.cpp:96] Setting up conv3
I1027 22:05:26.195708 25546 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1027 22:05:26.195724 25546 net.cpp:67] Creating Layer pool3
I1027 22:05:26.195729 25546 net.cpp:394] pool3 <- conv3
I1027 22:05:26.195734 25546 net.cpp:356] pool3 -> pool3
I1027 22:05:26.195741 25546 net.cpp:96] Setting up pool3
I1027 22:05:26.195747 25546 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 22:05:26.195754 25546 net.cpp:67] Creating Layer relu3
I1027 22:05:26.195757 25546 net.cpp:394] relu3 <- pool3
I1027 22:05:26.195762 25546 net.cpp:345] relu3 -> pool3 (in-place)
I1027 22:05:26.195768 25546 net.cpp:96] Setting up relu3
I1027 22:05:26.195772 25546 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 22:05:26.195778 25546 net.cpp:67] Creating Layer drop3
I1027 22:05:26.195782 25546 net.cpp:394] drop3 <- pool3
I1027 22:05:26.195787 25546 net.cpp:345] drop3 -> pool3 (in-place)
I1027 22:05:26.195793 25546 net.cpp:96] Setting up drop3
I1027 22:05:26.195797 25546 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1027 22:05:26.195804 25546 net.cpp:67] Creating Layer ip1
I1027 22:05:26.195808 25546 net.cpp:394] ip1 <- pool3
I1027 22:05:26.195814 25546 net.cpp:356] ip1 -> ip1
I1027 22:05:26.195822 25546 net.cpp:96] Setting up ip1
I1027 22:05:26.606294 25546 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 22:05:26.606360 25546 net.cpp:67] Creating Layer relu4
I1027 22:05:26.606369 25546 net.cpp:394] relu4 <- ip1
I1027 22:05:26.606379 25546 net.cpp:345] relu4 -> ip1 (in-place)
I1027 22:05:26.606400 25546 net.cpp:96] Setting up relu4
I1027 22:05:26.606407 25546 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 22:05:26.606415 25546 net.cpp:67] Creating Layer drop4
I1027 22:05:26.606420 25546 net.cpp:394] drop4 <- ip1
I1027 22:05:26.606427 25546 net.cpp:345] drop4 -> ip1 (in-place)
I1027 22:05:26.606434 25546 net.cpp:96] Setting up drop4
I1027 22:05:26.606441 25546 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1027 22:05:26.606451 25546 net.cpp:67] Creating Layer ip2
I1027 22:05:26.606456 25546 net.cpp:394] ip2 <- ip1
I1027 22:05:26.606464 25546 net.cpp:356] ip2 -> ip2
I1027 22:05:26.606477 25546 net.cpp:96] Setting up ip2
I1027 22:05:26.614213 25546 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 22:05:26.614279 25546 net.cpp:67] Creating Layer prob
I1027 22:05:26.614289 25546 net.cpp:394] prob <- ip2
I1027 22:05:26.614297 25546 net.cpp:356] prob -> prob
I1027 22:05:26.614310 25546 net.cpp:96] Setting up prob
I1027 22:05:26.614317 25546 net.cpp:103] Top shape: 1 378 1 1 (378)
I1027 22:05:26.614322 25546 net.cpp:172] prob does not need backward computation.
I1027 22:05:26.614327 25546 net.cpp:172] ip2 does not need backward computation.
I1027 22:05:26.614331 25546 net.cpp:172] drop4 does not need backward computation.
I1027 22:05:26.614336 25546 net.cpp:172] relu4 does not need backward computation.
I1027 22:05:26.614339 25546 net.cpp:172] ip1 does not need backward computation.
I1027 22:05:26.614343 25546 net.cpp:172] drop3 does not need backward computation.
I1027 22:05:26.614347 25546 net.cpp:172] relu3 does not need backward computation.
I1027 22:05:26.614351 25546 net.cpp:172] pool3 does not need backward computation.
I1027 22:05:26.614356 25546 net.cpp:172] conv3 does not need backward computation.
I1027 22:05:26.614361 25546 net.cpp:172] drop2 does not need backward computation.
I1027 22:05:26.614364 25546 net.cpp:172] relu2 does not need backward computation.
I1027 22:05:26.614367 25546 net.cpp:172] pool2 does not need backward computation.
I1027 22:05:26.614372 25546 net.cpp:172] conv2 does not need backward computation.
I1027 22:05:26.614375 25546 net.cpp:172] drop1 does not need backward computation.
I1027 22:05:26.614379 25546 net.cpp:172] relu1 does not need backward computation.
I1027 22:05:26.614383 25546 net.cpp:172] pool1 does not need backward computation.
I1027 22:05:26.614387 25546 net.cpp:172] conv1 does not need backward computation.
I1027 22:05:26.614392 25546 net.cpp:208] This network produces output prob
I1027 22:05:26.614405 25546 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 22:05:26.614416 25546 net.cpp:219] Network initialization done.
I1027 22:05:26.614420 25546 net.cpp:220] Memory required for data: 1837200
I1027 22:43:05.019171  1826 convert_imageset.cpp:70] Shuffling data
I1027 22:43:05.717169  1826 convert_imageset.cpp:73] A total of 60000 images.
I1027 22:43:05.717242  1826 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1027 22:43:07.936559  1826 convert_imageset.cpp:177] Processed 1000 files.
E1027 22:43:10.086580  1826 convert_imageset.cpp:177] Processed 2000 files.
E1027 22:43:12.143275  1826 convert_imageset.cpp:177] Processed 3000 files.
E1027 22:43:14.245944  1826 convert_imageset.cpp:177] Processed 4000 files.
E1027 22:43:16.255647  1826 convert_imageset.cpp:177] Processed 5000 files.
E1027 22:43:18.228224  1826 convert_imageset.cpp:177] Processed 6000 files.
E1027 22:43:20.033681  1826 convert_imageset.cpp:177] Processed 7000 files.
E1027 22:43:21.861608  1826 convert_imageset.cpp:177] Processed 8000 files.
E1027 22:43:23.638247  1826 convert_imageset.cpp:177] Processed 9000 files.
E1027 22:43:25.418275  1826 convert_imageset.cpp:177] Processed 10000 files.
E1027 22:43:27.244467  1826 convert_imageset.cpp:177] Processed 11000 files.
E1027 22:43:29.114599  1826 convert_imageset.cpp:177] Processed 12000 files.
E1027 22:43:31.071496  1826 convert_imageset.cpp:177] Processed 13000 files.
E1027 22:43:32.795771  1826 convert_imageset.cpp:177] Processed 14000 files.
E1027 22:43:34.576545  1826 convert_imageset.cpp:177] Processed 15000 files.
E1027 22:43:36.377521  1826 convert_imageset.cpp:177] Processed 16000 files.
E1027 22:43:38.151967  1826 convert_imageset.cpp:177] Processed 17000 files.
E1027 22:43:40.068481  1826 convert_imageset.cpp:177] Processed 18000 files.
E1027 22:43:41.743929  1826 convert_imageset.cpp:177] Processed 19000 files.
E1027 22:43:43.631943  1826 convert_imageset.cpp:177] Processed 20000 files.
E1027 22:43:45.324726  1826 convert_imageset.cpp:177] Processed 21000 files.
E1027 22:43:47.053638  1826 convert_imageset.cpp:177] Processed 22000 files.
E1027 22:43:48.861737  1826 convert_imageset.cpp:177] Processed 23000 files.
E1027 22:43:50.502579  1826 convert_imageset.cpp:177] Processed 24000 files.
E1027 22:43:52.196163  1826 convert_imageset.cpp:177] Processed 25000 files.
E1027 22:43:53.839663  1826 convert_imageset.cpp:177] Processed 26000 files.
E1027 22:43:55.484849  1826 convert_imageset.cpp:177] Processed 27000 files.
E1027 22:43:57.075636  1826 convert_imageset.cpp:177] Processed 28000 files.
E1027 22:43:58.689235  1826 convert_imageset.cpp:177] Processed 29000 files.
E1027 22:44:00.268717  1826 convert_imageset.cpp:177] Processed 30000 files.
E1027 22:44:01.825959  1826 convert_imageset.cpp:177] Processed 31000 files.
E1027 22:44:03.474818  1826 convert_imageset.cpp:177] Processed 32000 files.
E1027 22:44:05.003551  1826 convert_imageset.cpp:177] Processed 33000 files.
E1027 22:44:06.598501  1826 convert_imageset.cpp:177] Processed 34000 files.
E1027 22:44:08.200721  1826 convert_imageset.cpp:177] Processed 35000 files.
E1027 22:44:09.849793  1826 convert_imageset.cpp:177] Processed 36000 files.
E1027 22:44:11.411749  1826 convert_imageset.cpp:177] Processed 37000 files.
E1027 22:44:13.121822  1826 convert_imageset.cpp:177] Processed 38000 files.
E1027 22:44:14.949861  1826 convert_imageset.cpp:177] Processed 39000 files.
E1027 22:44:16.502521  1826 convert_imageset.cpp:177] Processed 40000 files.
E1027 22:44:18.021774  1826 convert_imageset.cpp:177] Processed 41000 files.
E1027 22:44:19.547925  1826 convert_imageset.cpp:177] Processed 42000 files.
E1027 22:44:21.194718  1826 convert_imageset.cpp:177] Processed 43000 files.
E1027 22:44:22.808838  1826 convert_imageset.cpp:177] Processed 44000 files.
E1027 22:44:24.482852  1826 convert_imageset.cpp:177] Processed 45000 files.
E1027 22:44:26.070431  1826 convert_imageset.cpp:177] Processed 46000 files.
E1027 22:44:27.602651  1826 convert_imageset.cpp:177] Processed 47000 files.
E1027 22:44:29.161077  1826 convert_imageset.cpp:177] Processed 48000 files.
E1027 22:44:30.690708  1826 convert_imageset.cpp:177] Processed 49000 files.
E1027 22:44:32.267197  1826 convert_imageset.cpp:177] Processed 50000 files.
E1027 22:44:33.869117  1826 convert_imageset.cpp:177] Processed 51000 files.
E1027 22:44:35.422297  1826 convert_imageset.cpp:177] Processed 52000 files.
E1027 22:44:36.982173  1826 convert_imageset.cpp:177] Processed 53000 files.
E1027 22:44:38.449013  1826 convert_imageset.cpp:177] Processed 54000 files.
E1027 22:44:39.961201  1826 convert_imageset.cpp:177] Processed 55000 files.
E1027 22:44:41.506217  1826 convert_imageset.cpp:177] Processed 56000 files.
E1027 22:44:43.020650  1826 convert_imageset.cpp:177] Processed 57000 files.
E1027 22:44:44.581156  1826 convert_imageset.cpp:177] Processed 58000 files.
E1027 22:44:46.142832  1826 convert_imageset.cpp:177] Processed 59000 files.
E1027 22:44:47.677216  1826 convert_imageset.cpp:177] Processed 60000 files.
I1027 22:44:47.874075  1932 caffe.cpp:99] Use GPU with device ID 0
I1027 22:44:48.209959  1932 caffe.cpp:107] Starting Optimization
I1027 22:44:48.210081  1932 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 335000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1027 22:44:48.210110  1932 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1027 22:44:48.219877  1932 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1027 22:44:48.220078  1932 net.cpp:67] Creating Layer mnist
I1027 22:44:48.220103  1932 net.cpp:356] mnist -> data
I1027 22:44:48.220135  1932 net.cpp:356] mnist -> label
I1027 22:44:48.220165  1932 net.cpp:96] Setting up mnist
I1027 22:44:48.229823  1932 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1027 22:44:48.229950  1932 data_layer.cpp:128] output data size: 64,1,50,180
I1027 22:44:48.231605  1932 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1027 22:44:48.231642  1932 net.cpp:103] Top shape: 64 1 1 1 (64)
I1027 22:44:48.231673  1932 net.cpp:67] Creating Layer conv1
I1027 22:44:48.231686  1932 net.cpp:394] conv1 <- data
I1027 22:44:48.231719  1932 net.cpp:356] conv1 -> conv1
I1027 22:44:48.231745  1932 net.cpp:96] Setting up conv1
I1027 22:44:48.232725  1932 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1027 22:44:48.232795  1932 net.cpp:67] Creating Layer pool1
I1027 22:44:48.232811  1932 net.cpp:394] pool1 <- conv1
I1027 22:44:48.232830  1932 net.cpp:356] pool1 -> pool1
I1027 22:44:48.232848  1932 net.cpp:96] Setting up pool1
I1027 22:44:48.232880  1932 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 22:44:48.232903  1932 net.cpp:67] Creating Layer relu1
I1027 22:44:48.232916  1932 net.cpp:394] relu1 <- pool1
I1027 22:44:48.232933  1932 net.cpp:345] relu1 -> pool1 (in-place)
I1027 22:44:48.232950  1932 net.cpp:96] Setting up relu1
I1027 22:44:48.232964  1932 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 22:44:48.232981  1932 net.cpp:67] Creating Layer drop1
I1027 22:44:48.232995  1932 net.cpp:394] drop1 <- pool1
I1027 22:44:48.233011  1932 net.cpp:345] drop1 -> pool1 (in-place)
I1027 22:44:48.233027  1932 net.cpp:96] Setting up drop1
I1027 22:44:48.233042  1932 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1027 22:44:48.233064  1932 net.cpp:67] Creating Layer conv2
I1027 22:44:48.233078  1932 net.cpp:394] conv2 <- pool1
I1027 22:44:48.233098  1932 net.cpp:356] conv2 -> conv2
I1027 22:44:48.233120  1932 net.cpp:96] Setting up conv2
I1027 22:44:48.234658  1932 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1027 22:44:48.234700  1932 net.cpp:67] Creating Layer pool2
I1027 22:44:48.234715  1932 net.cpp:394] pool2 <- conv2
I1027 22:44:48.234732  1932 net.cpp:356] pool2 -> pool2
I1027 22:44:48.234751  1932 net.cpp:96] Setting up pool2
I1027 22:44:48.234766  1932 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 22:44:48.234783  1932 net.cpp:67] Creating Layer relu2
I1027 22:44:48.234797  1932 net.cpp:394] relu2 <- pool2
I1027 22:44:48.234813  1932 net.cpp:345] relu2 -> pool2 (in-place)
I1027 22:44:48.234829  1932 net.cpp:96] Setting up relu2
I1027 22:44:48.234843  1932 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 22:44:48.234865  1932 net.cpp:67] Creating Layer drop2
I1027 22:44:48.234879  1932 net.cpp:394] drop2 <- pool2
I1027 22:44:48.234895  1932 net.cpp:345] drop2 -> pool2 (in-place)
I1027 22:44:48.234912  1932 net.cpp:96] Setting up drop2
I1027 22:44:48.234926  1932 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1027 22:44:48.234948  1932 net.cpp:67] Creating Layer conv3
I1027 22:44:48.234961  1932 net.cpp:394] conv3 <- pool2
I1027 22:44:48.234979  1932 net.cpp:356] conv3 -> conv3
I1027 22:44:48.234998  1932 net.cpp:96] Setting up conv3
I1027 22:44:48.239042  1932 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1027 22:44:48.239089  1932 net.cpp:67] Creating Layer pool3
I1027 22:44:48.239102  1932 net.cpp:394] pool3 <- conv3
I1027 22:44:48.239125  1932 net.cpp:356] pool3 -> pool3
I1027 22:44:48.239145  1932 net.cpp:96] Setting up pool3
I1027 22:44:48.239161  1932 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 22:44:48.239176  1932 net.cpp:67] Creating Layer relu3
I1027 22:44:48.239188  1932 net.cpp:394] relu3 <- pool3
I1027 22:44:48.239204  1932 net.cpp:345] relu3 -> pool3 (in-place)
I1027 22:44:48.239222  1932 net.cpp:96] Setting up relu3
I1027 22:44:48.239233  1932 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 22:44:48.239253  1932 net.cpp:67] Creating Layer drop3
I1027 22:44:48.239267  1932 net.cpp:394] drop3 <- pool3
I1027 22:44:48.239284  1932 net.cpp:345] drop3 -> pool3 (in-place)
I1027 22:44:48.239300  1932 net.cpp:96] Setting up drop3
I1027 22:44:48.239315  1932 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1027 22:44:48.239332  1932 net.cpp:67] Creating Layer ip1
I1027 22:44:48.239344  1932 net.cpp:394] ip1 <- pool3
I1027 22:44:48.239365  1932 net.cpp:356] ip1 -> ip1
I1027 22:44:48.239428  1932 net.cpp:96] Setting up ip1
I1027 22:44:48.659211  1932 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 22:44:48.659273  1932 net.cpp:67] Creating Layer relu4
I1027 22:44:48.659281  1932 net.cpp:394] relu4 <- ip1
I1027 22:44:48.659291  1932 net.cpp:345] relu4 -> ip1 (in-place)
I1027 22:44:48.659301  1932 net.cpp:96] Setting up relu4
I1027 22:44:48.659306  1932 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 22:44:48.659318  1932 net.cpp:67] Creating Layer drop4
I1027 22:44:48.659323  1932 net.cpp:394] drop4 <- ip1
I1027 22:44:48.659329  1932 net.cpp:345] drop4 -> ip1 (in-place)
I1027 22:44:48.659335  1932 net.cpp:96] Setting up drop4
I1027 22:44:48.659340  1932 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1027 22:44:48.659353  1932 net.cpp:67] Creating Layer ip2
I1027 22:44:48.659358  1932 net.cpp:394] ip2 <- ip1
I1027 22:44:48.659364  1932 net.cpp:356] ip2 -> ip2
I1027 22:44:48.659373  1932 net.cpp:96] Setting up ip2
I1027 22:44:48.672340  1932 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1027 22:44:48.672413  1932 net.cpp:67] Creating Layer loss
I1027 22:44:48.672446  1932 net.cpp:394] loss <- ip2
I1027 22:44:48.672454  1932 net.cpp:394] loss <- label
I1027 22:44:48.672461  1932 net.cpp:356] loss -> loss
I1027 22:44:48.672472  1932 net.cpp:96] Setting up loss
I1027 22:44:48.672484  1932 net.cpp:103] Top shape: 1 1 1 1 (1)
I1027 22:44:48.672489  1932 net.cpp:109]     with loss weight 1
I1027 22:44:48.672525  1932 net.cpp:170] loss needs backward computation.
I1027 22:44:48.672531  1932 net.cpp:170] ip2 needs backward computation.
I1027 22:44:48.672535  1932 net.cpp:170] drop4 needs backward computation.
I1027 22:44:48.672540  1932 net.cpp:170] relu4 needs backward computation.
I1027 22:44:48.672544  1932 net.cpp:170] ip1 needs backward computation.
I1027 22:44:48.672549  1932 net.cpp:170] drop3 needs backward computation.
I1027 22:44:48.672554  1932 net.cpp:170] relu3 needs backward computation.
I1027 22:44:48.672559  1932 net.cpp:170] pool3 needs backward computation.
I1027 22:44:48.672564  1932 net.cpp:170] conv3 needs backward computation.
I1027 22:44:48.672567  1932 net.cpp:170] drop2 needs backward computation.
I1027 22:44:48.672572  1932 net.cpp:170] relu2 needs backward computation.
I1027 22:44:48.672577  1932 net.cpp:170] pool2 needs backward computation.
I1027 22:44:48.672581  1932 net.cpp:170] conv2 needs backward computation.
I1027 22:44:48.672585  1932 net.cpp:170] drop1 needs backward computation.
I1027 22:44:48.672590  1932 net.cpp:170] relu1 needs backward computation.
I1027 22:44:48.672595  1932 net.cpp:170] pool1 needs backward computation.
I1027 22:44:48.672600  1932 net.cpp:170] conv1 needs backward computation.
I1027 22:44:48.672603  1932 net.cpp:172] mnist does not need backward computation.
I1027 22:44:48.672608  1932 net.cpp:208] This network produces output loss
I1027 22:44:48.672618  1932 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1027 22:44:48.672626  1932 net.cpp:219] Network initialization done.
I1027 22:44:48.672631  1932 net.cpp:220] Memory required for data: 119788292
I1027 22:44:48.672690  1932 solver.cpp:41] Solver scaffolding done.
I1027 22:44:48.672696  1932 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_320000.solverstate
I1027 22:44:48.672701  1932 solver.cpp:160] Solving Captcha
I1027 22:44:48.672720  1932 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_320000.solverstate
I1027 22:44:54.539788  1932 solver.cpp:502] SGDSolver: restoring history
I1027 22:44:55.541867  1932 solver.cpp:191] Iteration 320000, loss = 2.46331
I1027 22:44:55.541926  1932 solver.cpp:206]     Train net output #0: loss = 2.46331 (* 1 = 2.46331 loss)
I1027 22:44:55.541941  1932 solver.cpp:403] Iteration 320000, lr = 0.000726297
I1027 22:48:57.023355  1932 solver.cpp:191] Iteration 321000, loss = 2.47062
I1027 22:48:57.024178  1932 solver.cpp:206]     Train net output #0: loss = 2.47062 (* 1 = 2.47062 loss)
I1027 22:48:57.024209  1932 solver.cpp:403] Iteration 321000, lr = 0.000724651
I1027 22:52:58.046697  1932 solver.cpp:191] Iteration 322000, loss = 2.47902
I1027 22:52:58.047483  1932 solver.cpp:206]     Train net output #0: loss = 2.47902 (* 1 = 2.47902 loss)
I1027 22:52:58.047515  1932 solver.cpp:403] Iteration 322000, lr = 0.000723014
I1027 22:56:59.087496  1932 solver.cpp:191] Iteration 323000, loss = 2.54386
I1027 22:56:59.088080  1932 solver.cpp:206]     Train net output #0: loss = 2.54386 (* 1 = 2.54386 loss)
I1027 22:56:59.088124  1932 solver.cpp:403] Iteration 323000, lr = 0.000721385
I1027 23:01:00.163619  1932 solver.cpp:191] Iteration 324000, loss = 2.5008
I1027 23:01:00.164185  1932 solver.cpp:206]     Train net output #0: loss = 2.5008 (* 1 = 2.5008 loss)
I1027 23:01:00.164222  1932 solver.cpp:403] Iteration 324000, lr = 0.000719764
I1027 23:05:01.980289  1932 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_325000.caffemodel
I1027 23:05:06.333189  1932 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_325000.solverstate
I1027 23:05:10.251144  1932 solver.cpp:191] Iteration 325000, loss = 2.72398
I1027 23:05:10.251744  1932 solver.cpp:206]     Train net output #0: loss = 2.72398 (* 1 = 2.72398 loss)
I1027 23:05:10.251778  1932 solver.cpp:403] Iteration 325000, lr = 0.000718152
I1027 23:09:11.624475  1932 solver.cpp:191] Iteration 326000, loss = 2.33218
I1027 23:09:11.625144  1932 solver.cpp:206]     Train net output #0: loss = 2.33218 (* 1 = 2.33218 loss)
I1027 23:09:11.625175  1932 solver.cpp:403] Iteration 326000, lr = 0.000716548
I1027 23:13:12.876181  1932 solver.cpp:191] Iteration 327000, loss = 2.5068
I1027 23:13:12.877109  1932 solver.cpp:206]     Train net output #0: loss = 2.5068 (* 1 = 2.5068 loss)
I1027 23:13:12.877141  1932 solver.cpp:403] Iteration 327000, lr = 0.000714953
I1027 23:17:14.121438  1932 solver.cpp:191] Iteration 328000, loss = 2.19224
I1027 23:17:14.121978  1932 solver.cpp:206]     Train net output #0: loss = 2.19224 (* 1 = 2.19224 loss)
I1027 23:17:14.122010  1932 solver.cpp:403] Iteration 328000, lr = 0.000713366
I1027 23:21:15.304354  1932 solver.cpp:191] Iteration 329000, loss = 2.32242
I1027 23:21:15.304965  1932 solver.cpp:206]     Train net output #0: loss = 2.32242 (* 1 = 2.32242 loss)
I1027 23:21:15.304981  1932 solver.cpp:403] Iteration 329000, lr = 0.000711787
I1027 23:25:16.930435  1932 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_330000.caffemodel
I1027 23:25:21.578275  1932 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_330000.solverstate
I1027 23:25:25.185155  1932 solver.cpp:191] Iteration 330000, loss = 2.32464
I1027 23:25:25.185683  1932 solver.cpp:206]     Train net output #0: loss = 2.32464 (* 1 = 2.32464 loss)
I1027 23:25:25.185716  1932 solver.cpp:403] Iteration 330000, lr = 0.000710217
I1027 23:29:26.314543  1932 solver.cpp:191] Iteration 331000, loss = 2.3109
I1027 23:29:26.315107  1932 solver.cpp:206]     Train net output #0: loss = 2.3109 (* 1 = 2.3109 loss)
I1027 23:29:26.315145  1932 solver.cpp:403] Iteration 331000, lr = 0.000708654
I1027 23:33:27.383996  1932 solver.cpp:191] Iteration 332000, loss = 2.34389
I1027 23:33:27.385123  1932 solver.cpp:206]     Train net output #0: loss = 2.34389 (* 1 = 2.34389 loss)
I1027 23:33:27.385155  1932 solver.cpp:403] Iteration 332000, lr = 0.000707099
I1027 23:37:28.513926  1932 solver.cpp:191] Iteration 333000, loss = 2.50401
I1027 23:37:28.514489  1932 solver.cpp:206]     Train net output #0: loss = 2.50401 (* 1 = 2.50401 loss)
I1027 23:37:28.514521  1932 solver.cpp:403] Iteration 333000, lr = 0.000705553
I1027 23:41:29.823130  1932 solver.cpp:191] Iteration 334000, loss = 2.32209
I1027 23:41:29.823897  1932 solver.cpp:206]     Train net output #0: loss = 2.32209 (* 1 = 2.32209 loss)
I1027 23:41:29.823933  1932 solver.cpp:403] Iteration 334000, lr = 0.000704014
I1027 23:45:31.617439  1932 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_335000.caffemodel
I1027 23:45:36.247321  1932 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_335000.solverstate
I1027 23:45:39.643153  1932 solver.cpp:228] Iteration 335000, loss = 2.28902
I1027 23:45:39.643801  1932 solver.cpp:233] Optimization Done.
I1027 23:45:39.643834  1932 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 00:16:00.494158 30553 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 00:16:00.494269 30553 net.cpp:358] Input 0 -> data
I1028 00:16:00.494293 30553 net.cpp:67] Creating Layer conv1
I1028 00:16:00.494299 30553 net.cpp:394] conv1 <- data
I1028 00:16:00.494307 30553 net.cpp:356] conv1 -> conv1
I1028 00:16:00.494318 30553 net.cpp:96] Setting up conv1
I1028 00:16:00.494652 30553 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 00:16:00.494673 30553 net.cpp:67] Creating Layer pool1
I1028 00:16:00.494678 30553 net.cpp:394] pool1 <- conv1
I1028 00:16:00.494684 30553 net.cpp:356] pool1 -> pool1
I1028 00:16:00.494693 30553 net.cpp:96] Setting up pool1
I1028 00:16:00.494710 30553 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 00:16:00.494719 30553 net.cpp:67] Creating Layer relu1
I1028 00:16:00.494722 30553 net.cpp:394] relu1 <- pool1
I1028 00:16:00.494730 30553 net.cpp:345] relu1 -> pool1 (in-place)
I1028 00:16:00.494736 30553 net.cpp:96] Setting up relu1
I1028 00:16:00.494741 30553 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 00:16:00.494747 30553 net.cpp:67] Creating Layer drop1
I1028 00:16:00.494752 30553 net.cpp:394] drop1 <- pool1
I1028 00:16:00.494757 30553 net.cpp:345] drop1 -> pool1 (in-place)
I1028 00:16:00.494767 30553 net.cpp:96] Setting up drop1
I1028 00:16:00.494773 30553 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 00:16:00.494781 30553 net.cpp:67] Creating Layer conv2
I1028 00:16:00.494786 30553 net.cpp:394] conv2 <- pool1
I1028 00:16:00.494791 30553 net.cpp:356] conv2 -> conv2
I1028 00:16:00.494798 30553 net.cpp:96] Setting up conv2
I1028 00:16:00.495357 30553 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 00:16:00.495373 30553 net.cpp:67] Creating Layer pool2
I1028 00:16:00.495378 30553 net.cpp:394] pool2 <- conv2
I1028 00:16:00.495385 30553 net.cpp:356] pool2 -> pool2
I1028 00:16:00.495393 30553 net.cpp:96] Setting up pool2
I1028 00:16:00.495399 30553 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 00:16:00.495404 30553 net.cpp:67] Creating Layer relu2
I1028 00:16:00.495409 30553 net.cpp:394] relu2 <- pool2
I1028 00:16:00.495415 30553 net.cpp:345] relu2 -> pool2 (in-place)
I1028 00:16:00.495421 30553 net.cpp:96] Setting up relu2
I1028 00:16:00.495425 30553 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 00:16:00.495431 30553 net.cpp:67] Creating Layer drop2
I1028 00:16:00.495435 30553 net.cpp:394] drop2 <- pool2
I1028 00:16:00.495440 30553 net.cpp:345] drop2 -> pool2 (in-place)
I1028 00:16:00.495446 30553 net.cpp:96] Setting up drop2
I1028 00:16:00.495450 30553 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 00:16:00.495460 30553 net.cpp:67] Creating Layer conv3
I1028 00:16:00.495463 30553 net.cpp:394] conv3 <- pool2
I1028 00:16:00.495470 30553 net.cpp:356] conv3 -> conv3
I1028 00:16:00.495476 30553 net.cpp:96] Setting up conv3
I1028 00:16:00.496960 30553 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 00:16:00.496978 30553 net.cpp:67] Creating Layer pool3
I1028 00:16:00.496983 30553 net.cpp:394] pool3 <- conv3
I1028 00:16:00.496990 30553 net.cpp:356] pool3 -> pool3
I1028 00:16:00.496997 30553 net.cpp:96] Setting up pool3
I1028 00:16:00.497004 30553 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 00:16:00.497009 30553 net.cpp:67] Creating Layer relu3
I1028 00:16:00.497012 30553 net.cpp:394] relu3 <- pool3
I1028 00:16:00.497017 30553 net.cpp:345] relu3 -> pool3 (in-place)
I1028 00:16:00.497023 30553 net.cpp:96] Setting up relu3
I1028 00:16:00.497027 30553 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 00:16:00.497033 30553 net.cpp:67] Creating Layer drop3
I1028 00:16:00.497037 30553 net.cpp:394] drop3 <- pool3
I1028 00:16:00.497045 30553 net.cpp:345] drop3 -> pool3 (in-place)
I1028 00:16:00.497051 30553 net.cpp:96] Setting up drop3
I1028 00:16:00.497056 30553 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 00:16:00.497062 30553 net.cpp:67] Creating Layer ip1
I1028 00:16:00.497066 30553 net.cpp:394] ip1 <- pool3
I1028 00:16:00.497073 30553 net.cpp:356] ip1 -> ip1
I1028 00:16:00.497081 30553 net.cpp:96] Setting up ip1
I1028 00:16:01.015424 30553 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 00:16:01.015485 30553 net.cpp:67] Creating Layer relu4
I1028 00:16:01.015492 30553 net.cpp:394] relu4 <- ip1
I1028 00:16:01.015501 30553 net.cpp:345] relu4 -> ip1 (in-place)
I1028 00:16:01.015511 30553 net.cpp:96] Setting up relu4
I1028 00:16:01.015516 30553 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 00:16:01.015527 30553 net.cpp:67] Creating Layer drop4
I1028 00:16:01.015532 30553 net.cpp:394] drop4 <- ip1
I1028 00:16:01.015537 30553 net.cpp:345] drop4 -> ip1 (in-place)
I1028 00:16:01.015543 30553 net.cpp:96] Setting up drop4
I1028 00:16:01.015548 30553 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 00:16:01.015558 30553 net.cpp:67] Creating Layer ip2
I1028 00:16:01.015561 30553 net.cpp:394] ip2 <- ip1
I1028 00:16:01.015569 30553 net.cpp:356] ip2 -> ip2
I1028 00:16:01.015581 30553 net.cpp:96] Setting up ip2
I1028 00:16:01.025706 30553 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 00:16:01.025771 30553 net.cpp:67] Creating Layer prob
I1028 00:16:01.025779 30553 net.cpp:394] prob <- ip2
I1028 00:16:01.025787 30553 net.cpp:356] prob -> prob
I1028 00:16:01.025799 30553 net.cpp:96] Setting up prob
I1028 00:16:01.025806 30553 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 00:16:01.025811 30553 net.cpp:172] prob does not need backward computation.
I1028 00:16:01.025821 30553 net.cpp:172] ip2 does not need backward computation.
I1028 00:16:01.025826 30553 net.cpp:172] drop4 does not need backward computation.
I1028 00:16:01.025830 30553 net.cpp:172] relu4 does not need backward computation.
I1028 00:16:01.025835 30553 net.cpp:172] ip1 does not need backward computation.
I1028 00:16:01.025838 30553 net.cpp:172] drop3 does not need backward computation.
I1028 00:16:01.025841 30553 net.cpp:172] relu3 does not need backward computation.
I1028 00:16:01.025846 30553 net.cpp:172] pool3 does not need backward computation.
I1028 00:16:01.025849 30553 net.cpp:172] conv3 does not need backward computation.
I1028 00:16:01.025853 30553 net.cpp:172] drop2 does not need backward computation.
I1028 00:16:01.025857 30553 net.cpp:172] relu2 does not need backward computation.
I1028 00:16:01.025861 30553 net.cpp:172] pool2 does not need backward computation.
I1028 00:16:01.025864 30553 net.cpp:172] conv2 does not need backward computation.
I1028 00:16:01.025868 30553 net.cpp:172] drop1 does not need backward computation.
I1028 00:16:01.025871 30553 net.cpp:172] relu1 does not need backward computation.
I1028 00:16:01.025876 30553 net.cpp:172] pool1 does not need backward computation.
I1028 00:16:01.025879 30553 net.cpp:172] conv1 does not need backward computation.
I1028 00:16:01.025883 30553 net.cpp:208] This network produces output prob
I1028 00:16:01.025895 30553 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 00:16:01.025904 30553 net.cpp:219] Network initialization done.
I1028 00:16:01.025908 30553 net.cpp:220] Memory required for data: 1837200
I1028 00:16:50.358924 30553 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 00:16:50.359503 30553 net.cpp:358] Input 0 -> data
I1028 00:16:50.359554 30553 net.cpp:67] Creating Layer conv1
I1028 00:16:50.359565 30553 net.cpp:394] conv1 <- data
I1028 00:16:50.359581 30553 net.cpp:356] conv1 -> conv1
I1028 00:16:50.359602 30553 net.cpp:96] Setting up conv1
I1028 00:16:50.359658 30553 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 00:16:50.359688 30553 net.cpp:67] Creating Layer pool1
I1028 00:16:50.359699 30553 net.cpp:394] pool1 <- conv1
I1028 00:16:50.359714 30553 net.cpp:356] pool1 -> pool1
I1028 00:16:50.359730 30553 net.cpp:96] Setting up pool1
I1028 00:16:50.359745 30553 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 00:16:50.359761 30553 net.cpp:67] Creating Layer relu1
I1028 00:16:50.359771 30553 net.cpp:394] relu1 <- pool1
I1028 00:16:50.359783 30553 net.cpp:345] relu1 -> pool1 (in-place)
I1028 00:16:50.359797 30553 net.cpp:96] Setting up relu1
I1028 00:16:50.359807 30553 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 00:16:50.359822 30553 net.cpp:67] Creating Layer drop1
I1028 00:16:50.359830 30553 net.cpp:394] drop1 <- pool1
I1028 00:16:50.359843 30553 net.cpp:345] drop1 -> pool1 (in-place)
I1028 00:16:50.359858 30553 net.cpp:96] Setting up drop1
I1028 00:16:50.359869 30553 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 00:16:50.359884 30553 net.cpp:67] Creating Layer conv2
I1028 00:16:50.359894 30553 net.cpp:394] conv2 <- pool1
I1028 00:16:50.359908 30553 net.cpp:356] conv2 -> conv2
I1028 00:16:50.359925 30553 net.cpp:96] Setting up conv2
I1028 00:16:50.361089 30553 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 00:16:50.361120 30553 net.cpp:67] Creating Layer pool2
I1028 00:16:50.361131 30553 net.cpp:394] pool2 <- conv2
I1028 00:16:50.361145 30553 net.cpp:356] pool2 -> pool2
I1028 00:16:50.361162 30553 net.cpp:96] Setting up pool2
I1028 00:16:50.361176 30553 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 00:16:50.361189 30553 net.cpp:67] Creating Layer relu2
I1028 00:16:50.361199 30553 net.cpp:394] relu2 <- pool2
I1028 00:16:50.361212 30553 net.cpp:345] relu2 -> pool2 (in-place)
I1028 00:16:50.361225 30553 net.cpp:96] Setting up relu2
I1028 00:16:50.361235 30553 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 00:16:50.361254 30553 net.cpp:67] Creating Layer drop2
I1028 00:16:50.361268 30553 net.cpp:394] drop2 <- pool2
I1028 00:16:50.361282 30553 net.cpp:345] drop2 -> pool2 (in-place)
I1028 00:16:50.361299 30553 net.cpp:96] Setting up drop2
I1028 00:16:50.361311 30553 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 00:16:50.361332 30553 net.cpp:67] Creating Layer conv3
I1028 00:16:50.361345 30553 net.cpp:394] conv3 <- pool2
I1028 00:16:50.361361 30553 net.cpp:356] conv3 -> conv3
I1028 00:16:50.361382 30553 net.cpp:96] Setting up conv3
I1028 00:16:50.364720 30553 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 00:16:50.364737 30553 net.cpp:67] Creating Layer pool3
I1028 00:16:50.364742 30553 net.cpp:394] pool3 <- conv3
I1028 00:16:50.364749 30553 net.cpp:356] pool3 -> pool3
I1028 00:16:50.364756 30553 net.cpp:96] Setting up pool3
I1028 00:16:50.364763 30553 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 00:16:50.364768 30553 net.cpp:67] Creating Layer relu3
I1028 00:16:50.364771 30553 net.cpp:394] relu3 <- pool3
I1028 00:16:50.364778 30553 net.cpp:345] relu3 -> pool3 (in-place)
I1028 00:16:50.364784 30553 net.cpp:96] Setting up relu3
I1028 00:16:50.364787 30553 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 00:16:50.364792 30553 net.cpp:67] Creating Layer drop3
I1028 00:16:50.364796 30553 net.cpp:394] drop3 <- pool3
I1028 00:16:50.364802 30553 net.cpp:345] drop3 -> pool3 (in-place)
I1028 00:16:50.364809 30553 net.cpp:96] Setting up drop3
I1028 00:16:50.364812 30553 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 00:16:50.364822 30553 net.cpp:67] Creating Layer ip1
I1028 00:16:50.364827 30553 net.cpp:394] ip1 <- pool3
I1028 00:16:50.364833 30553 net.cpp:356] ip1 -> ip1
I1028 00:16:50.364841 30553 net.cpp:96] Setting up ip1
I1028 00:16:50.789798 30553 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 00:16:50.789866 30553 net.cpp:67] Creating Layer relu4
I1028 00:16:50.789875 30553 net.cpp:394] relu4 <- ip1
I1028 00:16:50.789885 30553 net.cpp:345] relu4 -> ip1 (in-place)
I1028 00:16:50.789896 30553 net.cpp:96] Setting up relu4
I1028 00:16:50.789901 30553 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 00:16:50.789909 30553 net.cpp:67] Creating Layer drop4
I1028 00:16:50.789913 30553 net.cpp:394] drop4 <- ip1
I1028 00:16:50.789921 30553 net.cpp:345] drop4 -> ip1 (in-place)
I1028 00:16:50.789927 30553 net.cpp:96] Setting up drop4
I1028 00:16:50.789933 30553 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 00:16:50.789943 30553 net.cpp:67] Creating Layer ip2
I1028 00:16:50.789948 30553 net.cpp:394] ip2 <- ip1
I1028 00:16:50.789955 30553 net.cpp:356] ip2 -> ip2
I1028 00:16:50.789969 30553 net.cpp:96] Setting up ip2
I1028 00:16:50.797565 30553 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 00:16:50.797626 30553 net.cpp:67] Creating Layer prob
I1028 00:16:50.797634 30553 net.cpp:394] prob <- ip2
I1028 00:16:50.797643 30553 net.cpp:356] prob -> prob
I1028 00:16:50.797654 30553 net.cpp:96] Setting up prob
I1028 00:16:50.797662 30553 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 00:16:50.797667 30553 net.cpp:172] prob does not need backward computation.
I1028 00:16:50.797672 30553 net.cpp:172] ip2 does not need backward computation.
I1028 00:16:50.797677 30553 net.cpp:172] drop4 does not need backward computation.
I1028 00:16:50.797680 30553 net.cpp:172] relu4 does not need backward computation.
I1028 00:16:50.797684 30553 net.cpp:172] ip1 does not need backward computation.
I1028 00:16:50.797688 30553 net.cpp:172] drop3 does not need backward computation.
I1028 00:16:50.797693 30553 net.cpp:172] relu3 does not need backward computation.
I1028 00:16:50.797696 30553 net.cpp:172] pool3 does not need backward computation.
I1028 00:16:50.797700 30553 net.cpp:172] conv3 does not need backward computation.
I1028 00:16:50.797704 30553 net.cpp:172] drop2 does not need backward computation.
I1028 00:16:50.797709 30553 net.cpp:172] relu2 does not need backward computation.
I1028 00:16:50.797713 30553 net.cpp:172] pool2 does not need backward computation.
I1028 00:16:50.797718 30553 net.cpp:172] conv2 does not need backward computation.
I1028 00:16:50.797721 30553 net.cpp:172] drop1 does not need backward computation.
I1028 00:16:50.797725 30553 net.cpp:172] relu1 does not need backward computation.
I1028 00:16:50.797729 30553 net.cpp:172] pool1 does not need backward computation.
I1028 00:16:50.797734 30553 net.cpp:172] conv1 does not need backward computation.
I1028 00:16:50.797737 30553 net.cpp:208] This network produces output prob
I1028 00:16:50.797752 30553 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 00:16:50.797762 30553 net.cpp:219] Network initialization done.
I1028 00:16:50.797766 30553 net.cpp:220] Memory required for data: 1837200
I1028 00:17:28.975271 30553 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 00:17:28.975870 30553 net.cpp:358] Input 0 -> data
I1028 00:17:28.975901 30553 net.cpp:67] Creating Layer conv1
I1028 00:17:28.975908 30553 net.cpp:394] conv1 <- data
I1028 00:17:28.975915 30553 net.cpp:356] conv1 -> conv1
I1028 00:17:28.975925 30553 net.cpp:96] Setting up conv1
I1028 00:17:28.975955 30553 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 00:17:28.975971 30553 net.cpp:67] Creating Layer pool1
I1028 00:17:28.975976 30553 net.cpp:394] pool1 <- conv1
I1028 00:17:28.975982 30553 net.cpp:356] pool1 -> pool1
I1028 00:17:28.975991 30553 net.cpp:96] Setting up pool1
I1028 00:17:28.975998 30553 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 00:17:28.976006 30553 net.cpp:67] Creating Layer relu1
I1028 00:17:28.976011 30553 net.cpp:394] relu1 <- pool1
I1028 00:17:28.976016 30553 net.cpp:345] relu1 -> pool1 (in-place)
I1028 00:17:28.976022 30553 net.cpp:96] Setting up relu1
I1028 00:17:28.976027 30553 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 00:17:28.976032 30553 net.cpp:67] Creating Layer drop1
I1028 00:17:28.976037 30553 net.cpp:394] drop1 <- pool1
I1028 00:17:28.976042 30553 net.cpp:345] drop1 -> pool1 (in-place)
I1028 00:17:28.976049 30553 net.cpp:96] Setting up drop1
I1028 00:17:28.976054 30553 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 00:17:28.976061 30553 net.cpp:67] Creating Layer conv2
I1028 00:17:28.976066 30553 net.cpp:394] conv2 <- pool1
I1028 00:17:28.976073 30553 net.cpp:356] conv2 -> conv2
I1028 00:17:28.976080 30553 net.cpp:96] Setting up conv2
I1028 00:17:28.976606 30553 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 00:17:28.976624 30553 net.cpp:67] Creating Layer pool2
I1028 00:17:28.976629 30553 net.cpp:394] pool2 <- conv2
I1028 00:17:28.976635 30553 net.cpp:356] pool2 -> pool2
I1028 00:17:28.976644 30553 net.cpp:96] Setting up pool2
I1028 00:17:28.976649 30553 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 00:17:28.976655 30553 net.cpp:67] Creating Layer relu2
I1028 00:17:28.976660 30553 net.cpp:394] relu2 <- pool2
I1028 00:17:28.976665 30553 net.cpp:345] relu2 -> pool2 (in-place)
I1028 00:17:28.976671 30553 net.cpp:96] Setting up relu2
I1028 00:17:28.976675 30553 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 00:17:28.976685 30553 net.cpp:67] Creating Layer drop2
I1028 00:17:28.976688 30553 net.cpp:394] drop2 <- pool2
I1028 00:17:28.976694 30553 net.cpp:345] drop2 -> pool2 (in-place)
I1028 00:17:28.976701 30553 net.cpp:96] Setting up drop2
I1028 00:17:28.976706 30553 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 00:17:28.976713 30553 net.cpp:67] Creating Layer conv3
I1028 00:17:28.976717 30553 net.cpp:394] conv3 <- pool2
I1028 00:17:28.976724 30553 net.cpp:356] conv3 -> conv3
I1028 00:17:28.976732 30553 net.cpp:96] Setting up conv3
I1028 00:17:28.978065 30553 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 00:17:28.978080 30553 net.cpp:67] Creating Layer pool3
I1028 00:17:28.978086 30553 net.cpp:394] pool3 <- conv3
I1028 00:17:28.978092 30553 net.cpp:356] pool3 -> pool3
I1028 00:17:28.978099 30553 net.cpp:96] Setting up pool3
I1028 00:17:28.978106 30553 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 00:17:28.978111 30553 net.cpp:67] Creating Layer relu3
I1028 00:17:28.978116 30553 net.cpp:394] relu3 <- pool3
I1028 00:17:28.978121 30553 net.cpp:345] relu3 -> pool3 (in-place)
I1028 00:17:28.978127 30553 net.cpp:96] Setting up relu3
I1028 00:17:28.978132 30553 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 00:17:28.978138 30553 net.cpp:67] Creating Layer drop3
I1028 00:17:28.978142 30553 net.cpp:394] drop3 <- pool3
I1028 00:17:28.978148 30553 net.cpp:345] drop3 -> pool3 (in-place)
I1028 00:17:28.978154 30553 net.cpp:96] Setting up drop3
I1028 00:17:28.978159 30553 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 00:17:28.978165 30553 net.cpp:67] Creating Layer ip1
I1028 00:17:28.978170 30553 net.cpp:394] ip1 <- pool3
I1028 00:17:28.978176 30553 net.cpp:356] ip1 -> ip1
I1028 00:17:28.978184 30553 net.cpp:96] Setting up ip1
I1028 00:17:29.376797 30553 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 00:17:29.376863 30553 net.cpp:67] Creating Layer relu4
I1028 00:17:29.376870 30553 net.cpp:394] relu4 <- ip1
I1028 00:17:29.376881 30553 net.cpp:345] relu4 -> ip1 (in-place)
I1028 00:17:29.376893 30553 net.cpp:96] Setting up relu4
I1028 00:17:29.376899 30553 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 00:17:29.376907 30553 net.cpp:67] Creating Layer drop4
I1028 00:17:29.376912 30553 net.cpp:394] drop4 <- ip1
I1028 00:17:29.376919 30553 net.cpp:345] drop4 -> ip1 (in-place)
I1028 00:17:29.376926 30553 net.cpp:96] Setting up drop4
I1028 00:17:29.376932 30553 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 00:17:29.376943 30553 net.cpp:67] Creating Layer ip2
I1028 00:17:29.376947 30553 net.cpp:394] ip2 <- ip1
I1028 00:17:29.376956 30553 net.cpp:356] ip2 -> ip2
I1028 00:17:29.376971 30553 net.cpp:96] Setting up ip2
I1028 00:17:29.384541 30553 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 00:17:29.384603 30553 net.cpp:67] Creating Layer prob
I1028 00:17:29.384610 30553 net.cpp:394] prob <- ip2
I1028 00:17:29.384620 30553 net.cpp:356] prob -> prob
I1028 00:17:29.384632 30553 net.cpp:96] Setting up prob
I1028 00:17:29.384640 30553 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 00:17:29.384645 30553 net.cpp:172] prob does not need backward computation.
I1028 00:17:29.384650 30553 net.cpp:172] ip2 does not need backward computation.
I1028 00:17:29.384654 30553 net.cpp:172] drop4 does not need backward computation.
I1028 00:17:29.384660 30553 net.cpp:172] relu4 does not need backward computation.
I1028 00:17:29.384663 30553 net.cpp:172] ip1 does not need backward computation.
I1028 00:17:29.384667 30553 net.cpp:172] drop3 does not need backward computation.
I1028 00:17:29.384671 30553 net.cpp:172] relu3 does not need backward computation.
I1028 00:17:29.384675 30553 net.cpp:172] pool3 does not need backward computation.
I1028 00:17:29.384680 30553 net.cpp:172] conv3 does not need backward computation.
I1028 00:17:29.384685 30553 net.cpp:172] drop2 does not need backward computation.
I1028 00:17:29.384688 30553 net.cpp:172] relu2 does not need backward computation.
I1028 00:17:29.384692 30553 net.cpp:172] pool2 does not need backward computation.
I1028 00:17:29.384696 30553 net.cpp:172] conv2 does not need backward computation.
I1028 00:17:29.384712 30553 net.cpp:172] drop1 does not need backward computation.
I1028 00:17:29.384716 30553 net.cpp:172] relu1 does not need backward computation.
I1028 00:17:29.384721 30553 net.cpp:172] pool1 does not need backward computation.
I1028 00:17:29.384726 30553 net.cpp:172] conv1 does not need backward computation.
I1028 00:17:29.384729 30553 net.cpp:208] This network produces output prob
I1028 00:17:29.384743 30553 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 00:17:29.384752 30553 net.cpp:219] Network initialization done.
I1028 00:17:29.384757 30553 net.cpp:220] Memory required for data: 1837200
I1028 00:54:28.993262  6519 convert_imageset.cpp:70] Shuffling data
I1028 00:54:29.773054  6519 convert_imageset.cpp:73] A total of 60000 images.
I1028 00:54:29.773135  6519 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 00:54:32.179419  6519 convert_imageset.cpp:177] Processed 1000 files.
E1028 00:54:34.297626  6519 convert_imageset.cpp:177] Processed 2000 files.
E1028 00:54:36.599712  6519 convert_imageset.cpp:177] Processed 3000 files.
E1028 00:54:38.758431  6519 convert_imageset.cpp:177] Processed 4000 files.
E1028 00:54:40.775557  6519 convert_imageset.cpp:177] Processed 5000 files.
E1028 00:54:42.838040  6519 convert_imageset.cpp:177] Processed 6000 files.
E1028 00:54:44.867360  6519 convert_imageset.cpp:177] Processed 7000 files.
E1028 00:54:46.902235  6519 convert_imageset.cpp:177] Processed 8000 files.
E1028 00:54:48.749975  6519 convert_imageset.cpp:177] Processed 9000 files.
E1028 00:54:50.510766  6519 convert_imageset.cpp:177] Processed 10000 files.
E1028 00:54:52.272152  6519 convert_imageset.cpp:177] Processed 11000 files.
E1028 00:54:54.153789  6519 convert_imageset.cpp:177] Processed 12000 files.
E1028 00:54:55.948454  6519 convert_imageset.cpp:177] Processed 13000 files.
E1028 00:54:57.692656  6519 convert_imageset.cpp:177] Processed 14000 files.
E1028 00:54:59.532477  6519 convert_imageset.cpp:177] Processed 15000 files.
E1028 00:55:01.435232  6519 convert_imageset.cpp:177] Processed 16000 files.
E1028 00:55:03.157264  6519 convert_imageset.cpp:177] Processed 17000 files.
E1028 00:55:05.058601  6519 convert_imageset.cpp:177] Processed 18000 files.
E1028 00:55:06.890769  6519 convert_imageset.cpp:177] Processed 19000 files.
E1028 00:55:08.702646  6519 convert_imageset.cpp:177] Processed 20000 files.
E1028 00:55:10.371116  6519 convert_imageset.cpp:177] Processed 21000 files.
E1028 00:55:12.120682  6519 convert_imageset.cpp:177] Processed 22000 files.
E1028 00:55:13.796493  6519 convert_imageset.cpp:177] Processed 23000 files.
E1028 00:55:15.627257  6519 convert_imageset.cpp:177] Processed 24000 files.
E1028 00:55:17.235647  6519 convert_imageset.cpp:177] Processed 25000 files.
E1028 00:55:18.949486  6519 convert_imageset.cpp:177] Processed 26000 files.
E1028 00:55:20.577718  6519 convert_imageset.cpp:177] Processed 27000 files.
E1028 00:55:22.124893  6519 convert_imageset.cpp:177] Processed 28000 files.
E1028 00:55:23.751785  6519 convert_imageset.cpp:177] Processed 29000 files.
E1028 00:55:25.431288  6519 convert_imageset.cpp:177] Processed 30000 files.
E1028 00:55:27.137919  6519 convert_imageset.cpp:177] Processed 31000 files.
E1028 00:55:28.741611  6519 convert_imageset.cpp:177] Processed 32000 files.
E1028 00:55:30.595214  6519 convert_imageset.cpp:177] Processed 33000 files.
E1028 00:55:32.208497  6519 convert_imageset.cpp:177] Processed 34000 files.
E1028 00:55:33.757886  6519 convert_imageset.cpp:177] Processed 35000 files.
E1028 00:55:35.375793  6519 convert_imageset.cpp:177] Processed 36000 files.
E1028 00:55:36.957844  6519 convert_imageset.cpp:177] Processed 37000 files.
E1028 00:55:38.663043  6519 convert_imageset.cpp:177] Processed 38000 files.
E1028 00:55:40.213213  6519 convert_imageset.cpp:177] Processed 39000 files.
E1028 00:55:41.771070  6519 convert_imageset.cpp:177] Processed 40000 files.
E1028 00:55:43.423959  6519 convert_imageset.cpp:177] Processed 41000 files.
E1028 00:55:45.047286  6519 convert_imageset.cpp:177] Processed 42000 files.
E1028 00:55:46.744153  6519 convert_imageset.cpp:177] Processed 43000 files.
E1028 00:55:48.276059  6519 convert_imageset.cpp:177] Processed 44000 files.
E1028 00:55:49.857933  6519 convert_imageset.cpp:177] Processed 45000 files.
E1028 00:55:51.482668  6519 convert_imageset.cpp:177] Processed 46000 files.
E1028 00:55:53.191056  6519 convert_imageset.cpp:177] Processed 47000 files.
E1028 00:55:54.740293  6519 convert_imageset.cpp:177] Processed 48000 files.
E1028 00:55:56.275187  6519 convert_imageset.cpp:177] Processed 49000 files.
E1028 00:55:57.789459  6519 convert_imageset.cpp:177] Processed 50000 files.
E1028 00:55:59.336309  6519 convert_imageset.cpp:177] Processed 51000 files.
E1028 00:56:00.974273  6519 convert_imageset.cpp:177] Processed 52000 files.
E1028 00:56:02.715538  6519 convert_imageset.cpp:177] Processed 53000 files.
E1028 00:56:04.273744  6519 convert_imageset.cpp:177] Processed 54000 files.
E1028 00:56:05.916939  6519 convert_imageset.cpp:177] Processed 55000 files.
E1028 00:56:07.761241  6519 convert_imageset.cpp:177] Processed 56000 files.
E1028 00:56:09.316792  6519 convert_imageset.cpp:177] Processed 57000 files.
E1028 00:56:11.000744  6519 convert_imageset.cpp:177] Processed 58000 files.
E1028 00:56:12.539420  6519 convert_imageset.cpp:177] Processed 59000 files.
E1028 00:56:14.123589  6519 convert_imageset.cpp:177] Processed 60000 files.
I1028 00:56:14.433665  6745 caffe.cpp:99] Use GPU with device ID 0
I1028 00:56:14.816848  6745 caffe.cpp:107] Starting Optimization
I1028 00:56:14.816970  6745 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 350000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 00:56:14.816997  6745 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 00:56:14.825207  6745 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 00:56:14.825312  6745 net.cpp:67] Creating Layer mnist
I1028 00:56:14.825325  6745 net.cpp:356] mnist -> data
I1028 00:56:14.825341  6745 net.cpp:356] mnist -> label
I1028 00:56:14.825356  6745 net.cpp:96] Setting up mnist
I1028 00:56:14.832295  6745 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 00:56:14.832392  6745 data_layer.cpp:128] output data size: 64,1,50,180
I1028 00:56:14.833232  6745 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 00:56:14.833256  6745 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 00:56:14.833269  6745 net.cpp:67] Creating Layer conv1
I1028 00:56:14.833274  6745 net.cpp:394] conv1 <- data
I1028 00:56:14.833292  6745 net.cpp:356] conv1 -> conv1
I1028 00:56:14.833304  6745 net.cpp:96] Setting up conv1
I1028 00:56:14.833665  6745 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 00:56:14.833699  6745 net.cpp:67] Creating Layer pool1
I1028 00:56:14.833705  6745 net.cpp:394] pool1 <- conv1
I1028 00:56:14.833711  6745 net.cpp:356] pool1 -> pool1
I1028 00:56:14.833719  6745 net.cpp:96] Setting up pool1
I1028 00:56:14.833734  6745 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 00:56:14.833742  6745 net.cpp:67] Creating Layer relu1
I1028 00:56:14.833747  6745 net.cpp:394] relu1 <- pool1
I1028 00:56:14.833755  6745 net.cpp:345] relu1 -> pool1 (in-place)
I1028 00:56:14.833762  6745 net.cpp:96] Setting up relu1
I1028 00:56:14.833767  6745 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 00:56:14.833775  6745 net.cpp:67] Creating Layer drop1
I1028 00:56:14.833780  6745 net.cpp:394] drop1 <- pool1
I1028 00:56:14.833786  6745 net.cpp:345] drop1 -> pool1 (in-place)
I1028 00:56:14.833791  6745 net.cpp:96] Setting up drop1
I1028 00:56:14.833796  6745 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 00:56:14.833806  6745 net.cpp:67] Creating Layer conv2
I1028 00:56:14.833811  6745 net.cpp:394] conv2 <- pool1
I1028 00:56:14.833817  6745 net.cpp:356] conv2 -> conv2
I1028 00:56:14.833824  6745 net.cpp:96] Setting up conv2
I1028 00:56:14.834405  6745 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 00:56:14.834422  6745 net.cpp:67] Creating Layer pool2
I1028 00:56:14.834429  6745 net.cpp:394] pool2 <- conv2
I1028 00:56:14.834436  6745 net.cpp:356] pool2 -> pool2
I1028 00:56:14.834444  6745 net.cpp:96] Setting up pool2
I1028 00:56:14.834450  6745 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 00:56:14.834457  6745 net.cpp:67] Creating Layer relu2
I1028 00:56:14.834461  6745 net.cpp:394] relu2 <- pool2
I1028 00:56:14.834470  6745 net.cpp:345] relu2 -> pool2 (in-place)
I1028 00:56:14.834475  6745 net.cpp:96] Setting up relu2
I1028 00:56:14.834481  6745 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 00:56:14.834487  6745 net.cpp:67] Creating Layer drop2
I1028 00:56:14.834492  6745 net.cpp:394] drop2 <- pool2
I1028 00:56:14.834498  6745 net.cpp:345] drop2 -> pool2 (in-place)
I1028 00:56:14.834509  6745 net.cpp:96] Setting up drop2
I1028 00:56:14.834514  6745 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 00:56:14.834522  6745 net.cpp:67] Creating Layer conv3
I1028 00:56:14.834527  6745 net.cpp:394] conv3 <- pool2
I1028 00:56:14.834535  6745 net.cpp:356] conv3 -> conv3
I1028 00:56:14.834543  6745 net.cpp:96] Setting up conv3
I1028 00:56:14.836092  6745 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 00:56:14.836119  6745 net.cpp:67] Creating Layer pool3
I1028 00:56:14.836124  6745 net.cpp:394] pool3 <- conv3
I1028 00:56:14.836133  6745 net.cpp:356] pool3 -> pool3
I1028 00:56:14.836141  6745 net.cpp:96] Setting up pool3
I1028 00:56:14.836148  6745 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 00:56:14.836154  6745 net.cpp:67] Creating Layer relu3
I1028 00:56:14.836158  6745 net.cpp:394] relu3 <- pool3
I1028 00:56:14.836164  6745 net.cpp:345] relu3 -> pool3 (in-place)
I1028 00:56:14.836170  6745 net.cpp:96] Setting up relu3
I1028 00:56:14.836174  6745 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 00:56:14.836180  6745 net.cpp:67] Creating Layer drop3
I1028 00:56:14.836185  6745 net.cpp:394] drop3 <- pool3
I1028 00:56:14.836192  6745 net.cpp:345] drop3 -> pool3 (in-place)
I1028 00:56:14.836199  6745 net.cpp:96] Setting up drop3
I1028 00:56:14.836205  6745 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 00:56:14.836212  6745 net.cpp:67] Creating Layer ip1
I1028 00:56:14.836216  6745 net.cpp:394] ip1 <- pool3
I1028 00:56:14.836225  6745 net.cpp:356] ip1 -> ip1
I1028 00:56:14.836261  6745 net.cpp:96] Setting up ip1
I1028 00:56:15.274320  6745 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 00:56:15.274379  6745 net.cpp:67] Creating Layer relu4
I1028 00:56:15.274387  6745 net.cpp:394] relu4 <- ip1
I1028 00:56:15.274399  6745 net.cpp:345] relu4 -> ip1 (in-place)
I1028 00:56:15.274408  6745 net.cpp:96] Setting up relu4
I1028 00:56:15.274413  6745 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 00:56:15.274420  6745 net.cpp:67] Creating Layer drop4
I1028 00:56:15.274425  6745 net.cpp:394] drop4 <- ip1
I1028 00:56:15.274430  6745 net.cpp:345] drop4 -> ip1 (in-place)
I1028 00:56:15.274436  6745 net.cpp:96] Setting up drop4
I1028 00:56:15.274441  6745 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 00:56:15.274454  6745 net.cpp:67] Creating Layer ip2
I1028 00:56:15.274458  6745 net.cpp:394] ip2 <- ip1
I1028 00:56:15.274466  6745 net.cpp:356] ip2 -> ip2
I1028 00:56:15.274473  6745 net.cpp:96] Setting up ip2
I1028 00:56:15.286046  6745 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 00:56:15.286106  6745 net.cpp:67] Creating Layer loss
I1028 00:56:15.286113  6745 net.cpp:394] loss <- ip2
I1028 00:56:15.286121  6745 net.cpp:394] loss <- label
I1028 00:56:15.286131  6745 net.cpp:356] loss -> loss
I1028 00:56:15.286141  6745 net.cpp:96] Setting up loss
I1028 00:56:15.286152  6745 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 00:56:15.286157  6745 net.cpp:109]     with loss weight 1
I1028 00:56:15.286193  6745 net.cpp:170] loss needs backward computation.
I1028 00:56:15.286197  6745 net.cpp:170] ip2 needs backward computation.
I1028 00:56:15.286202  6745 net.cpp:170] drop4 needs backward computation.
I1028 00:56:15.286206  6745 net.cpp:170] relu4 needs backward computation.
I1028 00:56:15.286211  6745 net.cpp:170] ip1 needs backward computation.
I1028 00:56:15.286216  6745 net.cpp:170] drop3 needs backward computation.
I1028 00:56:15.286219  6745 net.cpp:170] relu3 needs backward computation.
I1028 00:56:15.286224  6745 net.cpp:170] pool3 needs backward computation.
I1028 00:56:15.286228  6745 net.cpp:170] conv3 needs backward computation.
I1028 00:56:15.286233  6745 net.cpp:170] drop2 needs backward computation.
I1028 00:56:15.286237  6745 net.cpp:170] relu2 needs backward computation.
I1028 00:56:15.286242  6745 net.cpp:170] pool2 needs backward computation.
I1028 00:56:15.286247  6745 net.cpp:170] conv2 needs backward computation.
I1028 00:56:15.286252  6745 net.cpp:170] drop1 needs backward computation.
I1028 00:56:15.286255  6745 net.cpp:170] relu1 needs backward computation.
I1028 00:56:15.286272  6745 net.cpp:170] pool1 needs backward computation.
I1028 00:56:15.286278  6745 net.cpp:170] conv1 needs backward computation.
I1028 00:56:15.286283  6745 net.cpp:172] mnist does not need backward computation.
I1028 00:56:15.286286  6745 net.cpp:208] This network produces output loss
I1028 00:56:15.286299  6745 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 00:56:15.286309  6745 net.cpp:219] Network initialization done.
I1028 00:56:15.286312  6745 net.cpp:220] Memory required for data: 119788292
I1028 00:56:15.286373  6745 solver.cpp:41] Solver scaffolding done.
I1028 00:56:15.286380  6745 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_335000.solverstate
I1028 00:56:15.286384  6745 solver.cpp:160] Solving Captcha
I1028 00:56:15.286403  6745 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_335000.solverstate
I1028 00:56:21.029665  6745 solver.cpp:502] SGDSolver: restoring history
I1028 00:56:21.781837  6745 solver.cpp:191] Iteration 335000, loss = 2.43963
I1028 00:56:21.781895  6745 solver.cpp:206]     Train net output #0: loss = 2.43963 (* 1 = 2.43963 loss)
I1028 00:56:21.781909  6745 solver.cpp:403] Iteration 335000, lr = 0.000702483
I1028 01:00:23.479816  6745 solver.cpp:191] Iteration 336000, loss = 2.51896
I1028 01:00:23.480521  6745 solver.cpp:206]     Train net output #0: loss = 2.51896 (* 1 = 2.51896 loss)
I1028 01:00:23.480556  6745 solver.cpp:403] Iteration 336000, lr = 0.00070096
I1028 01:04:24.774914  6745 solver.cpp:191] Iteration 337000, loss = 2.50666
I1028 01:04:24.775468  6745 solver.cpp:206]     Train net output #0: loss = 2.50666 (* 1 = 2.50666 loss)
I1028 01:04:24.775504  6745 solver.cpp:403] Iteration 337000, lr = 0.000699444
I1028 01:08:26.117470  6745 solver.cpp:191] Iteration 338000, loss = 2.44412
I1028 01:08:26.118124  6745 solver.cpp:206]     Train net output #0: loss = 2.44412 (* 1 = 2.44412 loss)
I1028 01:08:26.118157  6745 solver.cpp:403] Iteration 338000, lr = 0.000697936
I1028 01:12:27.545578  6745 solver.cpp:191] Iteration 339000, loss = 2.40806
I1028 01:12:27.546375  6745 solver.cpp:206]     Train net output #0: loss = 2.40806 (* 1 = 2.40806 loss)
I1028 01:12:27.546407  6745 solver.cpp:403] Iteration 339000, lr = 0.000696436
I1028 01:16:29.558073  6745 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_340000.caffemodel
I1028 01:16:34.001590  6745 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_340000.solverstate
I1028 01:16:37.874541  6745 solver.cpp:191] Iteration 340000, loss = 2.34513
I1028 01:16:37.875064  6745 solver.cpp:206]     Train net output #0: loss = 2.34513 (* 1 = 2.34513 loss)
I1028 01:16:37.875099  6745 solver.cpp:403] Iteration 340000, lr = 0.000694943
I1028 01:20:39.252827  6745 solver.cpp:191] Iteration 341000, loss = 2.54357
I1028 01:20:39.253437  6745 solver.cpp:206]     Train net output #0: loss = 2.54357 (* 1 = 2.54357 loss)
I1028 01:20:39.253469  6745 solver.cpp:403] Iteration 341000, lr = 0.000693457
I1028 01:24:40.535733  6745 solver.cpp:191] Iteration 342000, loss = 2.45978
I1028 01:24:40.536382  6745 solver.cpp:206]     Train net output #0: loss = 2.45978 (* 1 = 2.45978 loss)
I1028 01:24:40.536414  6745 solver.cpp:403] Iteration 342000, lr = 0.000691979
I1028 01:28:42.015105  6745 solver.cpp:191] Iteration 343000, loss = 2.37942
I1028 01:28:42.015640  6745 solver.cpp:206]     Train net output #0: loss = 2.37942 (* 1 = 2.37942 loss)
I1028 01:28:42.015676  6745 solver.cpp:403] Iteration 343000, lr = 0.000690508
I1028 01:32:43.378233  6745 solver.cpp:191] Iteration 344000, loss = 2.52294
I1028 01:32:43.378798  6745 solver.cpp:206]     Train net output #0: loss = 2.52294 (* 1 = 2.52294 loss)
I1028 01:32:43.378831  6745 solver.cpp:403] Iteration 344000, lr = 0.000689045
I1028 01:36:45.226728  6745 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_345000.caffemodel
I1028 01:36:49.508172  6745 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_345000.solverstate
I1028 01:36:53.555261  6745 solver.cpp:191] Iteration 345000, loss = 2.35823
I1028 01:36:53.555790  6745 solver.cpp:206]     Train net output #0: loss = 2.35823 (* 1 = 2.35823 loss)
I1028 01:36:53.555827  6745 solver.cpp:403] Iteration 345000, lr = 0.000687589
I1028 01:40:54.919217  6745 solver.cpp:191] Iteration 346000, loss = 2.40139
I1028 01:40:54.919975  6745 solver.cpp:206]     Train net output #0: loss = 2.40139 (* 1 = 2.40139 loss)
I1028 01:40:54.920007  6745 solver.cpp:403] Iteration 346000, lr = 0.00068614
I1028 01:44:56.291693  6745 solver.cpp:191] Iteration 347000, loss = 2.42332
I1028 01:44:56.292271  6745 solver.cpp:206]     Train net output #0: loss = 2.42332 (* 1 = 2.42332 loss)
I1028 01:44:56.292304  6745 solver.cpp:403] Iteration 347000, lr = 0.000684698
I1028 01:48:57.650048  6745 solver.cpp:191] Iteration 348000, loss = 2.40069
I1028 01:48:57.650665  6745 solver.cpp:206]     Train net output #0: loss = 2.40069 (* 1 = 2.40069 loss)
I1028 01:48:57.650697  6745 solver.cpp:403] Iteration 348000, lr = 0.000683263
I1028 01:52:59.074306  6745 solver.cpp:191] Iteration 349000, loss = 2.24773
I1028 01:52:59.075000  6745 solver.cpp:206]     Train net output #0: loss = 2.24773 (* 1 = 2.24773 loss)
I1028 01:52:59.075032  6745 solver.cpp:403] Iteration 349000, lr = 0.000681835
I1028 01:57:00.950539  6745 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_350000.caffemodel
I1028 01:57:05.472832  6745 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_350000.solverstate
I1028 01:57:09.438168  6745 solver.cpp:228] Iteration 350000, loss = 2.18207
I1028 01:57:09.438827  6745 solver.cpp:233] Optimization Done.
I1028 01:57:09.447319  6745 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 02:27:36.700973  2951 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 02:27:36.701079  2951 net.cpp:358] Input 0 -> data
I1028 02:27:36.701108  2951 net.cpp:67] Creating Layer conv1
I1028 02:27:36.701114  2951 net.cpp:394] conv1 <- data
I1028 02:27:36.701120  2951 net.cpp:356] conv1 -> conv1
I1028 02:27:36.701130  2951 net.cpp:96] Setting up conv1
I1028 02:27:36.701457  2951 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 02:27:36.701478  2951 net.cpp:67] Creating Layer pool1
I1028 02:27:36.701483  2951 net.cpp:394] pool1 <- conv1
I1028 02:27:36.701488  2951 net.cpp:356] pool1 -> pool1
I1028 02:27:36.701496  2951 net.cpp:96] Setting up pool1
I1028 02:27:36.701508  2951 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 02:27:36.701515  2951 net.cpp:67] Creating Layer relu1
I1028 02:27:36.701519  2951 net.cpp:394] relu1 <- pool1
I1028 02:27:36.701527  2951 net.cpp:345] relu1 -> pool1 (in-place)
I1028 02:27:36.701534  2951 net.cpp:96] Setting up relu1
I1028 02:27:36.701537  2951 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 02:27:36.701544  2951 net.cpp:67] Creating Layer drop1
I1028 02:27:36.701547  2951 net.cpp:394] drop1 <- pool1
I1028 02:27:36.701552  2951 net.cpp:345] drop1 -> pool1 (in-place)
I1028 02:27:36.701558  2951 net.cpp:96] Setting up drop1
I1028 02:27:36.701563  2951 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 02:27:36.701570  2951 net.cpp:67] Creating Layer conv2
I1028 02:27:36.701575  2951 net.cpp:394] conv2 <- pool1
I1028 02:27:36.701580  2951 net.cpp:356] conv2 -> conv2
I1028 02:27:36.701586  2951 net.cpp:96] Setting up conv2
I1028 02:27:36.702144  2951 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 02:27:36.702159  2951 net.cpp:67] Creating Layer pool2
I1028 02:27:36.702163  2951 net.cpp:394] pool2 <- conv2
I1028 02:27:36.702172  2951 net.cpp:356] pool2 -> pool2
I1028 02:27:36.702178  2951 net.cpp:96] Setting up pool2
I1028 02:27:36.702184  2951 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 02:27:36.702189  2951 net.cpp:67] Creating Layer relu2
I1028 02:27:36.702193  2951 net.cpp:394] relu2 <- pool2
I1028 02:27:36.702200  2951 net.cpp:345] relu2 -> pool2 (in-place)
I1028 02:27:36.702206  2951 net.cpp:96] Setting up relu2
I1028 02:27:36.702210  2951 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 02:27:36.702215  2951 net.cpp:67] Creating Layer drop2
I1028 02:27:36.702219  2951 net.cpp:394] drop2 <- pool2
I1028 02:27:36.702224  2951 net.cpp:345] drop2 -> pool2 (in-place)
I1028 02:27:36.702229  2951 net.cpp:96] Setting up drop2
I1028 02:27:36.702234  2951 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 02:27:36.702241  2951 net.cpp:67] Creating Layer conv3
I1028 02:27:36.702245  2951 net.cpp:394] conv3 <- pool2
I1028 02:27:36.702252  2951 net.cpp:356] conv3 -> conv3
I1028 02:27:36.702260  2951 net.cpp:96] Setting up conv3
I1028 02:27:36.703740  2951 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 02:27:36.703758  2951 net.cpp:67] Creating Layer pool3
I1028 02:27:36.703761  2951 net.cpp:394] pool3 <- conv3
I1028 02:27:36.703769  2951 net.cpp:356] pool3 -> pool3
I1028 02:27:36.703776  2951 net.cpp:96] Setting up pool3
I1028 02:27:36.703781  2951 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 02:27:36.703786  2951 net.cpp:67] Creating Layer relu3
I1028 02:27:36.703790  2951 net.cpp:394] relu3 <- pool3
I1028 02:27:36.703799  2951 net.cpp:345] relu3 -> pool3 (in-place)
I1028 02:27:36.703804  2951 net.cpp:96] Setting up relu3
I1028 02:27:36.703809  2951 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 02:27:36.703814  2951 net.cpp:67] Creating Layer drop3
I1028 02:27:36.703817  2951 net.cpp:394] drop3 <- pool3
I1028 02:27:36.703824  2951 net.cpp:345] drop3 -> pool3 (in-place)
I1028 02:27:36.703830  2951 net.cpp:96] Setting up drop3
I1028 02:27:36.703835  2951 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 02:27:36.703841  2951 net.cpp:67] Creating Layer ip1
I1028 02:27:36.703845  2951 net.cpp:394] ip1 <- pool3
I1028 02:27:36.703852  2951 net.cpp:356] ip1 -> ip1
I1028 02:27:36.703860  2951 net.cpp:96] Setting up ip1
I1028 02:27:37.202807  2951 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 02:27:37.202868  2951 net.cpp:67] Creating Layer relu4
I1028 02:27:37.202875  2951 net.cpp:394] relu4 <- ip1
I1028 02:27:37.202883  2951 net.cpp:345] relu4 -> ip1 (in-place)
I1028 02:27:37.202893  2951 net.cpp:96] Setting up relu4
I1028 02:27:37.202898  2951 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 02:27:37.202908  2951 net.cpp:67] Creating Layer drop4
I1028 02:27:37.202911  2951 net.cpp:394] drop4 <- ip1
I1028 02:27:37.202918  2951 net.cpp:345] drop4 -> ip1 (in-place)
I1028 02:27:37.202924  2951 net.cpp:96] Setting up drop4
I1028 02:27:37.202929  2951 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 02:27:37.202936  2951 net.cpp:67] Creating Layer ip2
I1028 02:27:37.202940  2951 net.cpp:394] ip2 <- ip1
I1028 02:27:37.202950  2951 net.cpp:356] ip2 -> ip2
I1028 02:27:37.202962  2951 net.cpp:96] Setting up ip2
I1028 02:27:37.212777  2951 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 02:27:37.212841  2951 net.cpp:67] Creating Layer prob
I1028 02:27:37.212848  2951 net.cpp:394] prob <- ip2
I1028 02:27:37.212857  2951 net.cpp:356] prob -> prob
I1028 02:27:37.212867  2951 net.cpp:96] Setting up prob
I1028 02:27:37.212875  2951 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 02:27:37.212880  2951 net.cpp:172] prob does not need backward computation.
I1028 02:27:37.212884  2951 net.cpp:172] ip2 does not need backward computation.
I1028 02:27:37.212888  2951 net.cpp:172] drop4 does not need backward computation.
I1028 02:27:37.212891  2951 net.cpp:172] relu4 does not need backward computation.
I1028 02:27:37.212894  2951 net.cpp:172] ip1 does not need backward computation.
I1028 02:27:37.212898  2951 net.cpp:172] drop3 does not need backward computation.
I1028 02:27:37.212901  2951 net.cpp:172] relu3 does not need backward computation.
I1028 02:27:37.212905  2951 net.cpp:172] pool3 does not need backward computation.
I1028 02:27:37.212908  2951 net.cpp:172] conv3 does not need backward computation.
I1028 02:27:37.212913  2951 net.cpp:172] drop2 does not need backward computation.
I1028 02:27:37.212915  2951 net.cpp:172] relu2 does not need backward computation.
I1028 02:27:37.212919  2951 net.cpp:172] pool2 does not need backward computation.
I1028 02:27:37.212923  2951 net.cpp:172] conv2 does not need backward computation.
I1028 02:27:37.212926  2951 net.cpp:172] drop1 does not need backward computation.
I1028 02:27:37.212929  2951 net.cpp:172] relu1 does not need backward computation.
I1028 02:27:37.212934  2951 net.cpp:172] pool1 does not need backward computation.
I1028 02:27:37.212936  2951 net.cpp:172] conv1 does not need backward computation.
I1028 02:27:37.212940  2951 net.cpp:208] This network produces output prob
I1028 02:27:37.212954  2951 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 02:27:37.212960  2951 net.cpp:219] Network initialization done.
I1028 02:27:37.212965  2951 net.cpp:220] Memory required for data: 1837200
I1028 02:28:21.022735  2951 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 02:28:21.023352  2951 net.cpp:358] Input 0 -> data
I1028 02:28:21.023408  2951 net.cpp:67] Creating Layer conv1
I1028 02:28:21.023423  2951 net.cpp:394] conv1 <- data
I1028 02:28:21.023442  2951 net.cpp:356] conv1 -> conv1
I1028 02:28:21.023466  2951 net.cpp:96] Setting up conv1
I1028 02:28:21.023532  2951 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 02:28:21.023569  2951 net.cpp:67] Creating Layer pool1
I1028 02:28:21.023582  2951 net.cpp:394] pool1 <- conv1
I1028 02:28:21.023597  2951 net.cpp:356] pool1 -> pool1
I1028 02:28:21.023617  2951 net.cpp:96] Setting up pool1
I1028 02:28:21.023634  2951 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 02:28:21.023653  2951 net.cpp:67] Creating Layer relu1
I1028 02:28:21.023664  2951 net.cpp:394] relu1 <- pool1
I1028 02:28:21.023679  2951 net.cpp:345] relu1 -> pool1 (in-place)
I1028 02:28:21.023694  2951 net.cpp:96] Setting up relu1
I1028 02:28:21.023706  2951 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 02:28:21.023722  2951 net.cpp:67] Creating Layer drop1
I1028 02:28:21.023735  2951 net.cpp:394] drop1 <- pool1
I1028 02:28:21.023749  2951 net.cpp:345] drop1 -> pool1 (in-place)
I1028 02:28:21.023766  2951 net.cpp:96] Setting up drop1
I1028 02:28:21.023779  2951 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 02:28:21.023798  2951 net.cpp:67] Creating Layer conv2
I1028 02:28:21.023809  2951 net.cpp:394] conv2 <- pool1
I1028 02:28:21.023826  2951 net.cpp:356] conv2 -> conv2
I1028 02:28:21.023845  2951 net.cpp:96] Setting up conv2
I1028 02:28:21.025288  2951 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 02:28:21.025328  2951 net.cpp:67] Creating Layer pool2
I1028 02:28:21.025342  2951 net.cpp:394] pool2 <- conv2
I1028 02:28:21.025367  2951 net.cpp:356] pool2 -> pool2
I1028 02:28:21.025388  2951 net.cpp:96] Setting up pool2
I1028 02:28:21.025404  2951 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 02:28:21.025419  2951 net.cpp:67] Creating Layer relu2
I1028 02:28:21.025431  2951 net.cpp:394] relu2 <- pool2
I1028 02:28:21.025445  2951 net.cpp:345] relu2 -> pool2 (in-place)
I1028 02:28:21.025461  2951 net.cpp:96] Setting up relu2
I1028 02:28:21.025473  2951 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 02:28:21.025488  2951 net.cpp:67] Creating Layer drop2
I1028 02:28:21.025501  2951 net.cpp:394] drop2 <- pool2
I1028 02:28:21.025514  2951 net.cpp:345] drop2 -> pool2 (in-place)
I1028 02:28:21.025531  2951 net.cpp:96] Setting up drop2
I1028 02:28:21.025543  2951 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 02:28:21.025563  2951 net.cpp:67] Creating Layer conv3
I1028 02:28:21.025575  2951 net.cpp:394] conv3 <- pool2
I1028 02:28:21.025591  2951 net.cpp:356] conv3 -> conv3
I1028 02:28:21.025610  2951 net.cpp:96] Setting up conv3
I1028 02:28:21.029270  2951 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 02:28:21.029307  2951 net.cpp:67] Creating Layer pool3
I1028 02:28:21.029321  2951 net.cpp:394] pool3 <- conv3
I1028 02:28:21.029337  2951 net.cpp:356] pool3 -> pool3
I1028 02:28:21.029356  2951 net.cpp:96] Setting up pool3
I1028 02:28:21.029371  2951 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 02:28:21.029386  2951 net.cpp:67] Creating Layer relu3
I1028 02:28:21.029397  2951 net.cpp:394] relu3 <- pool3
I1028 02:28:21.029412  2951 net.cpp:345] relu3 -> pool3 (in-place)
I1028 02:28:21.029428  2951 net.cpp:96] Setting up relu3
I1028 02:28:21.029439  2951 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 02:28:21.029454  2951 net.cpp:67] Creating Layer drop3
I1028 02:28:21.029465  2951 net.cpp:394] drop3 <- pool3
I1028 02:28:21.029480  2951 net.cpp:345] drop3 -> pool3 (in-place)
I1028 02:28:21.029495  2951 net.cpp:96] Setting up drop3
I1028 02:28:21.029508  2951 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 02:28:21.029525  2951 net.cpp:67] Creating Layer ip1
I1028 02:28:21.029537  2951 net.cpp:394] ip1 <- pool3
I1028 02:28:21.029553  2951 net.cpp:356] ip1 -> ip1
I1028 02:28:21.029572  2951 net.cpp:96] Setting up ip1
I1028 02:28:21.439872  2951 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 02:28:21.439934  2951 net.cpp:67] Creating Layer relu4
I1028 02:28:21.439944  2951 net.cpp:394] relu4 <- ip1
I1028 02:28:21.439954  2951 net.cpp:345] relu4 -> ip1 (in-place)
I1028 02:28:21.439963  2951 net.cpp:96] Setting up relu4
I1028 02:28:21.439968  2951 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 02:28:21.439977  2951 net.cpp:67] Creating Layer drop4
I1028 02:28:21.439981  2951 net.cpp:394] drop4 <- ip1
I1028 02:28:21.439987  2951 net.cpp:345] drop4 -> ip1 (in-place)
I1028 02:28:21.439995  2951 net.cpp:96] Setting up drop4
I1028 02:28:21.440001  2951 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 02:28:21.440009  2951 net.cpp:67] Creating Layer ip2
I1028 02:28:21.440014  2951 net.cpp:394] ip2 <- ip1
I1028 02:28:21.440022  2951 net.cpp:356] ip2 -> ip2
I1028 02:28:21.440035  2951 net.cpp:96] Setting up ip2
I1028 02:28:21.447789  2951 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 02:28:21.447852  2951 net.cpp:67] Creating Layer prob
I1028 02:28:21.447860  2951 net.cpp:394] prob <- ip2
I1028 02:28:21.447870  2951 net.cpp:356] prob -> prob
I1028 02:28:21.447881  2951 net.cpp:96] Setting up prob
I1028 02:28:21.447890  2951 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 02:28:21.447895  2951 net.cpp:172] prob does not need backward computation.
I1028 02:28:21.447898  2951 net.cpp:172] ip2 does not need backward computation.
I1028 02:28:21.447902  2951 net.cpp:172] drop4 does not need backward computation.
I1028 02:28:21.447906  2951 net.cpp:172] relu4 does not need backward computation.
I1028 02:28:21.447911  2951 net.cpp:172] ip1 does not need backward computation.
I1028 02:28:21.447914  2951 net.cpp:172] drop3 does not need backward computation.
I1028 02:28:21.447918  2951 net.cpp:172] relu3 does not need backward computation.
I1028 02:28:21.447929  2951 net.cpp:172] pool3 does not need backward computation.
I1028 02:28:21.447934  2951 net.cpp:172] conv3 does not need backward computation.
I1028 02:28:21.447938  2951 net.cpp:172] drop2 does not need backward computation.
I1028 02:28:21.447942  2951 net.cpp:172] relu2 does not need backward computation.
I1028 02:28:21.447945  2951 net.cpp:172] pool2 does not need backward computation.
I1028 02:28:21.447949  2951 net.cpp:172] conv2 does not need backward computation.
I1028 02:28:21.447953  2951 net.cpp:172] drop1 does not need backward computation.
I1028 02:28:21.447957  2951 net.cpp:172] relu1 does not need backward computation.
I1028 02:28:21.447960  2951 net.cpp:172] pool1 does not need backward computation.
I1028 02:28:21.447964  2951 net.cpp:172] conv1 does not need backward computation.
I1028 02:28:21.447968  2951 net.cpp:208] This network produces output prob
I1028 02:28:21.447983  2951 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 02:28:21.447993  2951 net.cpp:219] Network initialization done.
I1028 02:28:21.447998  2951 net.cpp:220] Memory required for data: 1837200
I1028 02:28:58.994277  2951 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 02:28:58.994858  2951 net.cpp:358] Input 0 -> data
I1028 02:28:58.994915  2951 net.cpp:67] Creating Layer conv1
I1028 02:28:58.994945  2951 net.cpp:394] conv1 <- data
I1028 02:28:58.994966  2951 net.cpp:356] conv1 -> conv1
I1028 02:28:58.994990  2951 net.cpp:96] Setting up conv1
I1028 02:28:58.995059  2951 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 02:28:58.995095  2951 net.cpp:67] Creating Layer pool1
I1028 02:28:58.995110  2951 net.cpp:394] pool1 <- conv1
I1028 02:28:58.995126  2951 net.cpp:356] pool1 -> pool1
I1028 02:28:58.995144  2951 net.cpp:96] Setting up pool1
I1028 02:28:58.995162  2951 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 02:28:58.995179  2951 net.cpp:67] Creating Layer relu1
I1028 02:28:58.995192  2951 net.cpp:394] relu1 <- pool1
I1028 02:28:58.995205  2951 net.cpp:345] relu1 -> pool1 (in-place)
I1028 02:28:58.995221  2951 net.cpp:96] Setting up relu1
I1028 02:28:58.995231  2951 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 02:28:58.995237  2951 net.cpp:67] Creating Layer drop1
I1028 02:28:58.995241  2951 net.cpp:394] drop1 <- pool1
I1028 02:28:58.995247  2951 net.cpp:345] drop1 -> pool1 (in-place)
I1028 02:28:58.995254  2951 net.cpp:96] Setting up drop1
I1028 02:28:58.995260  2951 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 02:28:58.995266  2951 net.cpp:67] Creating Layer conv2
I1028 02:28:58.995270  2951 net.cpp:394] conv2 <- pool1
I1028 02:28:58.995277  2951 net.cpp:356] conv2 -> conv2
I1028 02:28:58.995301  2951 net.cpp:96] Setting up conv2
I1028 02:28:58.996776  2951 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 02:28:58.996817  2951 net.cpp:67] Creating Layer pool2
I1028 02:28:58.996830  2951 net.cpp:394] pool2 <- conv2
I1028 02:28:58.996847  2951 net.cpp:356] pool2 -> pool2
I1028 02:28:58.996867  2951 net.cpp:96] Setting up pool2
I1028 02:28:58.996884  2951 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 02:28:58.996899  2951 net.cpp:67] Creating Layer relu2
I1028 02:28:58.996911  2951 net.cpp:394] relu2 <- pool2
I1028 02:28:58.996924  2951 net.cpp:345] relu2 -> pool2 (in-place)
I1028 02:28:58.996940  2951 net.cpp:96] Setting up relu2
I1028 02:28:58.996951  2951 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 02:28:58.996968  2951 net.cpp:67] Creating Layer drop2
I1028 02:28:58.996978  2951 net.cpp:394] drop2 <- pool2
I1028 02:28:58.996994  2951 net.cpp:345] drop2 -> pool2 (in-place)
I1028 02:28:58.997009  2951 net.cpp:96] Setting up drop2
I1028 02:28:58.997022  2951 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 02:28:58.997042  2951 net.cpp:67] Creating Layer conv3
I1028 02:28:58.997055  2951 net.cpp:394] conv3 <- pool2
I1028 02:28:58.997071  2951 net.cpp:356] conv3 -> conv3
I1028 02:28:58.997089  2951 net.cpp:96] Setting up conv3
I1028 02:28:59.000799  2951 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 02:28:59.000840  2951 net.cpp:67] Creating Layer pool3
I1028 02:28:59.000854  2951 net.cpp:394] pool3 <- conv3
I1028 02:28:59.000870  2951 net.cpp:356] pool3 -> pool3
I1028 02:28:59.000890  2951 net.cpp:96] Setting up pool3
I1028 02:28:59.000905  2951 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 02:28:59.000919  2951 net.cpp:67] Creating Layer relu3
I1028 02:28:59.000931  2951 net.cpp:394] relu3 <- pool3
I1028 02:28:59.000946  2951 net.cpp:345] relu3 -> pool3 (in-place)
I1028 02:28:59.000962  2951 net.cpp:96] Setting up relu3
I1028 02:28:59.000973  2951 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 02:28:59.000988  2951 net.cpp:67] Creating Layer drop3
I1028 02:28:59.000999  2951 net.cpp:394] drop3 <- pool3
I1028 02:28:59.001014  2951 net.cpp:345] drop3 -> pool3 (in-place)
I1028 02:28:59.001030  2951 net.cpp:96] Setting up drop3
I1028 02:28:59.001042  2951 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 02:28:59.001060  2951 net.cpp:67] Creating Layer ip1
I1028 02:28:59.001071  2951 net.cpp:394] ip1 <- pool3
I1028 02:28:59.001088  2951 net.cpp:356] ip1 -> ip1
I1028 02:28:59.001107  2951 net.cpp:96] Setting up ip1
I1028 02:28:59.427819  2951 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 02:28:59.427883  2951 net.cpp:67] Creating Layer relu4
I1028 02:28:59.427892  2951 net.cpp:394] relu4 <- ip1
I1028 02:28:59.427902  2951 net.cpp:345] relu4 -> ip1 (in-place)
I1028 02:28:59.427912  2951 net.cpp:96] Setting up relu4
I1028 02:28:59.427929  2951 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 02:28:59.427939  2951 net.cpp:67] Creating Layer drop4
I1028 02:28:59.427943  2951 net.cpp:394] drop4 <- ip1
I1028 02:28:59.427950  2951 net.cpp:345] drop4 -> ip1 (in-place)
I1028 02:28:59.427958  2951 net.cpp:96] Setting up drop4
I1028 02:28:59.427963  2951 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 02:28:59.427973  2951 net.cpp:67] Creating Layer ip2
I1028 02:28:59.427978  2951 net.cpp:394] ip2 <- ip1
I1028 02:28:59.427986  2951 net.cpp:356] ip2 -> ip2
I1028 02:28:59.427999  2951 net.cpp:96] Setting up ip2
I1028 02:28:59.435575  2951 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 02:28:59.435637  2951 net.cpp:67] Creating Layer prob
I1028 02:28:59.435647  2951 net.cpp:394] prob <- ip2
I1028 02:28:59.435657  2951 net.cpp:356] prob -> prob
I1028 02:28:59.435667  2951 net.cpp:96] Setting up prob
I1028 02:28:59.435677  2951 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 02:28:59.435681  2951 net.cpp:172] prob does not need backward computation.
I1028 02:28:59.435686  2951 net.cpp:172] ip2 does not need backward computation.
I1028 02:28:59.435690  2951 net.cpp:172] drop4 does not need backward computation.
I1028 02:28:59.435694  2951 net.cpp:172] relu4 does not need backward computation.
I1028 02:28:59.435698  2951 net.cpp:172] ip1 does not need backward computation.
I1028 02:28:59.435703  2951 net.cpp:172] drop3 does not need backward computation.
I1028 02:28:59.435705  2951 net.cpp:172] relu3 does not need backward computation.
I1028 02:28:59.435710  2951 net.cpp:172] pool3 does not need backward computation.
I1028 02:28:59.435714  2951 net.cpp:172] conv3 does not need backward computation.
I1028 02:28:59.435717  2951 net.cpp:172] drop2 does not need backward computation.
I1028 02:28:59.435721  2951 net.cpp:172] relu2 does not need backward computation.
I1028 02:28:59.435725  2951 net.cpp:172] pool2 does not need backward computation.
I1028 02:28:59.435729  2951 net.cpp:172] conv2 does not need backward computation.
I1028 02:28:59.435734  2951 net.cpp:172] drop1 does not need backward computation.
I1028 02:28:59.435737  2951 net.cpp:172] relu1 does not need backward computation.
I1028 02:28:59.435741  2951 net.cpp:172] pool1 does not need backward computation.
I1028 02:28:59.435745  2951 net.cpp:172] conv1 does not need backward computation.
I1028 02:28:59.435750  2951 net.cpp:208] This network produces output prob
I1028 02:28:59.435763  2951 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 02:28:59.435773  2951 net.cpp:219] Network initialization done.
I1028 02:28:59.435777  2951 net.cpp:220] Memory required for data: 1837200
I1028 03:07:59.417676 12132 convert_imageset.cpp:70] Shuffling data
I1028 03:08:00.115921 12132 convert_imageset.cpp:73] A total of 60000 images.
I1028 03:08:00.116008 12132 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 03:08:02.658434 12132 convert_imageset.cpp:177] Processed 1000 files.
E1028 03:08:05.021497 12132 convert_imageset.cpp:177] Processed 2000 files.
E1028 03:08:07.235143 12132 convert_imageset.cpp:177] Processed 3000 files.
E1028 03:08:09.274778 12132 convert_imageset.cpp:177] Processed 4000 files.
E1028 03:08:11.240279 12132 convert_imageset.cpp:177] Processed 5000 files.
E1028 03:08:13.262461 12132 convert_imageset.cpp:177] Processed 6000 files.
E1028 03:08:15.297204 12132 convert_imageset.cpp:177] Processed 7000 files.
E1028 03:08:17.307894 12132 convert_imageset.cpp:177] Processed 8000 files.
E1028 03:08:19.287417 12132 convert_imageset.cpp:177] Processed 9000 files.
E1028 03:08:21.185931 12132 convert_imageset.cpp:177] Processed 10000 files.
E1028 03:08:23.129092 12132 convert_imageset.cpp:177] Processed 11000 files.
E1028 03:08:25.100174 12132 convert_imageset.cpp:177] Processed 12000 files.
E1028 03:08:26.993449 12132 convert_imageset.cpp:177] Processed 13000 files.
E1028 03:08:28.866641 12132 convert_imageset.cpp:177] Processed 14000 files.
E1028 03:08:30.606250 12132 convert_imageset.cpp:177] Processed 15000 files.
E1028 03:08:32.424686 12132 convert_imageset.cpp:177] Processed 16000 files.
E1028 03:08:34.231245 12132 convert_imageset.cpp:177] Processed 17000 files.
E1028 03:08:35.954044 12132 convert_imageset.cpp:177] Processed 18000 files.
E1028 03:08:37.778579 12132 convert_imageset.cpp:177] Processed 19000 files.
E1028 03:08:39.635383 12132 convert_imageset.cpp:177] Processed 20000 files.
E1028 03:08:41.410715 12132 convert_imageset.cpp:177] Processed 21000 files.
E1028 03:08:43.078739 12132 convert_imageset.cpp:177] Processed 22000 files.
E1028 03:08:44.672986 12132 convert_imageset.cpp:177] Processed 23000 files.
E1028 03:08:46.419714 12132 convert_imageset.cpp:177] Processed 24000 files.
E1028 03:08:48.154161 12132 convert_imageset.cpp:177] Processed 25000 files.
E1028 03:08:49.804167 12132 convert_imageset.cpp:177] Processed 26000 files.
E1028 03:08:51.506220 12132 convert_imageset.cpp:177] Processed 27000 files.
E1028 03:08:53.224293 12132 convert_imageset.cpp:177] Processed 28000 files.
E1028 03:08:54.976694 12132 convert_imageset.cpp:177] Processed 29000 files.
E1028 03:08:56.729873 12132 convert_imageset.cpp:177] Processed 30000 files.
E1028 03:08:58.460393 12132 convert_imageset.cpp:177] Processed 31000 files.
E1028 03:09:00.171274 12132 convert_imageset.cpp:177] Processed 32000 files.
E1028 03:09:01.873456 12132 convert_imageset.cpp:177] Processed 33000 files.
E1028 03:09:03.608669 12132 convert_imageset.cpp:177] Processed 34000 files.
E1028 03:09:05.333398 12132 convert_imageset.cpp:177] Processed 35000 files.
E1028 03:09:06.897277 12132 convert_imageset.cpp:177] Processed 36000 files.
E1028 03:09:08.571962 12132 convert_imageset.cpp:177] Processed 37000 files.
E1028 03:09:10.256322 12132 convert_imageset.cpp:177] Processed 38000 files.
E1028 03:09:11.920220 12132 convert_imageset.cpp:177] Processed 39000 files.
E1028 03:09:13.683734 12132 convert_imageset.cpp:177] Processed 40000 files.
E1028 03:09:15.727509 12132 convert_imageset.cpp:177] Processed 41000 files.
E1028 03:09:17.443202 12132 convert_imageset.cpp:177] Processed 42000 files.
E1028 03:09:19.078856 12132 convert_imageset.cpp:177] Processed 43000 files.
E1028 03:09:20.728308 12132 convert_imageset.cpp:177] Processed 44000 files.
E1028 03:09:22.381461 12132 convert_imageset.cpp:177] Processed 45000 files.
E1028 03:09:23.999017 12132 convert_imageset.cpp:177] Processed 46000 files.
E1028 03:09:25.627181 12132 convert_imageset.cpp:177] Processed 47000 files.
E1028 03:09:27.243098 12132 convert_imageset.cpp:177] Processed 48000 files.
E1028 03:09:28.856166 12132 convert_imageset.cpp:177] Processed 49000 files.
E1028 03:09:30.519747 12132 convert_imageset.cpp:177] Processed 50000 files.
E1028 03:09:32.200137 12132 convert_imageset.cpp:177] Processed 51000 files.
E1028 03:09:33.699506 12132 convert_imageset.cpp:177] Processed 52000 files.
E1028 03:09:35.296030 12132 convert_imageset.cpp:177] Processed 53000 files.
E1028 03:09:37.046385 12132 convert_imageset.cpp:177] Processed 54000 files.
E1028 03:09:38.646847 12132 convert_imageset.cpp:177] Processed 55000 files.
E1028 03:09:40.246434 12132 convert_imageset.cpp:177] Processed 56000 files.
E1028 03:09:41.836567 12132 convert_imageset.cpp:177] Processed 57000 files.
E1028 03:09:43.506330 12132 convert_imageset.cpp:177] Processed 58000 files.
E1028 03:09:45.098985 12132 convert_imageset.cpp:177] Processed 59000 files.
E1028 03:09:46.762460 12132 convert_imageset.cpp:177] Processed 60000 files.
I1028 03:09:46.992344 12249 caffe.cpp:99] Use GPU with device ID 0
I1028 03:09:47.337219 12249 caffe.cpp:107] Starting Optimization
I1028 03:09:47.337348 12249 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 360000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 03:09:47.337379 12249 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 03:09:47.362561 12249 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 03:09:47.362771 12249 net.cpp:67] Creating Layer mnist
I1028 03:09:47.362797 12249 net.cpp:356] mnist -> data
I1028 03:09:47.362831 12249 net.cpp:356] mnist -> label
I1028 03:09:47.362864 12249 net.cpp:96] Setting up mnist
I1028 03:09:47.385402 12249 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 03:09:47.385519 12249 data_layer.cpp:128] output data size: 64,1,50,180
I1028 03:09:47.386296 12249 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 03:09:47.386330 12249 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 03:09:47.386353 12249 net.cpp:67] Creating Layer conv1
I1028 03:09:47.386365 12249 net.cpp:394] conv1 <- data
I1028 03:09:47.386390 12249 net.cpp:356] conv1 -> conv1
I1028 03:09:47.386415 12249 net.cpp:96] Setting up conv1
I1028 03:09:47.387181 12249 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 03:09:47.387243 12249 net.cpp:67] Creating Layer pool1
I1028 03:09:47.387258 12249 net.cpp:394] pool1 <- conv1
I1028 03:09:47.387281 12249 net.cpp:356] pool1 -> pool1
I1028 03:09:47.387301 12249 net.cpp:96] Setting up pool1
I1028 03:09:47.387332 12249 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 03:09:47.387356 12249 net.cpp:67] Creating Layer relu1
I1028 03:09:47.387368 12249 net.cpp:394] relu1 <- pool1
I1028 03:09:47.387384 12249 net.cpp:345] relu1 -> pool1 (in-place)
I1028 03:09:47.387401 12249 net.cpp:96] Setting up relu1
I1028 03:09:47.387414 12249 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 03:09:47.387434 12249 net.cpp:67] Creating Layer drop1
I1028 03:09:47.387445 12249 net.cpp:394] drop1 <- pool1
I1028 03:09:47.387461 12249 net.cpp:345] drop1 -> pool1 (in-place)
I1028 03:09:47.387478 12249 net.cpp:96] Setting up drop1
I1028 03:09:47.387492 12249 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 03:09:47.387516 12249 net.cpp:67] Creating Layer conv2
I1028 03:09:47.387528 12249 net.cpp:394] conv2 <- pool1
I1028 03:09:47.387549 12249 net.cpp:356] conv2 -> conv2
I1028 03:09:47.387572 12249 net.cpp:96] Setting up conv2
I1028 03:09:47.389148 12249 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 03:09:47.389194 12249 net.cpp:67] Creating Layer pool2
I1028 03:09:47.389209 12249 net.cpp:394] pool2 <- conv2
I1028 03:09:47.389226 12249 net.cpp:356] pool2 -> pool2
I1028 03:09:47.389245 12249 net.cpp:96] Setting up pool2
I1028 03:09:47.389261 12249 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 03:09:47.389276 12249 net.cpp:67] Creating Layer relu2
I1028 03:09:47.389288 12249 net.cpp:394] relu2 <- pool2
I1028 03:09:47.389308 12249 net.cpp:345] relu2 -> pool2 (in-place)
I1028 03:09:47.389327 12249 net.cpp:96] Setting up relu2
I1028 03:09:47.389338 12249 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 03:09:47.389358 12249 net.cpp:67] Creating Layer drop2
I1028 03:09:47.389369 12249 net.cpp:394] drop2 <- pool2
I1028 03:09:47.389385 12249 net.cpp:345] drop2 -> pool2 (in-place)
I1028 03:09:47.389402 12249 net.cpp:96] Setting up drop2
I1028 03:09:47.389415 12249 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 03:09:47.389436 12249 net.cpp:67] Creating Layer conv3
I1028 03:09:47.389449 12249 net.cpp:394] conv3 <- pool2
I1028 03:09:47.389467 12249 net.cpp:356] conv3 -> conv3
I1028 03:09:47.389485 12249 net.cpp:96] Setting up conv3
I1028 03:09:47.393535 12249 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 03:09:47.393584 12249 net.cpp:67] Creating Layer pool3
I1028 03:09:47.393599 12249 net.cpp:394] pool3 <- conv3
I1028 03:09:47.393621 12249 net.cpp:356] pool3 -> pool3
I1028 03:09:47.393642 12249 net.cpp:96] Setting up pool3
I1028 03:09:47.393658 12249 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 03:09:47.393674 12249 net.cpp:67] Creating Layer relu3
I1028 03:09:47.393687 12249 net.cpp:394] relu3 <- pool3
I1028 03:09:47.393702 12249 net.cpp:345] relu3 -> pool3 (in-place)
I1028 03:09:47.393719 12249 net.cpp:96] Setting up relu3
I1028 03:09:47.393731 12249 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 03:09:47.393753 12249 net.cpp:67] Creating Layer drop3
I1028 03:09:47.393765 12249 net.cpp:394] drop3 <- pool3
I1028 03:09:47.393781 12249 net.cpp:345] drop3 -> pool3 (in-place)
I1028 03:09:47.393797 12249 net.cpp:96] Setting up drop3
I1028 03:09:47.393811 12249 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 03:09:47.393831 12249 net.cpp:67] Creating Layer ip1
I1028 03:09:47.393842 12249 net.cpp:394] ip1 <- pool3
I1028 03:09:47.393863 12249 net.cpp:356] ip1 -> ip1
I1028 03:09:47.393928 12249 net.cpp:96] Setting up ip1
I1028 03:09:47.863994 12249 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 03:09:47.864055 12249 net.cpp:67] Creating Layer relu4
I1028 03:09:47.864063 12249 net.cpp:394] relu4 <- ip1
I1028 03:09:47.864073 12249 net.cpp:345] relu4 -> ip1 (in-place)
I1028 03:09:47.864083 12249 net.cpp:96] Setting up relu4
I1028 03:09:47.864087 12249 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 03:09:47.864094 12249 net.cpp:67] Creating Layer drop4
I1028 03:09:47.864106 12249 net.cpp:394] drop4 <- ip1
I1028 03:09:47.864114 12249 net.cpp:345] drop4 -> ip1 (in-place)
I1028 03:09:47.864120 12249 net.cpp:96] Setting up drop4
I1028 03:09:47.864125 12249 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 03:09:47.864137 12249 net.cpp:67] Creating Layer ip2
I1028 03:09:47.864141 12249 net.cpp:394] ip2 <- ip1
I1028 03:09:47.864148 12249 net.cpp:356] ip2 -> ip2
I1028 03:09:47.864158 12249 net.cpp:96] Setting up ip2
I1028 03:09:47.877177 12249 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 03:09:47.877244 12249 net.cpp:67] Creating Layer loss
I1028 03:09:47.877251 12249 net.cpp:394] loss <- ip2
I1028 03:09:47.877260 12249 net.cpp:394] loss <- label
I1028 03:09:47.877266 12249 net.cpp:356] loss -> loss
I1028 03:09:47.877276 12249 net.cpp:96] Setting up loss
I1028 03:09:47.877288 12249 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 03:09:47.877293 12249 net.cpp:109]     with loss weight 1
I1028 03:09:47.877333 12249 net.cpp:170] loss needs backward computation.
I1028 03:09:47.877338 12249 net.cpp:170] ip2 needs backward computation.
I1028 03:09:47.877343 12249 net.cpp:170] drop4 needs backward computation.
I1028 03:09:47.877348 12249 net.cpp:170] relu4 needs backward computation.
I1028 03:09:47.877352 12249 net.cpp:170] ip1 needs backward computation.
I1028 03:09:47.877357 12249 net.cpp:170] drop3 needs backward computation.
I1028 03:09:47.877362 12249 net.cpp:170] relu3 needs backward computation.
I1028 03:09:47.877365 12249 net.cpp:170] pool3 needs backward computation.
I1028 03:09:47.877370 12249 net.cpp:170] conv3 needs backward computation.
I1028 03:09:47.877375 12249 net.cpp:170] drop2 needs backward computation.
I1028 03:09:47.877379 12249 net.cpp:170] relu2 needs backward computation.
I1028 03:09:47.877384 12249 net.cpp:170] pool2 needs backward computation.
I1028 03:09:47.877388 12249 net.cpp:170] conv2 needs backward computation.
I1028 03:09:47.877393 12249 net.cpp:170] drop1 needs backward computation.
I1028 03:09:47.877398 12249 net.cpp:170] relu1 needs backward computation.
I1028 03:09:47.877401 12249 net.cpp:170] pool1 needs backward computation.
I1028 03:09:47.877406 12249 net.cpp:170] conv1 needs backward computation.
I1028 03:09:47.877410 12249 net.cpp:172] mnist does not need backward computation.
I1028 03:09:47.877415 12249 net.cpp:208] This network produces output loss
I1028 03:09:47.877425 12249 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 03:09:47.877432 12249 net.cpp:219] Network initialization done.
I1028 03:09:47.877437 12249 net.cpp:220] Memory required for data: 119788292
I1028 03:09:47.877496 12249 solver.cpp:41] Solver scaffolding done.
I1028 03:09:47.877501 12249 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_350000.solverstate
I1028 03:09:47.877506 12249 solver.cpp:160] Solving Captcha
I1028 03:09:47.877524 12249 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_350000.solverstate
I1028 03:09:55.284469 12249 solver.cpp:502] SGDSolver: restoring history
I1028 03:09:56.055153 12249 solver.cpp:191] Iteration 350000, loss = 2.39427
I1028 03:09:56.055207 12249 solver.cpp:206]     Train net output #0: loss = 2.39427 (* 1 = 2.39427 loss)
I1028 03:09:56.055222 12249 solver.cpp:403] Iteration 350000, lr = 0.000680414
I1028 03:13:58.593741 12249 solver.cpp:191] Iteration 351000, loss = 2.58201
I1028 03:13:58.594530 12249 solver.cpp:206]     Train net output #0: loss = 2.58201 (* 1 = 2.58201 loss)
I1028 03:13:58.594564 12249 solver.cpp:403] Iteration 351000, lr = 0.000679
I1028 03:17:59.864948 12249 solver.cpp:191] Iteration 352000, loss = 2.64352
I1028 03:17:59.875730 12249 solver.cpp:206]     Train net output #0: loss = 2.64352 (* 1 = 2.64352 loss)
I1028 03:17:59.875766 12249 solver.cpp:403] Iteration 352000, lr = 0.000677592
I1028 03:22:01.192448 12249 solver.cpp:191] Iteration 353000, loss = 2.66404
I1028 03:22:01.193078 12249 solver.cpp:206]     Train net output #0: loss = 2.66404 (* 1 = 2.66404 loss)
I1028 03:22:01.193120 12249 solver.cpp:403] Iteration 353000, lr = 0.000676192
I1028 03:26:02.561568 12249 solver.cpp:191] Iteration 354000, loss = 2.4475
I1028 03:26:02.562228 12249 solver.cpp:206]     Train net output #0: loss = 2.4475 (* 1 = 2.4475 loss)
I1028 03:26:02.562265 12249 solver.cpp:403] Iteration 354000, lr = 0.000674798
I1028 03:30:04.483602 12249 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_355000.caffemodel
I1028 03:30:09.028362 12249 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_355000.solverstate
I1028 03:30:12.604511 12249 solver.cpp:191] Iteration 355000, loss = 2.54491
I1028 03:30:12.605101 12249 solver.cpp:206]     Train net output #0: loss = 2.54491 (* 1 = 2.54491 loss)
I1028 03:30:12.605134 12249 solver.cpp:403] Iteration 355000, lr = 0.000673411
I1028 03:34:14.033686 12249 solver.cpp:191] Iteration 356000, loss = 2.37334
I1028 03:34:14.034248 12249 solver.cpp:206]     Train net output #0: loss = 2.37334 (* 1 = 2.37334 loss)
I1028 03:34:14.034281 12249 solver.cpp:403] Iteration 356000, lr = 0.000672031
I1028 03:38:15.387313 12249 solver.cpp:191] Iteration 357000, loss = 2.31603
I1028 03:38:15.387996 12249 solver.cpp:206]     Train net output #0: loss = 2.31603 (* 1 = 2.31603 loss)
I1028 03:38:15.388030 12249 solver.cpp:403] Iteration 357000, lr = 0.000670657
I1028 03:42:16.722710 12249 solver.cpp:191] Iteration 358000, loss = 2.47343
I1028 03:42:16.723382 12249 solver.cpp:206]     Train net output #0: loss = 2.47343 (* 1 = 2.47343 loss)
I1028 03:42:16.723419 12249 solver.cpp:403] Iteration 358000, lr = 0.00066929
I1028 03:46:18.096698 12249 solver.cpp:191] Iteration 359000, loss = 2.35828
I1028 03:46:18.097290 12249 solver.cpp:206]     Train net output #0: loss = 2.35828 (* 1 = 2.35828 loss)
I1028 03:46:18.097327 12249 solver.cpp:403] Iteration 359000, lr = 0.000667929
I1028 03:50:19.916941 12249 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_360000.caffemodel
I1028 03:50:24.627010 12249 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_360000.solverstate
I1028 03:50:28.216933 12249 solver.cpp:228] Iteration 360000, loss = 2.2063
I1028 03:50:28.217427 12249 solver.cpp:233] Optimization Done.
I1028 03:50:28.217450 12249 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 04:12:15.851297 31570 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 04:12:15.851399 31570 net.cpp:358] Input 0 -> data
I1028 04:12:15.851424 31570 net.cpp:67] Creating Layer conv1
I1028 04:12:15.851429 31570 net.cpp:394] conv1 <- data
I1028 04:12:15.851436 31570 net.cpp:356] conv1 -> conv1
I1028 04:12:15.851446 31570 net.cpp:96] Setting up conv1
I1028 04:12:15.851764 31570 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 04:12:15.851785 31570 net.cpp:67] Creating Layer pool1
I1028 04:12:15.851791 31570 net.cpp:394] pool1 <- conv1
I1028 04:12:15.851797 31570 net.cpp:356] pool1 -> pool1
I1028 04:12:15.851804 31570 net.cpp:96] Setting up pool1
I1028 04:12:15.851815 31570 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 04:12:15.851822 31570 net.cpp:67] Creating Layer relu1
I1028 04:12:15.851826 31570 net.cpp:394] relu1 <- pool1
I1028 04:12:15.851831 31570 net.cpp:345] relu1 -> pool1 (in-place)
I1028 04:12:15.851837 31570 net.cpp:96] Setting up relu1
I1028 04:12:15.851841 31570 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 04:12:15.851847 31570 net.cpp:67] Creating Layer drop1
I1028 04:12:15.851850 31570 net.cpp:394] drop1 <- pool1
I1028 04:12:15.851856 31570 net.cpp:345] drop1 -> pool1 (in-place)
I1028 04:12:15.851862 31570 net.cpp:96] Setting up drop1
I1028 04:12:15.851867 31570 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 04:12:15.851876 31570 net.cpp:67] Creating Layer conv2
I1028 04:12:15.851881 31570 net.cpp:394] conv2 <- pool1
I1028 04:12:15.851886 31570 net.cpp:356] conv2 -> conv2
I1028 04:12:15.851892 31570 net.cpp:96] Setting up conv2
I1028 04:12:15.852488 31570 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 04:12:15.852507 31570 net.cpp:67] Creating Layer pool2
I1028 04:12:15.852515 31570 net.cpp:394] pool2 <- conv2
I1028 04:12:15.852521 31570 net.cpp:356] pool2 -> pool2
I1028 04:12:15.852529 31570 net.cpp:96] Setting up pool2
I1028 04:12:15.852535 31570 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 04:12:15.852543 31570 net.cpp:67] Creating Layer relu2
I1028 04:12:15.852548 31570 net.cpp:394] relu2 <- pool2
I1028 04:12:15.852553 31570 net.cpp:345] relu2 -> pool2 (in-place)
I1028 04:12:15.852558 31570 net.cpp:96] Setting up relu2
I1028 04:12:15.852562 31570 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 04:12:15.852571 31570 net.cpp:67] Creating Layer drop2
I1028 04:12:15.852574 31570 net.cpp:394] drop2 <- pool2
I1028 04:12:15.852581 31570 net.cpp:345] drop2 -> pool2 (in-place)
I1028 04:12:15.852586 31570 net.cpp:96] Setting up drop2
I1028 04:12:15.852591 31570 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 04:12:15.852598 31570 net.cpp:67] Creating Layer conv3
I1028 04:12:15.852603 31570 net.cpp:394] conv3 <- pool2
I1028 04:12:15.852608 31570 net.cpp:356] conv3 -> conv3
I1028 04:12:15.852615 31570 net.cpp:96] Setting up conv3
I1028 04:12:15.854085 31570 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 04:12:15.854105 31570 net.cpp:67] Creating Layer pool3
I1028 04:12:15.854110 31570 net.cpp:394] pool3 <- conv3
I1028 04:12:15.854116 31570 net.cpp:356] pool3 -> pool3
I1028 04:12:15.854122 31570 net.cpp:96] Setting up pool3
I1028 04:12:15.854130 31570 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 04:12:15.854135 31570 net.cpp:67] Creating Layer relu3
I1028 04:12:15.854140 31570 net.cpp:394] relu3 <- pool3
I1028 04:12:15.854145 31570 net.cpp:345] relu3 -> pool3 (in-place)
I1028 04:12:15.854151 31570 net.cpp:96] Setting up relu3
I1028 04:12:15.854154 31570 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 04:12:15.854161 31570 net.cpp:67] Creating Layer drop3
I1028 04:12:15.854166 31570 net.cpp:394] drop3 <- pool3
I1028 04:12:15.854171 31570 net.cpp:345] drop3 -> pool3 (in-place)
I1028 04:12:15.854176 31570 net.cpp:96] Setting up drop3
I1028 04:12:15.854181 31570 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 04:12:15.854187 31570 net.cpp:67] Creating Layer ip1
I1028 04:12:15.854190 31570 net.cpp:394] ip1 <- pool3
I1028 04:12:15.854195 31570 net.cpp:356] ip1 -> ip1
I1028 04:12:15.854202 31570 net.cpp:96] Setting up ip1
I1028 04:12:16.298378 31570 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 04:12:16.298441 31570 net.cpp:67] Creating Layer relu4
I1028 04:12:16.298449 31570 net.cpp:394] relu4 <- ip1
I1028 04:12:16.298460 31570 net.cpp:345] relu4 -> ip1 (in-place)
I1028 04:12:16.298470 31570 net.cpp:96] Setting up relu4
I1028 04:12:16.298475 31570 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 04:12:16.298481 31570 net.cpp:67] Creating Layer drop4
I1028 04:12:16.298486 31570 net.cpp:394] drop4 <- ip1
I1028 04:12:16.298491 31570 net.cpp:345] drop4 -> ip1 (in-place)
I1028 04:12:16.298497 31570 net.cpp:96] Setting up drop4
I1028 04:12:16.298502 31570 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 04:12:16.298511 31570 net.cpp:67] Creating Layer ip2
I1028 04:12:16.298514 31570 net.cpp:394] ip2 <- ip1
I1028 04:12:16.298524 31570 net.cpp:356] ip2 -> ip2
I1028 04:12:16.298537 31570 net.cpp:96] Setting up ip2
I1028 04:12:16.340106 31570 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 04:12:16.340170 31570 net.cpp:67] Creating Layer prob
I1028 04:12:16.340178 31570 net.cpp:394] prob <- ip2
I1028 04:12:16.340186 31570 net.cpp:356] prob -> prob
I1028 04:12:16.340196 31570 net.cpp:96] Setting up prob
I1028 04:12:16.340203 31570 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 04:12:16.340208 31570 net.cpp:172] prob does not need backward computation.
I1028 04:12:16.340211 31570 net.cpp:172] ip2 does not need backward computation.
I1028 04:12:16.340214 31570 net.cpp:172] drop4 does not need backward computation.
I1028 04:12:16.340219 31570 net.cpp:172] relu4 does not need backward computation.
I1028 04:12:16.340222 31570 net.cpp:172] ip1 does not need backward computation.
I1028 04:12:16.340225 31570 net.cpp:172] drop3 does not need backward computation.
I1028 04:12:16.340229 31570 net.cpp:172] relu3 does not need backward computation.
I1028 04:12:16.340232 31570 net.cpp:172] pool3 does not need backward computation.
I1028 04:12:16.340236 31570 net.cpp:172] conv3 does not need backward computation.
I1028 04:12:16.340240 31570 net.cpp:172] drop2 does not need backward computation.
I1028 04:12:16.340243 31570 net.cpp:172] relu2 does not need backward computation.
I1028 04:12:16.340247 31570 net.cpp:172] pool2 does not need backward computation.
I1028 04:12:16.340250 31570 net.cpp:172] conv2 does not need backward computation.
I1028 04:12:16.340255 31570 net.cpp:172] drop1 does not need backward computation.
I1028 04:12:16.340258 31570 net.cpp:172] relu1 does not need backward computation.
I1028 04:12:16.340261 31570 net.cpp:172] pool1 does not need backward computation.
I1028 04:12:16.340265 31570 net.cpp:172] conv1 does not need backward computation.
I1028 04:12:16.340268 31570 net.cpp:208] This network produces output prob
I1028 04:12:16.340283 31570 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 04:12:16.340291 31570 net.cpp:219] Network initialization done.
I1028 04:12:16.340294 31570 net.cpp:220] Memory required for data: 1837200
I1028 04:13:00.134346 31570 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 04:13:00.134886 31570 net.cpp:358] Input 0 -> data
I1028 04:13:00.134917 31570 net.cpp:67] Creating Layer conv1
I1028 04:13:00.134922 31570 net.cpp:394] conv1 <- data
I1028 04:13:00.134929 31570 net.cpp:356] conv1 -> conv1
I1028 04:13:00.134939 31570 net.cpp:96] Setting up conv1
I1028 04:13:00.134970 31570 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 04:13:00.134986 31570 net.cpp:67] Creating Layer pool1
I1028 04:13:00.134991 31570 net.cpp:394] pool1 <- conv1
I1028 04:13:00.134997 31570 net.cpp:356] pool1 -> pool1
I1028 04:13:00.135004 31570 net.cpp:96] Setting up pool1
I1028 04:13:00.135011 31570 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 04:13:00.135018 31570 net.cpp:67] Creating Layer relu1
I1028 04:13:00.135022 31570 net.cpp:394] relu1 <- pool1
I1028 04:13:00.135028 31570 net.cpp:345] relu1 -> pool1 (in-place)
I1028 04:13:00.135035 31570 net.cpp:96] Setting up relu1
I1028 04:13:00.135040 31570 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 04:13:00.135045 31570 net.cpp:67] Creating Layer drop1
I1028 04:13:00.135048 31570 net.cpp:394] drop1 <- pool1
I1028 04:13:00.135054 31570 net.cpp:345] drop1 -> pool1 (in-place)
I1028 04:13:00.135059 31570 net.cpp:96] Setting up drop1
I1028 04:13:00.135066 31570 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 04:13:00.135081 31570 net.cpp:67] Creating Layer conv2
I1028 04:13:00.135085 31570 net.cpp:394] conv2 <- pool1
I1028 04:13:00.135092 31570 net.cpp:356] conv2 -> conv2
I1028 04:13:00.135100 31570 net.cpp:96] Setting up conv2
I1028 04:13:00.135602 31570 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 04:13:00.135617 31570 net.cpp:67] Creating Layer pool2
I1028 04:13:00.135622 31570 net.cpp:394] pool2 <- conv2
I1028 04:13:00.135628 31570 net.cpp:356] pool2 -> pool2
I1028 04:13:00.135635 31570 net.cpp:96] Setting up pool2
I1028 04:13:00.135642 31570 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 04:13:00.135648 31570 net.cpp:67] Creating Layer relu2
I1028 04:13:00.135651 31570 net.cpp:394] relu2 <- pool2
I1028 04:13:00.135656 31570 net.cpp:345] relu2 -> pool2 (in-place)
I1028 04:13:00.135663 31570 net.cpp:96] Setting up relu2
I1028 04:13:00.135666 31570 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 04:13:00.135671 31570 net.cpp:67] Creating Layer drop2
I1028 04:13:00.135675 31570 net.cpp:394] drop2 <- pool2
I1028 04:13:00.135680 31570 net.cpp:345] drop2 -> pool2 (in-place)
I1028 04:13:00.135686 31570 net.cpp:96] Setting up drop2
I1028 04:13:00.135690 31570 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 04:13:00.135699 31570 net.cpp:67] Creating Layer conv3
I1028 04:13:00.135702 31570 net.cpp:394] conv3 <- pool2
I1028 04:13:00.135709 31570 net.cpp:356] conv3 -> conv3
I1028 04:13:00.135715 31570 net.cpp:96] Setting up conv3
I1028 04:13:00.137066 31570 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 04:13:00.137085 31570 net.cpp:67] Creating Layer pool3
I1028 04:13:00.137090 31570 net.cpp:394] pool3 <- conv3
I1028 04:13:00.137096 31570 net.cpp:356] pool3 -> pool3
I1028 04:13:00.137104 31570 net.cpp:96] Setting up pool3
I1028 04:13:00.137109 31570 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 04:13:00.137115 31570 net.cpp:67] Creating Layer relu3
I1028 04:13:00.137120 31570 net.cpp:394] relu3 <- pool3
I1028 04:13:00.137125 31570 net.cpp:345] relu3 -> pool3 (in-place)
I1028 04:13:00.137130 31570 net.cpp:96] Setting up relu3
I1028 04:13:00.137135 31570 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 04:13:00.137140 31570 net.cpp:67] Creating Layer drop3
I1028 04:13:00.137145 31570 net.cpp:394] drop3 <- pool3
I1028 04:13:00.137151 31570 net.cpp:345] drop3 -> pool3 (in-place)
I1028 04:13:00.137156 31570 net.cpp:96] Setting up drop3
I1028 04:13:00.137161 31570 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 04:13:00.137167 31570 net.cpp:67] Creating Layer ip1
I1028 04:13:00.137171 31570 net.cpp:394] ip1 <- pool3
I1028 04:13:00.137177 31570 net.cpp:356] ip1 -> ip1
I1028 04:13:00.137184 31570 net.cpp:96] Setting up ip1
I1028 04:13:00.542628 31570 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 04:13:00.542692 31570 net.cpp:67] Creating Layer relu4
I1028 04:13:00.542701 31570 net.cpp:394] relu4 <- ip1
I1028 04:13:00.542711 31570 net.cpp:345] relu4 -> ip1 (in-place)
I1028 04:13:00.542721 31570 net.cpp:96] Setting up relu4
I1028 04:13:00.542726 31570 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 04:13:00.542734 31570 net.cpp:67] Creating Layer drop4
I1028 04:13:00.542738 31570 net.cpp:394] drop4 <- ip1
I1028 04:13:00.542745 31570 net.cpp:345] drop4 -> ip1 (in-place)
I1028 04:13:00.542752 31570 net.cpp:96] Setting up drop4
I1028 04:13:00.542758 31570 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 04:13:00.542768 31570 net.cpp:67] Creating Layer ip2
I1028 04:13:00.542773 31570 net.cpp:394] ip2 <- ip1
I1028 04:13:00.542780 31570 net.cpp:356] ip2 -> ip2
I1028 04:13:00.542794 31570 net.cpp:96] Setting up ip2
I1028 04:13:00.550379 31570 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 04:13:00.550437 31570 net.cpp:67] Creating Layer prob
I1028 04:13:00.550446 31570 net.cpp:394] prob <- ip2
I1028 04:13:00.550456 31570 net.cpp:356] prob -> prob
I1028 04:13:00.550467 31570 net.cpp:96] Setting up prob
I1028 04:13:00.550474 31570 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 04:13:00.550479 31570 net.cpp:172] prob does not need backward computation.
I1028 04:13:00.550483 31570 net.cpp:172] ip2 does not need backward computation.
I1028 04:13:00.550498 31570 net.cpp:172] drop4 does not need backward computation.
I1028 04:13:00.550503 31570 net.cpp:172] relu4 does not need backward computation.
I1028 04:13:00.550506 31570 net.cpp:172] ip1 does not need backward computation.
I1028 04:13:00.550510 31570 net.cpp:172] drop3 does not need backward computation.
I1028 04:13:00.550514 31570 net.cpp:172] relu3 does not need backward computation.
I1028 04:13:00.550518 31570 net.cpp:172] pool3 does not need backward computation.
I1028 04:13:00.550523 31570 net.cpp:172] conv3 does not need backward computation.
I1028 04:13:00.550526 31570 net.cpp:172] drop2 does not need backward computation.
I1028 04:13:00.550530 31570 net.cpp:172] relu2 does not need backward computation.
I1028 04:13:00.550535 31570 net.cpp:172] pool2 does not need backward computation.
I1028 04:13:00.550539 31570 net.cpp:172] conv2 does not need backward computation.
I1028 04:13:00.550544 31570 net.cpp:172] drop1 does not need backward computation.
I1028 04:13:00.550547 31570 net.cpp:172] relu1 does not need backward computation.
I1028 04:13:00.550551 31570 net.cpp:172] pool1 does not need backward computation.
I1028 04:13:00.550555 31570 net.cpp:172] conv1 does not need backward computation.
I1028 04:13:00.550559 31570 net.cpp:208] This network produces output prob
I1028 04:13:00.550575 31570 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 04:13:00.550585 31570 net.cpp:219] Network initialization done.
I1028 04:13:00.550588 31570 net.cpp:220] Memory required for data: 1837200
I1028 04:45:40.121264  7662 convert_imageset.cpp:70] Shuffling data
I1028 04:45:40.692531  7662 convert_imageset.cpp:73] A total of 60000 images.
I1028 04:45:40.692613  7662 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 04:45:42.713722  7662 convert_imageset.cpp:177] Processed 1000 files.
E1028 04:45:44.698169  7662 convert_imageset.cpp:177] Processed 2000 files.
E1028 04:45:46.724167  7662 convert_imageset.cpp:177] Processed 3000 files.
E1028 04:45:48.718161  7662 convert_imageset.cpp:177] Processed 4000 files.
E1028 04:45:50.773000  7662 convert_imageset.cpp:177] Processed 5000 files.
E1028 04:45:52.706440  7662 convert_imageset.cpp:177] Processed 6000 files.
E1028 04:45:54.590301  7662 convert_imageset.cpp:177] Processed 7000 files.
E1028 04:45:56.429059  7662 convert_imageset.cpp:177] Processed 8000 files.
E1028 04:45:58.426345  7662 convert_imageset.cpp:177] Processed 9000 files.
E1028 04:46:00.132010  7662 convert_imageset.cpp:177] Processed 10000 files.
E1028 04:46:01.928403  7662 convert_imageset.cpp:177] Processed 11000 files.
E1028 04:46:03.810979  7662 convert_imageset.cpp:177] Processed 12000 files.
E1028 04:46:05.662340  7662 convert_imageset.cpp:177] Processed 13000 files.
E1028 04:46:07.359524  7662 convert_imageset.cpp:177] Processed 14000 files.
E1028 04:46:09.186563  7662 convert_imageset.cpp:177] Processed 15000 files.
E1028 04:46:10.864989  7662 convert_imageset.cpp:177] Processed 16000 files.
E1028 04:46:12.549916  7662 convert_imageset.cpp:177] Processed 17000 files.
E1028 04:46:14.359913  7662 convert_imageset.cpp:177] Processed 18000 files.
E1028 04:46:15.968721  7662 convert_imageset.cpp:177] Processed 19000 files.
E1028 04:46:17.763216  7662 convert_imageset.cpp:177] Processed 20000 files.
E1028 04:46:19.404781  7662 convert_imageset.cpp:177] Processed 21000 files.
E1028 04:46:21.078527  7662 convert_imageset.cpp:177] Processed 22000 files.
E1028 04:46:22.647862  7662 convert_imageset.cpp:177] Processed 23000 files.
E1028 04:46:24.366441  7662 convert_imageset.cpp:177] Processed 24000 files.
E1028 04:46:26.170835  7662 convert_imageset.cpp:177] Processed 25000 files.
E1028 04:46:27.743530  7662 convert_imageset.cpp:177] Processed 26000 files.
E1028 04:46:29.331068  7662 convert_imageset.cpp:177] Processed 27000 files.
E1028 04:46:31.018908  7662 convert_imageset.cpp:177] Processed 28000 files.
E1028 04:46:32.676935  7662 convert_imageset.cpp:177] Processed 29000 files.
E1028 04:46:34.371351  7662 convert_imageset.cpp:177] Processed 30000 files.
E1028 04:46:35.984524  7662 convert_imageset.cpp:177] Processed 31000 files.
E1028 04:46:37.498334  7662 convert_imageset.cpp:177] Processed 32000 files.
E1028 04:46:39.038343  7662 convert_imageset.cpp:177] Processed 33000 files.
E1028 04:46:40.551784  7662 convert_imageset.cpp:177] Processed 34000 files.
E1028 04:46:42.171195  7662 convert_imageset.cpp:177] Processed 35000 files.
E1028 04:46:43.774173  7662 convert_imageset.cpp:177] Processed 36000 files.
E1028 04:46:45.438104  7662 convert_imageset.cpp:177] Processed 37000 files.
E1028 04:46:47.047977  7662 convert_imageset.cpp:177] Processed 38000 files.
E1028 04:46:48.595075  7662 convert_imageset.cpp:177] Processed 39000 files.
E1028 04:46:50.157057  7662 convert_imageset.cpp:177] Processed 40000 files.
E1028 04:46:51.694497  7662 convert_imageset.cpp:177] Processed 41000 files.
E1028 04:46:53.206073  7662 convert_imageset.cpp:177] Processed 42000 files.
E1028 04:46:54.799695  7662 convert_imageset.cpp:177] Processed 43000 files.
E1028 04:46:56.377209  7662 convert_imageset.cpp:177] Processed 44000 files.
E1028 04:46:57.899999  7662 convert_imageset.cpp:177] Processed 45000 files.
E1028 04:46:59.365072  7662 convert_imageset.cpp:177] Processed 46000 files.
E1028 04:47:00.820935  7662 convert_imageset.cpp:177] Processed 47000 files.
E1028 04:47:02.346730  7662 convert_imageset.cpp:177] Processed 48000 files.
E1028 04:47:03.793215  7662 convert_imageset.cpp:177] Processed 49000 files.
E1028 04:47:05.441967  7662 convert_imageset.cpp:177] Processed 50000 files.
E1028 04:47:06.965705  7662 convert_imageset.cpp:177] Processed 51000 files.
E1028 04:47:08.483499  7662 convert_imageset.cpp:177] Processed 52000 files.
E1028 04:47:09.969895  7662 convert_imageset.cpp:177] Processed 53000 files.
E1028 04:47:11.633923  7662 convert_imageset.cpp:177] Processed 54000 files.
E1028 04:47:13.269965  7662 convert_imageset.cpp:177] Processed 55000 files.
E1028 04:47:14.808029  7662 convert_imageset.cpp:177] Processed 56000 files.
E1028 04:47:16.507529  7662 convert_imageset.cpp:177] Processed 57000 files.
E1028 04:47:18.037583  7662 convert_imageset.cpp:177] Processed 58000 files.
E1028 04:47:19.555853  7662 convert_imageset.cpp:177] Processed 59000 files.
E1028 04:47:21.052651  7662 convert_imageset.cpp:177] Processed 60000 files.
I1028 04:47:21.257360  7821 caffe.cpp:99] Use GPU with device ID 0
I1028 04:47:21.607036  7821 caffe.cpp:107] Starting Optimization
I1028 04:47:21.607164  7821 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 370000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 04:47:21.607189  7821 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 04:47:21.618727  7821 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 04:47:21.618940  7821 net.cpp:67] Creating Layer mnist
I1028 04:47:21.618968  7821 net.cpp:356] mnist -> data
I1028 04:47:21.619002  7821 net.cpp:356] mnist -> label
I1028 04:47:21.619035  7821 net.cpp:96] Setting up mnist
I1028 04:47:21.625630  7821 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 04:47:21.625758  7821 data_layer.cpp:128] output data size: 64,1,50,180
I1028 04:47:21.627411  7821 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 04:47:21.627446  7821 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 04:47:21.627478  7821 net.cpp:67] Creating Layer conv1
I1028 04:47:21.627493  7821 net.cpp:394] conv1 <- data
I1028 04:47:21.627524  7821 net.cpp:356] conv1 -> conv1
I1028 04:47:21.627555  7821 net.cpp:96] Setting up conv1
I1028 04:47:21.627919  7821 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 04:47:21.627954  7821 net.cpp:67] Creating Layer pool1
I1028 04:47:21.627960  7821 net.cpp:394] pool1 <- conv1
I1028 04:47:21.627969  7821 net.cpp:356] pool1 -> pool1
I1028 04:47:21.627976  7821 net.cpp:96] Setting up pool1
I1028 04:47:21.627992  7821 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 04:47:21.628000  7821 net.cpp:67] Creating Layer relu1
I1028 04:47:21.628005  7821 net.cpp:394] relu1 <- pool1
I1028 04:47:21.628010  7821 net.cpp:345] relu1 -> pool1 (in-place)
I1028 04:47:21.628017  7821 net.cpp:96] Setting up relu1
I1028 04:47:21.628022  7821 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 04:47:21.628031  7821 net.cpp:67] Creating Layer drop1
I1028 04:47:21.628034  7821 net.cpp:394] drop1 <- pool1
I1028 04:47:21.628043  7821 net.cpp:345] drop1 -> pool1 (in-place)
I1028 04:47:21.628051  7821 net.cpp:96] Setting up drop1
I1028 04:47:21.628056  7821 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 04:47:21.628063  7821 net.cpp:67] Creating Layer conv2
I1028 04:47:21.628067  7821 net.cpp:394] conv2 <- pool1
I1028 04:47:21.628075  7821 net.cpp:356] conv2 -> conv2
I1028 04:47:21.628088  7821 net.cpp:96] Setting up conv2
I1028 04:47:21.629132  7821 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 04:47:21.629179  7821 net.cpp:67] Creating Layer pool2
I1028 04:47:21.629194  7821 net.cpp:394] pool2 <- conv2
I1028 04:47:21.629212  7821 net.cpp:356] pool2 -> pool2
I1028 04:47:21.629231  7821 net.cpp:96] Setting up pool2
I1028 04:47:21.629247  7821 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 04:47:21.629264  7821 net.cpp:67] Creating Layer relu2
I1028 04:47:21.629276  7821 net.cpp:394] relu2 <- pool2
I1028 04:47:21.629297  7821 net.cpp:345] relu2 -> pool2 (in-place)
I1028 04:47:21.629315  7821 net.cpp:96] Setting up relu2
I1028 04:47:21.629328  7821 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 04:47:21.629348  7821 net.cpp:67] Creating Layer drop2
I1028 04:47:21.629360  7821 net.cpp:394] drop2 <- pool2
I1028 04:47:21.629382  7821 net.cpp:345] drop2 -> pool2 (in-place)
I1028 04:47:21.629401  7821 net.cpp:96] Setting up drop2
I1028 04:47:21.629415  7821 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 04:47:21.629433  7821 net.cpp:67] Creating Layer conv3
I1028 04:47:21.629446  7821 net.cpp:394] conv3 <- pool2
I1028 04:47:21.629464  7821 net.cpp:356] conv3 -> conv3
I1028 04:47:21.629483  7821 net.cpp:96] Setting up conv3
I1028 04:47:21.633522  7821 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 04:47:21.633574  7821 net.cpp:67] Creating Layer pool3
I1028 04:47:21.633589  7821 net.cpp:394] pool3 <- conv3
I1028 04:47:21.633610  7821 net.cpp:356] pool3 -> pool3
I1028 04:47:21.633631  7821 net.cpp:96] Setting up pool3
I1028 04:47:21.633648  7821 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 04:47:21.633664  7821 net.cpp:67] Creating Layer relu3
I1028 04:47:21.633677  7821 net.cpp:394] relu3 <- pool3
I1028 04:47:21.633697  7821 net.cpp:345] relu3 -> pool3 (in-place)
I1028 04:47:21.633714  7821 net.cpp:96] Setting up relu3
I1028 04:47:21.633726  7821 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 04:47:21.633744  7821 net.cpp:67] Creating Layer drop3
I1028 04:47:21.633755  7821 net.cpp:394] drop3 <- pool3
I1028 04:47:21.633772  7821 net.cpp:345] drop3 -> pool3 (in-place)
I1028 04:47:21.633790  7821 net.cpp:96] Setting up drop3
I1028 04:47:21.633803  7821 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 04:47:21.633821  7821 net.cpp:67] Creating Layer ip1
I1028 04:47:21.633833  7821 net.cpp:394] ip1 <- pool3
I1028 04:47:21.633855  7821 net.cpp:356] ip1 -> ip1
I1028 04:47:21.633919  7821 net.cpp:96] Setting up ip1
I1028 04:47:22.101454  7821 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 04:47:22.101508  7821 net.cpp:67] Creating Layer relu4
I1028 04:47:22.101516  7821 net.cpp:394] relu4 <- ip1
I1028 04:47:22.101524  7821 net.cpp:345] relu4 -> ip1 (in-place)
I1028 04:47:22.101533  7821 net.cpp:96] Setting up relu4
I1028 04:47:22.101538  7821 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 04:47:22.101546  7821 net.cpp:67] Creating Layer drop4
I1028 04:47:22.101550  7821 net.cpp:394] drop4 <- ip1
I1028 04:47:22.101558  7821 net.cpp:345] drop4 -> ip1 (in-place)
I1028 04:47:22.101565  7821 net.cpp:96] Setting up drop4
I1028 04:47:22.101570  7821 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 04:47:22.101580  7821 net.cpp:67] Creating Layer ip2
I1028 04:47:22.101584  7821 net.cpp:394] ip2 <- ip1
I1028 04:47:22.101593  7821 net.cpp:356] ip2 -> ip2
I1028 04:47:22.101601  7821 net.cpp:96] Setting up ip2
I1028 04:47:22.110491  7821 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 04:47:22.110553  7821 net.cpp:67] Creating Layer loss
I1028 04:47:22.110559  7821 net.cpp:394] loss <- ip2
I1028 04:47:22.110568  7821 net.cpp:394] loss <- label
I1028 04:47:22.110574  7821 net.cpp:356] loss -> loss
I1028 04:47:22.110584  7821 net.cpp:96] Setting up loss
I1028 04:47:22.110595  7821 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 04:47:22.110601  7821 net.cpp:109]     with loss weight 1
I1028 04:47:22.110637  7821 net.cpp:170] loss needs backward computation.
I1028 04:47:22.110642  7821 net.cpp:170] ip2 needs backward computation.
I1028 04:47:22.110647  7821 net.cpp:170] drop4 needs backward computation.
I1028 04:47:22.110658  7821 net.cpp:170] relu4 needs backward computation.
I1028 04:47:22.110663  7821 net.cpp:170] ip1 needs backward computation.
I1028 04:47:22.110667  7821 net.cpp:170] drop3 needs backward computation.
I1028 04:47:22.110672  7821 net.cpp:170] relu3 needs backward computation.
I1028 04:47:22.110677  7821 net.cpp:170] pool3 needs backward computation.
I1028 04:47:22.110682  7821 net.cpp:170] conv3 needs backward computation.
I1028 04:47:22.110687  7821 net.cpp:170] drop2 needs backward computation.
I1028 04:47:22.110690  7821 net.cpp:170] relu2 needs backward computation.
I1028 04:47:22.110695  7821 net.cpp:170] pool2 needs backward computation.
I1028 04:47:22.110699  7821 net.cpp:170] conv2 needs backward computation.
I1028 04:47:22.110704  7821 net.cpp:170] drop1 needs backward computation.
I1028 04:47:22.110709  7821 net.cpp:170] relu1 needs backward computation.
I1028 04:47:22.110713  7821 net.cpp:170] pool1 needs backward computation.
I1028 04:47:22.110718  7821 net.cpp:170] conv1 needs backward computation.
I1028 04:47:22.110723  7821 net.cpp:172] mnist does not need backward computation.
I1028 04:47:22.110728  7821 net.cpp:208] This network produces output loss
I1028 04:47:22.110738  7821 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 04:47:22.110745  7821 net.cpp:219] Network initialization done.
I1028 04:47:22.110749  7821 net.cpp:220] Memory required for data: 119788292
I1028 04:47:22.110808  7821 solver.cpp:41] Solver scaffolding done.
I1028 04:47:22.110816  7821 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_360000.solverstate
I1028 04:47:22.110819  7821 solver.cpp:160] Solving Captcha
I1028 04:47:22.110838  7821 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_360000.solverstate
I1028 04:47:27.072644  7821 solver.cpp:502] SGDSolver: restoring history
I1028 04:47:27.905380  7821 solver.cpp:191] Iteration 360000, loss = 2.69213
I1028 04:47:27.905438  7821 solver.cpp:206]     Train net output #0: loss = 2.69213 (* 1 = 2.69213 loss)
I1028 04:47:27.905453  7821 solver.cpp:403] Iteration 360000, lr = 0.000666575
I1028 04:51:29.565699  7821 solver.cpp:191] Iteration 361000, loss = 2.39226
I1028 04:51:29.566582  7821 solver.cpp:206]     Train net output #0: loss = 2.39226 (* 1 = 2.39226 loss)
I1028 04:51:29.566615  7821 solver.cpp:403] Iteration 361000, lr = 0.000665227
I1028 04:55:30.625851  7821 solver.cpp:191] Iteration 362000, loss = 2.71859
I1028 04:55:30.626593  7821 solver.cpp:206]     Train net output #0: loss = 2.71859 (* 1 = 2.71859 loss)
I1028 04:55:30.626626  7821 solver.cpp:403] Iteration 362000, lr = 0.000663885
I1028 04:59:31.715132  7821 solver.cpp:191] Iteration 363000, loss = 2.35723
I1028 04:59:31.715692  7821 solver.cpp:206]     Train net output #0: loss = 2.35723 (* 1 = 2.35723 loss)
I1028 04:59:31.715725  7821 solver.cpp:403] Iteration 363000, lr = 0.00066255
I1028 05:03:32.884382  7821 solver.cpp:191] Iteration 364000, loss = 2.2495
I1028 05:03:32.885021  7821 solver.cpp:206]     Train net output #0: loss = 2.2495 (* 1 = 2.2495 loss)
I1028 05:03:32.885054  7821 solver.cpp:403] Iteration 364000, lr = 0.000661221
I1028 05:07:34.694224  7821 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_365000.caffemodel
I1028 05:07:40.398849  7821 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_365000.solverstate
I1028 05:07:45.435642  7821 solver.cpp:191] Iteration 365000, loss = 2.42932
I1028 05:07:45.436169  7821 solver.cpp:206]     Train net output #0: loss = 2.42932 (* 1 = 2.42932 loss)
I1028 05:07:45.436183  7821 solver.cpp:403] Iteration 365000, lr = 0.000659898
I1028 05:11:46.594074  7821 solver.cpp:191] Iteration 366000, loss = 2.31612
I1028 05:11:46.594604  7821 solver.cpp:206]     Train net output #0: loss = 2.31612 (* 1 = 2.31612 loss)
I1028 05:11:46.594626  7821 solver.cpp:403] Iteration 366000, lr = 0.000658581
I1028 05:15:47.792314  7821 solver.cpp:191] Iteration 367000, loss = 2.51693
I1028 05:15:47.792925  7821 solver.cpp:206]     Train net output #0: loss = 2.51693 (* 1 = 2.51693 loss)
I1028 05:15:47.792948  7821 solver.cpp:403] Iteration 367000, lr = 0.00065727
I1028 05:19:48.908962  7821 solver.cpp:191] Iteration 368000, loss = 2.35314
I1028 05:19:48.909577  7821 solver.cpp:206]     Train net output #0: loss = 2.35314 (* 1 = 2.35314 loss)
I1028 05:19:48.909610  7821 solver.cpp:403] Iteration 368000, lr = 0.000655966
I1028 05:23:49.992564  7821 solver.cpp:191] Iteration 369000, loss = 2.36244
I1028 05:23:49.993197  7821 solver.cpp:206]     Train net output #0: loss = 2.36244 (* 1 = 2.36244 loss)
I1028 05:23:49.993230  7821 solver.cpp:403] Iteration 369000, lr = 0.000654667
I1028 05:27:51.621765  7821 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_370000.caffemodel
I1028 05:27:55.683194  7821 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_370000.solverstate
I1028 05:27:59.097889  7821 solver.cpp:228] Iteration 370000, loss = 2.44408
I1028 05:27:59.098461  7821 solver.cpp:233] Optimization Done.
I1028 05:27:59.098485  7821 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 05:49:50.740185 27021 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 05:49:50.740298 27021 net.cpp:358] Input 0 -> data
I1028 05:49:50.740322 27021 net.cpp:67] Creating Layer conv1
I1028 05:49:50.740327 27021 net.cpp:394] conv1 <- data
I1028 05:49:50.740334 27021 net.cpp:356] conv1 -> conv1
I1028 05:49:50.740345 27021 net.cpp:96] Setting up conv1
I1028 05:49:50.740692 27021 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 05:49:50.740720 27021 net.cpp:67] Creating Layer pool1
I1028 05:49:50.740725 27021 net.cpp:394] pool1 <- conv1
I1028 05:49:50.740731 27021 net.cpp:356] pool1 -> pool1
I1028 05:49:50.740739 27021 net.cpp:96] Setting up pool1
I1028 05:49:50.740756 27021 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 05:49:50.740767 27021 net.cpp:67] Creating Layer relu1
I1028 05:49:50.740772 27021 net.cpp:394] relu1 <- pool1
I1028 05:49:50.740779 27021 net.cpp:345] relu1 -> pool1 (in-place)
I1028 05:49:50.740785 27021 net.cpp:96] Setting up relu1
I1028 05:49:50.740789 27021 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 05:49:50.740795 27021 net.cpp:67] Creating Layer drop1
I1028 05:49:50.740799 27021 net.cpp:394] drop1 <- pool1
I1028 05:49:50.740804 27021 net.cpp:345] drop1 -> pool1 (in-place)
I1028 05:49:50.740810 27021 net.cpp:96] Setting up drop1
I1028 05:49:50.740815 27021 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 05:49:50.740823 27021 net.cpp:67] Creating Layer conv2
I1028 05:49:50.740826 27021 net.cpp:394] conv2 <- pool1
I1028 05:49:50.740833 27021 net.cpp:356] conv2 -> conv2
I1028 05:49:50.740839 27021 net.cpp:96] Setting up conv2
I1028 05:49:50.741402 27021 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 05:49:50.741416 27021 net.cpp:67] Creating Layer pool2
I1028 05:49:50.741421 27021 net.cpp:394] pool2 <- conv2
I1028 05:49:50.741430 27021 net.cpp:356] pool2 -> pool2
I1028 05:49:50.741436 27021 net.cpp:96] Setting up pool2
I1028 05:49:50.741442 27021 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 05:49:50.741447 27021 net.cpp:67] Creating Layer relu2
I1028 05:49:50.741451 27021 net.cpp:394] relu2 <- pool2
I1028 05:49:50.741458 27021 net.cpp:345] relu2 -> pool2 (in-place)
I1028 05:49:50.741464 27021 net.cpp:96] Setting up relu2
I1028 05:49:50.741468 27021 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 05:49:50.741473 27021 net.cpp:67] Creating Layer drop2
I1028 05:49:50.741477 27021 net.cpp:394] drop2 <- pool2
I1028 05:49:50.741482 27021 net.cpp:345] drop2 -> pool2 (in-place)
I1028 05:49:50.741487 27021 net.cpp:96] Setting up drop2
I1028 05:49:50.741492 27021 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 05:49:50.741498 27021 net.cpp:67] Creating Layer conv3
I1028 05:49:50.741503 27021 net.cpp:394] conv3 <- pool2
I1028 05:49:50.741513 27021 net.cpp:356] conv3 -> conv3
I1028 05:49:50.741521 27021 net.cpp:96] Setting up conv3
I1028 05:49:50.742998 27021 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 05:49:50.743013 27021 net.cpp:67] Creating Layer pool3
I1028 05:49:50.743018 27021 net.cpp:394] pool3 <- conv3
I1028 05:49:50.743026 27021 net.cpp:356] pool3 -> pool3
I1028 05:49:50.743033 27021 net.cpp:96] Setting up pool3
I1028 05:49:50.743038 27021 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 05:49:50.743043 27021 net.cpp:67] Creating Layer relu3
I1028 05:49:50.743047 27021 net.cpp:394] relu3 <- pool3
I1028 05:49:50.743052 27021 net.cpp:345] relu3 -> pool3 (in-place)
I1028 05:49:50.743058 27021 net.cpp:96] Setting up relu3
I1028 05:49:50.743062 27021 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 05:49:50.743067 27021 net.cpp:67] Creating Layer drop3
I1028 05:49:50.743072 27021 net.cpp:394] drop3 <- pool3
I1028 05:49:50.743078 27021 net.cpp:345] drop3 -> pool3 (in-place)
I1028 05:49:50.743084 27021 net.cpp:96] Setting up drop3
I1028 05:49:50.743088 27021 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 05:49:50.743094 27021 net.cpp:67] Creating Layer ip1
I1028 05:49:50.743098 27021 net.cpp:394] ip1 <- pool3
I1028 05:49:50.743105 27021 net.cpp:356] ip1 -> ip1
I1028 05:49:50.743113 27021 net.cpp:96] Setting up ip1
I1028 05:49:51.240892 27021 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 05:49:51.240949 27021 net.cpp:67] Creating Layer relu4
I1028 05:49:51.240957 27021 net.cpp:394] relu4 <- ip1
I1028 05:49:51.240965 27021 net.cpp:345] relu4 -> ip1 (in-place)
I1028 05:49:51.240974 27021 net.cpp:96] Setting up relu4
I1028 05:49:51.240980 27021 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 05:49:51.240990 27021 net.cpp:67] Creating Layer drop4
I1028 05:49:51.240994 27021 net.cpp:394] drop4 <- ip1
I1028 05:49:51.241000 27021 net.cpp:345] drop4 -> ip1 (in-place)
I1028 05:49:51.241006 27021 net.cpp:96] Setting up drop4
I1028 05:49:51.241011 27021 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 05:49:51.241019 27021 net.cpp:67] Creating Layer ip2
I1028 05:49:51.241024 27021 net.cpp:394] ip2 <- ip1
I1028 05:49:51.241034 27021 net.cpp:356] ip2 -> ip2
I1028 05:49:51.241047 27021 net.cpp:96] Setting up ip2
I1028 05:49:51.250136 27021 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 05:49:51.250200 27021 net.cpp:67] Creating Layer prob
I1028 05:49:51.250206 27021 net.cpp:394] prob <- ip2
I1028 05:49:51.250216 27021 net.cpp:356] prob -> prob
I1028 05:49:51.250226 27021 net.cpp:96] Setting up prob
I1028 05:49:51.250233 27021 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 05:49:51.250238 27021 net.cpp:172] prob does not need backward computation.
I1028 05:49:51.250242 27021 net.cpp:172] ip2 does not need backward computation.
I1028 05:49:51.250246 27021 net.cpp:172] drop4 does not need backward computation.
I1028 05:49:51.250249 27021 net.cpp:172] relu4 does not need backward computation.
I1028 05:49:51.250252 27021 net.cpp:172] ip1 does not need backward computation.
I1028 05:49:51.250257 27021 net.cpp:172] drop3 does not need backward computation.
I1028 05:49:51.250260 27021 net.cpp:172] relu3 does not need backward computation.
I1028 05:49:51.250263 27021 net.cpp:172] pool3 does not need backward computation.
I1028 05:49:51.250267 27021 net.cpp:172] conv3 does not need backward computation.
I1028 05:49:51.250272 27021 net.cpp:172] drop2 does not need backward computation.
I1028 05:49:51.250274 27021 net.cpp:172] relu2 does not need backward computation.
I1028 05:49:51.250278 27021 net.cpp:172] pool2 does not need backward computation.
I1028 05:49:51.250282 27021 net.cpp:172] conv2 does not need backward computation.
I1028 05:49:51.250285 27021 net.cpp:172] drop1 does not need backward computation.
I1028 05:49:51.250288 27021 net.cpp:172] relu1 does not need backward computation.
I1028 05:49:51.250293 27021 net.cpp:172] pool1 does not need backward computation.
I1028 05:49:51.250296 27021 net.cpp:172] conv1 does not need backward computation.
I1028 05:49:51.250299 27021 net.cpp:208] This network produces output prob
I1028 05:49:51.250313 27021 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 05:49:51.250320 27021 net.cpp:219] Network initialization done.
I1028 05:49:51.250324 27021 net.cpp:220] Memory required for data: 1837200
I1028 05:50:32.406561 27021 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 05:50:32.407138 27021 net.cpp:358] Input 0 -> data
I1028 05:50:32.407196 27021 net.cpp:67] Creating Layer conv1
I1028 05:50:32.407212 27021 net.cpp:394] conv1 <- data
I1028 05:50:32.407230 27021 net.cpp:356] conv1 -> conv1
I1028 05:50:32.407255 27021 net.cpp:96] Setting up conv1
I1028 05:50:32.407320 27021 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 05:50:32.407356 27021 net.cpp:67] Creating Layer pool1
I1028 05:50:32.407371 27021 net.cpp:394] pool1 <- conv1
I1028 05:50:32.407387 27021 net.cpp:356] pool1 -> pool1
I1028 05:50:32.407405 27021 net.cpp:96] Setting up pool1
I1028 05:50:32.407423 27021 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 05:50:32.407441 27021 net.cpp:67] Creating Layer relu1
I1028 05:50:32.407454 27021 net.cpp:394] relu1 <- pool1
I1028 05:50:32.407469 27021 net.cpp:345] relu1 -> pool1 (in-place)
I1028 05:50:32.407485 27021 net.cpp:96] Setting up relu1
I1028 05:50:32.407496 27021 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 05:50:32.407512 27021 net.cpp:67] Creating Layer drop1
I1028 05:50:32.407523 27021 net.cpp:394] drop1 <- pool1
I1028 05:50:32.407538 27021 net.cpp:345] drop1 -> pool1 (in-place)
I1028 05:50:32.407555 27021 net.cpp:96] Setting up drop1
I1028 05:50:32.407568 27021 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 05:50:32.407588 27021 net.cpp:67] Creating Layer conv2
I1028 05:50:32.407600 27021 net.cpp:394] conv2 <- pool1
I1028 05:50:32.407616 27021 net.cpp:356] conv2 -> conv2
I1028 05:50:32.407636 27021 net.cpp:96] Setting up conv2
I1028 05:50:32.409060 27021 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 05:50:32.409096 27021 net.cpp:67] Creating Layer pool2
I1028 05:50:32.409109 27021 net.cpp:394] pool2 <- conv2
I1028 05:50:32.409126 27021 net.cpp:356] pool2 -> pool2
I1028 05:50:32.409147 27021 net.cpp:96] Setting up pool2
I1028 05:50:32.409162 27021 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 05:50:32.409178 27021 net.cpp:67] Creating Layer relu2
I1028 05:50:32.409190 27021 net.cpp:394] relu2 <- pool2
I1028 05:50:32.409205 27021 net.cpp:345] relu2 -> pool2 (in-place)
I1028 05:50:32.409221 27021 net.cpp:96] Setting up relu2
I1028 05:50:32.409232 27021 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 05:50:32.409247 27021 net.cpp:67] Creating Layer drop2
I1028 05:50:32.409260 27021 net.cpp:394] drop2 <- pool2
I1028 05:50:32.409274 27021 net.cpp:345] drop2 -> pool2 (in-place)
I1028 05:50:32.409291 27021 net.cpp:96] Setting up drop2
I1028 05:50:32.409313 27021 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 05:50:32.409333 27021 net.cpp:67] Creating Layer conv3
I1028 05:50:32.409345 27021 net.cpp:394] conv3 <- pool2
I1028 05:50:32.409363 27021 net.cpp:356] conv3 -> conv3
I1028 05:50:32.409382 27021 net.cpp:96] Setting up conv3
I1028 05:50:32.412727 27021 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 05:50:32.412745 27021 net.cpp:67] Creating Layer pool3
I1028 05:50:32.412750 27021 net.cpp:394] pool3 <- conv3
I1028 05:50:32.412756 27021 net.cpp:356] pool3 -> pool3
I1028 05:50:32.412763 27021 net.cpp:96] Setting up pool3
I1028 05:50:32.412770 27021 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 05:50:32.412775 27021 net.cpp:67] Creating Layer relu3
I1028 05:50:32.412778 27021 net.cpp:394] relu3 <- pool3
I1028 05:50:32.412785 27021 net.cpp:345] relu3 -> pool3 (in-place)
I1028 05:50:32.412789 27021 net.cpp:96] Setting up relu3
I1028 05:50:32.412793 27021 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 05:50:32.412799 27021 net.cpp:67] Creating Layer drop3
I1028 05:50:32.412803 27021 net.cpp:394] drop3 <- pool3
I1028 05:50:32.412808 27021 net.cpp:345] drop3 -> pool3 (in-place)
I1028 05:50:32.412814 27021 net.cpp:96] Setting up drop3
I1028 05:50:32.412819 27021 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 05:50:32.412825 27021 net.cpp:67] Creating Layer ip1
I1028 05:50:32.412829 27021 net.cpp:394] ip1 <- pool3
I1028 05:50:32.412837 27021 net.cpp:356] ip1 -> ip1
I1028 05:50:32.412843 27021 net.cpp:96] Setting up ip1
I1028 05:50:32.833633 27021 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 05:50:32.833698 27021 net.cpp:67] Creating Layer relu4
I1028 05:50:32.833708 27021 net.cpp:394] relu4 <- ip1
I1028 05:50:32.833719 27021 net.cpp:345] relu4 -> ip1 (in-place)
I1028 05:50:32.833729 27021 net.cpp:96] Setting up relu4
I1028 05:50:32.833734 27021 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 05:50:32.833742 27021 net.cpp:67] Creating Layer drop4
I1028 05:50:32.833746 27021 net.cpp:394] drop4 <- ip1
I1028 05:50:32.833753 27021 net.cpp:345] drop4 -> ip1 (in-place)
I1028 05:50:32.833760 27021 net.cpp:96] Setting up drop4
I1028 05:50:32.833766 27021 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 05:50:32.833776 27021 net.cpp:67] Creating Layer ip2
I1028 05:50:32.833781 27021 net.cpp:394] ip2 <- ip1
I1028 05:50:32.833788 27021 net.cpp:356] ip2 -> ip2
I1028 05:50:32.833802 27021 net.cpp:96] Setting up ip2
I1028 05:50:32.841383 27021 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 05:50:32.841445 27021 net.cpp:67] Creating Layer prob
I1028 05:50:32.841454 27021 net.cpp:394] prob <- ip2
I1028 05:50:32.841462 27021 net.cpp:356] prob -> prob
I1028 05:50:32.841475 27021 net.cpp:96] Setting up prob
I1028 05:50:32.841482 27021 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 05:50:32.841487 27021 net.cpp:172] prob does not need backward computation.
I1028 05:50:32.841492 27021 net.cpp:172] ip2 does not need backward computation.
I1028 05:50:32.841496 27021 net.cpp:172] drop4 does not need backward computation.
I1028 05:50:32.841500 27021 net.cpp:172] relu4 does not need backward computation.
I1028 05:50:32.841505 27021 net.cpp:172] ip1 does not need backward computation.
I1028 05:50:32.841508 27021 net.cpp:172] drop3 does not need backward computation.
I1028 05:50:32.841512 27021 net.cpp:172] relu3 does not need backward computation.
I1028 05:50:32.841516 27021 net.cpp:172] pool3 does not need backward computation.
I1028 05:50:32.841521 27021 net.cpp:172] conv3 does not need backward computation.
I1028 05:50:32.841524 27021 net.cpp:172] drop2 does not need backward computation.
I1028 05:50:32.841528 27021 net.cpp:172] relu2 does not need backward computation.
I1028 05:50:32.841533 27021 net.cpp:172] pool2 does not need backward computation.
I1028 05:50:32.841537 27021 net.cpp:172] conv2 does not need backward computation.
I1028 05:50:32.841541 27021 net.cpp:172] drop1 does not need backward computation.
I1028 05:50:32.841545 27021 net.cpp:172] relu1 does not need backward computation.
I1028 05:50:32.841549 27021 net.cpp:172] pool1 does not need backward computation.
I1028 05:50:32.841564 27021 net.cpp:172] conv1 does not need backward computation.
I1028 05:50:32.841568 27021 net.cpp:208] This network produces output prob
I1028 05:50:32.841584 27021 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 05:50:32.841594 27021 net.cpp:219] Network initialization done.
I1028 05:50:32.841598 27021 net.cpp:220] Memory required for data: 1837200
I1028 06:23:07.314208  2941 convert_imageset.cpp:70] Shuffling data
I1028 06:23:07.893273  2941 convert_imageset.cpp:73] A total of 60000 images.
I1028 06:23:07.893349  2941 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 06:23:09.973764  2941 convert_imageset.cpp:177] Processed 1000 files.
E1028 06:23:12.445268  2941 convert_imageset.cpp:177] Processed 2000 files.
E1028 06:23:14.290024  2941 convert_imageset.cpp:177] Processed 3000 files.
E1028 06:23:16.164921  2941 convert_imageset.cpp:177] Processed 4000 files.
E1028 06:23:17.978793  2941 convert_imageset.cpp:177] Processed 5000 files.
E1028 06:23:19.853811  2941 convert_imageset.cpp:177] Processed 6000 files.
E1028 06:23:21.659682  2941 convert_imageset.cpp:177] Processed 7000 files.
E1028 06:23:23.623863  2941 convert_imageset.cpp:177] Processed 8000 files.
E1028 06:23:25.336455  2941 convert_imageset.cpp:177] Processed 9000 files.
E1028 06:23:27.123862  2941 convert_imageset.cpp:177] Processed 10000 files.
E1028 06:23:28.845868  2941 convert_imageset.cpp:177] Processed 11000 files.
E1028 06:23:30.545042  2941 convert_imageset.cpp:177] Processed 12000 files.
E1028 06:23:32.281461  2941 convert_imageset.cpp:177] Processed 13000 files.
E1028 06:23:33.924471  2941 convert_imageset.cpp:177] Processed 14000 files.
E1028 06:23:35.678112  2941 convert_imageset.cpp:177] Processed 15000 files.
E1028 06:23:37.414994  2941 convert_imageset.cpp:177] Processed 16000 files.
E1028 06:23:39.185518  2941 convert_imageset.cpp:177] Processed 17000 files.
E1028 06:23:40.893993  2941 convert_imageset.cpp:177] Processed 18000 files.
E1028 06:23:42.470927  2941 convert_imageset.cpp:177] Processed 19000 files.
E1028 06:23:44.082648  2941 convert_imageset.cpp:177] Processed 20000 files.
E1028 06:23:45.736209  2941 convert_imageset.cpp:177] Processed 21000 files.
E1028 06:23:47.376266  2941 convert_imageset.cpp:177] Processed 22000 files.
E1028 06:23:48.969830  2941 convert_imageset.cpp:177] Processed 23000 files.
E1028 06:23:50.576004  2941 convert_imageset.cpp:177] Processed 24000 files.
E1028 06:23:52.175992  2941 convert_imageset.cpp:177] Processed 25000 files.
E1028 06:23:53.744513  2941 convert_imageset.cpp:177] Processed 26000 files.
E1028 06:23:55.302669  2941 convert_imageset.cpp:177] Processed 27000 files.
E1028 06:23:56.842756  2941 convert_imageset.cpp:177] Processed 28000 files.
E1028 06:23:58.425009  2941 convert_imageset.cpp:177] Processed 29000 files.
E1028 06:23:59.998505  2941 convert_imageset.cpp:177] Processed 30000 files.
E1028 06:24:01.590225  2941 convert_imageset.cpp:177] Processed 31000 files.
E1028 06:24:03.129220  2941 convert_imageset.cpp:177] Processed 32000 files.
E1028 06:24:04.665092  2941 convert_imageset.cpp:177] Processed 33000 files.
E1028 06:24:06.150081  2941 convert_imageset.cpp:177] Processed 34000 files.
E1028 06:24:07.773875  2941 convert_imageset.cpp:177] Processed 35000 files.
E1028 06:24:09.337705  2941 convert_imageset.cpp:177] Processed 36000 files.
E1028 06:24:10.878448  2941 convert_imageset.cpp:177] Processed 37000 files.
E1028 06:24:12.489087  2941 convert_imageset.cpp:177] Processed 38000 files.
E1028 06:24:14.419870  2941 convert_imageset.cpp:177] Processed 39000 files.
E1028 06:24:16.008222  2941 convert_imageset.cpp:177] Processed 40000 files.
E1028 06:24:17.501431  2941 convert_imageset.cpp:177] Processed 41000 files.
E1028 06:24:19.098484  2941 convert_imageset.cpp:177] Processed 42000 files.
E1028 06:24:20.622927  2941 convert_imageset.cpp:177] Processed 43000 files.
E1028 06:24:22.185061  2941 convert_imageset.cpp:177] Processed 44000 files.
E1028 06:24:23.745544  2941 convert_imageset.cpp:177] Processed 45000 files.
E1028 06:24:25.211402  2941 convert_imageset.cpp:177] Processed 46000 files.
E1028 06:24:26.775352  2941 convert_imageset.cpp:177] Processed 47000 files.
E1028 06:24:28.347857  2941 convert_imageset.cpp:177] Processed 48000 files.
E1028 06:24:29.930838  2941 convert_imageset.cpp:177] Processed 49000 files.
E1028 06:24:31.401139  2941 convert_imageset.cpp:177] Processed 50000 files.
E1028 06:24:32.981729  2941 convert_imageset.cpp:177] Processed 51000 files.
E1028 06:24:34.450960  2941 convert_imageset.cpp:177] Processed 52000 files.
E1028 06:24:35.988474  2941 convert_imageset.cpp:177] Processed 53000 files.
E1028 06:24:37.440021  2941 convert_imageset.cpp:177] Processed 54000 files.
E1028 06:24:39.044468  2941 convert_imageset.cpp:177] Processed 55000 files.
E1028 06:24:40.667086  2941 convert_imageset.cpp:177] Processed 56000 files.
E1028 06:24:42.161401  2941 convert_imageset.cpp:177] Processed 57000 files.
E1028 06:24:43.665648  2941 convert_imageset.cpp:177] Processed 58000 files.
E1028 06:24:45.132650  2941 convert_imageset.cpp:177] Processed 59000 files.
E1028 06:24:46.650606  2941 convert_imageset.cpp:177] Processed 60000 files.
I1028 06:24:46.839161  3042 caffe.cpp:99] Use GPU with device ID 0
I1028 06:24:47.185142  3042 caffe.cpp:107] Starting Optimization
I1028 06:24:47.185271  3042 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 380000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 06:24:47.185298  3042 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 06:24:47.187644  3042 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 06:24:47.187772  3042 net.cpp:67] Creating Layer mnist
I1028 06:24:47.187788  3042 net.cpp:356] mnist -> data
I1028 06:24:47.187809  3042 net.cpp:356] mnist -> label
I1028 06:24:47.187829  3042 net.cpp:96] Setting up mnist
I1028 06:24:47.195446  3042 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 06:24:47.195580  3042 data_layer.cpp:128] output data size: 64,1,50,180
I1028 06:24:47.197273  3042 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 06:24:47.197311  3042 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 06:24:47.197340  3042 net.cpp:67] Creating Layer conv1
I1028 06:24:47.197355  3042 net.cpp:394] conv1 <- data
I1028 06:24:47.197388  3042 net.cpp:356] conv1 -> conv1
I1028 06:24:47.197413  3042 net.cpp:96] Setting up conv1
I1028 06:24:47.197767  3042 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 06:24:47.197798  3042 net.cpp:67] Creating Layer pool1
I1028 06:24:47.197805  3042 net.cpp:394] pool1 <- conv1
I1028 06:24:47.197811  3042 net.cpp:356] pool1 -> pool1
I1028 06:24:47.197819  3042 net.cpp:96] Setting up pool1
I1028 06:24:47.197835  3042 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 06:24:47.197845  3042 net.cpp:67] Creating Layer relu1
I1028 06:24:47.197850  3042 net.cpp:394] relu1 <- pool1
I1028 06:24:47.197856  3042 net.cpp:345] relu1 -> pool1 (in-place)
I1028 06:24:47.197863  3042 net.cpp:96] Setting up relu1
I1028 06:24:47.197867  3042 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 06:24:47.197875  3042 net.cpp:67] Creating Layer drop1
I1028 06:24:47.197880  3042 net.cpp:394] drop1 <- pool1
I1028 06:24:47.197885  3042 net.cpp:345] drop1 -> pool1 (in-place)
I1028 06:24:47.197892  3042 net.cpp:96] Setting up drop1
I1028 06:24:47.197897  3042 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 06:24:47.197906  3042 net.cpp:67] Creating Layer conv2
I1028 06:24:47.197911  3042 net.cpp:394] conv2 <- pool1
I1028 06:24:47.197921  3042 net.cpp:356] conv2 -> conv2
I1028 06:24:47.197928  3042 net.cpp:96] Setting up conv2
I1028 06:24:47.198508  3042 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 06:24:47.198526  3042 net.cpp:67] Creating Layer pool2
I1028 06:24:47.198532  3042 net.cpp:394] pool2 <- conv2
I1028 06:24:47.198539  3042 net.cpp:356] pool2 -> pool2
I1028 06:24:47.198546  3042 net.cpp:96] Setting up pool2
I1028 06:24:47.198552  3042 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 06:24:47.198559  3042 net.cpp:67] Creating Layer relu2
I1028 06:24:47.198562  3042 net.cpp:394] relu2 <- pool2
I1028 06:24:47.198570  3042 net.cpp:345] relu2 -> pool2 (in-place)
I1028 06:24:47.198577  3042 net.cpp:96] Setting up relu2
I1028 06:24:47.198582  3042 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 06:24:47.198590  3042 net.cpp:67] Creating Layer drop2
I1028 06:24:47.198595  3042 net.cpp:394] drop2 <- pool2
I1028 06:24:47.198599  3042 net.cpp:345] drop2 -> pool2 (in-place)
I1028 06:24:47.198606  3042 net.cpp:96] Setting up drop2
I1028 06:24:47.198611  3042 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 06:24:47.198619  3042 net.cpp:67] Creating Layer conv3
I1028 06:24:47.198632  3042 net.cpp:394] conv3 <- pool2
I1028 06:24:47.198638  3042 net.cpp:356] conv3 -> conv3
I1028 06:24:47.198645  3042 net.cpp:96] Setting up conv3
I1028 06:24:47.201577  3042 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 06:24:47.201630  3042 net.cpp:67] Creating Layer pool3
I1028 06:24:47.201645  3042 net.cpp:394] pool3 <- conv3
I1028 06:24:47.201668  3042 net.cpp:356] pool3 -> pool3
I1028 06:24:47.201689  3042 net.cpp:96] Setting up pool3
I1028 06:24:47.201705  3042 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 06:24:47.201722  3042 net.cpp:67] Creating Layer relu3
I1028 06:24:47.201735  3042 net.cpp:394] relu3 <- pool3
I1028 06:24:47.201751  3042 net.cpp:345] relu3 -> pool3 (in-place)
I1028 06:24:47.201768  3042 net.cpp:96] Setting up relu3
I1028 06:24:47.201781  3042 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 06:24:47.201802  3042 net.cpp:67] Creating Layer drop3
I1028 06:24:47.201815  3042 net.cpp:394] drop3 <- pool3
I1028 06:24:47.201831  3042 net.cpp:345] drop3 -> pool3 (in-place)
I1028 06:24:47.201849  3042 net.cpp:96] Setting up drop3
I1028 06:24:47.201864  3042 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 06:24:47.201881  3042 net.cpp:67] Creating Layer ip1
I1028 06:24:47.201894  3042 net.cpp:394] ip1 <- pool3
I1028 06:24:47.201915  3042 net.cpp:356] ip1 -> ip1
I1028 06:24:47.201979  3042 net.cpp:96] Setting up ip1
I1028 06:24:47.735596  3042 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 06:24:47.735657  3042 net.cpp:67] Creating Layer relu4
I1028 06:24:47.735664  3042 net.cpp:394] relu4 <- ip1
I1028 06:24:47.735674  3042 net.cpp:345] relu4 -> ip1 (in-place)
I1028 06:24:47.735683  3042 net.cpp:96] Setting up relu4
I1028 06:24:47.735689  3042 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 06:24:47.735697  3042 net.cpp:67] Creating Layer drop4
I1028 06:24:47.735700  3042 net.cpp:394] drop4 <- ip1
I1028 06:24:47.735707  3042 net.cpp:345] drop4 -> ip1 (in-place)
I1028 06:24:47.735713  3042 net.cpp:96] Setting up drop4
I1028 06:24:47.735718  3042 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 06:24:47.735729  3042 net.cpp:67] Creating Layer ip2
I1028 06:24:47.735734  3042 net.cpp:394] ip2 <- ip1
I1028 06:24:47.735741  3042 net.cpp:356] ip2 -> ip2
I1028 06:24:47.735751  3042 net.cpp:96] Setting up ip2
I1028 06:24:47.746496  3042 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 06:24:47.746561  3042 net.cpp:67] Creating Layer loss
I1028 06:24:47.746568  3042 net.cpp:394] loss <- ip2
I1028 06:24:47.746577  3042 net.cpp:394] loss <- label
I1028 06:24:47.746583  3042 net.cpp:356] loss -> loss
I1028 06:24:47.746593  3042 net.cpp:96] Setting up loss
I1028 06:24:47.746605  3042 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 06:24:47.746610  3042 net.cpp:109]     with loss weight 1
I1028 06:24:47.746645  3042 net.cpp:170] loss needs backward computation.
I1028 06:24:47.746650  3042 net.cpp:170] ip2 needs backward computation.
I1028 06:24:47.746655  3042 net.cpp:170] drop4 needs backward computation.
I1028 06:24:47.746659  3042 net.cpp:170] relu4 needs backward computation.
I1028 06:24:47.746664  3042 net.cpp:170] ip1 needs backward computation.
I1028 06:24:47.746668  3042 net.cpp:170] drop3 needs backward computation.
I1028 06:24:47.746672  3042 net.cpp:170] relu3 needs backward computation.
I1028 06:24:47.746676  3042 net.cpp:170] pool3 needs backward computation.
I1028 06:24:47.746681  3042 net.cpp:170] conv3 needs backward computation.
I1028 06:24:47.746686  3042 net.cpp:170] drop2 needs backward computation.
I1028 06:24:47.746690  3042 net.cpp:170] relu2 needs backward computation.
I1028 06:24:47.746695  3042 net.cpp:170] pool2 needs backward computation.
I1028 06:24:47.746700  3042 net.cpp:170] conv2 needs backward computation.
I1028 06:24:47.746703  3042 net.cpp:170] drop1 needs backward computation.
I1028 06:24:47.746708  3042 net.cpp:170] relu1 needs backward computation.
I1028 06:24:47.746712  3042 net.cpp:170] pool1 needs backward computation.
I1028 06:24:47.746716  3042 net.cpp:170] conv1 needs backward computation.
I1028 06:24:47.746721  3042 net.cpp:172] mnist does not need backward computation.
I1028 06:24:47.746731  3042 net.cpp:208] This network produces output loss
I1028 06:24:47.746743  3042 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 06:24:47.746750  3042 net.cpp:219] Network initialization done.
I1028 06:24:47.746754  3042 net.cpp:220] Memory required for data: 119788292
I1028 06:24:47.746816  3042 solver.cpp:41] Solver scaffolding done.
I1028 06:24:47.746822  3042 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_370000.solverstate
I1028 06:24:47.746827  3042 solver.cpp:160] Solving Captcha
I1028 06:24:47.746845  3042 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_370000.solverstate
I1028 06:24:51.595815  3042 solver.cpp:502] SGDSolver: restoring history
I1028 06:24:52.360401  3042 solver.cpp:191] Iteration 370000, loss = 2.48235
I1028 06:24:52.360469  3042 solver.cpp:206]     Train net output #0: loss = 2.48235 (* 1 = 2.48235 loss)
I1028 06:24:52.360484  3042 solver.cpp:403] Iteration 370000, lr = 0.000653375
I1028 06:28:54.404803  3042 solver.cpp:191] Iteration 371000, loss = 2.39707
I1028 06:28:54.405424  3042 solver.cpp:206]     Train net output #0: loss = 2.39707 (* 1 = 2.39707 loss)
I1028 06:28:54.405458  3042 solver.cpp:403] Iteration 371000, lr = 0.000652088
I1028 06:32:55.829015  3042 solver.cpp:191] Iteration 372000, loss = 2.63112
I1028 06:32:55.829653  3042 solver.cpp:206]     Train net output #0: loss = 2.63112 (* 1 = 2.63112 loss)
I1028 06:32:55.829685  3042 solver.cpp:403] Iteration 372000, lr = 0.000650807
I1028 06:36:57.183542  3042 solver.cpp:191] Iteration 373000, loss = 2.45595
I1028 06:36:57.184073  3042 solver.cpp:206]     Train net output #0: loss = 2.45595 (* 1 = 2.45595 loss)
I1028 06:36:57.184106  3042 solver.cpp:403] Iteration 373000, lr = 0.000649533
I1028 06:40:58.512042  3042 solver.cpp:191] Iteration 374000, loss = 2.70149
I1028 06:40:58.512614  3042 solver.cpp:206]     Train net output #0: loss = 2.70149 (* 1 = 2.70149 loss)
I1028 06:40:58.512645  3042 solver.cpp:403] Iteration 374000, lr = 0.000648264
I1028 06:45:00.312212  3042 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_375000.caffemodel
I1028 06:45:05.222512  3042 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_375000.solverstate
I1028 06:45:09.108522  3042 solver.cpp:191] Iteration 375000, loss = 2.43598
I1028 06:45:09.109014  3042 solver.cpp:206]     Train net output #0: loss = 2.43598 (* 1 = 2.43598 loss)
I1028 06:45:09.109046  3042 solver.cpp:403] Iteration 375000, lr = 0.000647
I1028 06:49:10.475471  3042 solver.cpp:191] Iteration 376000, loss = 2.43754
I1028 06:49:10.476178  3042 solver.cpp:206]     Train net output #0: loss = 2.43754 (* 1 = 2.43754 loss)
I1028 06:49:10.476212  3042 solver.cpp:403] Iteration 376000, lr = 0.000645743
I1028 06:53:11.982205  3042 solver.cpp:191] Iteration 377000, loss = 2.25011
I1028 06:53:11.982816  3042 solver.cpp:206]     Train net output #0: loss = 2.25011 (* 1 = 2.25011 loss)
I1028 06:53:11.982848  3042 solver.cpp:403] Iteration 377000, lr = 0.000644491
I1028 06:57:13.511600  3042 solver.cpp:191] Iteration 378000, loss = 2.24954
I1028 06:57:13.512253  3042 solver.cpp:206]     Train net output #0: loss = 2.24954 (* 1 = 2.24954 loss)
I1028 06:57:13.512285  3042 solver.cpp:403] Iteration 378000, lr = 0.000643245
I1028 07:01:14.998044  3042 solver.cpp:191] Iteration 379000, loss = 2.50073
I1028 07:01:14.998708  3042 solver.cpp:206]     Train net output #0: loss = 2.50073 (* 1 = 2.50073 loss)
I1028 07:01:14.998740  3042 solver.cpp:403] Iteration 379000, lr = 0.000642004
I1028 07:05:16.963560  3042 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_380000.caffemodel
I1028 07:05:21.637513  3042 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_380000.solverstate
I1028 07:05:25.241819  3042 solver.cpp:228] Iteration 380000, loss = 2.40093
I1028 07:05:25.242400  3042 solver.cpp:233] Optimization Done.
I1028 07:05:25.242427  3042 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 07:27:21.373736 22309 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 07:27:21.373831 22309 net.cpp:358] Input 0 -> data
I1028 07:27:21.373855 22309 net.cpp:67] Creating Layer conv1
I1028 07:27:21.373860 22309 net.cpp:394] conv1 <- data
I1028 07:27:21.373867 22309 net.cpp:356] conv1 -> conv1
I1028 07:27:21.373877 22309 net.cpp:96] Setting up conv1
I1028 07:27:21.374189 22309 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 07:27:21.374209 22309 net.cpp:67] Creating Layer pool1
I1028 07:27:21.374213 22309 net.cpp:394] pool1 <- conv1
I1028 07:27:21.374219 22309 net.cpp:356] pool1 -> pool1
I1028 07:27:21.374228 22309 net.cpp:96] Setting up pool1
I1028 07:27:21.374238 22309 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 07:27:21.374244 22309 net.cpp:67] Creating Layer relu1
I1028 07:27:21.374248 22309 net.cpp:394] relu1 <- pool1
I1028 07:27:21.374254 22309 net.cpp:345] relu1 -> pool1 (in-place)
I1028 07:27:21.374259 22309 net.cpp:96] Setting up relu1
I1028 07:27:21.374264 22309 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 07:27:21.374274 22309 net.cpp:67] Creating Layer drop1
I1028 07:27:21.374279 22309 net.cpp:394] drop1 <- pool1
I1028 07:27:21.374286 22309 net.cpp:345] drop1 -> pool1 (in-place)
I1028 07:27:21.374294 22309 net.cpp:96] Setting up drop1
I1028 07:27:21.374299 22309 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 07:27:21.374305 22309 net.cpp:67] Creating Layer conv2
I1028 07:27:21.374310 22309 net.cpp:394] conv2 <- pool1
I1028 07:27:21.374316 22309 net.cpp:356] conv2 -> conv2
I1028 07:27:21.374323 22309 net.cpp:96] Setting up conv2
I1028 07:27:21.374888 22309 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 07:27:21.374905 22309 net.cpp:67] Creating Layer pool2
I1028 07:27:21.374910 22309 net.cpp:394] pool2 <- conv2
I1028 07:27:21.374915 22309 net.cpp:356] pool2 -> pool2
I1028 07:27:21.374922 22309 net.cpp:96] Setting up pool2
I1028 07:27:21.374928 22309 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 07:27:21.374933 22309 net.cpp:67] Creating Layer relu2
I1028 07:27:21.374938 22309 net.cpp:394] relu2 <- pool2
I1028 07:27:21.374944 22309 net.cpp:345] relu2 -> pool2 (in-place)
I1028 07:27:21.374950 22309 net.cpp:96] Setting up relu2
I1028 07:27:21.374954 22309 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 07:27:21.374960 22309 net.cpp:67] Creating Layer drop2
I1028 07:27:21.374963 22309 net.cpp:394] drop2 <- pool2
I1028 07:27:21.374969 22309 net.cpp:345] drop2 -> pool2 (in-place)
I1028 07:27:21.374974 22309 net.cpp:96] Setting up drop2
I1028 07:27:21.374979 22309 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 07:27:21.374987 22309 net.cpp:67] Creating Layer conv3
I1028 07:27:21.374991 22309 net.cpp:394] conv3 <- pool2
I1028 07:27:21.374997 22309 net.cpp:356] conv3 -> conv3
I1028 07:27:21.375005 22309 net.cpp:96] Setting up conv3
I1028 07:27:21.376705 22309 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 07:27:21.376723 22309 net.cpp:67] Creating Layer pool3
I1028 07:27:21.376727 22309 net.cpp:394] pool3 <- conv3
I1028 07:27:21.376736 22309 net.cpp:356] pool3 -> pool3
I1028 07:27:21.376742 22309 net.cpp:96] Setting up pool3
I1028 07:27:21.376749 22309 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 07:27:21.376754 22309 net.cpp:67] Creating Layer relu3
I1028 07:27:21.376757 22309 net.cpp:394] relu3 <- pool3
I1028 07:27:21.376765 22309 net.cpp:345] relu3 -> pool3 (in-place)
I1028 07:27:21.376770 22309 net.cpp:96] Setting up relu3
I1028 07:27:21.376775 22309 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 07:27:21.376780 22309 net.cpp:67] Creating Layer drop3
I1028 07:27:21.376783 22309 net.cpp:394] drop3 <- pool3
I1028 07:27:21.376788 22309 net.cpp:345] drop3 -> pool3 (in-place)
I1028 07:27:21.376795 22309 net.cpp:96] Setting up drop3
I1028 07:27:21.376798 22309 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 07:27:21.376804 22309 net.cpp:67] Creating Layer ip1
I1028 07:27:21.376808 22309 net.cpp:394] ip1 <- pool3
I1028 07:27:21.376816 22309 net.cpp:356] ip1 -> ip1
I1028 07:27:21.376823 22309 net.cpp:96] Setting up ip1
I1028 07:27:21.902920 22309 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 07:27:21.902981 22309 net.cpp:67] Creating Layer relu4
I1028 07:27:21.902988 22309 net.cpp:394] relu4 <- ip1
I1028 07:27:21.903000 22309 net.cpp:345] relu4 -> ip1 (in-place)
I1028 07:27:21.903009 22309 net.cpp:96] Setting up relu4
I1028 07:27:21.903015 22309 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 07:27:21.903023 22309 net.cpp:67] Creating Layer drop4
I1028 07:27:21.903026 22309 net.cpp:394] drop4 <- ip1
I1028 07:27:21.903031 22309 net.cpp:345] drop4 -> ip1 (in-place)
I1028 07:27:21.903038 22309 net.cpp:96] Setting up drop4
I1028 07:27:21.903043 22309 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 07:27:21.903055 22309 net.cpp:67] Creating Layer ip2
I1028 07:27:21.903060 22309 net.cpp:394] ip2 <- ip1
I1028 07:27:21.903066 22309 net.cpp:356] ip2 -> ip2
I1028 07:27:21.903079 22309 net.cpp:96] Setting up ip2
I1028 07:27:21.912904 22309 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 07:27:21.912971 22309 net.cpp:67] Creating Layer prob
I1028 07:27:21.912978 22309 net.cpp:394] prob <- ip2
I1028 07:27:21.912986 22309 net.cpp:356] prob -> prob
I1028 07:27:21.913003 22309 net.cpp:96] Setting up prob
I1028 07:27:21.913010 22309 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 07:27:21.913014 22309 net.cpp:172] prob does not need backward computation.
I1028 07:27:21.913018 22309 net.cpp:172] ip2 does not need backward computation.
I1028 07:27:21.913022 22309 net.cpp:172] drop4 does not need backward computation.
I1028 07:27:21.913025 22309 net.cpp:172] relu4 does not need backward computation.
I1028 07:27:21.913029 22309 net.cpp:172] ip1 does not need backward computation.
I1028 07:27:21.913033 22309 net.cpp:172] drop3 does not need backward computation.
I1028 07:27:21.913036 22309 net.cpp:172] relu3 does not need backward computation.
I1028 07:27:21.913040 22309 net.cpp:172] pool3 does not need backward computation.
I1028 07:27:21.913044 22309 net.cpp:172] conv3 does not need backward computation.
I1028 07:27:21.913048 22309 net.cpp:172] drop2 does not need backward computation.
I1028 07:27:21.913051 22309 net.cpp:172] relu2 does not need backward computation.
I1028 07:27:21.913055 22309 net.cpp:172] pool2 does not need backward computation.
I1028 07:27:21.913058 22309 net.cpp:172] conv2 does not need backward computation.
I1028 07:27:21.913063 22309 net.cpp:172] drop1 does not need backward computation.
I1028 07:27:21.913065 22309 net.cpp:172] relu1 does not need backward computation.
I1028 07:27:21.913069 22309 net.cpp:172] pool1 does not need backward computation.
I1028 07:27:21.913074 22309 net.cpp:172] conv1 does not need backward computation.
I1028 07:27:21.913076 22309 net.cpp:208] This network produces output prob
I1028 07:27:21.913089 22309 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 07:27:21.913097 22309 net.cpp:219] Network initialization done.
I1028 07:27:21.913101 22309 net.cpp:220] Memory required for data: 1837200
I1028 07:28:05.551260 22309 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 07:28:05.551805 22309 net.cpp:358] Input 0 -> data
I1028 07:28:05.551862 22309 net.cpp:67] Creating Layer conv1
I1028 07:28:05.551877 22309 net.cpp:394] conv1 <- data
I1028 07:28:05.551895 22309 net.cpp:356] conv1 -> conv1
I1028 07:28:05.551919 22309 net.cpp:96] Setting up conv1
I1028 07:28:05.551985 22309 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 07:28:05.552021 22309 net.cpp:67] Creating Layer pool1
I1028 07:28:05.552034 22309 net.cpp:394] pool1 <- conv1
I1028 07:28:05.552052 22309 net.cpp:356] pool1 -> pool1
I1028 07:28:05.552070 22309 net.cpp:96] Setting up pool1
I1028 07:28:05.552088 22309 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 07:28:05.552106 22309 net.cpp:67] Creating Layer relu1
I1028 07:28:05.552117 22309 net.cpp:394] relu1 <- pool1
I1028 07:28:05.552132 22309 net.cpp:345] relu1 -> pool1 (in-place)
I1028 07:28:05.552148 22309 net.cpp:96] Setting up relu1
I1028 07:28:05.552160 22309 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 07:28:05.552176 22309 net.cpp:67] Creating Layer drop1
I1028 07:28:05.552187 22309 net.cpp:394] drop1 <- pool1
I1028 07:28:05.552202 22309 net.cpp:345] drop1 -> pool1 (in-place)
I1028 07:28:05.552218 22309 net.cpp:96] Setting up drop1
I1028 07:28:05.552232 22309 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 07:28:05.552250 22309 net.cpp:67] Creating Layer conv2
I1028 07:28:05.552263 22309 net.cpp:394] conv2 <- pool1
I1028 07:28:05.552279 22309 net.cpp:356] conv2 -> conv2
I1028 07:28:05.552299 22309 net.cpp:96] Setting up conv2
I1028 07:28:05.553730 22309 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 07:28:05.553766 22309 net.cpp:67] Creating Layer pool2
I1028 07:28:05.553781 22309 net.cpp:394] pool2 <- conv2
I1028 07:28:05.553797 22309 net.cpp:356] pool2 -> pool2
I1028 07:28:05.553817 22309 net.cpp:96] Setting up pool2
I1028 07:28:05.553833 22309 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 07:28:05.553848 22309 net.cpp:67] Creating Layer relu2
I1028 07:28:05.553860 22309 net.cpp:394] relu2 <- pool2
I1028 07:28:05.553875 22309 net.cpp:345] relu2 -> pool2 (in-place)
I1028 07:28:05.553891 22309 net.cpp:96] Setting up relu2
I1028 07:28:05.553902 22309 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 07:28:05.553917 22309 net.cpp:67] Creating Layer drop2
I1028 07:28:05.553928 22309 net.cpp:394] drop2 <- pool2
I1028 07:28:05.553943 22309 net.cpp:345] drop2 -> pool2 (in-place)
I1028 07:28:05.553959 22309 net.cpp:96] Setting up drop2
I1028 07:28:05.553972 22309 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 07:28:05.553992 22309 net.cpp:67] Creating Layer conv3
I1028 07:28:05.554003 22309 net.cpp:394] conv3 <- pool2
I1028 07:28:05.554020 22309 net.cpp:356] conv3 -> conv3
I1028 07:28:05.554039 22309 net.cpp:96] Setting up conv3
I1028 07:28:05.556965 22309 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 07:28:05.556982 22309 net.cpp:67] Creating Layer pool3
I1028 07:28:05.556988 22309 net.cpp:394] pool3 <- conv3
I1028 07:28:05.556994 22309 net.cpp:356] pool3 -> pool3
I1028 07:28:05.557001 22309 net.cpp:96] Setting up pool3
I1028 07:28:05.557008 22309 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 07:28:05.557013 22309 net.cpp:67] Creating Layer relu3
I1028 07:28:05.557018 22309 net.cpp:394] relu3 <- pool3
I1028 07:28:05.557024 22309 net.cpp:345] relu3 -> pool3 (in-place)
I1028 07:28:05.557029 22309 net.cpp:96] Setting up relu3
I1028 07:28:05.557034 22309 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 07:28:05.557039 22309 net.cpp:67] Creating Layer drop3
I1028 07:28:05.557044 22309 net.cpp:394] drop3 <- pool3
I1028 07:28:05.557052 22309 net.cpp:345] drop3 -> pool3 (in-place)
I1028 07:28:05.557059 22309 net.cpp:96] Setting up drop3
I1028 07:28:05.557063 22309 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 07:28:05.557070 22309 net.cpp:67] Creating Layer ip1
I1028 07:28:05.557075 22309 net.cpp:394] ip1 <- pool3
I1028 07:28:05.557080 22309 net.cpp:356] ip1 -> ip1
I1028 07:28:05.557087 22309 net.cpp:96] Setting up ip1
I1028 07:28:05.974769 22309 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 07:28:05.974836 22309 net.cpp:67] Creating Layer relu4
I1028 07:28:05.974845 22309 net.cpp:394] relu4 <- ip1
I1028 07:28:05.974856 22309 net.cpp:345] relu4 -> ip1 (in-place)
I1028 07:28:05.974866 22309 net.cpp:96] Setting up relu4
I1028 07:28:05.974872 22309 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 07:28:05.974880 22309 net.cpp:67] Creating Layer drop4
I1028 07:28:05.974885 22309 net.cpp:394] drop4 <- ip1
I1028 07:28:05.974891 22309 net.cpp:345] drop4 -> ip1 (in-place)
I1028 07:28:05.974899 22309 net.cpp:96] Setting up drop4
I1028 07:28:05.974905 22309 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 07:28:05.974915 22309 net.cpp:67] Creating Layer ip2
I1028 07:28:05.974920 22309 net.cpp:394] ip2 <- ip1
I1028 07:28:05.974927 22309 net.cpp:356] ip2 -> ip2
I1028 07:28:05.974941 22309 net.cpp:96] Setting up ip2
I1028 07:28:05.982489 22309 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 07:28:05.982555 22309 net.cpp:67] Creating Layer prob
I1028 07:28:05.982563 22309 net.cpp:394] prob <- ip2
I1028 07:28:05.982573 22309 net.cpp:356] prob -> prob
I1028 07:28:05.982583 22309 net.cpp:96] Setting up prob
I1028 07:28:05.982591 22309 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 07:28:05.982596 22309 net.cpp:172] prob does not need backward computation.
I1028 07:28:05.982601 22309 net.cpp:172] ip2 does not need backward computation.
I1028 07:28:05.982605 22309 net.cpp:172] drop4 does not need backward computation.
I1028 07:28:05.982609 22309 net.cpp:172] relu4 does not need backward computation.
I1028 07:28:05.982614 22309 net.cpp:172] ip1 does not need backward computation.
I1028 07:28:05.982617 22309 net.cpp:172] drop3 does not need backward computation.
I1028 07:28:05.982621 22309 net.cpp:172] relu3 does not need backward computation.
I1028 07:28:05.982625 22309 net.cpp:172] pool3 does not need backward computation.
I1028 07:28:05.982630 22309 net.cpp:172] conv3 does not need backward computation.
I1028 07:28:05.982632 22309 net.cpp:172] drop2 does not need backward computation.
I1028 07:28:05.982636 22309 net.cpp:172] relu2 does not need backward computation.
I1028 07:28:05.982640 22309 net.cpp:172] pool2 does not need backward computation.
I1028 07:28:05.982645 22309 net.cpp:172] conv2 does not need backward computation.
I1028 07:28:05.982648 22309 net.cpp:172] drop1 does not need backward computation.
I1028 07:28:05.982652 22309 net.cpp:172] relu1 does not need backward computation.
I1028 07:28:05.982656 22309 net.cpp:172] pool1 does not need backward computation.
I1028 07:28:05.982661 22309 net.cpp:172] conv1 does not need backward computation.
I1028 07:28:05.982664 22309 net.cpp:208] This network produces output prob
I1028 07:28:05.982679 22309 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 07:28:05.982689 22309 net.cpp:219] Network initialization done.
I1028 07:28:05.982693 22309 net.cpp:220] Memory required for data: 1837200
I1028 08:02:20.068317 28191 convert_imageset.cpp:70] Shuffling data
I1028 08:02:20.760757 28191 convert_imageset.cpp:73] A total of 60000 images.
I1028 08:02:20.760840 28191 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 08:02:22.726490 28191 convert_imageset.cpp:177] Processed 1000 files.
E1028 08:02:24.969097 28191 convert_imageset.cpp:177] Processed 2000 files.
E1028 08:02:27.145908 28191 convert_imageset.cpp:177] Processed 3000 files.
E1028 08:02:29.176820 28191 convert_imageset.cpp:177] Processed 4000 files.
E1028 08:02:31.198112 28191 convert_imageset.cpp:177] Processed 5000 files.
E1028 08:02:33.091277 28191 convert_imageset.cpp:177] Processed 6000 files.
E1028 08:02:35.093703 28191 convert_imageset.cpp:177] Processed 7000 files.
E1028 08:02:36.861408 28191 convert_imageset.cpp:177] Processed 8000 files.
E1028 08:02:38.913815 28191 convert_imageset.cpp:177] Processed 9000 files.
E1028 08:02:40.745193 28191 convert_imageset.cpp:177] Processed 10000 files.
E1028 08:02:42.580113 28191 convert_imageset.cpp:177] Processed 11000 files.
E1028 08:02:44.507073 28191 convert_imageset.cpp:177] Processed 12000 files.
E1028 08:02:46.213933 28191 convert_imageset.cpp:177] Processed 13000 files.
E1028 08:02:48.053954 28191 convert_imageset.cpp:177] Processed 14000 files.
E1028 08:02:49.847679 28191 convert_imageset.cpp:177] Processed 15000 files.
E1028 08:02:51.711046 28191 convert_imageset.cpp:177] Processed 16000 files.
E1028 08:02:53.521793 28191 convert_imageset.cpp:177] Processed 17000 files.
E1028 08:02:55.378700 28191 convert_imageset.cpp:177] Processed 18000 files.
E1028 08:02:57.470690 28191 convert_imageset.cpp:177] Processed 19000 files.
E1028 08:02:59.271209 28191 convert_imageset.cpp:177] Processed 20000 files.
E1028 08:03:00.894165 28191 convert_imageset.cpp:177] Processed 21000 files.
E1028 08:03:02.945322 28191 convert_imageset.cpp:177] Processed 22000 files.
E1028 08:03:04.621330 28191 convert_imageset.cpp:177] Processed 23000 files.
E1028 08:03:06.157497 28191 convert_imageset.cpp:177] Processed 24000 files.
E1028 08:03:07.874583 28191 convert_imageset.cpp:177] Processed 25000 files.
E1028 08:03:09.606582 28191 convert_imageset.cpp:177] Processed 26000 files.
E1028 08:03:11.446344 28191 convert_imageset.cpp:177] Processed 27000 files.
E1028 08:03:13.004338 28191 convert_imageset.cpp:177] Processed 28000 files.
E1028 08:03:14.659647 28191 convert_imageset.cpp:177] Processed 29000 files.
E1028 08:03:16.207726 28191 convert_imageset.cpp:177] Processed 30000 files.
E1028 08:03:17.948766 28191 convert_imageset.cpp:177] Processed 31000 files.
E1028 08:03:19.565645 28191 convert_imageset.cpp:177] Processed 32000 files.
E1028 08:03:21.194643 28191 convert_imageset.cpp:177] Processed 33000 files.
E1028 08:03:22.836123 28191 convert_imageset.cpp:177] Processed 34000 files.
E1028 08:03:24.630044 28191 convert_imageset.cpp:177] Processed 35000 files.
E1028 08:03:26.281313 28191 convert_imageset.cpp:177] Processed 36000 files.
E1028 08:03:27.838637 28191 convert_imageset.cpp:177] Processed 37000 files.
E1028 08:03:29.473217 28191 convert_imageset.cpp:177] Processed 38000 files.
E1028 08:03:31.072211 28191 convert_imageset.cpp:177] Processed 39000 files.
E1028 08:03:33.063076 28191 convert_imageset.cpp:177] Processed 40000 files.
E1028 08:03:34.565886 28191 convert_imageset.cpp:177] Processed 41000 files.
E1028 08:03:36.247869 28191 convert_imageset.cpp:177] Processed 42000 files.
E1028 08:03:37.807500 28191 convert_imageset.cpp:177] Processed 43000 files.
E1028 08:03:39.438347 28191 convert_imageset.cpp:177] Processed 44000 files.
E1028 08:03:41.120235 28191 convert_imageset.cpp:177] Processed 45000 files.
E1028 08:03:42.786774 28191 convert_imageset.cpp:177] Processed 46000 files.
E1028 08:03:44.442435 28191 convert_imageset.cpp:177] Processed 47000 files.
E1028 08:03:45.916082 28191 convert_imageset.cpp:177] Processed 48000 files.
E1028 08:03:47.514257 28191 convert_imageset.cpp:177] Processed 49000 files.
E1028 08:03:49.059535 28191 convert_imageset.cpp:177] Processed 50000 files.
E1028 08:03:50.549798 28191 convert_imageset.cpp:177] Processed 51000 files.
E1028 08:03:52.193825 28191 convert_imageset.cpp:177] Processed 52000 files.
E1028 08:03:53.915328 28191 convert_imageset.cpp:177] Processed 53000 files.
E1028 08:03:55.476239 28191 convert_imageset.cpp:177] Processed 54000 files.
E1028 08:03:57.048516 28191 convert_imageset.cpp:177] Processed 55000 files.
E1028 08:03:58.669528 28191 convert_imageset.cpp:177] Processed 56000 files.
E1028 08:04:00.147508 28191 convert_imageset.cpp:177] Processed 57000 files.
E1028 08:04:01.752209 28191 convert_imageset.cpp:177] Processed 58000 files.
E1028 08:04:03.341408 28191 convert_imageset.cpp:177] Processed 59000 files.
E1028 08:04:04.851065 28191 convert_imageset.cpp:177] Processed 60000 files.
I1028 08:04:05.151643 28293 caffe.cpp:99] Use GPU with device ID 0
I1028 08:04:05.515626 28293 caffe.cpp:107] Starting Optimization
I1028 08:04:05.515749 28293 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 390000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 08:04:05.515774 28293 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 08:04:05.535406 28293 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 08:04:05.535626 28293 net.cpp:67] Creating Layer mnist
I1028 08:04:05.535652 28293 net.cpp:356] mnist -> data
I1028 08:04:05.535688 28293 net.cpp:356] mnist -> label
I1028 08:04:05.535732 28293 net.cpp:96] Setting up mnist
I1028 08:04:05.542667 28293 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 08:04:05.542793 28293 data_layer.cpp:128] output data size: 64,1,50,180
I1028 08:04:05.544477 28293 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 08:04:05.544551 28293 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 08:04:05.544582 28293 net.cpp:67] Creating Layer conv1
I1028 08:04:05.544592 28293 net.cpp:394] conv1 <- data
I1028 08:04:05.544612 28293 net.cpp:356] conv1 -> conv1
I1028 08:04:05.544628 28293 net.cpp:96] Setting up conv1
I1028 08:04:05.545130 28293 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 08:04:05.545171 28293 net.cpp:67] Creating Layer pool1
I1028 08:04:05.545181 28293 net.cpp:394] pool1 <- conv1
I1028 08:04:05.545189 28293 net.cpp:356] pool1 -> pool1
I1028 08:04:05.545200 28293 net.cpp:96] Setting up pool1
I1028 08:04:05.545220 28293 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 08:04:05.545233 28293 net.cpp:67] Creating Layer relu1
I1028 08:04:05.545241 28293 net.cpp:394] relu1 <- pool1
I1028 08:04:05.545249 28293 net.cpp:345] relu1 -> pool1 (in-place)
I1028 08:04:05.545259 28293 net.cpp:96] Setting up relu1
I1028 08:04:05.545266 28293 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 08:04:05.545276 28293 net.cpp:67] Creating Layer drop1
I1028 08:04:05.545284 28293 net.cpp:394] drop1 <- pool1
I1028 08:04:05.545292 28293 net.cpp:345] drop1 -> pool1 (in-place)
I1028 08:04:05.545301 28293 net.cpp:96] Setting up drop1
I1028 08:04:05.545310 28293 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 08:04:05.545322 28293 net.cpp:67] Creating Layer conv2
I1028 08:04:05.545330 28293 net.cpp:394] conv2 <- pool1
I1028 08:04:05.545341 28293 net.cpp:356] conv2 -> conv2
I1028 08:04:05.545353 28293 net.cpp:96] Setting up conv2
I1028 08:04:05.546185 28293 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 08:04:05.546210 28293 net.cpp:67] Creating Layer pool2
I1028 08:04:05.546217 28293 net.cpp:394] pool2 <- conv2
I1028 08:04:05.546227 28293 net.cpp:356] pool2 -> pool2
I1028 08:04:05.546237 28293 net.cpp:96] Setting up pool2
I1028 08:04:05.546247 28293 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 08:04:05.546254 28293 net.cpp:67] Creating Layer relu2
I1028 08:04:05.546262 28293 net.cpp:394] relu2 <- pool2
I1028 08:04:05.546269 28293 net.cpp:345] relu2 -> pool2 (in-place)
I1028 08:04:05.546278 28293 net.cpp:96] Setting up relu2
I1028 08:04:05.546285 28293 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 08:04:05.546298 28293 net.cpp:67] Creating Layer drop2
I1028 08:04:05.546305 28293 net.cpp:394] drop2 <- pool2
I1028 08:04:05.546314 28293 net.cpp:345] drop2 -> pool2 (in-place)
I1028 08:04:05.546324 28293 net.cpp:96] Setting up drop2
I1028 08:04:05.546331 28293 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 08:04:05.546344 28293 net.cpp:67] Creating Layer conv3
I1028 08:04:05.546350 28293 net.cpp:394] conv3 <- pool2
I1028 08:04:05.546360 28293 net.cpp:356] conv3 -> conv3
I1028 08:04:05.546371 28293 net.cpp:96] Setting up conv3
I1028 08:04:05.549594 28293 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 08:04:05.549643 28293 net.cpp:67] Creating Layer pool3
I1028 08:04:05.549659 28293 net.cpp:394] pool3 <- conv3
I1028 08:04:05.549680 28293 net.cpp:356] pool3 -> pool3
I1028 08:04:05.549701 28293 net.cpp:96] Setting up pool3
I1028 08:04:05.549717 28293 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 08:04:05.549736 28293 net.cpp:67] Creating Layer relu3
I1028 08:04:05.549748 28293 net.cpp:394] relu3 <- pool3
I1028 08:04:05.549764 28293 net.cpp:345] relu3 -> pool3 (in-place)
I1028 08:04:05.549782 28293 net.cpp:96] Setting up relu3
I1028 08:04:05.549794 28293 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 08:04:05.549815 28293 net.cpp:67] Creating Layer drop3
I1028 08:04:05.549829 28293 net.cpp:394] drop3 <- pool3
I1028 08:04:05.549845 28293 net.cpp:345] drop3 -> pool3 (in-place)
I1028 08:04:05.549864 28293 net.cpp:96] Setting up drop3
I1028 08:04:05.549877 28293 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 08:04:05.549906 28293 net.cpp:67] Creating Layer ip1
I1028 08:04:05.549919 28293 net.cpp:394] ip1 <- pool3
I1028 08:04:05.549942 28293 net.cpp:356] ip1 -> ip1
I1028 08:04:05.550004 28293 net.cpp:96] Setting up ip1
I1028 08:04:06.155725 28293 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 08:04:06.155784 28293 net.cpp:67] Creating Layer relu4
I1028 08:04:06.155792 28293 net.cpp:394] relu4 <- ip1
I1028 08:04:06.155802 28293 net.cpp:345] relu4 -> ip1 (in-place)
I1028 08:04:06.155810 28293 net.cpp:96] Setting up relu4
I1028 08:04:06.155815 28293 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 08:04:06.155822 28293 net.cpp:67] Creating Layer drop4
I1028 08:04:06.155827 28293 net.cpp:394] drop4 <- ip1
I1028 08:04:06.155833 28293 net.cpp:345] drop4 -> ip1 (in-place)
I1028 08:04:06.155839 28293 net.cpp:96] Setting up drop4
I1028 08:04:06.155844 28293 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 08:04:06.155858 28293 net.cpp:67] Creating Layer ip2
I1028 08:04:06.155863 28293 net.cpp:394] ip2 <- ip1
I1028 08:04:06.155869 28293 net.cpp:356] ip2 -> ip2
I1028 08:04:06.155877 28293 net.cpp:96] Setting up ip2
I1028 08:04:06.165282 28293 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 08:04:06.165354 28293 net.cpp:67] Creating Layer loss
I1028 08:04:06.165362 28293 net.cpp:394] loss <- ip2
I1028 08:04:06.165370 28293 net.cpp:394] loss <- label
I1028 08:04:06.165377 28293 net.cpp:356] loss -> loss
I1028 08:04:06.165388 28293 net.cpp:96] Setting up loss
I1028 08:04:06.165400 28293 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 08:04:06.165405 28293 net.cpp:109]     with loss weight 1
I1028 08:04:06.165446 28293 net.cpp:170] loss needs backward computation.
I1028 08:04:06.165452 28293 net.cpp:170] ip2 needs backward computation.
I1028 08:04:06.165457 28293 net.cpp:170] drop4 needs backward computation.
I1028 08:04:06.165460 28293 net.cpp:170] relu4 needs backward computation.
I1028 08:04:06.165465 28293 net.cpp:170] ip1 needs backward computation.
I1028 08:04:06.165469 28293 net.cpp:170] drop3 needs backward computation.
I1028 08:04:06.165474 28293 net.cpp:170] relu3 needs backward computation.
I1028 08:04:06.165478 28293 net.cpp:170] pool3 needs backward computation.
I1028 08:04:06.165483 28293 net.cpp:170] conv3 needs backward computation.
I1028 08:04:06.165488 28293 net.cpp:170] drop2 needs backward computation.
I1028 08:04:06.165493 28293 net.cpp:170] relu2 needs backward computation.
I1028 08:04:06.165498 28293 net.cpp:170] pool2 needs backward computation.
I1028 08:04:06.165503 28293 net.cpp:170] conv2 needs backward computation.
I1028 08:04:06.165508 28293 net.cpp:170] drop1 needs backward computation.
I1028 08:04:06.165511 28293 net.cpp:170] relu1 needs backward computation.
I1028 08:04:06.165515 28293 net.cpp:170] pool1 needs backward computation.
I1028 08:04:06.165520 28293 net.cpp:170] conv1 needs backward computation.
I1028 08:04:06.165525 28293 net.cpp:172] mnist does not need backward computation.
I1028 08:04:06.165529 28293 net.cpp:208] This network produces output loss
I1028 08:04:06.165541 28293 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 08:04:06.165549 28293 net.cpp:219] Network initialization done.
I1028 08:04:06.165554 28293 net.cpp:220] Memory required for data: 119788292
I1028 08:04:06.165613 28293 solver.cpp:41] Solver scaffolding done.
I1028 08:04:06.165621 28293 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_380000.solverstate
I1028 08:04:06.165626 28293 solver.cpp:160] Solving Captcha
I1028 08:04:06.165644 28293 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_380000.solverstate
I1028 08:04:13.713529 28293 solver.cpp:502] SGDSolver: restoring history
I1028 08:04:14.447983 28293 solver.cpp:191] Iteration 380000, loss = 2.63467
I1028 08:04:14.448037 28293 solver.cpp:206]     Train net output #0: loss = 2.63467 (* 1 = 2.63467 loss)
I1028 08:04:14.448052 28293 solver.cpp:403] Iteration 380000, lr = 0.000640769
I1028 08:08:16.329869 28293 solver.cpp:191] Iteration 381000, loss = 2.42142
I1028 08:08:16.330530 28293 solver.cpp:206]     Train net output #0: loss = 2.42142 (* 1 = 2.42142 loss)
I1028 08:08:16.330556 28293 solver.cpp:403] Iteration 381000, lr = 0.00063954
I1028 08:12:17.913880 28293 solver.cpp:191] Iteration 382000, loss = 2.5475
I1028 08:12:17.914541 28293 solver.cpp:206]     Train net output #0: loss = 2.5475 (* 1 = 2.5475 loss)
I1028 08:12:17.914576 28293 solver.cpp:403] Iteration 382000, lr = 0.000638316
I1028 08:16:19.576761 28293 solver.cpp:191] Iteration 383000, loss = 2.40911
I1028 08:16:19.577394 28293 solver.cpp:206]     Train net output #0: loss = 2.40911 (* 1 = 2.40911 loss)
I1028 08:16:19.577427 28293 solver.cpp:403] Iteration 383000, lr = 0.000637097
I1028 08:20:21.249076 28293 solver.cpp:191] Iteration 384000, loss = 2.28261
I1028 08:20:21.249809 28293 solver.cpp:206]     Train net output #0: loss = 2.28261 (* 1 = 2.28261 loss)
I1028 08:20:21.249847 28293 solver.cpp:403] Iteration 384000, lr = 0.000635884
I1028 08:24:23.410655 28293 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_385000.caffemodel
I1028 08:24:27.892119 28293 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_385000.solverstate
I1028 08:24:31.611811 28293 solver.cpp:191] Iteration 385000, loss = 2.47515
I1028 08:24:31.612627 28293 solver.cpp:206]     Train net output #0: loss = 2.47515 (* 1 = 2.47515 loss)
I1028 08:24:31.612663 28293 solver.cpp:403] Iteration 385000, lr = 0.000634676
I1028 08:28:33.321153 28293 solver.cpp:191] Iteration 386000, loss = 2.46687
I1028 08:28:33.321763 28293 solver.cpp:206]     Train net output #0: loss = 2.46687 (* 1 = 2.46687 loss)
I1028 08:28:33.321800 28293 solver.cpp:403] Iteration 386000, lr = 0.000633474
I1028 08:32:35.212698 28293 solver.cpp:191] Iteration 387000, loss = 2.36336
I1028 08:32:35.213454 28293 solver.cpp:206]     Train net output #0: loss = 2.36336 (* 1 = 2.36336 loss)
I1028 08:32:35.213486 28293 solver.cpp:403] Iteration 387000, lr = 0.000632277
I1028 08:36:37.083598 28293 solver.cpp:191] Iteration 388000, loss = 2.13664
I1028 08:36:37.084211 28293 solver.cpp:206]     Train net output #0: loss = 2.13664 (* 1 = 2.13664 loss)
I1028 08:36:37.084244 28293 solver.cpp:403] Iteration 388000, lr = 0.000631085
I1028 08:40:38.835290 28293 solver.cpp:191] Iteration 389000, loss = 2.43863
I1028 08:40:38.835942 28293 solver.cpp:206]     Train net output #0: loss = 2.43863 (* 1 = 2.43863 loss)
I1028 08:40:38.835976 28293 solver.cpp:403] Iteration 389000, lr = 0.000629898
I1028 08:44:41.013442 28293 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_390000.caffemodel
I1028 08:44:45.519778 28293 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_390000.solverstate
I1028 08:44:49.130199 28293 solver.cpp:228] Iteration 390000, loss = 2.32993
I1028 08:44:49.130719 28293 solver.cpp:233] Optimization Done.
I1028 08:44:49.130743 28293 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 09:06:54.893343 15388 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 09:06:54.894047 15388 net.cpp:358] Input 0 -> data
I1028 09:06:54.894078 15388 net.cpp:67] Creating Layer conv1
I1028 09:06:54.894085 15388 net.cpp:394] conv1 <- data
I1028 09:06:54.894094 15388 net.cpp:356] conv1 -> conv1
I1028 09:06:54.894109 15388 net.cpp:96] Setting up conv1
I1028 09:06:54.894553 15388 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 09:06:54.894579 15388 net.cpp:67] Creating Layer pool1
I1028 09:06:54.894587 15388 net.cpp:394] pool1 <- conv1
I1028 09:06:54.894594 15388 net.cpp:356] pool1 -> pool1
I1028 09:06:54.894604 15388 net.cpp:96] Setting up pool1
I1028 09:06:54.894621 15388 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 09:06:54.894630 15388 net.cpp:67] Creating Layer relu1
I1028 09:06:54.894636 15388 net.cpp:394] relu1 <- pool1
I1028 09:06:54.894647 15388 net.cpp:345] relu1 -> pool1 (in-place)
I1028 09:06:54.894655 15388 net.cpp:96] Setting up relu1
I1028 09:06:54.894662 15388 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 09:06:54.894670 15388 net.cpp:67] Creating Layer drop1
I1028 09:06:54.894675 15388 net.cpp:394] drop1 <- pool1
I1028 09:06:54.894683 15388 net.cpp:345] drop1 -> pool1 (in-place)
I1028 09:06:54.894691 15388 net.cpp:96] Setting up drop1
I1028 09:06:54.894698 15388 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 09:06:54.894707 15388 net.cpp:67] Creating Layer conv2
I1028 09:06:54.894713 15388 net.cpp:394] conv2 <- pool1
I1028 09:06:54.894721 15388 net.cpp:356] conv2 -> conv2
I1028 09:06:54.894731 15388 net.cpp:96] Setting up conv2
I1028 09:06:54.895491 15388 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 09:06:54.895509 15388 net.cpp:67] Creating Layer pool2
I1028 09:06:54.895515 15388 net.cpp:394] pool2 <- conv2
I1028 09:06:54.895526 15388 net.cpp:356] pool2 -> pool2
I1028 09:06:54.895537 15388 net.cpp:96] Setting up pool2
I1028 09:06:54.895545 15388 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 09:06:54.895553 15388 net.cpp:67] Creating Layer relu2
I1028 09:06:54.895558 15388 net.cpp:394] relu2 <- pool2
I1028 09:06:54.895568 15388 net.cpp:345] relu2 -> pool2 (in-place)
I1028 09:06:54.895577 15388 net.cpp:96] Setting up relu2
I1028 09:06:54.895586 15388 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 09:06:54.895593 15388 net.cpp:67] Creating Layer drop2
I1028 09:06:54.895599 15388 net.cpp:394] drop2 <- pool2
I1028 09:06:54.895607 15388 net.cpp:345] drop2 -> pool2 (in-place)
I1028 09:06:54.895615 15388 net.cpp:96] Setting up drop2
I1028 09:06:54.895622 15388 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 09:06:54.895632 15388 net.cpp:67] Creating Layer conv3
I1028 09:06:54.895637 15388 net.cpp:394] conv3 <- pool2
I1028 09:06:54.895648 15388 net.cpp:356] conv3 -> conv3
I1028 09:06:54.895658 15388 net.cpp:96] Setting up conv3
I1028 09:06:54.897678 15388 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 09:06:54.897706 15388 net.cpp:67] Creating Layer pool3
I1028 09:06:54.897711 15388 net.cpp:394] pool3 <- conv3
I1028 09:06:54.897722 15388 net.cpp:356] pool3 -> pool3
I1028 09:06:54.897732 15388 net.cpp:96] Setting up pool3
I1028 09:06:54.897739 15388 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 09:06:54.897747 15388 net.cpp:67] Creating Layer relu3
I1028 09:06:54.897753 15388 net.cpp:394] relu3 <- pool3
I1028 09:06:54.897760 15388 net.cpp:345] relu3 -> pool3 (in-place)
I1028 09:06:54.897768 15388 net.cpp:96] Setting up relu3
I1028 09:06:54.897774 15388 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 09:06:54.897783 15388 net.cpp:67] Creating Layer drop3
I1028 09:06:54.897788 15388 net.cpp:394] drop3 <- pool3
I1028 09:06:54.897797 15388 net.cpp:345] drop3 -> pool3 (in-place)
I1028 09:06:54.897806 15388 net.cpp:96] Setting up drop3
I1028 09:06:54.897812 15388 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 09:06:54.897822 15388 net.cpp:67] Creating Layer ip1
I1028 09:06:54.897827 15388 net.cpp:394] ip1 <- pool3
I1028 09:06:54.897840 15388 net.cpp:356] ip1 -> ip1
I1028 09:06:54.897850 15388 net.cpp:96] Setting up ip1
I1028 09:06:55.396356 15388 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 09:06:55.396416 15388 net.cpp:67] Creating Layer relu4
I1028 09:06:55.396433 15388 net.cpp:394] relu4 <- ip1
I1028 09:06:55.396442 15388 net.cpp:345] relu4 -> ip1 (in-place)
I1028 09:06:55.396452 15388 net.cpp:96] Setting up relu4
I1028 09:06:55.396457 15388 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 09:06:55.396467 15388 net.cpp:67] Creating Layer drop4
I1028 09:06:55.396472 15388 net.cpp:394] drop4 <- ip1
I1028 09:06:55.396478 15388 net.cpp:345] drop4 -> ip1 (in-place)
I1028 09:06:55.396484 15388 net.cpp:96] Setting up drop4
I1028 09:06:55.396489 15388 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 09:06:55.396497 15388 net.cpp:67] Creating Layer ip2
I1028 09:06:55.396502 15388 net.cpp:394] ip2 <- ip1
I1028 09:06:55.396510 15388 net.cpp:356] ip2 -> ip2
I1028 09:06:55.396522 15388 net.cpp:96] Setting up ip2
I1028 09:06:55.406599 15388 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 09:06:55.406661 15388 net.cpp:67] Creating Layer prob
I1028 09:06:55.406669 15388 net.cpp:394] prob <- ip2
I1028 09:06:55.406677 15388 net.cpp:356] prob -> prob
I1028 09:06:55.406687 15388 net.cpp:96] Setting up prob
I1028 09:06:55.406697 15388 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 09:06:55.406702 15388 net.cpp:172] prob does not need backward computation.
I1028 09:06:55.406707 15388 net.cpp:172] ip2 does not need backward computation.
I1028 09:06:55.406710 15388 net.cpp:172] drop4 does not need backward computation.
I1028 09:06:55.406714 15388 net.cpp:172] relu4 does not need backward computation.
I1028 09:06:55.406718 15388 net.cpp:172] ip1 does not need backward computation.
I1028 09:06:55.406723 15388 net.cpp:172] drop3 does not need backward computation.
I1028 09:06:55.406725 15388 net.cpp:172] relu3 does not need backward computation.
I1028 09:06:55.406729 15388 net.cpp:172] pool3 does not need backward computation.
I1028 09:06:55.406733 15388 net.cpp:172] conv3 does not need backward computation.
I1028 09:06:55.406738 15388 net.cpp:172] drop2 does not need backward computation.
I1028 09:06:55.406741 15388 net.cpp:172] relu2 does not need backward computation.
I1028 09:06:55.406744 15388 net.cpp:172] pool2 does not need backward computation.
I1028 09:06:55.406755 15388 net.cpp:172] conv2 does not need backward computation.
I1028 09:06:55.406760 15388 net.cpp:172] drop1 does not need backward computation.
I1028 09:06:55.406764 15388 net.cpp:172] relu1 does not need backward computation.
I1028 09:06:55.406767 15388 net.cpp:172] pool1 does not need backward computation.
I1028 09:06:55.406771 15388 net.cpp:172] conv1 does not need backward computation.
I1028 09:06:55.406775 15388 net.cpp:208] This network produces output prob
I1028 09:06:55.406787 15388 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 09:06:55.406795 15388 net.cpp:219] Network initialization done.
I1028 09:06:55.406798 15388 net.cpp:220] Memory required for data: 1837200
I1028 09:07:35.692389 15388 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 09:07:35.692915 15388 net.cpp:358] Input 0 -> data
I1028 09:07:35.692947 15388 net.cpp:67] Creating Layer conv1
I1028 09:07:35.692955 15388 net.cpp:394] conv1 <- data
I1028 09:07:35.692962 15388 net.cpp:356] conv1 -> conv1
I1028 09:07:35.692973 15388 net.cpp:96] Setting up conv1
I1028 09:07:35.693004 15388 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 09:07:35.693022 15388 net.cpp:67] Creating Layer pool1
I1028 09:07:35.693027 15388 net.cpp:394] pool1 <- conv1
I1028 09:07:35.693033 15388 net.cpp:356] pool1 -> pool1
I1028 09:07:35.693042 15388 net.cpp:96] Setting up pool1
I1028 09:07:35.693064 15388 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 09:07:35.693073 15388 net.cpp:67] Creating Layer relu1
I1028 09:07:35.693078 15388 net.cpp:394] relu1 <- pool1
I1028 09:07:35.693084 15388 net.cpp:345] relu1 -> pool1 (in-place)
I1028 09:07:35.693090 15388 net.cpp:96] Setting up relu1
I1028 09:07:35.693095 15388 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 09:07:35.693102 15388 net.cpp:67] Creating Layer drop1
I1028 09:07:35.693106 15388 net.cpp:394] drop1 <- pool1
I1028 09:07:35.693112 15388 net.cpp:345] drop1 -> pool1 (in-place)
I1028 09:07:35.693120 15388 net.cpp:96] Setting up drop1
I1028 09:07:35.693125 15388 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 09:07:35.693132 15388 net.cpp:67] Creating Layer conv2
I1028 09:07:35.693137 15388 net.cpp:394] conv2 <- pool1
I1028 09:07:35.693145 15388 net.cpp:356] conv2 -> conv2
I1028 09:07:35.693151 15388 net.cpp:96] Setting up conv2
I1028 09:07:35.693673 15388 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 09:07:35.693689 15388 net.cpp:67] Creating Layer pool2
I1028 09:07:35.693694 15388 net.cpp:394] pool2 <- conv2
I1028 09:07:35.693701 15388 net.cpp:356] pool2 -> pool2
I1028 09:07:35.693708 15388 net.cpp:96] Setting up pool2
I1028 09:07:35.693716 15388 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 09:07:35.693722 15388 net.cpp:67] Creating Layer relu2
I1028 09:07:35.693725 15388 net.cpp:394] relu2 <- pool2
I1028 09:07:35.693732 15388 net.cpp:345] relu2 -> pool2 (in-place)
I1028 09:07:35.693737 15388 net.cpp:96] Setting up relu2
I1028 09:07:35.693742 15388 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 09:07:35.693748 15388 net.cpp:67] Creating Layer drop2
I1028 09:07:35.693753 15388 net.cpp:394] drop2 <- pool2
I1028 09:07:35.693758 15388 net.cpp:345] drop2 -> pool2 (in-place)
I1028 09:07:35.693765 15388 net.cpp:96] Setting up drop2
I1028 09:07:35.693770 15388 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 09:07:35.693778 15388 net.cpp:67] Creating Layer conv3
I1028 09:07:35.693783 15388 net.cpp:394] conv3 <- pool2
I1028 09:07:35.693789 15388 net.cpp:356] conv3 -> conv3
I1028 09:07:35.693797 15388 net.cpp:96] Setting up conv3
I1028 09:07:35.695168 15388 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 09:07:35.695183 15388 net.cpp:67] Creating Layer pool3
I1028 09:07:35.695188 15388 net.cpp:394] pool3 <- conv3
I1028 09:07:35.695194 15388 net.cpp:356] pool3 -> pool3
I1028 09:07:35.695202 15388 net.cpp:96] Setting up pool3
I1028 09:07:35.695207 15388 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 09:07:35.695214 15388 net.cpp:67] Creating Layer relu3
I1028 09:07:35.695219 15388 net.cpp:394] relu3 <- pool3
I1028 09:07:35.695225 15388 net.cpp:345] relu3 -> pool3 (in-place)
I1028 09:07:35.695230 15388 net.cpp:96] Setting up relu3
I1028 09:07:35.695235 15388 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 09:07:35.695241 15388 net.cpp:67] Creating Layer drop3
I1028 09:07:35.695245 15388 net.cpp:394] drop3 <- pool3
I1028 09:07:35.695251 15388 net.cpp:345] drop3 -> pool3 (in-place)
I1028 09:07:35.695258 15388 net.cpp:96] Setting up drop3
I1028 09:07:35.695263 15388 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 09:07:35.695271 15388 net.cpp:67] Creating Layer ip1
I1028 09:07:35.695274 15388 net.cpp:394] ip1 <- pool3
I1028 09:07:35.695281 15388 net.cpp:356] ip1 -> ip1
I1028 09:07:35.695289 15388 net.cpp:96] Setting up ip1
I1028 09:07:36.122994 15388 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 09:07:36.123059 15388 net.cpp:67] Creating Layer relu4
I1028 09:07:36.123067 15388 net.cpp:394] relu4 <- ip1
I1028 09:07:36.123078 15388 net.cpp:345] relu4 -> ip1 (in-place)
I1028 09:07:36.123088 15388 net.cpp:96] Setting up relu4
I1028 09:07:36.123093 15388 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 09:07:36.123102 15388 net.cpp:67] Creating Layer drop4
I1028 09:07:36.123106 15388 net.cpp:394] drop4 <- ip1
I1028 09:07:36.123113 15388 net.cpp:345] drop4 -> ip1 (in-place)
I1028 09:07:36.123121 15388 net.cpp:96] Setting up drop4
I1028 09:07:36.123126 15388 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 09:07:36.123137 15388 net.cpp:67] Creating Layer ip2
I1028 09:07:36.123153 15388 net.cpp:394] ip2 <- ip1
I1028 09:07:36.123162 15388 net.cpp:356] ip2 -> ip2
I1028 09:07:36.123175 15388 net.cpp:96] Setting up ip2
I1028 09:07:36.130772 15388 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 09:07:36.130838 15388 net.cpp:67] Creating Layer prob
I1028 09:07:36.130846 15388 net.cpp:394] prob <- ip2
I1028 09:07:36.130856 15388 net.cpp:356] prob -> prob
I1028 09:07:36.130867 15388 net.cpp:96] Setting up prob
I1028 09:07:36.130875 15388 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 09:07:36.130880 15388 net.cpp:172] prob does not need backward computation.
I1028 09:07:36.130885 15388 net.cpp:172] ip2 does not need backward computation.
I1028 09:07:36.130889 15388 net.cpp:172] drop4 does not need backward computation.
I1028 09:07:36.130893 15388 net.cpp:172] relu4 does not need backward computation.
I1028 09:07:36.130898 15388 net.cpp:172] ip1 does not need backward computation.
I1028 09:07:36.130903 15388 net.cpp:172] drop3 does not need backward computation.
I1028 09:07:36.130906 15388 net.cpp:172] relu3 does not need backward computation.
I1028 09:07:36.130911 15388 net.cpp:172] pool3 does not need backward computation.
I1028 09:07:36.130915 15388 net.cpp:172] conv3 does not need backward computation.
I1028 09:07:36.130919 15388 net.cpp:172] drop2 does not need backward computation.
I1028 09:07:36.130924 15388 net.cpp:172] relu2 does not need backward computation.
I1028 09:07:36.130928 15388 net.cpp:172] pool2 does not need backward computation.
I1028 09:07:36.130934 15388 net.cpp:172] conv2 does not need backward computation.
I1028 09:07:36.130938 15388 net.cpp:172] drop1 does not need backward computation.
I1028 09:07:36.130941 15388 net.cpp:172] relu1 does not need backward computation.
I1028 09:07:36.130946 15388 net.cpp:172] pool1 does not need backward computation.
I1028 09:07:36.130950 15388 net.cpp:172] conv1 does not need backward computation.
I1028 09:07:36.130954 15388 net.cpp:208] This network produces output prob
I1028 09:07:36.130970 15388 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 09:07:36.130980 15388 net.cpp:219] Network initialization done.
I1028 09:07:36.130985 15388 net.cpp:220] Memory required for data: 1837200
I1028 09:40:21.513192 23608 convert_imageset.cpp:70] Shuffling data
I1028 09:40:22.109359 23608 convert_imageset.cpp:73] A total of 60000 images.
I1028 09:40:22.109441 23608 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 09:40:24.125264 23608 convert_imageset.cpp:177] Processed 1000 files.
E1028 09:40:26.209576 23608 convert_imageset.cpp:177] Processed 2000 files.
E1028 09:40:28.305393 23608 convert_imageset.cpp:177] Processed 3000 files.
E1028 09:40:30.218724 23608 convert_imageset.cpp:177] Processed 4000 files.
E1028 09:40:32.065141 23608 convert_imageset.cpp:177] Processed 5000 files.
E1028 09:40:33.935019 23608 convert_imageset.cpp:177] Processed 6000 files.
E1028 09:40:35.651476 23608 convert_imageset.cpp:177] Processed 7000 files.
E1028 09:40:37.508019 23608 convert_imageset.cpp:177] Processed 8000 files.
E1028 09:40:39.259050 23608 convert_imageset.cpp:177] Processed 9000 files.
E1028 09:40:41.079192 23608 convert_imageset.cpp:177] Processed 10000 files.
E1028 09:40:42.785125 23608 convert_imageset.cpp:177] Processed 11000 files.
E1028 09:40:44.713804 23608 convert_imageset.cpp:177] Processed 12000 files.
E1028 09:40:46.385467 23608 convert_imageset.cpp:177] Processed 13000 files.
E1028 09:40:48.058384 23608 convert_imageset.cpp:177] Processed 14000 files.
E1028 09:40:49.814330 23608 convert_imageset.cpp:177] Processed 15000 files.
E1028 09:40:51.503146 23608 convert_imageset.cpp:177] Processed 16000 files.
E1028 09:40:53.245617 23608 convert_imageset.cpp:177] Processed 17000 files.
E1028 09:40:54.908514 23608 convert_imageset.cpp:177] Processed 18000 files.
E1028 09:40:56.478615 23608 convert_imageset.cpp:177] Processed 19000 files.
E1028 09:40:58.138020 23608 convert_imageset.cpp:177] Processed 20000 files.
E1028 09:41:00.226706 23608 convert_imageset.cpp:177] Processed 21000 files.
E1028 09:41:02.147467 23608 convert_imageset.cpp:177] Processed 22000 files.
E1028 09:41:03.790746 23608 convert_imageset.cpp:177] Processed 23000 files.
E1028 09:41:05.468201 23608 convert_imageset.cpp:177] Processed 24000 files.
E1028 09:41:07.143460 23608 convert_imageset.cpp:177] Processed 25000 files.
E1028 09:41:08.761075 23608 convert_imageset.cpp:177] Processed 26000 files.
E1028 09:41:10.417109 23608 convert_imageset.cpp:177] Processed 27000 files.
E1028 09:41:12.067468 23608 convert_imageset.cpp:177] Processed 28000 files.
E1028 09:41:13.791492 23608 convert_imageset.cpp:177] Processed 29000 files.
E1028 09:41:15.309355 23608 convert_imageset.cpp:177] Processed 30000 files.
E1028 09:41:16.892988 23608 convert_imageset.cpp:177] Processed 31000 files.
E1028 09:41:18.405263 23608 convert_imageset.cpp:177] Processed 32000 files.
E1028 09:41:19.882167 23608 convert_imageset.cpp:177] Processed 33000 files.
E1028 09:41:21.682670 23608 convert_imageset.cpp:177] Processed 34000 files.
E1028 09:41:23.173387 23608 convert_imageset.cpp:177] Processed 35000 files.
E1028 09:41:24.664636 23608 convert_imageset.cpp:177] Processed 36000 files.
E1028 09:41:26.286550 23608 convert_imageset.cpp:177] Processed 37000 files.
E1028 09:41:27.869153 23608 convert_imageset.cpp:177] Processed 38000 files.
E1028 09:41:29.380462 23608 convert_imageset.cpp:177] Processed 39000 files.
E1028 09:41:30.966748 23608 convert_imageset.cpp:177] Processed 40000 files.
E1028 09:41:32.487944 23608 convert_imageset.cpp:177] Processed 41000 files.
E1028 09:41:33.968017 23608 convert_imageset.cpp:177] Processed 42000 files.
E1028 09:41:35.623306 23608 convert_imageset.cpp:177] Processed 43000 files.
E1028 09:41:37.183949 23608 convert_imageset.cpp:177] Processed 44000 files.
E1028 09:41:38.632688 23608 convert_imageset.cpp:177] Processed 45000 files.
E1028 09:41:40.284138 23608 convert_imageset.cpp:177] Processed 46000 files.
E1028 09:41:41.798686 23608 convert_imageset.cpp:177] Processed 47000 files.
E1028 09:41:43.303769 23608 convert_imageset.cpp:177] Processed 48000 files.
E1028 09:41:44.764844 23608 convert_imageset.cpp:177] Processed 49000 files.
E1028 09:41:46.270885 23608 convert_imageset.cpp:177] Processed 50000 files.
E1028 09:41:47.789458 23608 convert_imageset.cpp:177] Processed 51000 files.
E1028 09:41:49.263711 23608 convert_imageset.cpp:177] Processed 52000 files.
E1028 09:41:50.751857 23608 convert_imageset.cpp:177] Processed 53000 files.
E1028 09:41:52.273419 23608 convert_imageset.cpp:177] Processed 54000 files.
E1028 09:41:53.759277 23608 convert_imageset.cpp:177] Processed 55000 files.
E1028 09:41:55.344763 23608 convert_imageset.cpp:177] Processed 56000 files.
E1028 09:41:56.842952 23608 convert_imageset.cpp:177] Processed 57000 files.
E1028 09:41:58.315089 23608 convert_imageset.cpp:177] Processed 58000 files.
E1028 09:41:59.762194 23608 convert_imageset.cpp:177] Processed 59000 files.
E1028 09:42:01.249761 23608 convert_imageset.cpp:177] Processed 60000 files.
I1028 09:42:01.515151 23825 caffe.cpp:99] Use GPU with device ID 0
I1028 09:42:01.856231 23825 caffe.cpp:107] Starting Optimization
I1028 09:42:01.856355 23825 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 400000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 09:42:01.856381 23825 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 09:42:01.872020 23825 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 09:42:01.872120 23825 net.cpp:67] Creating Layer mnist
I1028 09:42:01.872131 23825 net.cpp:356] mnist -> data
I1028 09:42:01.872148 23825 net.cpp:356] mnist -> label
I1028 09:42:01.872161 23825 net.cpp:96] Setting up mnist
I1028 09:42:01.880209 23825 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 09:42:01.880342 23825 data_layer.cpp:128] output data size: 64,1,50,180
I1028 09:42:01.882071 23825 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 09:42:01.882109 23825 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 09:42:01.882136 23825 net.cpp:67] Creating Layer conv1
I1028 09:42:01.882150 23825 net.cpp:394] conv1 <- data
I1028 09:42:01.882181 23825 net.cpp:356] conv1 -> conv1
I1028 09:42:01.882207 23825 net.cpp:96] Setting up conv1
I1028 09:42:01.883129 23825 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 09:42:01.883189 23825 net.cpp:67] Creating Layer pool1
I1028 09:42:01.883204 23825 net.cpp:394] pool1 <- conv1
I1028 09:42:01.883226 23825 net.cpp:356] pool1 -> pool1
I1028 09:42:01.883247 23825 net.cpp:96] Setting up pool1
I1028 09:42:01.883280 23825 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 09:42:01.883297 23825 net.cpp:67] Creating Layer relu1
I1028 09:42:01.883311 23825 net.cpp:394] relu1 <- pool1
I1028 09:42:01.883335 23825 net.cpp:345] relu1 -> pool1 (in-place)
I1028 09:42:01.883353 23825 net.cpp:96] Setting up relu1
I1028 09:42:01.883368 23825 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 09:42:01.883390 23825 net.cpp:67] Creating Layer drop1
I1028 09:42:01.883404 23825 net.cpp:394] drop1 <- pool1
I1028 09:42:01.883420 23825 net.cpp:345] drop1 -> pool1 (in-place)
I1028 09:42:01.883438 23825 net.cpp:96] Setting up drop1
I1028 09:42:01.883452 23825 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 09:42:01.883471 23825 net.cpp:67] Creating Layer conv2
I1028 09:42:01.883487 23825 net.cpp:394] conv2 <- pool1
I1028 09:42:01.883505 23825 net.cpp:356] conv2 -> conv2
I1028 09:42:01.883527 23825 net.cpp:96] Setting up conv2
I1028 09:42:01.884743 23825 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 09:42:01.884763 23825 net.cpp:67] Creating Layer pool2
I1028 09:42:01.884768 23825 net.cpp:394] pool2 <- conv2
I1028 09:42:01.884774 23825 net.cpp:356] pool2 -> pool2
I1028 09:42:01.884783 23825 net.cpp:96] Setting up pool2
I1028 09:42:01.884788 23825 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 09:42:01.884798 23825 net.cpp:67] Creating Layer relu2
I1028 09:42:01.884801 23825 net.cpp:394] relu2 <- pool2
I1028 09:42:01.884807 23825 net.cpp:345] relu2 -> pool2 (in-place)
I1028 09:42:01.884814 23825 net.cpp:96] Setting up relu2
I1028 09:42:01.884819 23825 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 09:42:01.884827 23825 net.cpp:67] Creating Layer drop2
I1028 09:42:01.884832 23825 net.cpp:394] drop2 <- pool2
I1028 09:42:01.884838 23825 net.cpp:345] drop2 -> pool2 (in-place)
I1028 09:42:01.884845 23825 net.cpp:96] Setting up drop2
I1028 09:42:01.884850 23825 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 09:42:01.884857 23825 net.cpp:67] Creating Layer conv3
I1028 09:42:01.884861 23825 net.cpp:394] conv3 <- pool2
I1028 09:42:01.884871 23825 net.cpp:356] conv3 -> conv3
I1028 09:42:01.884878 23825 net.cpp:96] Setting up conv3
I1028 09:42:01.886401 23825 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 09:42:01.886430 23825 net.cpp:67] Creating Layer pool3
I1028 09:42:01.886435 23825 net.cpp:394] pool3 <- conv3
I1028 09:42:01.886442 23825 net.cpp:356] pool3 -> pool3
I1028 09:42:01.886450 23825 net.cpp:96] Setting up pool3
I1028 09:42:01.886456 23825 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 09:42:01.886464 23825 net.cpp:67] Creating Layer relu3
I1028 09:42:01.886468 23825 net.cpp:394] relu3 <- pool3
I1028 09:42:01.886474 23825 net.cpp:345] relu3 -> pool3 (in-place)
I1028 09:42:01.886481 23825 net.cpp:96] Setting up relu3
I1028 09:42:01.886486 23825 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 09:42:01.886492 23825 net.cpp:67] Creating Layer drop3
I1028 09:42:01.886495 23825 net.cpp:394] drop3 <- pool3
I1028 09:42:01.886502 23825 net.cpp:345] drop3 -> pool3 (in-place)
I1028 09:42:01.886507 23825 net.cpp:96] Setting up drop3
I1028 09:42:01.886512 23825 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 09:42:01.886521 23825 net.cpp:67] Creating Layer ip1
I1028 09:42:01.886526 23825 net.cpp:394] ip1 <- pool3
I1028 09:42:01.886533 23825 net.cpp:356] ip1 -> ip1
I1028 09:42:01.886569 23825 net.cpp:96] Setting up ip1
I1028 09:42:02.330811 23825 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 09:42:02.330869 23825 net.cpp:67] Creating Layer relu4
I1028 09:42:02.330876 23825 net.cpp:394] relu4 <- ip1
I1028 09:42:02.330886 23825 net.cpp:345] relu4 -> ip1 (in-place)
I1028 09:42:02.330895 23825 net.cpp:96] Setting up relu4
I1028 09:42:02.330900 23825 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 09:42:02.330907 23825 net.cpp:67] Creating Layer drop4
I1028 09:42:02.330911 23825 net.cpp:394] drop4 <- ip1
I1028 09:42:02.330922 23825 net.cpp:345] drop4 -> ip1 (in-place)
I1028 09:42:02.330929 23825 net.cpp:96] Setting up drop4
I1028 09:42:02.330935 23825 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 09:42:02.330945 23825 net.cpp:67] Creating Layer ip2
I1028 09:42:02.330951 23825 net.cpp:394] ip2 <- ip1
I1028 09:42:02.330958 23825 net.cpp:356] ip2 -> ip2
I1028 09:42:02.330967 23825 net.cpp:96] Setting up ip2
I1028 09:42:02.341930 23825 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 09:42:02.341990 23825 net.cpp:67] Creating Layer loss
I1028 09:42:02.341997 23825 net.cpp:394] loss <- ip2
I1028 09:42:02.342005 23825 net.cpp:394] loss <- label
I1028 09:42:02.342011 23825 net.cpp:356] loss -> loss
I1028 09:42:02.342021 23825 net.cpp:96] Setting up loss
I1028 09:42:02.342034 23825 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 09:42:02.342039 23825 net.cpp:109]     with loss weight 1
I1028 09:42:02.342075 23825 net.cpp:170] loss needs backward computation.
I1028 09:42:02.342080 23825 net.cpp:170] ip2 needs backward computation.
I1028 09:42:02.342085 23825 net.cpp:170] drop4 needs backward computation.
I1028 09:42:02.342089 23825 net.cpp:170] relu4 needs backward computation.
I1028 09:42:02.342094 23825 net.cpp:170] ip1 needs backward computation.
I1028 09:42:02.342099 23825 net.cpp:170] drop3 needs backward computation.
I1028 09:42:02.342103 23825 net.cpp:170] relu3 needs backward computation.
I1028 09:42:02.342108 23825 net.cpp:170] pool3 needs backward computation.
I1028 09:42:02.342113 23825 net.cpp:170] conv3 needs backward computation.
I1028 09:42:02.342118 23825 net.cpp:170] drop2 needs backward computation.
I1028 09:42:02.342121 23825 net.cpp:170] relu2 needs backward computation.
I1028 09:42:02.342126 23825 net.cpp:170] pool2 needs backward computation.
I1028 09:42:02.342130 23825 net.cpp:170] conv2 needs backward computation.
I1028 09:42:02.342135 23825 net.cpp:170] drop1 needs backward computation.
I1028 09:42:02.342139 23825 net.cpp:170] relu1 needs backward computation.
I1028 09:42:02.342144 23825 net.cpp:170] pool1 needs backward computation.
I1028 09:42:02.342149 23825 net.cpp:170] conv1 needs backward computation.
I1028 09:42:02.342154 23825 net.cpp:172] mnist does not need backward computation.
I1028 09:42:02.342159 23825 net.cpp:208] This network produces output loss
I1028 09:42:02.342170 23825 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 09:42:02.342177 23825 net.cpp:219] Network initialization done.
I1028 09:42:02.342181 23825 net.cpp:220] Memory required for data: 119788292
I1028 09:42:02.342242 23825 solver.cpp:41] Solver scaffolding done.
I1028 09:42:02.342248 23825 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_390000.solverstate
I1028 09:42:02.342253 23825 solver.cpp:160] Solving Captcha
I1028 09:42:02.342272 23825 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_390000.solverstate
I1028 09:42:06.929536 23825 solver.cpp:502] SGDSolver: restoring history
I1028 09:42:07.655197 23825 solver.cpp:191] Iteration 390000, loss = 2.36035
I1028 09:42:07.655258 23825 solver.cpp:206]     Train net output #0: loss = 2.36035 (* 1 = 2.36035 loss)
I1028 09:42:07.655274 23825 solver.cpp:403] Iteration 390000, lr = 0.000628717
I1028 09:46:09.628557 23825 solver.cpp:191] Iteration 391000, loss = 2.57024
I1028 09:46:09.638381 23825 solver.cpp:206]     Train net output #0: loss = 2.57024 (* 1 = 2.57024 loss)
I1028 09:46:09.638401 23825 solver.cpp:403] Iteration 391000, lr = 0.00062754
I1028 09:50:10.816278 23825 solver.cpp:191] Iteration 392000, loss = 2.51921
I1028 09:50:10.817337 23825 solver.cpp:206]     Train net output #0: loss = 2.51921 (* 1 = 2.51921 loss)
I1028 09:50:10.817380 23825 solver.cpp:403] Iteration 392000, lr = 0.000626369
I1028 09:54:11.906800 23825 solver.cpp:191] Iteration 393000, loss = 2.49734
I1028 09:54:11.907382 23825 solver.cpp:206]     Train net output #0: loss = 2.49734 (* 1 = 2.49734 loss)
I1028 09:54:11.907414 23825 solver.cpp:403] Iteration 393000, lr = 0.000625203
I1028 09:58:13.182560 23825 solver.cpp:191] Iteration 394000, loss = 2.33398
I1028 09:58:13.183173 23825 solver.cpp:206]     Train net output #0: loss = 2.33398 (* 1 = 2.33398 loss)
I1028 09:58:13.183207 23825 solver.cpp:403] Iteration 394000, lr = 0.000624042
I1028 10:02:14.975711 23825 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_395000.caffemodel
I1028 10:02:19.955425 23825 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_395000.solverstate
I1028 10:02:23.844810 23825 solver.cpp:191] Iteration 395000, loss = 2.34134
I1028 10:02:23.845422 23825 solver.cpp:206]     Train net output #0: loss = 2.34134 (* 1 = 2.34134 loss)
I1028 10:02:23.845451 23825 solver.cpp:403] Iteration 395000, lr = 0.000622886
I1028 10:06:25.545543 23825 solver.cpp:191] Iteration 396000, loss = 2.40377
I1028 10:06:25.546130 23825 solver.cpp:206]     Train net output #0: loss = 2.40377 (* 1 = 2.40377 loss)
I1028 10:06:25.546164 23825 solver.cpp:403] Iteration 396000, lr = 0.000621735
I1028 10:10:26.820127 23825 solver.cpp:191] Iteration 397000, loss = 2.43777
I1028 10:10:26.820714 23825 solver.cpp:206]     Train net output #0: loss = 2.43777 (* 1 = 2.43777 loss)
I1028 10:10:26.820749 23825 solver.cpp:403] Iteration 397000, lr = 0.000620589
I1028 10:14:28.157111 23825 solver.cpp:191] Iteration 398000, loss = 2.35236
I1028 10:14:28.157709 23825 solver.cpp:206]     Train net output #0: loss = 2.35236 (* 1 = 2.35236 loss)
I1028 10:14:28.157742 23825 solver.cpp:403] Iteration 398000, lr = 0.000619448
I1028 10:18:29.468665 23825 solver.cpp:191] Iteration 399000, loss = 2.34255
I1028 10:18:29.469269 23825 solver.cpp:206]     Train net output #0: loss = 2.34255 (* 1 = 2.34255 loss)
I1028 10:18:29.469300 23825 solver.cpp:403] Iteration 399000, lr = 0.000618312
I1028 10:22:31.237766 23825 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_400000.caffemodel
I1028 10:22:35.872531 23825 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_400000.solverstate
I1028 10:22:39.417079 23825 solver.cpp:228] Iteration 400000, loss = 2.5131
I1028 10:22:39.417681 23825 solver.cpp:233] Optimization Done.
I1028 10:22:39.432653 23825 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 10:45:13.984684 10964 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 10:45:13.984794 10964 net.cpp:358] Input 0 -> data
I1028 10:45:13.984819 10964 net.cpp:67] Creating Layer conv1
I1028 10:45:13.984824 10964 net.cpp:394] conv1 <- data
I1028 10:45:13.984832 10964 net.cpp:356] conv1 -> conv1
I1028 10:45:13.984840 10964 net.cpp:96] Setting up conv1
I1028 10:45:13.985162 10964 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 10:45:13.985182 10964 net.cpp:67] Creating Layer pool1
I1028 10:45:13.985187 10964 net.cpp:394] pool1 <- conv1
I1028 10:45:13.985193 10964 net.cpp:356] pool1 -> pool1
I1028 10:45:13.985199 10964 net.cpp:96] Setting up pool1
I1028 10:45:13.985214 10964 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 10:45:13.985222 10964 net.cpp:67] Creating Layer relu1
I1028 10:45:13.985226 10964 net.cpp:394] relu1 <- pool1
I1028 10:45:13.985231 10964 net.cpp:345] relu1 -> pool1 (in-place)
I1028 10:45:13.985237 10964 net.cpp:96] Setting up relu1
I1028 10:45:13.985241 10964 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 10:45:13.985249 10964 net.cpp:67] Creating Layer drop1
I1028 10:45:13.985255 10964 net.cpp:394] drop1 <- pool1
I1028 10:45:13.985260 10964 net.cpp:345] drop1 -> pool1 (in-place)
I1028 10:45:13.985265 10964 net.cpp:96] Setting up drop1
I1028 10:45:13.985270 10964 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 10:45:13.985277 10964 net.cpp:67] Creating Layer conv2
I1028 10:45:13.985283 10964 net.cpp:394] conv2 <- pool1
I1028 10:45:13.985290 10964 net.cpp:356] conv2 -> conv2
I1028 10:45:13.985296 10964 net.cpp:96] Setting up conv2
I1028 10:45:13.985857 10964 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 10:45:13.985872 10964 net.cpp:67] Creating Layer pool2
I1028 10:45:13.985877 10964 net.cpp:394] pool2 <- conv2
I1028 10:45:13.985882 10964 net.cpp:356] pool2 -> pool2
I1028 10:45:13.985889 10964 net.cpp:96] Setting up pool2
I1028 10:45:13.985894 10964 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 10:45:13.985903 10964 net.cpp:67] Creating Layer relu2
I1028 10:45:13.985908 10964 net.cpp:394] relu2 <- pool2
I1028 10:45:13.985913 10964 net.cpp:345] relu2 -> pool2 (in-place)
I1028 10:45:13.985918 10964 net.cpp:96] Setting up relu2
I1028 10:45:13.985923 10964 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 10:45:13.985927 10964 net.cpp:67] Creating Layer drop2
I1028 10:45:13.985931 10964 net.cpp:394] drop2 <- pool2
I1028 10:45:13.985939 10964 net.cpp:345] drop2 -> pool2 (in-place)
I1028 10:45:13.985944 10964 net.cpp:96] Setting up drop2
I1028 10:45:13.985949 10964 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 10:45:13.985955 10964 net.cpp:67] Creating Layer conv3
I1028 10:45:13.985960 10964 net.cpp:394] conv3 <- pool2
I1028 10:45:13.985967 10964 net.cpp:356] conv3 -> conv3
I1028 10:45:13.985975 10964 net.cpp:96] Setting up conv3
I1028 10:45:13.987447 10964 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 10:45:13.987467 10964 net.cpp:67] Creating Layer pool3
I1028 10:45:13.987471 10964 net.cpp:394] pool3 <- conv3
I1028 10:45:13.987478 10964 net.cpp:356] pool3 -> pool3
I1028 10:45:13.987483 10964 net.cpp:96] Setting up pool3
I1028 10:45:13.987489 10964 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 10:45:13.987498 10964 net.cpp:67] Creating Layer relu3
I1028 10:45:13.987504 10964 net.cpp:394] relu3 <- pool3
I1028 10:45:13.987509 10964 net.cpp:345] relu3 -> pool3 (in-place)
I1028 10:45:13.987517 10964 net.cpp:96] Setting up relu3
I1028 10:45:13.987521 10964 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 10:45:13.987527 10964 net.cpp:67] Creating Layer drop3
I1028 10:45:13.987531 10964 net.cpp:394] drop3 <- pool3
I1028 10:45:13.987536 10964 net.cpp:345] drop3 -> pool3 (in-place)
I1028 10:45:13.987541 10964 net.cpp:96] Setting up drop3
I1028 10:45:13.987546 10964 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 10:45:13.987555 10964 net.cpp:67] Creating Layer ip1
I1028 10:45:13.987560 10964 net.cpp:394] ip1 <- pool3
I1028 10:45:13.987565 10964 net.cpp:356] ip1 -> ip1
I1028 10:45:13.987572 10964 net.cpp:96] Setting up ip1
I1028 10:45:14.526049 10964 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 10:45:14.526111 10964 net.cpp:67] Creating Layer relu4
I1028 10:45:14.526119 10964 net.cpp:394] relu4 <- ip1
I1028 10:45:14.526129 10964 net.cpp:345] relu4 -> ip1 (in-place)
I1028 10:45:14.526137 10964 net.cpp:96] Setting up relu4
I1028 10:45:14.526142 10964 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 10:45:14.526149 10964 net.cpp:67] Creating Layer drop4
I1028 10:45:14.526154 10964 net.cpp:394] drop4 <- ip1
I1028 10:45:14.526161 10964 net.cpp:345] drop4 -> ip1 (in-place)
I1028 10:45:14.526168 10964 net.cpp:96] Setting up drop4
I1028 10:45:14.526175 10964 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 10:45:14.526182 10964 net.cpp:67] Creating Layer ip2
I1028 10:45:14.526186 10964 net.cpp:394] ip2 <- ip1
I1028 10:45:14.526192 10964 net.cpp:356] ip2 -> ip2
I1028 10:45:14.526207 10964 net.cpp:96] Setting up ip2
I1028 10:45:14.538094 10964 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 10:45:14.538166 10964 net.cpp:67] Creating Layer prob
I1028 10:45:14.538173 10964 net.cpp:394] prob <- ip2
I1028 10:45:14.538182 10964 net.cpp:356] prob -> prob
I1028 10:45:14.538192 10964 net.cpp:96] Setting up prob
I1028 10:45:14.538198 10964 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 10:45:14.538203 10964 net.cpp:172] prob does not need backward computation.
I1028 10:45:14.538208 10964 net.cpp:172] ip2 does not need backward computation.
I1028 10:45:14.538211 10964 net.cpp:172] drop4 does not need backward computation.
I1028 10:45:14.538215 10964 net.cpp:172] relu4 does not need backward computation.
I1028 10:45:14.538219 10964 net.cpp:172] ip1 does not need backward computation.
I1028 10:45:14.538223 10964 net.cpp:172] drop3 does not need backward computation.
I1028 10:45:14.538226 10964 net.cpp:172] relu3 does not need backward computation.
I1028 10:45:14.538230 10964 net.cpp:172] pool3 does not need backward computation.
I1028 10:45:14.538233 10964 net.cpp:172] conv3 does not need backward computation.
I1028 10:45:14.538238 10964 net.cpp:172] drop2 does not need backward computation.
I1028 10:45:14.538241 10964 net.cpp:172] relu2 does not need backward computation.
I1028 10:45:14.538244 10964 net.cpp:172] pool2 does not need backward computation.
I1028 10:45:14.538249 10964 net.cpp:172] conv2 does not need backward computation.
I1028 10:45:14.538252 10964 net.cpp:172] drop1 does not need backward computation.
I1028 10:45:14.538255 10964 net.cpp:172] relu1 does not need backward computation.
I1028 10:45:14.538259 10964 net.cpp:172] pool1 does not need backward computation.
I1028 10:45:14.538262 10964 net.cpp:172] conv1 does not need backward computation.
I1028 10:45:14.538266 10964 net.cpp:208] This network produces output prob
I1028 10:45:14.538280 10964 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 10:45:14.538288 10964 net.cpp:219] Network initialization done.
I1028 10:45:14.538292 10964 net.cpp:220] Memory required for data: 1837200
I1028 10:46:00.475164 10964 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 10:46:00.475710 10964 net.cpp:358] Input 0 -> data
I1028 10:46:00.475760 10964 net.cpp:67] Creating Layer conv1
I1028 10:46:00.475774 10964 net.cpp:394] conv1 <- data
I1028 10:46:00.475790 10964 net.cpp:356] conv1 -> conv1
I1028 10:46:00.475810 10964 net.cpp:96] Setting up conv1
I1028 10:46:00.475867 10964 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 10:46:00.475896 10964 net.cpp:67] Creating Layer pool1
I1028 10:46:00.475908 10964 net.cpp:394] pool1 <- conv1
I1028 10:46:00.475921 10964 net.cpp:356] pool1 -> pool1
I1028 10:46:00.475937 10964 net.cpp:96] Setting up pool1
I1028 10:46:00.475952 10964 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 10:46:00.475968 10964 net.cpp:67] Creating Layer relu1
I1028 10:46:00.475978 10964 net.cpp:394] relu1 <- pool1
I1028 10:46:00.475991 10964 net.cpp:345] relu1 -> pool1 (in-place)
I1028 10:46:00.476004 10964 net.cpp:96] Setting up relu1
I1028 10:46:00.476014 10964 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 10:46:00.476027 10964 net.cpp:67] Creating Layer drop1
I1028 10:46:00.476037 10964 net.cpp:394] drop1 <- pool1
I1028 10:46:00.476050 10964 net.cpp:345] drop1 -> pool1 (in-place)
I1028 10:46:00.476063 10964 net.cpp:96] Setting up drop1
I1028 10:46:00.476075 10964 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 10:46:00.476091 10964 net.cpp:67] Creating Layer conv2
I1028 10:46:00.476101 10964 net.cpp:394] conv2 <- pool1
I1028 10:46:00.476115 10964 net.cpp:356] conv2 -> conv2
I1028 10:46:00.476131 10964 net.cpp:96] Setting up conv2
I1028 10:46:00.477308 10964 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 10:46:00.477339 10964 net.cpp:67] Creating Layer pool2
I1028 10:46:00.477351 10964 net.cpp:394] pool2 <- conv2
I1028 10:46:00.477371 10964 net.cpp:356] pool2 -> pool2
I1028 10:46:00.477390 10964 net.cpp:96] Setting up pool2
I1028 10:46:00.477403 10964 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 10:46:00.477417 10964 net.cpp:67] Creating Layer relu2
I1028 10:46:00.477432 10964 net.cpp:394] relu2 <- pool2
I1028 10:46:00.477449 10964 net.cpp:345] relu2 -> pool2 (in-place)
I1028 10:46:00.477524 10964 net.cpp:96] Setting up relu2
I1028 10:46:00.477550 10964 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 10:46:00.477576 10964 net.cpp:67] Creating Layer drop2
I1028 10:46:00.477588 10964 net.cpp:394] drop2 <- pool2
I1028 10:46:00.477604 10964 net.cpp:345] drop2 -> pool2 (in-place)
I1028 10:46:00.477622 10964 net.cpp:96] Setting up drop2
I1028 10:46:00.477635 10964 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 10:46:00.477656 10964 net.cpp:67] Creating Layer conv3
I1028 10:46:00.477669 10964 net.cpp:394] conv3 <- pool2
I1028 10:46:00.477686 10964 net.cpp:356] conv3 -> conv3
I1028 10:46:00.477706 10964 net.cpp:96] Setting up conv3
I1028 10:46:00.480837 10964 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 10:46:00.480855 10964 net.cpp:67] Creating Layer pool3
I1028 10:46:00.480861 10964 net.cpp:394] pool3 <- conv3
I1028 10:46:00.480867 10964 net.cpp:356] pool3 -> pool3
I1028 10:46:00.480875 10964 net.cpp:96] Setting up pool3
I1028 10:46:00.480880 10964 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 10:46:00.480886 10964 net.cpp:67] Creating Layer relu3
I1028 10:46:00.480890 10964 net.cpp:394] relu3 <- pool3
I1028 10:46:00.480895 10964 net.cpp:345] relu3 -> pool3 (in-place)
I1028 10:46:00.480901 10964 net.cpp:96] Setting up relu3
I1028 10:46:00.480906 10964 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 10:46:00.480911 10964 net.cpp:67] Creating Layer drop3
I1028 10:46:00.480916 10964 net.cpp:394] drop3 <- pool3
I1028 10:46:00.480921 10964 net.cpp:345] drop3 -> pool3 (in-place)
I1028 10:46:00.480926 10964 net.cpp:96] Setting up drop3
I1028 10:46:00.480931 10964 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 10:46:00.480938 10964 net.cpp:67] Creating Layer ip1
I1028 10:46:00.480942 10964 net.cpp:394] ip1 <- pool3
I1028 10:46:00.480948 10964 net.cpp:356] ip1 -> ip1
I1028 10:46:00.480957 10964 net.cpp:96] Setting up ip1
I1028 10:46:00.887078 10964 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 10:46:00.887148 10964 net.cpp:67] Creating Layer relu4
I1028 10:46:00.887157 10964 net.cpp:394] relu4 <- ip1
I1028 10:46:00.887167 10964 net.cpp:345] relu4 -> ip1 (in-place)
I1028 10:46:00.887177 10964 net.cpp:96] Setting up relu4
I1028 10:46:00.887183 10964 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 10:46:00.887192 10964 net.cpp:67] Creating Layer drop4
I1028 10:46:00.887195 10964 net.cpp:394] drop4 <- ip1
I1028 10:46:00.887202 10964 net.cpp:345] drop4 -> ip1 (in-place)
I1028 10:46:00.887209 10964 net.cpp:96] Setting up drop4
I1028 10:46:00.887215 10964 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 10:46:00.887225 10964 net.cpp:67] Creating Layer ip2
I1028 10:46:00.887229 10964 net.cpp:394] ip2 <- ip1
I1028 10:46:00.887236 10964 net.cpp:356] ip2 -> ip2
I1028 10:46:00.887250 10964 net.cpp:96] Setting up ip2
I1028 10:46:00.894847 10964 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 10:46:00.894912 10964 net.cpp:67] Creating Layer prob
I1028 10:46:00.894920 10964 net.cpp:394] prob <- ip2
I1028 10:46:00.894930 10964 net.cpp:356] prob -> prob
I1028 10:46:00.894940 10964 net.cpp:96] Setting up prob
I1028 10:46:00.894949 10964 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 10:46:00.894954 10964 net.cpp:172] prob does not need backward computation.
I1028 10:46:00.894958 10964 net.cpp:172] ip2 does not need backward computation.
I1028 10:46:00.894963 10964 net.cpp:172] drop4 does not need backward computation.
I1028 10:46:00.894966 10964 net.cpp:172] relu4 does not need backward computation.
I1028 10:46:00.894970 10964 net.cpp:172] ip1 does not need backward computation.
I1028 10:46:00.894975 10964 net.cpp:172] drop3 does not need backward computation.
I1028 10:46:00.894979 10964 net.cpp:172] relu3 does not need backward computation.
I1028 10:46:00.894984 10964 net.cpp:172] pool3 does not need backward computation.
I1028 10:46:00.894999 10964 net.cpp:172] conv3 does not need backward computation.
I1028 10:46:00.895004 10964 net.cpp:172] drop2 does not need backward computation.
I1028 10:46:00.895007 10964 net.cpp:172] relu2 does not need backward computation.
I1028 10:46:00.895011 10964 net.cpp:172] pool2 does not need backward computation.
I1028 10:46:00.895015 10964 net.cpp:172] conv2 does not need backward computation.
I1028 10:46:00.895020 10964 net.cpp:172] drop1 does not need backward computation.
I1028 10:46:00.895023 10964 net.cpp:172] relu1 does not need backward computation.
I1028 10:46:00.895027 10964 net.cpp:172] pool1 does not need backward computation.
I1028 10:46:00.895031 10964 net.cpp:172] conv1 does not need backward computation.
I1028 10:46:00.895035 10964 net.cpp:208] This network produces output prob
I1028 10:46:00.895052 10964 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 10:46:00.895061 10964 net.cpp:219] Network initialization done.
I1028 10:46:00.895066 10964 net.cpp:220] Memory required for data: 1837200
I1028 11:21:47.119958 18609 convert_imageset.cpp:70] Shuffling data
I1028 11:21:47.742377 18609 convert_imageset.cpp:73] A total of 60000 images.
I1028 11:21:47.742455 18609 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 11:21:49.873739 18609 convert_imageset.cpp:177] Processed 1000 files.
E1028 11:21:52.303190 18609 convert_imageset.cpp:177] Processed 2000 files.
E1028 11:21:54.422390 18609 convert_imageset.cpp:177] Processed 3000 files.
E1028 11:21:56.465844 18609 convert_imageset.cpp:177] Processed 4000 files.
E1028 11:21:58.527190 18609 convert_imageset.cpp:177] Processed 5000 files.
E1028 11:22:00.311499 18609 convert_imageset.cpp:177] Processed 6000 files.
E1028 11:22:02.140779 18609 convert_imageset.cpp:177] Processed 7000 files.
E1028 11:22:04.177273 18609 convert_imageset.cpp:177] Processed 8000 files.
E1028 11:22:06.029268 18609 convert_imageset.cpp:177] Processed 9000 files.
E1028 11:22:07.741912 18609 convert_imageset.cpp:177] Processed 10000 files.
E1028 11:22:10.155679 18609 convert_imageset.cpp:177] Processed 11000 files.
E1028 11:22:12.092566 18609 convert_imageset.cpp:177] Processed 12000 files.
E1028 11:22:13.906741 18609 convert_imageset.cpp:177] Processed 13000 files.
E1028 11:22:15.575474 18609 convert_imageset.cpp:177] Processed 14000 files.
E1028 11:22:17.332021 18609 convert_imageset.cpp:177] Processed 15000 files.
E1028 11:22:19.193717 18609 convert_imageset.cpp:177] Processed 16000 files.
E1028 11:22:20.801300 18609 convert_imageset.cpp:177] Processed 17000 files.
E1028 11:22:22.449705 18609 convert_imageset.cpp:177] Processed 18000 files.
E1028 11:22:24.071297 18609 convert_imageset.cpp:177] Processed 19000 files.
E1028 11:22:25.714123 18609 convert_imageset.cpp:177] Processed 20000 files.
E1028 11:22:27.353910 18609 convert_imageset.cpp:177] Processed 21000 files.
E1028 11:22:28.971464 18609 convert_imageset.cpp:177] Processed 22000 files.
E1028 11:22:30.802806 18609 convert_imageset.cpp:177] Processed 23000 files.
E1028 11:22:32.665506 18609 convert_imageset.cpp:177] Processed 24000 files.
E1028 11:22:34.350229 18609 convert_imageset.cpp:177] Processed 25000 files.
E1028 11:22:36.077764 18609 convert_imageset.cpp:177] Processed 26000 files.
E1028 11:22:37.626871 18609 convert_imageset.cpp:177] Processed 27000 files.
E1028 11:22:39.168364 18609 convert_imageset.cpp:177] Processed 28000 files.
E1028 11:22:40.752960 18609 convert_imageset.cpp:177] Processed 29000 files.
E1028 11:22:42.304661 18609 convert_imageset.cpp:177] Processed 30000 files.
E1028 11:22:43.922818 18609 convert_imageset.cpp:177] Processed 31000 files.
E1028 11:22:45.442195 18609 convert_imageset.cpp:177] Processed 32000 files.
E1028 11:22:47.048480 18609 convert_imageset.cpp:177] Processed 33000 files.
E1028 11:22:48.730635 18609 convert_imageset.cpp:177] Processed 34000 files.
E1028 11:22:50.275136 18609 convert_imageset.cpp:177] Processed 35000 files.
E1028 11:22:51.790035 18609 convert_imageset.cpp:177] Processed 36000 files.
E1028 11:22:53.321207 18609 convert_imageset.cpp:177] Processed 37000 files.
E1028 11:22:54.889657 18609 convert_imageset.cpp:177] Processed 38000 files.
E1028 11:22:56.522920 18609 convert_imageset.cpp:177] Processed 39000 files.
E1028 11:22:58.161581 18609 convert_imageset.cpp:177] Processed 40000 files.
E1028 11:22:59.787230 18609 convert_imageset.cpp:177] Processed 41000 files.
E1028 11:23:01.428979 18609 convert_imageset.cpp:177] Processed 42000 files.
E1028 11:23:03.417207 18609 convert_imageset.cpp:177] Processed 43000 files.
E1028 11:23:05.253074 18609 convert_imageset.cpp:177] Processed 44000 files.
E1028 11:23:06.831400 18609 convert_imageset.cpp:177] Processed 45000 files.
E1028 11:23:08.308161 18609 convert_imageset.cpp:177] Processed 46000 files.
E1028 11:23:09.777925 18609 convert_imageset.cpp:177] Processed 47000 files.
E1028 11:23:11.361310 18609 convert_imageset.cpp:177] Processed 48000 files.
E1028 11:23:12.900221 18609 convert_imageset.cpp:177] Processed 49000 files.
E1028 11:23:14.419956 18609 convert_imageset.cpp:177] Processed 50000 files.
E1028 11:23:16.100563 18609 convert_imageset.cpp:177] Processed 51000 files.
E1028 11:23:17.691378 18609 convert_imageset.cpp:177] Processed 52000 files.
E1028 11:23:19.215059 18609 convert_imageset.cpp:177] Processed 53000 files.
E1028 11:23:20.808619 18609 convert_imageset.cpp:177] Processed 54000 files.
E1028 11:23:22.300048 18609 convert_imageset.cpp:177] Processed 55000 files.
E1028 11:23:23.741319 18609 convert_imageset.cpp:177] Processed 56000 files.
E1028 11:23:25.296308 18609 convert_imageset.cpp:177] Processed 57000 files.
E1028 11:23:26.835520 18609 convert_imageset.cpp:177] Processed 58000 files.
E1028 11:23:28.273589 18609 convert_imageset.cpp:177] Processed 59000 files.
E1028 11:23:29.788411 18609 convert_imageset.cpp:177] Processed 60000 files.
I1028 11:23:30.036856 18711 caffe.cpp:99] Use GPU with device ID 0
I1028 11:23:30.392951 18711 caffe.cpp:107] Starting Optimization
I1028 11:23:30.393074 18711 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 405000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 11:23:30.393098 18711 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 11:23:30.417448 18711 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 11:23:30.417675 18711 net.cpp:67] Creating Layer mnist
I1028 11:23:30.417701 18711 net.cpp:356] mnist -> data
I1028 11:23:30.417737 18711 net.cpp:356] mnist -> label
I1028 11:23:30.417769 18711 net.cpp:96] Setting up mnist
I1028 11:23:30.425678 18711 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 11:23:30.425773 18711 data_layer.cpp:128] output data size: 64,1,50,180
I1028 11:23:30.426633 18711 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 11:23:30.426654 18711 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 11:23:30.426668 18711 net.cpp:67] Creating Layer conv1
I1028 11:23:30.426674 18711 net.cpp:394] conv1 <- data
I1028 11:23:30.426689 18711 net.cpp:356] conv1 -> conv1
I1028 11:23:30.426702 18711 net.cpp:96] Setting up conv1
I1028 11:23:30.427111 18711 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 11:23:30.427144 18711 net.cpp:67] Creating Layer pool1
I1028 11:23:30.427150 18711 net.cpp:394] pool1 <- conv1
I1028 11:23:30.427160 18711 net.cpp:356] pool1 -> pool1
I1028 11:23:30.427170 18711 net.cpp:96] Setting up pool1
I1028 11:23:30.427187 18711 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 11:23:30.427196 18711 net.cpp:67] Creating Layer relu1
I1028 11:23:30.427201 18711 net.cpp:394] relu1 <- pool1
I1028 11:23:30.427207 18711 net.cpp:345] relu1 -> pool1 (in-place)
I1028 11:23:30.427216 18711 net.cpp:96] Setting up relu1
I1028 11:23:30.427220 18711 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 11:23:30.427228 18711 net.cpp:67] Creating Layer drop1
I1028 11:23:30.427233 18711 net.cpp:394] drop1 <- pool1
I1028 11:23:30.427242 18711 net.cpp:345] drop1 -> pool1 (in-place)
I1028 11:23:30.427250 18711 net.cpp:96] Setting up drop1
I1028 11:23:30.427256 18711 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 11:23:30.427264 18711 net.cpp:67] Creating Layer conv2
I1028 11:23:30.427269 18711 net.cpp:394] conv2 <- pool1
I1028 11:23:30.427278 18711 net.cpp:356] conv2 -> conv2
I1028 11:23:30.427289 18711 net.cpp:96] Setting up conv2
I1028 11:23:30.427935 18711 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 11:23:30.427956 18711 net.cpp:67] Creating Layer pool2
I1028 11:23:30.427963 18711 net.cpp:394] pool2 <- conv2
I1028 11:23:30.427970 18711 net.cpp:356] pool2 -> pool2
I1028 11:23:30.427978 18711 net.cpp:96] Setting up pool2
I1028 11:23:30.427984 18711 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 11:23:30.427997 18711 net.cpp:67] Creating Layer relu2
I1028 11:23:30.428004 18711 net.cpp:394] relu2 <- pool2
I1028 11:23:30.428012 18711 net.cpp:345] relu2 -> pool2 (in-place)
I1028 11:23:30.428020 18711 net.cpp:96] Setting up relu2
I1028 11:23:30.428025 18711 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 11:23:30.428033 18711 net.cpp:67] Creating Layer drop2
I1028 11:23:30.428038 18711 net.cpp:394] drop2 <- pool2
I1028 11:23:30.428047 18711 net.cpp:345] drop2 -> pool2 (in-place)
I1028 11:23:30.428055 18711 net.cpp:96] Setting up drop2
I1028 11:23:30.428061 18711 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 11:23:30.428068 18711 net.cpp:67] Creating Layer conv3
I1028 11:23:30.428073 18711 net.cpp:394] conv3 <- pool2
I1028 11:23:30.428081 18711 net.cpp:356] conv3 -> conv3
I1028 11:23:30.428088 18711 net.cpp:96] Setting up conv3
I1028 11:23:30.431179 18711 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 11:23:30.431232 18711 net.cpp:67] Creating Layer pool3
I1028 11:23:30.431247 18711 net.cpp:394] pool3 <- conv3
I1028 11:23:30.431270 18711 net.cpp:356] pool3 -> pool3
I1028 11:23:30.431293 18711 net.cpp:96] Setting up pool3
I1028 11:23:30.431308 18711 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 11:23:30.431325 18711 net.cpp:67] Creating Layer relu3
I1028 11:23:30.431337 18711 net.cpp:394] relu3 <- pool3
I1028 11:23:30.431357 18711 net.cpp:345] relu3 -> pool3 (in-place)
I1028 11:23:30.431375 18711 net.cpp:96] Setting up relu3
I1028 11:23:30.431388 18711 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 11:23:30.431406 18711 net.cpp:67] Creating Layer drop3
I1028 11:23:30.431417 18711 net.cpp:394] drop3 <- pool3
I1028 11:23:30.431434 18711 net.cpp:345] drop3 -> pool3 (in-place)
I1028 11:23:30.431450 18711 net.cpp:96] Setting up drop3
I1028 11:23:30.431464 18711 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 11:23:30.431483 18711 net.cpp:67] Creating Layer ip1
I1028 11:23:30.431494 18711 net.cpp:394] ip1 <- pool3
I1028 11:23:30.431516 18711 net.cpp:356] ip1 -> ip1
I1028 11:23:30.431579 18711 net.cpp:96] Setting up ip1
I1028 11:23:30.860450 18711 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 11:23:30.860509 18711 net.cpp:67] Creating Layer relu4
I1028 11:23:30.860517 18711 net.cpp:394] relu4 <- ip1
I1028 11:23:30.860527 18711 net.cpp:345] relu4 -> ip1 (in-place)
I1028 11:23:30.860537 18711 net.cpp:96] Setting up relu4
I1028 11:23:30.860541 18711 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 11:23:30.860548 18711 net.cpp:67] Creating Layer drop4
I1028 11:23:30.860553 18711 net.cpp:394] drop4 <- ip1
I1028 11:23:30.860561 18711 net.cpp:345] drop4 -> ip1 (in-place)
I1028 11:23:30.860569 18711 net.cpp:96] Setting up drop4
I1028 11:23:30.860574 18711 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 11:23:30.860584 18711 net.cpp:67] Creating Layer ip2
I1028 11:23:30.860589 18711 net.cpp:394] ip2 <- ip1
I1028 11:23:30.860596 18711 net.cpp:356] ip2 -> ip2
I1028 11:23:30.860605 18711 net.cpp:96] Setting up ip2
I1028 11:23:30.868715 18711 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 11:23:30.868768 18711 net.cpp:67] Creating Layer loss
I1028 11:23:30.868774 18711 net.cpp:394] loss <- ip2
I1028 11:23:30.868782 18711 net.cpp:394] loss <- label
I1028 11:23:30.868789 18711 net.cpp:356] loss -> loss
I1028 11:23:30.868798 18711 net.cpp:96] Setting up loss
I1028 11:23:30.868810 18711 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 11:23:30.868815 18711 net.cpp:109]     with loss weight 1
I1028 11:23:30.868854 18711 net.cpp:170] loss needs backward computation.
I1028 11:23:30.868860 18711 net.cpp:170] ip2 needs backward computation.
I1028 11:23:30.868863 18711 net.cpp:170] drop4 needs backward computation.
I1028 11:23:30.868868 18711 net.cpp:170] relu4 needs backward computation.
I1028 11:23:30.868872 18711 net.cpp:170] ip1 needs backward computation.
I1028 11:23:30.868877 18711 net.cpp:170] drop3 needs backward computation.
I1028 11:23:30.868882 18711 net.cpp:170] relu3 needs backward computation.
I1028 11:23:30.868886 18711 net.cpp:170] pool3 needs backward computation.
I1028 11:23:30.868891 18711 net.cpp:170] conv3 needs backward computation.
I1028 11:23:30.868903 18711 net.cpp:170] drop2 needs backward computation.
I1028 11:23:30.868908 18711 net.cpp:170] relu2 needs backward computation.
I1028 11:23:30.868913 18711 net.cpp:170] pool2 needs backward computation.
I1028 11:23:30.868917 18711 net.cpp:170] conv2 needs backward computation.
I1028 11:23:30.868922 18711 net.cpp:170] drop1 needs backward computation.
I1028 11:23:30.868927 18711 net.cpp:170] relu1 needs backward computation.
I1028 11:23:30.868932 18711 net.cpp:170] pool1 needs backward computation.
I1028 11:23:30.868937 18711 net.cpp:170] conv1 needs backward computation.
I1028 11:23:30.868940 18711 net.cpp:172] mnist does not need backward computation.
I1028 11:23:30.868945 18711 net.cpp:208] This network produces output loss
I1028 11:23:30.868955 18711 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 11:23:30.868963 18711 net.cpp:219] Network initialization done.
I1028 11:23:30.868968 18711 net.cpp:220] Memory required for data: 119788292
I1028 11:23:30.869027 18711 solver.cpp:41] Solver scaffolding done.
I1028 11:23:30.869035 18711 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_400000.solverstate
I1028 11:23:30.869038 18711 solver.cpp:160] Solving Captcha
I1028 11:23:30.869058 18711 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_400000.solverstate
I1028 11:23:34.961659 18711 solver.cpp:502] SGDSolver: restoring history
I1028 11:23:35.707131 18711 solver.cpp:191] Iteration 400000, loss = 2.19621
I1028 11:23:35.707190 18711 solver.cpp:206]     Train net output #0: loss = 2.19621 (* 1 = 2.19621 loss)
I1028 11:23:35.707204 18711 solver.cpp:403] Iteration 400000, lr = 0.00061718
I1028 11:27:37.767573 18711 solver.cpp:191] Iteration 401000, loss = 2.32326
I1028 11:27:37.768270 18711 solver.cpp:206]     Train net output #0: loss = 2.32326 (* 1 = 2.32326 loss)
I1028 11:27:37.768303 18711 solver.cpp:403] Iteration 401000, lr = 0.000616054
I1028 11:31:38.711724 18711 solver.cpp:191] Iteration 402000, loss = 2.52468
I1028 11:31:38.712280 18711 solver.cpp:206]     Train net output #0: loss = 2.52468 (* 1 = 2.52468 loss)
I1028 11:31:38.712317 18711 solver.cpp:403] Iteration 402000, lr = 0.000614932
I1028 11:35:39.702850 18711 solver.cpp:191] Iteration 403000, loss = 2.24221
I1028 11:35:39.703521 18711 solver.cpp:206]     Train net output #0: loss = 2.24221 (* 1 = 2.24221 loss)
I1028 11:35:39.703560 18711 solver.cpp:403] Iteration 403000, lr = 0.000613815
I1028 11:39:40.766984 18711 solver.cpp:191] Iteration 404000, loss = 2.56869
I1028 11:39:40.767765 18711 solver.cpp:206]     Train net output #0: loss = 2.56869 (* 1 = 2.56869 loss)
I1028 11:39:40.767799 18711 solver.cpp:403] Iteration 404000, lr = 0.000612703
I1028 11:43:42.356024 18711 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_405000.caffemodel
I1028 11:43:46.760087 18711 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_405000.solverstate
I1028 11:43:50.332955 18711 solver.cpp:228] Iteration 405000, loss = 2.39831
I1028 11:43:50.333508 18711 solver.cpp:233] Optimization Done.
I1028 11:43:50.333533 18711 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 12:06:35.863260 32446 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 12:06:35.863833 32446 net.cpp:358] Input 0 -> data
I1028 12:06:35.863864 32446 net.cpp:67] Creating Layer conv1
I1028 12:06:35.863872 32446 net.cpp:394] conv1 <- data
I1028 12:06:35.863879 32446 net.cpp:356] conv1 -> conv1
I1028 12:06:35.863891 32446 net.cpp:96] Setting up conv1
I1028 12:06:35.864284 32446 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 12:06:35.864306 32446 net.cpp:67] Creating Layer pool1
I1028 12:06:35.864312 32446 net.cpp:394] pool1 <- conv1
I1028 12:06:35.864320 32446 net.cpp:356] pool1 -> pool1
I1028 12:06:35.864328 32446 net.cpp:96] Setting up pool1
I1028 12:06:35.864346 32446 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 12:06:35.864356 32446 net.cpp:67] Creating Layer relu1
I1028 12:06:35.864361 32446 net.cpp:394] relu1 <- pool1
I1028 12:06:35.864367 32446 net.cpp:345] relu1 -> pool1 (in-place)
I1028 12:06:35.864373 32446 net.cpp:96] Setting up relu1
I1028 12:06:35.864379 32446 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 12:06:35.864385 32446 net.cpp:67] Creating Layer drop1
I1028 12:06:35.864390 32446 net.cpp:394] drop1 <- pool1
I1028 12:06:35.864405 32446 net.cpp:345] drop1 -> pool1 (in-place)
I1028 12:06:35.864414 32446 net.cpp:96] Setting up drop1
I1028 12:06:35.864445 32446 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 12:06:35.864461 32446 net.cpp:67] Creating Layer conv2
I1028 12:06:35.864475 32446 net.cpp:394] conv2 <- pool1
I1028 12:06:35.864498 32446 net.cpp:356] conv2 -> conv2
I1028 12:06:35.864518 32446 net.cpp:96] Setting up conv2
I1028 12:06:35.866046 32446 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 12:06:35.866087 32446 net.cpp:67] Creating Layer pool2
I1028 12:06:35.866101 32446 net.cpp:394] pool2 <- conv2
I1028 12:06:35.866117 32446 net.cpp:356] pool2 -> pool2
I1028 12:06:35.866135 32446 net.cpp:96] Setting up pool2
I1028 12:06:35.866152 32446 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 12:06:35.866166 32446 net.cpp:67] Creating Layer relu2
I1028 12:06:35.866184 32446 net.cpp:394] relu2 <- pool2
I1028 12:06:35.866204 32446 net.cpp:345] relu2 -> pool2 (in-place)
I1028 12:06:35.866221 32446 net.cpp:96] Setting up relu2
I1028 12:06:35.866233 32446 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 12:06:35.866248 32446 net.cpp:67] Creating Layer drop2
I1028 12:06:35.866260 32446 net.cpp:394] drop2 <- pool2
I1028 12:06:35.866277 32446 net.cpp:345] drop2 -> pool2 (in-place)
I1028 12:06:35.866294 32446 net.cpp:96] Setting up drop2
I1028 12:06:35.866307 32446 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 12:06:35.866325 32446 net.cpp:67] Creating Layer conv3
I1028 12:06:35.866336 32446 net.cpp:394] conv3 <- pool2
I1028 12:06:35.866353 32446 net.cpp:356] conv3 -> conv3
I1028 12:06:35.866370 32446 net.cpp:96] Setting up conv3
I1028 12:06:35.870417 32446 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 12:06:35.870457 32446 net.cpp:67] Creating Layer pool3
I1028 12:06:35.870471 32446 net.cpp:394] pool3 <- conv3
I1028 12:06:35.870491 32446 net.cpp:356] pool3 -> pool3
I1028 12:06:35.870512 32446 net.cpp:96] Setting up pool3
I1028 12:06:35.870525 32446 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 12:06:35.870540 32446 net.cpp:67] Creating Layer relu3
I1028 12:06:35.870551 32446 net.cpp:394] relu3 <- pool3
I1028 12:06:35.870570 32446 net.cpp:345] relu3 -> pool3 (in-place)
I1028 12:06:35.870586 32446 net.cpp:96] Setting up relu3
I1028 12:06:35.870599 32446 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 12:06:35.870614 32446 net.cpp:67] Creating Layer drop3
I1028 12:06:35.870625 32446 net.cpp:394] drop3 <- pool3
I1028 12:06:35.870640 32446 net.cpp:345] drop3 -> pool3 (in-place)
I1028 12:06:35.870656 32446 net.cpp:96] Setting up drop3
I1028 12:06:35.870667 32446 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 12:06:35.870683 32446 net.cpp:67] Creating Layer ip1
I1028 12:06:35.870694 32446 net.cpp:394] ip1 <- pool3
I1028 12:06:35.870715 32446 net.cpp:356] ip1 -> ip1
I1028 12:06:35.870735 32446 net.cpp:96] Setting up ip1
I1028 12:06:36.333442 32446 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 12:06:36.333506 32446 net.cpp:67] Creating Layer relu4
I1028 12:06:36.333513 32446 net.cpp:394] relu4 <- ip1
I1028 12:06:36.333523 32446 net.cpp:345] relu4 -> ip1 (in-place)
I1028 12:06:36.333533 32446 net.cpp:96] Setting up relu4
I1028 12:06:36.333537 32446 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 12:06:36.333545 32446 net.cpp:67] Creating Layer drop4
I1028 12:06:36.333549 32446 net.cpp:394] drop4 <- ip1
I1028 12:06:36.333555 32446 net.cpp:345] drop4 -> ip1 (in-place)
I1028 12:06:36.333561 32446 net.cpp:96] Setting up drop4
I1028 12:06:36.333566 32446 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 12:06:36.333577 32446 net.cpp:67] Creating Layer ip2
I1028 12:06:36.333581 32446 net.cpp:394] ip2 <- ip1
I1028 12:06:36.333587 32446 net.cpp:356] ip2 -> ip2
I1028 12:06:36.333600 32446 net.cpp:96] Setting up ip2
I1028 12:06:36.343436 32446 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 12:06:36.343511 32446 net.cpp:67] Creating Layer prob
I1028 12:06:36.343519 32446 net.cpp:394] prob <- ip2
I1028 12:06:36.343528 32446 net.cpp:356] prob -> prob
I1028 12:06:36.343538 32446 net.cpp:96] Setting up prob
I1028 12:06:36.343544 32446 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 12:06:36.343549 32446 net.cpp:172] prob does not need backward computation.
I1028 12:06:36.343554 32446 net.cpp:172] ip2 does not need backward computation.
I1028 12:06:36.343557 32446 net.cpp:172] drop4 does not need backward computation.
I1028 12:06:36.343560 32446 net.cpp:172] relu4 does not need backward computation.
I1028 12:06:36.343564 32446 net.cpp:172] ip1 does not need backward computation.
I1028 12:06:36.343569 32446 net.cpp:172] drop3 does not need backward computation.
I1028 12:06:36.343572 32446 net.cpp:172] relu3 does not need backward computation.
I1028 12:06:36.343575 32446 net.cpp:172] pool3 does not need backward computation.
I1028 12:06:36.343580 32446 net.cpp:172] conv3 does not need backward computation.
I1028 12:06:36.343583 32446 net.cpp:172] drop2 does not need backward computation.
I1028 12:06:36.343595 32446 net.cpp:172] relu2 does not need backward computation.
I1028 12:06:36.343598 32446 net.cpp:172] pool2 does not need backward computation.
I1028 12:06:36.343602 32446 net.cpp:172] conv2 does not need backward computation.
I1028 12:06:36.343606 32446 net.cpp:172] drop1 does not need backward computation.
I1028 12:06:36.343610 32446 net.cpp:172] relu1 does not need backward computation.
I1028 12:06:36.343613 32446 net.cpp:172] pool1 does not need backward computation.
I1028 12:06:36.343617 32446 net.cpp:172] conv1 does not need backward computation.
I1028 12:06:36.343621 32446 net.cpp:208] This network produces output prob
I1028 12:06:36.343634 32446 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 12:06:36.343642 32446 net.cpp:219] Network initialization done.
I1028 12:06:36.343647 32446 net.cpp:220] Memory required for data: 1837200
I1028 12:38:37.130913  7800 convert_imageset.cpp:70] Shuffling data
I1028 12:38:37.742346  7800 convert_imageset.cpp:73] A total of 60000 images.
I1028 12:38:37.742424  7800 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 12:38:40.288554  7800 convert_imageset.cpp:177] Processed 1000 files.
E1028 12:38:42.391227  7800 convert_imageset.cpp:177] Processed 2000 files.
E1028 12:38:44.390915  7800 convert_imageset.cpp:177] Processed 3000 files.
E1028 12:38:46.356292  7800 convert_imageset.cpp:177] Processed 4000 files.
E1028 12:38:48.303159  7800 convert_imageset.cpp:177] Processed 5000 files.
E1028 12:38:50.270190  7800 convert_imageset.cpp:177] Processed 6000 files.
E1028 12:38:52.058295  7800 convert_imageset.cpp:177] Processed 7000 files.
E1028 12:38:53.858844  7800 convert_imageset.cpp:177] Processed 8000 files.
E1028 12:38:55.740914  7800 convert_imageset.cpp:177] Processed 9000 files.
E1028 12:38:57.639196  7800 convert_imageset.cpp:177] Processed 10000 files.
E1028 12:38:59.320004  7800 convert_imageset.cpp:177] Processed 11000 files.
E1028 12:39:01.239163  7800 convert_imageset.cpp:177] Processed 12000 files.
E1028 12:39:03.145294  7800 convert_imageset.cpp:177] Processed 13000 files.
E1028 12:39:05.050135  7800 convert_imageset.cpp:177] Processed 14000 files.
E1028 12:39:06.749446  7800 convert_imageset.cpp:177] Processed 15000 files.
E1028 12:39:08.449362  7800 convert_imageset.cpp:177] Processed 16000 files.
E1028 12:39:10.307016  7800 convert_imageset.cpp:177] Processed 17000 files.
E1028 12:39:12.438304  7800 convert_imageset.cpp:177] Processed 18000 files.
E1028 12:39:14.290434  7800 convert_imageset.cpp:177] Processed 19000 files.
E1028 12:39:15.899632  7800 convert_imageset.cpp:177] Processed 20000 files.
E1028 12:39:17.599901  7800 convert_imageset.cpp:177] Processed 21000 files.
E1028 12:39:19.207710  7800 convert_imageset.cpp:177] Processed 22000 files.
E1028 12:39:20.842461  7800 convert_imageset.cpp:177] Processed 23000 files.
E1028 12:39:22.438505  7800 convert_imageset.cpp:177] Processed 24000 files.
E1028 12:39:24.035320  7800 convert_imageset.cpp:177] Processed 25000 files.
E1028 12:39:25.735054  7800 convert_imageset.cpp:177] Processed 26000 files.
E1028 12:39:27.285049  7800 convert_imageset.cpp:177] Processed 27000 files.
E1028 12:39:28.944090  7800 convert_imageset.cpp:177] Processed 28000 files.
E1028 12:39:30.526152  7800 convert_imageset.cpp:177] Processed 29000 files.
E1028 12:39:32.127898  7800 convert_imageset.cpp:177] Processed 30000 files.
E1028 12:39:33.864311  7800 convert_imageset.cpp:177] Processed 31000 files.
E1028 12:39:35.510490  7800 convert_imageset.cpp:177] Processed 32000 files.
E1028 12:39:37.091007  7800 convert_imageset.cpp:177] Processed 33000 files.
E1028 12:39:38.620026  7800 convert_imageset.cpp:177] Processed 34000 files.
E1028 12:39:40.396891  7800 convert_imageset.cpp:177] Processed 35000 files.
E1028 12:39:42.166281  7800 convert_imageset.cpp:177] Processed 36000 files.
E1028 12:39:43.898478  7800 convert_imageset.cpp:177] Processed 37000 files.
E1028 12:39:45.398936  7800 convert_imageset.cpp:177] Processed 38000 files.
E1028 12:39:47.012969  7800 convert_imageset.cpp:177] Processed 39000 files.
E1028 12:39:48.672456  7800 convert_imageset.cpp:177] Processed 40000 files.
E1028 12:39:50.291757  7800 convert_imageset.cpp:177] Processed 41000 files.
E1028 12:39:51.886818  7800 convert_imageset.cpp:177] Processed 42000 files.
E1028 12:39:53.498304  7800 convert_imageset.cpp:177] Processed 43000 files.
E1028 12:39:54.987586  7800 convert_imageset.cpp:177] Processed 44000 files.
E1028 12:39:56.567473  7800 convert_imageset.cpp:177] Processed 45000 files.
E1028 12:39:58.227223  7800 convert_imageset.cpp:177] Processed 46000 files.
E1028 12:39:59.797646  7800 convert_imageset.cpp:177] Processed 47000 files.
E1028 12:40:01.468933  7800 convert_imageset.cpp:177] Processed 48000 files.
E1028 12:40:03.115432  7800 convert_imageset.cpp:177] Processed 49000 files.
E1028 12:40:04.875402  7800 convert_imageset.cpp:177] Processed 50000 files.
E1028 12:40:06.495575  7800 convert_imageset.cpp:177] Processed 51000 files.
E1028 12:40:08.034488  7800 convert_imageset.cpp:177] Processed 52000 files.
E1028 12:40:09.617967  7800 convert_imageset.cpp:177] Processed 53000 files.
E1028 12:40:11.207221  7800 convert_imageset.cpp:177] Processed 54000 files.
E1028 12:40:12.814872  7800 convert_imageset.cpp:177] Processed 55000 files.
E1028 12:40:14.298926  7800 convert_imageset.cpp:177] Processed 56000 files.
E1028 12:40:15.850440  7800 convert_imageset.cpp:177] Processed 57000 files.
E1028 12:40:17.378857  7800 convert_imageset.cpp:177] Processed 58000 files.
E1028 12:40:18.961189  7800 convert_imageset.cpp:177] Processed 59000 files.
E1028 12:40:20.456522  7800 convert_imageset.cpp:177] Processed 60000 files.
I1028 12:40:20.659909  7987 caffe.cpp:99] Use GPU with device ID 0
I1028 12:40:21.007235  7987 caffe.cpp:107] Starting Optimization
I1028 12:40:21.007351  7987 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 410000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 12:40:21.007376  7987 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 12:40:21.021541  7987 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 12:40:21.021757  7987 net.cpp:67] Creating Layer mnist
I1028 12:40:21.021785  7987 net.cpp:356] mnist -> data
I1028 12:40:21.021821  7987 net.cpp:356] mnist -> label
I1028 12:40:21.021852  7987 net.cpp:96] Setting up mnist
I1028 12:40:21.029849  7987 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 12:40:21.029942  7987 data_layer.cpp:128] output data size: 64,1,50,180
I1028 12:40:21.030488  7987 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 12:40:21.030519  7987 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 12:40:21.030531  7987 net.cpp:67] Creating Layer conv1
I1028 12:40:21.030539  7987 net.cpp:394] conv1 <- data
I1028 12:40:21.030553  7987 net.cpp:356] conv1 -> conv1
I1028 12:40:21.030565  7987 net.cpp:96] Setting up conv1
I1028 12:40:21.030923  7987 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 12:40:21.030962  7987 net.cpp:67] Creating Layer pool1
I1028 12:40:21.030969  7987 net.cpp:394] pool1 <- conv1
I1028 12:40:21.030975  7987 net.cpp:356] pool1 -> pool1
I1028 12:40:21.030983  7987 net.cpp:96] Setting up pool1
I1028 12:40:21.030999  7987 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 12:40:21.031008  7987 net.cpp:67] Creating Layer relu1
I1028 12:40:21.031011  7987 net.cpp:394] relu1 <- pool1
I1028 12:40:21.031018  7987 net.cpp:345] relu1 -> pool1 (in-place)
I1028 12:40:21.031024  7987 net.cpp:96] Setting up relu1
I1028 12:40:21.031029  7987 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 12:40:21.031036  7987 net.cpp:67] Creating Layer drop1
I1028 12:40:21.031040  7987 net.cpp:394] drop1 <- pool1
I1028 12:40:21.031046  7987 net.cpp:345] drop1 -> pool1 (in-place)
I1028 12:40:21.031052  7987 net.cpp:96] Setting up drop1
I1028 12:40:21.031059  7987 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 12:40:21.031067  7987 net.cpp:67] Creating Layer conv2
I1028 12:40:21.031072  7987 net.cpp:394] conv2 <- pool1
I1028 12:40:21.031080  7987 net.cpp:356] conv2 -> conv2
I1028 12:40:21.031086  7987 net.cpp:96] Setting up conv2
I1028 12:40:21.031658  7987 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 12:40:21.031678  7987 net.cpp:67] Creating Layer pool2
I1028 12:40:21.031683  7987 net.cpp:394] pool2 <- conv2
I1028 12:40:21.031690  7987 net.cpp:356] pool2 -> pool2
I1028 12:40:21.031697  7987 net.cpp:96] Setting up pool2
I1028 12:40:21.031703  7987 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 12:40:21.031709  7987 net.cpp:67] Creating Layer relu2
I1028 12:40:21.031713  7987 net.cpp:394] relu2 <- pool2
I1028 12:40:21.031719  7987 net.cpp:345] relu2 -> pool2 (in-place)
I1028 12:40:21.031724  7987 net.cpp:96] Setting up relu2
I1028 12:40:21.031735  7987 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 12:40:21.031745  7987 net.cpp:67] Creating Layer drop2
I1028 12:40:21.031750  7987 net.cpp:394] drop2 <- pool2
I1028 12:40:21.031756  7987 net.cpp:345] drop2 -> pool2 (in-place)
I1028 12:40:21.031762  7987 net.cpp:96] Setting up drop2
I1028 12:40:21.031767  7987 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 12:40:21.031776  7987 net.cpp:67] Creating Layer conv3
I1028 12:40:21.031780  7987 net.cpp:394] conv3 <- pool2
I1028 12:40:21.031786  7987 net.cpp:356] conv3 -> conv3
I1028 12:40:21.031793  7987 net.cpp:96] Setting up conv3
I1028 12:40:21.033344  7987 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 12:40:21.033373  7987 net.cpp:67] Creating Layer pool3
I1028 12:40:21.033380  7987 net.cpp:394] pool3 <- conv3
I1028 12:40:21.033386  7987 net.cpp:356] pool3 -> pool3
I1028 12:40:21.033393  7987 net.cpp:96] Setting up pool3
I1028 12:40:21.033402  7987 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 12:40:21.033408  7987 net.cpp:67] Creating Layer relu3
I1028 12:40:21.033413  7987 net.cpp:394] relu3 <- pool3
I1028 12:40:21.033418  7987 net.cpp:345] relu3 -> pool3 (in-place)
I1028 12:40:21.033424  7987 net.cpp:96] Setting up relu3
I1028 12:40:21.033429  7987 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 12:40:21.033437  7987 net.cpp:67] Creating Layer drop3
I1028 12:40:21.033442  7987 net.cpp:394] drop3 <- pool3
I1028 12:40:21.033447  7987 net.cpp:345] drop3 -> pool3 (in-place)
I1028 12:40:21.033453  7987 net.cpp:96] Setting up drop3
I1028 12:40:21.033458  7987 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 12:40:21.033465  7987 net.cpp:67] Creating Layer ip1
I1028 12:40:21.033469  7987 net.cpp:394] ip1 <- pool3
I1028 12:40:21.033478  7987 net.cpp:356] ip1 -> ip1
I1028 12:40:21.033514  7987 net.cpp:96] Setting up ip1
I1028 12:40:21.574118  7987 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 12:40:21.574179  7987 net.cpp:67] Creating Layer relu4
I1028 12:40:21.574189  7987 net.cpp:394] relu4 <- ip1
I1028 12:40:21.574198  7987 net.cpp:345] relu4 -> ip1 (in-place)
I1028 12:40:21.574208  7987 net.cpp:96] Setting up relu4
I1028 12:40:21.574213  7987 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 12:40:21.574219  7987 net.cpp:67] Creating Layer drop4
I1028 12:40:21.574223  7987 net.cpp:394] drop4 <- ip1
I1028 12:40:21.574229  7987 net.cpp:345] drop4 -> ip1 (in-place)
I1028 12:40:21.574235  7987 net.cpp:96] Setting up drop4
I1028 12:40:21.574241  7987 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 12:40:21.574254  7987 net.cpp:67] Creating Layer ip2
I1028 12:40:21.574259  7987 net.cpp:394] ip2 <- ip1
I1028 12:40:21.574265  7987 net.cpp:356] ip2 -> ip2
I1028 12:40:21.574273  7987 net.cpp:96] Setting up ip2
I1028 12:40:21.583602  7987 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 12:40:21.583667  7987 net.cpp:67] Creating Layer loss
I1028 12:40:21.583674  7987 net.cpp:394] loss <- ip2
I1028 12:40:21.583681  7987 net.cpp:394] loss <- label
I1028 12:40:21.583688  7987 net.cpp:356] loss -> loss
I1028 12:40:21.583698  7987 net.cpp:96] Setting up loss
I1028 12:40:21.583710  7987 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 12:40:21.583715  7987 net.cpp:109]     with loss weight 1
I1028 12:40:21.583751  7987 net.cpp:170] loss needs backward computation.
I1028 12:40:21.583756  7987 net.cpp:170] ip2 needs backward computation.
I1028 12:40:21.583760  7987 net.cpp:170] drop4 needs backward computation.
I1028 12:40:21.583765  7987 net.cpp:170] relu4 needs backward computation.
I1028 12:40:21.583768  7987 net.cpp:170] ip1 needs backward computation.
I1028 12:40:21.583773  7987 net.cpp:170] drop3 needs backward computation.
I1028 12:40:21.583777  7987 net.cpp:170] relu3 needs backward computation.
I1028 12:40:21.583781  7987 net.cpp:170] pool3 needs backward computation.
I1028 12:40:21.583786  7987 net.cpp:170] conv3 needs backward computation.
I1028 12:40:21.583791  7987 net.cpp:170] drop2 needs backward computation.
I1028 12:40:21.583794  7987 net.cpp:170] relu2 needs backward computation.
I1028 12:40:21.583806  7987 net.cpp:170] pool2 needs backward computation.
I1028 12:40:21.583811  7987 net.cpp:170] conv2 needs backward computation.
I1028 12:40:21.583817  7987 net.cpp:170] drop1 needs backward computation.
I1028 12:40:21.583820  7987 net.cpp:170] relu1 needs backward computation.
I1028 12:40:21.583824  7987 net.cpp:170] pool1 needs backward computation.
I1028 12:40:21.583828  7987 net.cpp:170] conv1 needs backward computation.
I1028 12:40:21.583833  7987 net.cpp:172] mnist does not need backward computation.
I1028 12:40:21.583837  7987 net.cpp:208] This network produces output loss
I1028 12:40:21.583847  7987 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 12:40:21.583855  7987 net.cpp:219] Network initialization done.
I1028 12:40:21.583859  7987 net.cpp:220] Memory required for data: 119788292
I1028 12:40:21.583917  7987 solver.cpp:41] Solver scaffolding done.
I1028 12:40:21.583923  7987 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_405000.solverstate
I1028 12:40:21.583927  7987 solver.cpp:160] Solving Captcha
I1028 12:40:21.583946  7987 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_405000.solverstate
I1028 12:40:26.040705  7987 solver.cpp:502] SGDSolver: restoring history
I1028 12:40:26.791966  7987 solver.cpp:191] Iteration 405000, loss = 2.63954
I1028 12:40:26.792026  7987 solver.cpp:206]     Train net output #0: loss = 2.63954 (* 1 = 2.63954 loss)
I1028 12:40:26.792042  7987 solver.cpp:403] Iteration 405000, lr = 0.000611595
I1028 12:44:28.619388  7987 solver.cpp:191] Iteration 406000, loss = 2.5067
I1028 12:44:28.619979  7987 solver.cpp:206]     Train net output #0: loss = 2.5067 (* 1 = 2.5067 loss)
I1028 12:44:28.620018  7987 solver.cpp:403] Iteration 406000, lr = 0.000610492
I1028 12:48:30.062311  7987 solver.cpp:191] Iteration 407000, loss = 2.38842
I1028 12:48:30.062958  7987 solver.cpp:206]     Train net output #0: loss = 2.38842 (* 1 = 2.38842 loss)
I1028 12:48:30.062989  7987 solver.cpp:403] Iteration 407000, lr = 0.000609394
I1028 12:52:31.482484  7987 solver.cpp:191] Iteration 408000, loss = 2.31098
I1028 12:52:31.483263  7987 solver.cpp:206]     Train net output #0: loss = 2.31098 (* 1 = 2.31098 loss)
I1028 12:52:31.483295  7987 solver.cpp:403] Iteration 408000, lr = 0.0006083
I1028 12:56:32.982364  7987 solver.cpp:191] Iteration 409000, loss = 2.6747
I1028 12:56:32.982981  7987 solver.cpp:206]     Train net output #0: loss = 2.6747 (* 1 = 2.6747 loss)
I1028 12:56:32.983001  7987 solver.cpp:403] Iteration 409000, lr = 0.000607211
I1028 13:00:34.981072  7987 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_410000.caffemodel
I1028 13:00:39.857450  7987 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_410000.solverstate
I1028 13:00:43.432507  7987 solver.cpp:228] Iteration 410000, loss = 2.31589
I1028 13:00:43.433193  7987 solver.cpp:233] Optimization Done.
I1028 13:00:43.433220  7987 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 13:23:23.135089 21686 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 13:23:23.135188 21686 net.cpp:358] Input 0 -> data
I1028 13:23:23.135213 21686 net.cpp:67] Creating Layer conv1
I1028 13:23:23.135218 21686 net.cpp:394] conv1 <- data
I1028 13:23:23.135226 21686 net.cpp:356] conv1 -> conv1
I1028 13:23:23.135234 21686 net.cpp:96] Setting up conv1
I1028 13:23:23.135586 21686 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 13:23:23.135607 21686 net.cpp:67] Creating Layer pool1
I1028 13:23:23.135612 21686 net.cpp:394] pool1 <- conv1
I1028 13:23:23.135617 21686 net.cpp:356] pool1 -> pool1
I1028 13:23:23.135624 21686 net.cpp:96] Setting up pool1
I1028 13:23:23.135704 21686 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 13:23:23.135715 21686 net.cpp:67] Creating Layer relu1
I1028 13:23:23.135720 21686 net.cpp:394] relu1 <- pool1
I1028 13:23:23.135725 21686 net.cpp:345] relu1 -> pool1 (in-place)
I1028 13:23:23.135731 21686 net.cpp:96] Setting up relu1
I1028 13:23:23.135736 21686 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 13:23:23.135741 21686 net.cpp:67] Creating Layer drop1
I1028 13:23:23.135746 21686 net.cpp:394] drop1 <- pool1
I1028 13:23:23.135754 21686 net.cpp:345] drop1 -> pool1 (in-place)
I1028 13:23:23.135761 21686 net.cpp:96] Setting up drop1
I1028 13:23:23.135766 21686 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 13:23:23.135773 21686 net.cpp:67] Creating Layer conv2
I1028 13:23:23.135777 21686 net.cpp:394] conv2 <- pool1
I1028 13:23:23.135784 21686 net.cpp:356] conv2 -> conv2
I1028 13:23:23.135792 21686 net.cpp:96] Setting up conv2
I1028 13:23:23.136466 21686 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 13:23:23.136486 21686 net.cpp:67] Creating Layer pool2
I1028 13:23:23.136490 21686 net.cpp:394] pool2 <- conv2
I1028 13:23:23.136497 21686 net.cpp:356] pool2 -> pool2
I1028 13:23:23.136503 21686 net.cpp:96] Setting up pool2
I1028 13:23:23.136509 21686 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 13:23:23.136514 21686 net.cpp:67] Creating Layer relu2
I1028 13:23:23.136518 21686 net.cpp:394] relu2 <- pool2
I1028 13:23:23.136526 21686 net.cpp:345] relu2 -> pool2 (in-place)
I1028 13:23:23.136549 21686 net.cpp:96] Setting up relu2
I1028 13:23:23.136559 21686 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 13:23:23.136566 21686 net.cpp:67] Creating Layer drop2
I1028 13:23:23.136570 21686 net.cpp:394] drop2 <- pool2
I1028 13:23:23.136575 21686 net.cpp:345] drop2 -> pool2 (in-place)
I1028 13:23:23.136584 21686 net.cpp:96] Setting up drop2
I1028 13:23:23.136589 21686 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 13:23:23.136596 21686 net.cpp:67] Creating Layer conv3
I1028 13:23:23.136600 21686 net.cpp:394] conv3 <- pool2
I1028 13:23:23.136606 21686 net.cpp:356] conv3 -> conv3
I1028 13:23:23.136613 21686 net.cpp:96] Setting up conv3
I1028 13:23:23.138149 21686 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 13:23:23.138171 21686 net.cpp:67] Creating Layer pool3
I1028 13:23:23.138176 21686 net.cpp:394] pool3 <- conv3
I1028 13:23:23.138185 21686 net.cpp:356] pool3 -> pool3
I1028 13:23:23.138192 21686 net.cpp:96] Setting up pool3
I1028 13:23:23.138198 21686 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 13:23:23.138205 21686 net.cpp:67] Creating Layer relu3
I1028 13:23:23.138208 21686 net.cpp:394] relu3 <- pool3
I1028 13:23:23.138214 21686 net.cpp:345] relu3 -> pool3 (in-place)
I1028 13:23:23.138221 21686 net.cpp:96] Setting up relu3
I1028 13:23:23.138226 21686 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 13:23:23.138231 21686 net.cpp:67] Creating Layer drop3
I1028 13:23:23.138234 21686 net.cpp:394] drop3 <- pool3
I1028 13:23:23.138239 21686 net.cpp:345] drop3 -> pool3 (in-place)
I1028 13:23:23.138245 21686 net.cpp:96] Setting up drop3
I1028 13:23:23.138249 21686 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 13:23:23.138257 21686 net.cpp:67] Creating Layer ip1
I1028 13:23:23.138260 21686 net.cpp:394] ip1 <- pool3
I1028 13:23:23.138269 21686 net.cpp:356] ip1 -> ip1
I1028 13:23:23.138278 21686 net.cpp:96] Setting up ip1
I1028 13:23:23.623767 21686 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 13:23:23.623828 21686 net.cpp:67] Creating Layer relu4
I1028 13:23:23.623837 21686 net.cpp:394] relu4 <- ip1
I1028 13:23:23.623847 21686 net.cpp:345] relu4 -> ip1 (in-place)
I1028 13:23:23.623855 21686 net.cpp:96] Setting up relu4
I1028 13:23:23.623860 21686 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 13:23:23.623867 21686 net.cpp:67] Creating Layer drop4
I1028 13:23:23.623872 21686 net.cpp:394] drop4 <- ip1
I1028 13:23:23.623877 21686 net.cpp:345] drop4 -> ip1 (in-place)
I1028 13:23:23.623883 21686 net.cpp:96] Setting up drop4
I1028 13:23:23.623888 21686 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 13:23:23.623898 21686 net.cpp:67] Creating Layer ip2
I1028 13:23:23.623903 21686 net.cpp:394] ip2 <- ip1
I1028 13:23:23.623908 21686 net.cpp:356] ip2 -> ip2
I1028 13:23:23.623920 21686 net.cpp:96] Setting up ip2
I1028 13:23:23.633091 21686 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 13:23:23.633155 21686 net.cpp:67] Creating Layer prob
I1028 13:23:23.633162 21686 net.cpp:394] prob <- ip2
I1028 13:23:23.633170 21686 net.cpp:356] prob -> prob
I1028 13:23:23.633180 21686 net.cpp:96] Setting up prob
I1028 13:23:23.633186 21686 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 13:23:23.633190 21686 net.cpp:172] prob does not need backward computation.
I1028 13:23:23.633194 21686 net.cpp:172] ip2 does not need backward computation.
I1028 13:23:23.633198 21686 net.cpp:172] drop4 does not need backward computation.
I1028 13:23:23.633201 21686 net.cpp:172] relu4 does not need backward computation.
I1028 13:23:23.633204 21686 net.cpp:172] ip1 does not need backward computation.
I1028 13:23:23.633208 21686 net.cpp:172] drop3 does not need backward computation.
I1028 13:23:23.633213 21686 net.cpp:172] relu3 does not need backward computation.
I1028 13:23:23.633215 21686 net.cpp:172] pool3 does not need backward computation.
I1028 13:23:23.633219 21686 net.cpp:172] conv3 does not need backward computation.
I1028 13:23:23.633222 21686 net.cpp:172] drop2 does not need backward computation.
I1028 13:23:23.633226 21686 net.cpp:172] relu2 does not need backward computation.
I1028 13:23:23.633229 21686 net.cpp:172] pool2 does not need backward computation.
I1028 13:23:23.633234 21686 net.cpp:172] conv2 does not need backward computation.
I1028 13:23:23.633244 21686 net.cpp:172] drop1 does not need backward computation.
I1028 13:23:23.633247 21686 net.cpp:172] relu1 does not need backward computation.
I1028 13:23:23.633250 21686 net.cpp:172] pool1 does not need backward computation.
I1028 13:23:23.633255 21686 net.cpp:172] conv1 does not need backward computation.
I1028 13:23:23.633257 21686 net.cpp:208] This network produces output prob
I1028 13:23:23.633272 21686 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 13:23:23.633280 21686 net.cpp:219] Network initialization done.
I1028 13:23:23.633283 21686 net.cpp:220] Memory required for data: 1837200
I1028 13:56:22.708309 29822 convert_imageset.cpp:70] Shuffling data
I1028 13:56:23.325481 29822 convert_imageset.cpp:73] A total of 60000 images.
I1028 13:56:23.325559 29822 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 13:56:25.668040 29822 convert_imageset.cpp:177] Processed 1000 files.
E1028 13:56:27.992918 29822 convert_imageset.cpp:177] Processed 2000 files.
E1028 13:56:30.435120 29822 convert_imageset.cpp:177] Processed 3000 files.
E1028 13:56:32.643091 29822 convert_imageset.cpp:177] Processed 4000 files.
E1028 13:56:34.836714 29822 convert_imageset.cpp:177] Processed 5000 files.
E1028 13:56:36.853359 29822 convert_imageset.cpp:177] Processed 6000 files.
E1028 13:56:38.764955 29822 convert_imageset.cpp:177] Processed 7000 files.
E1028 13:56:40.875432 29822 convert_imageset.cpp:177] Processed 8000 files.
E1028 13:56:42.789724 29822 convert_imageset.cpp:177] Processed 9000 files.
E1028 13:56:44.713286 29822 convert_imageset.cpp:177] Processed 10000 files.
E1028 13:56:46.536903 29822 convert_imageset.cpp:177] Processed 11000 files.
E1028 13:56:48.480188 29822 convert_imageset.cpp:177] Processed 12000 files.
E1028 13:56:50.277425 29822 convert_imageset.cpp:177] Processed 13000 files.
E1028 13:56:52.245782 29822 convert_imageset.cpp:177] Processed 14000 files.
E1028 13:56:54.160815 29822 convert_imageset.cpp:177] Processed 15000 files.
E1028 13:56:55.956655 29822 convert_imageset.cpp:177] Processed 16000 files.
E1028 13:56:57.806987 29822 convert_imageset.cpp:177] Processed 17000 files.
E1028 13:56:59.607404 29822 convert_imageset.cpp:177] Processed 18000 files.
E1028 13:57:01.193670 29822 convert_imageset.cpp:177] Processed 19000 files.
E1028 13:57:02.779388 29822 convert_imageset.cpp:177] Processed 20000 files.
E1028 13:57:04.549320 29822 convert_imageset.cpp:177] Processed 21000 files.
E1028 13:57:06.417134 29822 convert_imageset.cpp:177] Processed 22000 files.
E1028 13:57:08.104624 29822 convert_imageset.cpp:177] Processed 23000 files.
E1028 13:57:09.873734 29822 convert_imageset.cpp:177] Processed 24000 files.
E1028 13:57:11.583658 29822 convert_imageset.cpp:177] Processed 25000 files.
E1028 13:57:13.325891 29822 convert_imageset.cpp:177] Processed 26000 files.
E1028 13:57:14.960640 29822 convert_imageset.cpp:177] Processed 27000 files.
E1028 13:57:16.677510 29822 convert_imageset.cpp:177] Processed 28000 files.
E1028 13:57:18.393229 29822 convert_imageset.cpp:177] Processed 29000 files.
E1028 13:57:19.999385 29822 convert_imageset.cpp:177] Processed 30000 files.
E1028 13:57:21.600337 29822 convert_imageset.cpp:177] Processed 31000 files.
E1028 13:57:23.348117 29822 convert_imageset.cpp:177] Processed 32000 files.
E1028 13:57:24.956110 29822 convert_imageset.cpp:177] Processed 33000 files.
E1028 13:57:26.696871 29822 convert_imageset.cpp:177] Processed 34000 files.
E1028 13:57:28.188642 29822 convert_imageset.cpp:177] Processed 35000 files.
E1028 13:57:29.785262 29822 convert_imageset.cpp:177] Processed 36000 files.
E1028 13:57:31.283022 29822 convert_imageset.cpp:177] Processed 37000 files.
E1028 13:57:32.808639 29822 convert_imageset.cpp:177] Processed 38000 files.
E1028 13:57:34.408258 29822 convert_imageset.cpp:177] Processed 39000 files.
E1028 13:57:36.064585 29822 convert_imageset.cpp:177] Processed 40000 files.
E1028 13:57:37.667763 29822 convert_imageset.cpp:177] Processed 41000 files.
E1028 13:57:39.236681 29822 convert_imageset.cpp:177] Processed 42000 files.
E1028 13:57:40.821559 29822 convert_imageset.cpp:177] Processed 43000 files.
E1028 13:57:42.465908 29822 convert_imageset.cpp:177] Processed 44000 files.
E1028 13:57:44.035673 29822 convert_imageset.cpp:177] Processed 45000 files.
E1028 13:57:45.488814 29822 convert_imageset.cpp:177] Processed 46000 files.
E1028 13:57:46.954027 29822 convert_imageset.cpp:177] Processed 47000 files.
E1028 13:57:48.434619 29822 convert_imageset.cpp:177] Processed 48000 files.
E1028 13:57:50.018313 29822 convert_imageset.cpp:177] Processed 49000 files.
E1028 13:57:51.910315 29822 convert_imageset.cpp:177] Processed 50000 files.
E1028 13:57:53.756567 29822 convert_imageset.cpp:177] Processed 51000 files.
E1028 13:57:55.680557 29822 convert_imageset.cpp:177] Processed 52000 files.
E1028 13:57:57.148036 29822 convert_imageset.cpp:177] Processed 53000 files.
E1028 13:57:58.681663 29822 convert_imageset.cpp:177] Processed 54000 files.
E1028 13:58:00.175788 29822 convert_imageset.cpp:177] Processed 55000 files.
E1028 13:58:01.962321 29822 convert_imageset.cpp:177] Processed 56000 files.
E1028 13:58:03.621655 29822 convert_imageset.cpp:177] Processed 57000 files.
E1028 13:58:05.297029 29822 convert_imageset.cpp:177] Processed 58000 files.
E1028 13:58:06.879889 29822 convert_imageset.cpp:177] Processed 59000 files.
E1028 13:58:09.140254 29822 convert_imageset.cpp:177] Processed 60000 files.
I1028 13:58:09.343000 29979 caffe.cpp:99] Use GPU with device ID 0
I1028 13:58:09.682952 29979 caffe.cpp:107] Starting Optimization
I1028 13:58:09.683065 29979 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 415000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 13:58:09.683091 29979 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 13:58:09.691287 29979 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 13:58:09.691392 29979 net.cpp:67] Creating Layer mnist
I1028 13:58:09.691404 29979 net.cpp:356] mnist -> data
I1028 13:58:09.691422 29979 net.cpp:356] mnist -> label
I1028 13:58:09.691437 29979 net.cpp:96] Setting up mnist
I1028 13:58:09.697954 29979 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 13:58:09.698087 29979 data_layer.cpp:128] output data size: 64,1,50,180
I1028 13:58:09.699784 29979 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 13:58:09.699820 29979 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 13:58:09.699849 29979 net.cpp:67] Creating Layer conv1
I1028 13:58:09.699864 29979 net.cpp:394] conv1 <- data
I1028 13:58:09.699898 29979 net.cpp:356] conv1 -> conv1
I1028 13:58:09.699921 29979 net.cpp:96] Setting up conv1
I1028 13:58:09.700319 29979 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 13:58:09.700355 29979 net.cpp:67] Creating Layer pool1
I1028 13:58:09.700361 29979 net.cpp:394] pool1 <- conv1
I1028 13:58:09.700368 29979 net.cpp:356] pool1 -> pool1
I1028 13:58:09.700376 29979 net.cpp:96] Setting up pool1
I1028 13:58:09.700392 29979 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 13:58:09.700402 29979 net.cpp:67] Creating Layer relu1
I1028 13:58:09.700408 29979 net.cpp:394] relu1 <- pool1
I1028 13:58:09.700414 29979 net.cpp:345] relu1 -> pool1 (in-place)
I1028 13:58:09.700439 29979 net.cpp:96] Setting up relu1
I1028 13:58:09.700445 29979 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 13:58:09.700453 29979 net.cpp:67] Creating Layer drop1
I1028 13:58:09.700459 29979 net.cpp:394] drop1 <- pool1
I1028 13:58:09.700469 29979 net.cpp:345] drop1 -> pool1 (in-place)
I1028 13:58:09.700475 29979 net.cpp:96] Setting up drop1
I1028 13:58:09.700481 29979 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 13:58:09.700489 29979 net.cpp:67] Creating Layer conv2
I1028 13:58:09.700495 29979 net.cpp:394] conv2 <- pool1
I1028 13:58:09.700503 29979 net.cpp:356] conv2 -> conv2
I1028 13:58:09.700512 29979 net.cpp:96] Setting up conv2
I1028 13:58:09.701138 29979 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 13:58:09.701159 29979 net.cpp:67] Creating Layer pool2
I1028 13:58:09.701165 29979 net.cpp:394] pool2 <- conv2
I1028 13:58:09.701172 29979 net.cpp:356] pool2 -> pool2
I1028 13:58:09.701180 29979 net.cpp:96] Setting up pool2
I1028 13:58:09.701186 29979 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 13:58:09.701194 29979 net.cpp:67] Creating Layer relu2
I1028 13:58:09.701198 29979 net.cpp:394] relu2 <- pool2
I1028 13:58:09.701207 29979 net.cpp:345] relu2 -> pool2 (in-place)
I1028 13:58:09.701215 29979 net.cpp:96] Setting up relu2
I1028 13:58:09.701220 29979 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 13:58:09.701227 29979 net.cpp:67] Creating Layer drop2
I1028 13:58:09.701232 29979 net.cpp:394] drop2 <- pool2
I1028 13:58:09.701244 29979 net.cpp:345] drop2 -> pool2 (in-place)
I1028 13:58:09.701251 29979 net.cpp:96] Setting up drop2
I1028 13:58:09.701257 29979 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 13:58:09.701267 29979 net.cpp:67] Creating Layer conv3
I1028 13:58:09.701272 29979 net.cpp:394] conv3 <- pool2
I1028 13:58:09.701279 29979 net.cpp:356] conv3 -> conv3
I1028 13:58:09.701287 29979 net.cpp:96] Setting up conv3
I1028 13:58:09.704206 29979 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 13:58:09.704254 29979 net.cpp:67] Creating Layer pool3
I1028 13:58:09.704268 29979 net.cpp:394] pool3 <- conv3
I1028 13:58:09.704291 29979 net.cpp:356] pool3 -> pool3
I1028 13:58:09.704313 29979 net.cpp:96] Setting up pool3
I1028 13:58:09.704329 29979 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 13:58:09.704345 29979 net.cpp:67] Creating Layer relu3
I1028 13:58:09.704358 29979 net.cpp:394] relu3 <- pool3
I1028 13:58:09.704375 29979 net.cpp:345] relu3 -> pool3 (in-place)
I1028 13:58:09.704391 29979 net.cpp:96] Setting up relu3
I1028 13:58:09.704403 29979 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 13:58:09.704459 29979 net.cpp:67] Creating Layer drop3
I1028 13:58:09.704478 29979 net.cpp:394] drop3 <- pool3
I1028 13:58:09.704494 29979 net.cpp:345] drop3 -> pool3 (in-place)
I1028 13:58:09.704511 29979 net.cpp:96] Setting up drop3
I1028 13:58:09.704525 29979 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 13:58:09.704545 29979 net.cpp:67] Creating Layer ip1
I1028 13:58:09.704556 29979 net.cpp:394] ip1 <- pool3
I1028 13:58:09.704579 29979 net.cpp:356] ip1 -> ip1
I1028 13:58:09.704643 29979 net.cpp:96] Setting up ip1
I1028 13:58:10.131508 29979 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 13:58:10.131568 29979 net.cpp:67] Creating Layer relu4
I1028 13:58:10.131577 29979 net.cpp:394] relu4 <- ip1
I1028 13:58:10.131587 29979 net.cpp:345] relu4 -> ip1 (in-place)
I1028 13:58:10.131595 29979 net.cpp:96] Setting up relu4
I1028 13:58:10.131600 29979 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 13:58:10.131608 29979 net.cpp:67] Creating Layer drop4
I1028 13:58:10.131613 29979 net.cpp:394] drop4 <- ip1
I1028 13:58:10.131618 29979 net.cpp:345] drop4 -> ip1 (in-place)
I1028 13:58:10.131624 29979 net.cpp:96] Setting up drop4
I1028 13:58:10.131630 29979 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 13:58:10.131644 29979 net.cpp:67] Creating Layer ip2
I1028 13:58:10.131649 29979 net.cpp:394] ip2 <- ip1
I1028 13:58:10.131656 29979 net.cpp:356] ip2 -> ip2
I1028 13:58:10.131665 29979 net.cpp:96] Setting up ip2
I1028 13:58:10.140611 29979 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 13:58:10.140682 29979 net.cpp:67] Creating Layer loss
I1028 13:58:10.140691 29979 net.cpp:394] loss <- ip2
I1028 13:58:10.140698 29979 net.cpp:394] loss <- label
I1028 13:58:10.140705 29979 net.cpp:356] loss -> loss
I1028 13:58:10.140714 29979 net.cpp:96] Setting up loss
I1028 13:58:10.140727 29979 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 13:58:10.140732 29979 net.cpp:109]     with loss weight 1
I1028 13:58:10.140774 29979 net.cpp:170] loss needs backward computation.
I1028 13:58:10.140779 29979 net.cpp:170] ip2 needs backward computation.
I1028 13:58:10.140784 29979 net.cpp:170] drop4 needs backward computation.
I1028 13:58:10.140787 29979 net.cpp:170] relu4 needs backward computation.
I1028 13:58:10.140792 29979 net.cpp:170] ip1 needs backward computation.
I1028 13:58:10.140796 29979 net.cpp:170] drop3 needs backward computation.
I1028 13:58:10.140801 29979 net.cpp:170] relu3 needs backward computation.
I1028 13:58:10.140805 29979 net.cpp:170] pool3 needs backward computation.
I1028 13:58:10.140810 29979 net.cpp:170] conv3 needs backward computation.
I1028 13:58:10.140815 29979 net.cpp:170] drop2 needs backward computation.
I1028 13:58:10.140818 29979 net.cpp:170] relu2 needs backward computation.
I1028 13:58:10.140823 29979 net.cpp:170] pool2 needs backward computation.
I1028 13:58:10.140827 29979 net.cpp:170] conv2 needs backward computation.
I1028 13:58:10.140832 29979 net.cpp:170] drop1 needs backward computation.
I1028 13:58:10.140843 29979 net.cpp:170] relu1 needs backward computation.
I1028 13:58:10.140848 29979 net.cpp:170] pool1 needs backward computation.
I1028 13:58:10.140852 29979 net.cpp:170] conv1 needs backward computation.
I1028 13:58:10.140857 29979 net.cpp:172] mnist does not need backward computation.
I1028 13:58:10.140861 29979 net.cpp:208] This network produces output loss
I1028 13:58:10.140872 29979 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 13:58:10.140880 29979 net.cpp:219] Network initialization done.
I1028 13:58:10.140884 29979 net.cpp:220] Memory required for data: 119788292
I1028 13:58:10.140946 29979 solver.cpp:41] Solver scaffolding done.
I1028 13:58:10.140952 29979 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_410000.solverstate
I1028 13:58:10.140957 29979 solver.cpp:160] Solving Captcha
I1028 13:58:10.140976 29979 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_410000.solverstate
I1028 13:58:14.983412 29979 solver.cpp:502] SGDSolver: restoring history
I1028 13:58:15.713292 29979 solver.cpp:191] Iteration 410000, loss = 2.61333
I1028 13:58:15.713356 29979 solver.cpp:206]     Train net output #0: loss = 2.61333 (* 1 = 2.61333 loss)
I1028 13:58:15.713371 29979 solver.cpp:403] Iteration 410000, lr = 0.000606126
I1028 14:02:17.229755 29979 solver.cpp:191] Iteration 411000, loss = 2.41159
I1028 14:02:17.230716 29979 solver.cpp:206]     Train net output #0: loss = 2.41159 (* 1 = 2.41159 loss)
I1028 14:02:17.230749 29979 solver.cpp:403] Iteration 411000, lr = 0.000605046
I1028 14:06:18.350299 29979 solver.cpp:191] Iteration 412000, loss = 2.48061
I1028 14:06:18.350906 29979 solver.cpp:206]     Train net output #0: loss = 2.48061 (* 1 = 2.48061 loss)
I1028 14:06:18.350939 29979 solver.cpp:403] Iteration 412000, lr = 0.00060397
I1028 14:10:19.516451 29979 solver.cpp:191] Iteration 413000, loss = 2.68175
I1028 14:10:19.517105 29979 solver.cpp:206]     Train net output #0: loss = 2.68175 (* 1 = 2.68175 loss)
I1028 14:10:19.517138 29979 solver.cpp:403] Iteration 413000, lr = 0.000602899
I1028 14:14:20.628085 29979 solver.cpp:191] Iteration 414000, loss = 2.37465
I1028 14:14:20.628914 29979 solver.cpp:206]     Train net output #0: loss = 2.37465 (* 1 = 2.37465 loss)
I1028 14:14:20.628947 29979 solver.cpp:403] Iteration 414000, lr = 0.000601832
I1028 14:18:22.302580 29979 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_415000.caffemodel
I1028 14:18:27.210928 29979 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_415000.solverstate
I1028 14:18:31.850422 29979 solver.cpp:228] Iteration 415000, loss = 2.41968
I1028 14:18:31.851202 29979 solver.cpp:233] Optimization Done.
I1028 14:18:31.851227 29979 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 14:41:06.866035 11469 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 14:41:06.866137 11469 net.cpp:358] Input 0 -> data
I1028 14:41:06.866168 11469 net.cpp:67] Creating Layer conv1
I1028 14:41:06.866173 11469 net.cpp:394] conv1 <- data
I1028 14:41:06.866179 11469 net.cpp:356] conv1 -> conv1
I1028 14:41:06.866189 11469 net.cpp:96] Setting up conv1
I1028 14:41:06.866518 11469 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 14:41:06.866538 11469 net.cpp:67] Creating Layer pool1
I1028 14:41:06.866542 11469 net.cpp:394] pool1 <- conv1
I1028 14:41:06.866549 11469 net.cpp:356] pool1 -> pool1
I1028 14:41:06.866556 11469 net.cpp:96] Setting up pool1
I1028 14:41:06.866575 11469 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 14:41:06.866585 11469 net.cpp:67] Creating Layer relu1
I1028 14:41:06.866590 11469 net.cpp:394] relu1 <- pool1
I1028 14:41:06.866596 11469 net.cpp:345] relu1 -> pool1 (in-place)
I1028 14:41:06.866601 11469 net.cpp:96] Setting up relu1
I1028 14:41:06.866606 11469 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 14:41:06.866614 11469 net.cpp:67] Creating Layer drop1
I1028 14:41:06.866619 11469 net.cpp:394] drop1 <- pool1
I1028 14:41:06.866624 11469 net.cpp:345] drop1 -> pool1 (in-place)
I1028 14:41:06.866631 11469 net.cpp:96] Setting up drop1
I1028 14:41:06.866636 11469 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 14:41:06.866642 11469 net.cpp:67] Creating Layer conv2
I1028 14:41:06.866647 11469 net.cpp:394] conv2 <- pool1
I1028 14:41:06.866652 11469 net.cpp:356] conv2 -> conv2
I1028 14:41:06.866659 11469 net.cpp:96] Setting up conv2
I1028 14:41:06.867213 11469 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 14:41:06.867228 11469 net.cpp:67] Creating Layer pool2
I1028 14:41:06.867233 11469 net.cpp:394] pool2 <- conv2
I1028 14:41:06.867238 11469 net.cpp:356] pool2 -> pool2
I1028 14:41:06.867245 11469 net.cpp:96] Setting up pool2
I1028 14:41:06.867251 11469 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 14:41:06.867259 11469 net.cpp:67] Creating Layer relu2
I1028 14:41:06.867264 11469 net.cpp:394] relu2 <- pool2
I1028 14:41:06.867269 11469 net.cpp:345] relu2 -> pool2 (in-place)
I1028 14:41:06.867274 11469 net.cpp:96] Setting up relu2
I1028 14:41:06.867279 11469 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 14:41:06.867283 11469 net.cpp:67] Creating Layer drop2
I1028 14:41:06.867287 11469 net.cpp:394] drop2 <- pool2
I1028 14:41:06.867295 11469 net.cpp:345] drop2 -> pool2 (in-place)
I1028 14:41:06.867303 11469 net.cpp:96] Setting up drop2
I1028 14:41:06.867307 11469 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 14:41:06.867314 11469 net.cpp:67] Creating Layer conv3
I1028 14:41:06.867319 11469 net.cpp:394] conv3 <- pool2
I1028 14:41:06.867326 11469 net.cpp:356] conv3 -> conv3
I1028 14:41:06.867333 11469 net.cpp:96] Setting up conv3
I1028 14:41:06.868814 11469 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 14:41:06.868832 11469 net.cpp:67] Creating Layer pool3
I1028 14:41:06.868839 11469 net.cpp:394] pool3 <- conv3
I1028 14:41:06.868844 11469 net.cpp:356] pool3 -> pool3
I1028 14:41:06.868850 11469 net.cpp:96] Setting up pool3
I1028 14:41:06.868856 11469 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 14:41:06.868863 11469 net.cpp:67] Creating Layer relu3
I1028 14:41:06.868867 11469 net.cpp:394] relu3 <- pool3
I1028 14:41:06.868872 11469 net.cpp:345] relu3 -> pool3 (in-place)
I1028 14:41:06.868878 11469 net.cpp:96] Setting up relu3
I1028 14:41:06.868882 11469 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 14:41:06.868887 11469 net.cpp:67] Creating Layer drop3
I1028 14:41:06.868891 11469 net.cpp:394] drop3 <- pool3
I1028 14:41:06.868897 11469 net.cpp:345] drop3 -> pool3 (in-place)
I1028 14:41:06.868902 11469 net.cpp:96] Setting up drop3
I1028 14:41:06.868907 11469 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 14:41:06.868916 11469 net.cpp:67] Creating Layer ip1
I1028 14:41:06.868919 11469 net.cpp:394] ip1 <- pool3
I1028 14:41:06.868926 11469 net.cpp:356] ip1 -> ip1
I1028 14:41:06.868932 11469 net.cpp:96] Setting up ip1
I1028 14:41:07.384914 11469 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 14:41:07.384977 11469 net.cpp:67] Creating Layer relu4
I1028 14:41:07.384985 11469 net.cpp:394] relu4 <- ip1
I1028 14:41:07.384994 11469 net.cpp:345] relu4 -> ip1 (in-place)
I1028 14:41:07.385002 11469 net.cpp:96] Setting up relu4
I1028 14:41:07.385007 11469 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 14:41:07.385015 11469 net.cpp:67] Creating Layer drop4
I1028 14:41:07.385020 11469 net.cpp:394] drop4 <- ip1
I1028 14:41:07.385026 11469 net.cpp:345] drop4 -> ip1 (in-place)
I1028 14:41:07.385033 11469 net.cpp:96] Setting up drop4
I1028 14:41:07.385040 11469 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 14:41:07.385047 11469 net.cpp:67] Creating Layer ip2
I1028 14:41:07.385051 11469 net.cpp:394] ip2 <- ip1
I1028 14:41:07.385059 11469 net.cpp:356] ip2 -> ip2
I1028 14:41:07.385071 11469 net.cpp:96] Setting up ip2
I1028 14:41:07.393297 11469 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 14:41:07.393369 11469 net.cpp:67] Creating Layer prob
I1028 14:41:07.393376 11469 net.cpp:394] prob <- ip2
I1028 14:41:07.393385 11469 net.cpp:356] prob -> prob
I1028 14:41:07.393395 11469 net.cpp:96] Setting up prob
I1028 14:41:07.393401 11469 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 14:41:07.393405 11469 net.cpp:172] prob does not need backward computation.
I1028 14:41:07.393409 11469 net.cpp:172] ip2 does not need backward computation.
I1028 14:41:07.393414 11469 net.cpp:172] drop4 does not need backward computation.
I1028 14:41:07.393417 11469 net.cpp:172] relu4 does not need backward computation.
I1028 14:41:07.393420 11469 net.cpp:172] ip1 does not need backward computation.
I1028 14:41:07.393424 11469 net.cpp:172] drop3 does not need backward computation.
I1028 14:41:07.393427 11469 net.cpp:172] relu3 does not need backward computation.
I1028 14:41:07.393431 11469 net.cpp:172] pool3 does not need backward computation.
I1028 14:41:07.393435 11469 net.cpp:172] conv3 does not need backward computation.
I1028 14:41:07.393438 11469 net.cpp:172] drop2 does not need backward computation.
I1028 14:41:07.393442 11469 net.cpp:172] relu2 does not need backward computation.
I1028 14:41:07.393446 11469 net.cpp:172] pool2 does not need backward computation.
I1028 14:41:07.393450 11469 net.cpp:172] conv2 does not need backward computation.
I1028 14:41:07.393453 11469 net.cpp:172] drop1 does not need backward computation.
I1028 14:41:07.393457 11469 net.cpp:172] relu1 does not need backward computation.
I1028 14:41:07.393467 11469 net.cpp:172] pool1 does not need backward computation.
I1028 14:41:07.393472 11469 net.cpp:172] conv1 does not need backward computation.
I1028 14:41:07.393476 11469 net.cpp:208] This network produces output prob
I1028 14:41:07.393488 11469 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 14:41:07.393496 11469 net.cpp:219] Network initialization done.
I1028 14:41:07.393501 11469 net.cpp:220] Memory required for data: 1837200
I1028 15:13:04.831209 19065 convert_imageset.cpp:70] Shuffling data
I1028 15:13:05.433374 19065 convert_imageset.cpp:73] A total of 60000 images.
I1028 15:13:05.433459 19065 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 15:13:07.445190 19065 convert_imageset.cpp:177] Processed 1000 files.
E1028 15:13:09.364472 19065 convert_imageset.cpp:177] Processed 2000 files.
E1028 15:13:11.267391 19065 convert_imageset.cpp:177] Processed 3000 files.
E1028 15:13:13.393034 19065 convert_imageset.cpp:177] Processed 4000 files.
E1028 15:13:15.479846 19065 convert_imageset.cpp:177] Processed 5000 files.
E1028 15:13:17.257905 19065 convert_imageset.cpp:177] Processed 6000 files.
E1028 15:13:18.999459 19065 convert_imageset.cpp:177] Processed 7000 files.
E1028 15:13:20.722339 19065 convert_imageset.cpp:177] Processed 8000 files.
E1028 15:13:22.404568 19065 convert_imageset.cpp:177] Processed 9000 files.
E1028 15:13:24.234400 19065 convert_imageset.cpp:177] Processed 10000 files.
E1028 15:13:26.113956 19065 convert_imageset.cpp:177] Processed 11000 files.
E1028 15:13:27.829264 19065 convert_imageset.cpp:177] Processed 12000 files.
E1028 15:13:29.527746 19065 convert_imageset.cpp:177] Processed 13000 files.
E1028 15:13:31.206400 19065 convert_imageset.cpp:177] Processed 14000 files.
E1028 15:13:32.840533 19065 convert_imageset.cpp:177] Processed 15000 files.
E1028 15:13:34.449755 19065 convert_imageset.cpp:177] Processed 16000 files.
E1028 15:13:36.226312 19065 convert_imageset.cpp:177] Processed 17000 files.
E1028 15:13:37.894971 19065 convert_imageset.cpp:177] Processed 18000 files.
E1028 15:13:39.782850 19065 convert_imageset.cpp:177] Processed 19000 files.
E1028 15:13:41.369060 19065 convert_imageset.cpp:177] Processed 20000 files.
E1028 15:13:43.061189 19065 convert_imageset.cpp:177] Processed 21000 files.
E1028 15:13:44.675712 19065 convert_imageset.cpp:177] Processed 22000 files.
E1028 15:13:46.363256 19065 convert_imageset.cpp:177] Processed 23000 files.
E1028 15:13:47.982897 19065 convert_imageset.cpp:177] Processed 24000 files.
E1028 15:13:49.960144 19065 convert_imageset.cpp:177] Processed 25000 files.
E1028 15:13:51.636167 19065 convert_imageset.cpp:177] Processed 26000 files.
E1028 15:13:53.282845 19065 convert_imageset.cpp:177] Processed 27000 files.
E1028 15:13:54.868317 19065 convert_imageset.cpp:177] Processed 28000 files.
E1028 15:13:56.474931 19065 convert_imageset.cpp:177] Processed 29000 files.
E1028 15:13:58.097939 19065 convert_imageset.cpp:177] Processed 30000 files.
E1028 15:13:59.725746 19065 convert_imageset.cpp:177] Processed 31000 files.
E1028 15:14:01.406400 19065 convert_imageset.cpp:177] Processed 32000 files.
E1028 15:14:02.986810 19065 convert_imageset.cpp:177] Processed 33000 files.
E1028 15:14:04.577518 19065 convert_imageset.cpp:177] Processed 34000 files.
E1028 15:14:06.065093 19065 convert_imageset.cpp:177] Processed 35000 files.
E1028 15:14:07.590184 19065 convert_imageset.cpp:177] Processed 36000 files.
E1028 15:14:09.115617 19065 convert_imageset.cpp:177] Processed 37000 files.
E1028 15:14:10.613888 19065 convert_imageset.cpp:177] Processed 38000 files.
E1028 15:14:12.180865 19065 convert_imageset.cpp:177] Processed 39000 files.
E1028 15:14:13.890167 19065 convert_imageset.cpp:177] Processed 40000 files.
E1028 15:14:15.393596 19065 convert_imageset.cpp:177] Processed 41000 files.
E1028 15:14:16.873821 19065 convert_imageset.cpp:177] Processed 42000 files.
E1028 15:14:18.341053 19065 convert_imageset.cpp:177] Processed 43000 files.
E1028 15:14:19.812002 19065 convert_imageset.cpp:177] Processed 44000 files.
E1028 15:14:21.438591 19065 convert_imageset.cpp:177] Processed 45000 files.
E1028 15:14:22.927058 19065 convert_imageset.cpp:177] Processed 46000 files.
E1028 15:14:24.394759 19065 convert_imageset.cpp:177] Processed 47000 files.
E1028 15:14:25.928725 19065 convert_imageset.cpp:177] Processed 48000 files.
E1028 15:14:27.439959 19065 convert_imageset.cpp:177] Processed 49000 files.
E1028 15:14:29.032896 19065 convert_imageset.cpp:177] Processed 50000 files.
E1028 15:14:30.759685 19065 convert_imageset.cpp:177] Processed 51000 files.
E1028 15:14:32.380987 19065 convert_imageset.cpp:177] Processed 52000 files.
E1028 15:14:33.813314 19065 convert_imageset.cpp:177] Processed 53000 files.
E1028 15:14:35.363905 19065 convert_imageset.cpp:177] Processed 54000 files.
E1028 15:14:36.877250 19065 convert_imageset.cpp:177] Processed 55000 files.
E1028 15:14:38.335217 19065 convert_imageset.cpp:177] Processed 56000 files.
E1028 15:14:39.786425 19065 convert_imageset.cpp:177] Processed 57000 files.
E1028 15:14:41.318981 19065 convert_imageset.cpp:177] Processed 58000 files.
E1028 15:14:42.823775 19065 convert_imageset.cpp:177] Processed 59000 files.
E1028 15:14:44.337731 19065 convert_imageset.cpp:177] Processed 60000 files.
I1028 15:14:44.497874 19166 caffe.cpp:99] Use GPU with device ID 0
I1028 15:14:44.848649 19166 caffe.cpp:107] Starting Optimization
I1028 15:14:44.848774 19166 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 420000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 15:14:44.848800 19166 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 15:14:44.851572 19166 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 15:14:44.851714 19166 net.cpp:67] Creating Layer mnist
I1028 15:14:44.851732 19166 net.cpp:356] mnist -> data
I1028 15:14:44.851755 19166 net.cpp:356] mnist -> label
I1028 15:14:44.851776 19166 net.cpp:96] Setting up mnist
I1028 15:14:44.859694 19166 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 15:14:44.859794 19166 data_layer.cpp:128] output data size: 64,1,50,180
I1028 15:14:44.860924 19166 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 15:14:44.860952 19166 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 15:14:44.860973 19166 net.cpp:67] Creating Layer conv1
I1028 15:14:44.860982 19166 net.cpp:394] conv1 <- data
I1028 15:14:44.861002 19166 net.cpp:356] conv1 -> conv1
I1028 15:14:44.861022 19166 net.cpp:96] Setting up conv1
I1028 15:14:44.861392 19166 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 15:14:44.861425 19166 net.cpp:67] Creating Layer pool1
I1028 15:14:44.861431 19166 net.cpp:394] pool1 <- conv1
I1028 15:14:44.861438 19166 net.cpp:356] pool1 -> pool1
I1028 15:14:44.861446 19166 net.cpp:96] Setting up pool1
I1028 15:14:44.861464 19166 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 15:14:44.861471 19166 net.cpp:67] Creating Layer relu1
I1028 15:14:44.861476 19166 net.cpp:394] relu1 <- pool1
I1028 15:14:44.861482 19166 net.cpp:345] relu1 -> pool1 (in-place)
I1028 15:14:44.861490 19166 net.cpp:96] Setting up relu1
I1028 15:14:44.861495 19166 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 15:14:44.861501 19166 net.cpp:67] Creating Layer drop1
I1028 15:14:44.861506 19166 net.cpp:394] drop1 <- pool1
I1028 15:14:44.861515 19166 net.cpp:345] drop1 -> pool1 (in-place)
I1028 15:14:44.861521 19166 net.cpp:96] Setting up drop1
I1028 15:14:44.861527 19166 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 15:14:44.861534 19166 net.cpp:67] Creating Layer conv2
I1028 15:14:44.861539 19166 net.cpp:394] conv2 <- pool1
I1028 15:14:44.861547 19166 net.cpp:356] conv2 -> conv2
I1028 15:14:44.861556 19166 net.cpp:96] Setting up conv2
I1028 15:14:44.862128 19166 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 15:14:44.862148 19166 net.cpp:67] Creating Layer pool2
I1028 15:14:44.862154 19166 net.cpp:394] pool2 <- conv2
I1028 15:14:44.862160 19166 net.cpp:356] pool2 -> pool2
I1028 15:14:44.862167 19166 net.cpp:96] Setting up pool2
I1028 15:14:44.862174 19166 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 15:14:44.862179 19166 net.cpp:67] Creating Layer relu2
I1028 15:14:44.862185 19166 net.cpp:394] relu2 <- pool2
I1028 15:14:44.862192 19166 net.cpp:345] relu2 -> pool2 (in-place)
I1028 15:14:44.862200 19166 net.cpp:96] Setting up relu2
I1028 15:14:44.862203 19166 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 15:14:44.862211 19166 net.cpp:67] Creating Layer drop2
I1028 15:14:44.862216 19166 net.cpp:394] drop2 <- pool2
I1028 15:14:44.862222 19166 net.cpp:345] drop2 -> pool2 (in-place)
I1028 15:14:44.862228 19166 net.cpp:96] Setting up drop2
I1028 15:14:44.862233 19166 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 15:14:44.862248 19166 net.cpp:67] Creating Layer conv3
I1028 15:14:44.862253 19166 net.cpp:394] conv3 <- pool2
I1028 15:14:44.862260 19166 net.cpp:356] conv3 -> conv3
I1028 15:14:44.862267 19166 net.cpp:96] Setting up conv3
I1028 15:14:44.864269 19166 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 15:14:44.864303 19166 net.cpp:67] Creating Layer pool3
I1028 15:14:44.864312 19166 net.cpp:394] pool3 <- conv3
I1028 15:14:44.864326 19166 net.cpp:356] pool3 -> pool3
I1028 15:14:44.864339 19166 net.cpp:96] Setting up pool3
I1028 15:14:44.864351 19166 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 15:14:44.864361 19166 net.cpp:67] Creating Layer relu3
I1028 15:14:44.864368 19166 net.cpp:394] relu3 <- pool3
I1028 15:14:44.864377 19166 net.cpp:345] relu3 -> pool3 (in-place)
I1028 15:14:44.864390 19166 net.cpp:96] Setting up relu3
I1028 15:14:44.864398 19166 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 15:14:44.864409 19166 net.cpp:67] Creating Layer drop3
I1028 15:14:44.864433 19166 net.cpp:394] drop3 <- pool3
I1028 15:14:44.864444 19166 net.cpp:345] drop3 -> pool3 (in-place)
I1028 15:14:44.864455 19166 net.cpp:96] Setting up drop3
I1028 15:14:44.864464 19166 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 15:14:44.864475 19166 net.cpp:67] Creating Layer ip1
I1028 15:14:44.864483 19166 net.cpp:394] ip1 <- pool3
I1028 15:14:44.864496 19166 net.cpp:356] ip1 -> ip1
I1028 15:14:44.864544 19166 net.cpp:96] Setting up ip1
I1028 15:14:45.387327 19166 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 15:14:45.387389 19166 net.cpp:67] Creating Layer relu4
I1028 15:14:45.387398 19166 net.cpp:394] relu4 <- ip1
I1028 15:14:45.387408 19166 net.cpp:345] relu4 -> ip1 (in-place)
I1028 15:14:45.387416 19166 net.cpp:96] Setting up relu4
I1028 15:14:45.387421 19166 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 15:14:45.387428 19166 net.cpp:67] Creating Layer drop4
I1028 15:14:45.387433 19166 net.cpp:394] drop4 <- ip1
I1028 15:14:45.387439 19166 net.cpp:345] drop4 -> ip1 (in-place)
I1028 15:14:45.387445 19166 net.cpp:96] Setting up drop4
I1028 15:14:45.387450 19166 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 15:14:45.387462 19166 net.cpp:67] Creating Layer ip2
I1028 15:14:45.387467 19166 net.cpp:394] ip2 <- ip1
I1028 15:14:45.387475 19166 net.cpp:356] ip2 -> ip2
I1028 15:14:45.387485 19166 net.cpp:96] Setting up ip2
I1028 15:14:45.396379 19166 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 15:14:45.396471 19166 net.cpp:67] Creating Layer loss
I1028 15:14:45.396481 19166 net.cpp:394] loss <- ip2
I1028 15:14:45.396488 19166 net.cpp:394] loss <- label
I1028 15:14:45.396495 19166 net.cpp:356] loss -> loss
I1028 15:14:45.396505 19166 net.cpp:96] Setting up loss
I1028 15:14:45.396517 19166 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 15:14:45.396523 19166 net.cpp:109]     with loss weight 1
I1028 15:14:45.396565 19166 net.cpp:170] loss needs backward computation.
I1028 15:14:45.396570 19166 net.cpp:170] ip2 needs backward computation.
I1028 15:14:45.396575 19166 net.cpp:170] drop4 needs backward computation.
I1028 15:14:45.396579 19166 net.cpp:170] relu4 needs backward computation.
I1028 15:14:45.396584 19166 net.cpp:170] ip1 needs backward computation.
I1028 15:14:45.396589 19166 net.cpp:170] drop3 needs backward computation.
I1028 15:14:45.396594 19166 net.cpp:170] relu3 needs backward computation.
I1028 15:14:45.396597 19166 net.cpp:170] pool3 needs backward computation.
I1028 15:14:45.396602 19166 net.cpp:170] conv3 needs backward computation.
I1028 15:14:45.396607 19166 net.cpp:170] drop2 needs backward computation.
I1028 15:14:45.396611 19166 net.cpp:170] relu2 needs backward computation.
I1028 15:14:45.396616 19166 net.cpp:170] pool2 needs backward computation.
I1028 15:14:45.396620 19166 net.cpp:170] conv2 needs backward computation.
I1028 15:14:45.396625 19166 net.cpp:170] drop1 needs backward computation.
I1028 15:14:45.396631 19166 net.cpp:170] relu1 needs backward computation.
I1028 15:14:45.396634 19166 net.cpp:170] pool1 needs backward computation.
I1028 15:14:45.396638 19166 net.cpp:170] conv1 needs backward computation.
I1028 15:14:45.396651 19166 net.cpp:172] mnist does not need backward computation.
I1028 15:14:45.396656 19166 net.cpp:208] This network produces output loss
I1028 15:14:45.396667 19166 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 15:14:45.396673 19166 net.cpp:219] Network initialization done.
I1028 15:14:45.396678 19166 net.cpp:220] Memory required for data: 119788292
I1028 15:14:45.396738 19166 solver.cpp:41] Solver scaffolding done.
I1028 15:14:45.396744 19166 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_415000.solverstate
I1028 15:14:45.396749 19166 solver.cpp:160] Solving Captcha
I1028 15:14:45.396767 19166 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_415000.solverstate
I1028 15:14:48.607925 19166 solver.cpp:502] SGDSolver: restoring history
I1028 15:14:49.350858 19166 solver.cpp:191] Iteration 415000, loss = 2.3465
I1028 15:14:49.350924 19166 solver.cpp:206]     Train net output #0: loss = 2.3465 (* 1 = 2.3465 loss)
I1028 15:14:49.350941 19166 solver.cpp:403] Iteration 415000, lr = 0.00060077
I1028 15:18:50.852694 19166 solver.cpp:191] Iteration 416000, loss = 2.33458
I1028 15:18:50.853656 19166 solver.cpp:206]     Train net output #0: loss = 2.33458 (* 1 = 2.33458 loss)
I1028 15:18:50.853689 19166 solver.cpp:403] Iteration 416000, lr = 0.000599712
I1028 15:22:51.846714 19166 solver.cpp:191] Iteration 417000, loss = 2.43313
I1028 15:22:51.847476 19166 solver.cpp:206]     Train net output #0: loss = 2.43313 (* 1 = 2.43313 loss)
I1028 15:22:51.847508 19166 solver.cpp:403] Iteration 417000, lr = 0.000598658
I1028 15:26:52.855669 19166 solver.cpp:191] Iteration 418000, loss = 2.45143
I1028 15:26:52.856490 19166 solver.cpp:206]     Train net output #0: loss = 2.45143 (* 1 = 2.45143 loss)
I1028 15:26:52.856523 19166 solver.cpp:403] Iteration 418000, lr = 0.000597609
I1028 15:30:53.839717 19166 solver.cpp:191] Iteration 419000, loss = 2.50953
I1028 15:30:53.840354 19166 solver.cpp:206]     Train net output #0: loss = 2.50953 (* 1 = 2.50953 loss)
I1028 15:30:53.840386 19166 solver.cpp:403] Iteration 419000, lr = 0.000596564
I1028 15:34:55.399794 19166 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_420000.caffemodel
I1028 15:35:00.405839 19166 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_420000.solverstate
I1028 15:35:04.196954 19166 solver.cpp:228] Iteration 420000, loss = 2.50863
I1028 15:35:04.197664 19166 solver.cpp:233] Optimization Done.
I1028 15:35:04.197690 19166 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 15:57:31.527369   424 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 15:57:31.527477   424 net.cpp:358] Input 0 -> data
I1028 15:57:31.527501   424 net.cpp:67] Creating Layer conv1
I1028 15:57:31.527506   424 net.cpp:394] conv1 <- data
I1028 15:57:31.527513   424 net.cpp:356] conv1 -> conv1
I1028 15:57:31.527523   424 net.cpp:96] Setting up conv1
I1028 15:57:31.527845   424 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 15:57:31.527868   424 net.cpp:67] Creating Layer pool1
I1028 15:57:31.527873   424 net.cpp:394] pool1 <- conv1
I1028 15:57:31.527878   424 net.cpp:356] pool1 -> pool1
I1028 15:57:31.527885   424 net.cpp:96] Setting up pool1
I1028 15:57:31.527899   424 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 15:57:31.527907   424 net.cpp:67] Creating Layer relu1
I1028 15:57:31.527911   424 net.cpp:394] relu1 <- pool1
I1028 15:57:31.527916   424 net.cpp:345] relu1 -> pool1 (in-place)
I1028 15:57:31.527921   424 net.cpp:96] Setting up relu1
I1028 15:57:31.527926   424 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 15:57:31.527932   424 net.cpp:67] Creating Layer drop1
I1028 15:57:31.527936   424 net.cpp:394] drop1 <- pool1
I1028 15:57:31.527943   424 net.cpp:345] drop1 -> pool1 (in-place)
I1028 15:57:31.527950   424 net.cpp:96] Setting up drop1
I1028 15:57:31.527954   424 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 15:57:31.527961   424 net.cpp:67] Creating Layer conv2
I1028 15:57:31.527966   424 net.cpp:394] conv2 <- pool1
I1028 15:57:31.527973   424 net.cpp:356] conv2 -> conv2
I1028 15:57:31.527981   424 net.cpp:96] Setting up conv2
I1028 15:57:31.528558   424 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 15:57:31.528576   424 net.cpp:67] Creating Layer pool2
I1028 15:57:31.528581   424 net.cpp:394] pool2 <- conv2
I1028 15:57:31.528587   424 net.cpp:356] pool2 -> pool2
I1028 15:57:31.528594   424 net.cpp:96] Setting up pool2
I1028 15:57:31.528599   424 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 15:57:31.528605   424 net.cpp:67] Creating Layer relu2
I1028 15:57:31.528609   424 net.cpp:394] relu2 <- pool2
I1028 15:57:31.528616   424 net.cpp:345] relu2 -> pool2 (in-place)
I1028 15:57:31.528622   424 net.cpp:96] Setting up relu2
I1028 15:57:31.528626   424 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 15:57:31.528631   424 net.cpp:67] Creating Layer drop2
I1028 15:57:31.528635   424 net.cpp:394] drop2 <- pool2
I1028 15:57:31.528642   424 net.cpp:345] drop2 -> pool2 (in-place)
I1028 15:57:31.528648   424 net.cpp:96] Setting up drop2
I1028 15:57:31.528653   424 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 15:57:31.528661   424 net.cpp:67] Creating Layer conv3
I1028 15:57:31.528666   424 net.cpp:394] conv3 <- pool2
I1028 15:57:31.528672   424 net.cpp:356] conv3 -> conv3
I1028 15:57:31.528679   424 net.cpp:96] Setting up conv3
I1028 15:57:31.530128   424 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 15:57:31.530144   424 net.cpp:67] Creating Layer pool3
I1028 15:57:31.530149   424 net.cpp:394] pool3 <- conv3
I1028 15:57:31.530158   424 net.cpp:356] pool3 -> pool3
I1028 15:57:31.530164   424 net.cpp:96] Setting up pool3
I1028 15:57:31.530169   424 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 15:57:31.530175   424 net.cpp:67] Creating Layer relu3
I1028 15:57:31.530179   424 net.cpp:394] relu3 <- pool3
I1028 15:57:31.530185   424 net.cpp:345] relu3 -> pool3 (in-place)
I1028 15:57:31.530191   424 net.cpp:96] Setting up relu3
I1028 15:57:31.530195   424 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 15:57:31.530201   424 net.cpp:67] Creating Layer drop3
I1028 15:57:31.530205   424 net.cpp:394] drop3 <- pool3
I1028 15:57:31.530210   424 net.cpp:345] drop3 -> pool3 (in-place)
I1028 15:57:31.530215   424 net.cpp:96] Setting up drop3
I1028 15:57:31.530220   424 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 15:57:31.530226   424 net.cpp:67] Creating Layer ip1
I1028 15:57:31.530230   424 net.cpp:394] ip1 <- pool3
I1028 15:57:31.530237   424 net.cpp:356] ip1 -> ip1
I1028 15:57:31.530246   424 net.cpp:96] Setting up ip1
I1028 15:57:31.988729   424 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 15:57:31.988792   424 net.cpp:67] Creating Layer relu4
I1028 15:57:31.988801   424 net.cpp:394] relu4 <- ip1
I1028 15:57:31.988809   424 net.cpp:345] relu4 -> ip1 (in-place)
I1028 15:57:31.988819   424 net.cpp:96] Setting up relu4
I1028 15:57:31.988824   424 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 15:57:31.988831   424 net.cpp:67] Creating Layer drop4
I1028 15:57:31.988836   424 net.cpp:394] drop4 <- ip1
I1028 15:57:31.988840   424 net.cpp:345] drop4 -> ip1 (in-place)
I1028 15:57:31.988847   424 net.cpp:96] Setting up drop4
I1028 15:57:31.988852   424 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 15:57:31.988862   424 net.cpp:67] Creating Layer ip2
I1028 15:57:31.988867   424 net.cpp:394] ip2 <- ip1
I1028 15:57:31.988873   424 net.cpp:356] ip2 -> ip2
I1028 15:57:31.988884   424 net.cpp:96] Setting up ip2
I1028 15:57:31.998057   424 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 15:57:31.998129   424 net.cpp:67] Creating Layer prob
I1028 15:57:31.998137   424 net.cpp:394] prob <- ip2
I1028 15:57:31.998145   424 net.cpp:356] prob -> prob
I1028 15:57:31.998155   424 net.cpp:96] Setting up prob
I1028 15:57:31.998162   424 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 15:57:31.998167   424 net.cpp:172] prob does not need backward computation.
I1028 15:57:31.998170   424 net.cpp:172] ip2 does not need backward computation.
I1028 15:57:31.998174   424 net.cpp:172] drop4 does not need backward computation.
I1028 15:57:31.998178   424 net.cpp:172] relu4 does not need backward computation.
I1028 15:57:31.998181   424 net.cpp:172] ip1 does not need backward computation.
I1028 15:57:31.998185   424 net.cpp:172] drop3 does not need backward computation.
I1028 15:57:31.998188   424 net.cpp:172] relu3 does not need backward computation.
I1028 15:57:31.998193   424 net.cpp:172] pool3 does not need backward computation.
I1028 15:57:31.998196   424 net.cpp:172] conv3 does not need backward computation.
I1028 15:57:31.998199   424 net.cpp:172] drop2 does not need backward computation.
I1028 15:57:31.998203   424 net.cpp:172] relu2 does not need backward computation.
I1028 15:57:31.998208   424 net.cpp:172] pool2 does not need backward computation.
I1028 15:57:31.998210   424 net.cpp:172] conv2 does not need backward computation.
I1028 15:57:31.998214   424 net.cpp:172] drop1 does not need backward computation.
I1028 15:57:31.998217   424 net.cpp:172] relu1 does not need backward computation.
I1028 15:57:31.998221   424 net.cpp:172] pool1 does not need backward computation.
I1028 15:57:31.998225   424 net.cpp:172] conv1 does not need backward computation.
I1028 15:57:31.998229   424 net.cpp:208] This network produces output prob
I1028 15:57:31.998248   424 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 15:57:31.998256   424 net.cpp:219] Network initialization done.
I1028 15:57:31.998260   424 net.cpp:220] Memory required for data: 1837200
I1028 16:29:58.735198  8322 convert_imageset.cpp:70] Shuffling data
I1028 16:29:59.320914  8322 convert_imageset.cpp:73] A total of 60000 images.
I1028 16:29:59.320994  8322 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 16:30:01.526641  8322 convert_imageset.cpp:177] Processed 1000 files.
E1028 16:30:03.643671  8322 convert_imageset.cpp:177] Processed 2000 files.
E1028 16:30:05.756600  8322 convert_imageset.cpp:177] Processed 3000 files.
E1028 16:30:07.968117  8322 convert_imageset.cpp:177] Processed 4000 files.
E1028 16:30:09.755578  8322 convert_imageset.cpp:177] Processed 5000 files.
E1028 16:30:11.700919  8322 convert_imageset.cpp:177] Processed 6000 files.
E1028 16:30:13.499644  8322 convert_imageset.cpp:177] Processed 7000 files.
E1028 16:30:15.250095  8322 convert_imageset.cpp:177] Processed 8000 files.
E1028 16:30:17.038055  8322 convert_imageset.cpp:177] Processed 9000 files.
E1028 16:30:18.729481  8322 convert_imageset.cpp:177] Processed 10000 files.
E1028 16:30:20.463182  8322 convert_imageset.cpp:177] Processed 11000 files.
E1028 16:30:22.265969  8322 convert_imageset.cpp:177] Processed 12000 files.
E1028 16:30:23.998464  8322 convert_imageset.cpp:177] Processed 13000 files.
E1028 16:30:25.893517  8322 convert_imageset.cpp:177] Processed 14000 files.
E1028 16:30:27.625288  8322 convert_imageset.cpp:177] Processed 15000 files.
E1028 16:30:29.315547  8322 convert_imageset.cpp:177] Processed 16000 files.
E1028 16:30:31.152508  8322 convert_imageset.cpp:177] Processed 17000 files.
E1028 16:30:32.869632  8322 convert_imageset.cpp:177] Processed 18000 files.
E1028 16:30:34.586397  8322 convert_imageset.cpp:177] Processed 19000 files.
E1028 16:30:36.196141  8322 convert_imageset.cpp:177] Processed 20000 files.
E1028 16:30:37.772411  8322 convert_imageset.cpp:177] Processed 21000 files.
E1028 16:30:39.357244  8322 convert_imageset.cpp:177] Processed 22000 files.
E1028 16:30:41.080698  8322 convert_imageset.cpp:177] Processed 23000 files.
E1028 16:30:42.611770  8322 convert_imageset.cpp:177] Processed 24000 files.
E1028 16:30:44.159255  8322 convert_imageset.cpp:177] Processed 25000 files.
E1028 16:30:45.701691  8322 convert_imageset.cpp:177] Processed 26000 files.
E1028 16:30:47.244406  8322 convert_imageset.cpp:177] Processed 27000 files.
E1028 16:30:48.863526  8322 convert_imageset.cpp:177] Processed 28000 files.
E1028 16:30:50.398574  8322 convert_imageset.cpp:177] Processed 29000 files.
E1028 16:30:51.951980  8322 convert_imageset.cpp:177] Processed 30000 files.
E1028 16:30:53.453054  8322 convert_imageset.cpp:177] Processed 31000 files.
E1028 16:30:55.019546  8322 convert_imageset.cpp:177] Processed 32000 files.
E1028 16:30:56.606564  8322 convert_imageset.cpp:177] Processed 33000 files.
E1028 16:30:58.117741  8322 convert_imageset.cpp:177] Processed 34000 files.
E1028 16:30:59.645577  8322 convert_imageset.cpp:177] Processed 35000 files.
E1028 16:31:01.207759  8322 convert_imageset.cpp:177] Processed 36000 files.
E1028 16:31:02.832083  8322 convert_imageset.cpp:177] Processed 37000 files.
E1028 16:31:04.350625  8322 convert_imageset.cpp:177] Processed 38000 files.
E1028 16:31:05.878903  8322 convert_imageset.cpp:177] Processed 39000 files.
E1028 16:31:07.455654  8322 convert_imageset.cpp:177] Processed 40000 files.
E1028 16:31:09.321162  8322 convert_imageset.cpp:177] Processed 41000 files.
E1028 16:31:10.921115  8322 convert_imageset.cpp:177] Processed 42000 files.
E1028 16:31:12.462766  8322 convert_imageset.cpp:177] Processed 43000 files.
E1028 16:31:14.033059  8322 convert_imageset.cpp:177] Processed 44000 files.
E1028 16:31:15.506750  8322 convert_imageset.cpp:177] Processed 45000 files.
E1028 16:31:17.123316  8322 convert_imageset.cpp:177] Processed 46000 files.
E1028 16:31:18.636207  8322 convert_imageset.cpp:177] Processed 47000 files.
E1028 16:31:20.223018  8322 convert_imageset.cpp:177] Processed 48000 files.
E1028 16:31:21.727810  8322 convert_imageset.cpp:177] Processed 49000 files.
E1028 16:31:23.376767  8322 convert_imageset.cpp:177] Processed 50000 files.
E1028 16:31:25.050335  8322 convert_imageset.cpp:177] Processed 51000 files.
E1028 16:31:26.499783  8322 convert_imageset.cpp:177] Processed 52000 files.
E1028 16:31:28.036337  8322 convert_imageset.cpp:177] Processed 53000 files.
E1028 16:31:29.486191  8322 convert_imageset.cpp:177] Processed 54000 files.
E1028 16:31:30.994088  8322 convert_imageset.cpp:177] Processed 55000 files.
E1028 16:31:32.586035  8322 convert_imageset.cpp:177] Processed 56000 files.
E1028 16:31:34.124743  8322 convert_imageset.cpp:177] Processed 57000 files.
E1028 16:31:35.624564  8322 convert_imageset.cpp:177] Processed 58000 files.
E1028 16:31:37.199517  8322 convert_imageset.cpp:177] Processed 59000 files.
E1028 16:31:38.640043  8322 convert_imageset.cpp:177] Processed 60000 files.
I1028 16:31:38.878418  8487 caffe.cpp:99] Use GPU with device ID 0
I1028 16:31:39.227141  8487 caffe.cpp:107] Starting Optimization
I1028 16:31:39.227272  8487 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 425000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 16:31:39.227298  8487 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 16:31:39.242756  8487 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 16:31:39.242861  8487 net.cpp:67] Creating Layer mnist
I1028 16:31:39.242872  8487 net.cpp:356] mnist -> data
I1028 16:31:39.242892  8487 net.cpp:356] mnist -> label
I1028 16:31:39.242905  8487 net.cpp:96] Setting up mnist
I1028 16:31:39.255512  8487 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 16:31:39.255614  8487 data_layer.cpp:128] output data size: 64,1,50,180
I1028 16:31:39.256791  8487 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 16:31:39.256822  8487 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 16:31:39.256844  8487 net.cpp:67] Creating Layer conv1
I1028 16:31:39.256852  8487 net.cpp:394] conv1 <- data
I1028 16:31:39.256875  8487 net.cpp:356] conv1 -> conv1
I1028 16:31:39.256894  8487 net.cpp:96] Setting up conv1
I1028 16:31:39.257530  8487 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 16:31:39.257575  8487 net.cpp:67] Creating Layer pool1
I1028 16:31:39.257586  8487 net.cpp:394] pool1 <- conv1
I1028 16:31:39.257601  8487 net.cpp:356] pool1 -> pool1
I1028 16:31:39.257614  8487 net.cpp:96] Setting up pool1
I1028 16:31:39.257637  8487 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 16:31:39.257649  8487 net.cpp:67] Creating Layer relu1
I1028 16:31:39.257658  8487 net.cpp:394] relu1 <- pool1
I1028 16:31:39.257668  8487 net.cpp:345] relu1 -> pool1 (in-place)
I1028 16:31:39.257679  8487 net.cpp:96] Setting up relu1
I1028 16:31:39.257688  8487 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 16:31:39.257700  8487 net.cpp:67] Creating Layer drop1
I1028 16:31:39.257709  8487 net.cpp:394] drop1 <- pool1
I1028 16:31:39.257722  8487 net.cpp:345] drop1 -> pool1 (in-place)
I1028 16:31:39.257735  8487 net.cpp:96] Setting up drop1
I1028 16:31:39.257743  8487 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 16:31:39.257756  8487 net.cpp:67] Creating Layer conv2
I1028 16:31:39.257764  8487 net.cpp:394] conv2 <- pool1
I1028 16:31:39.257779  8487 net.cpp:356] conv2 -> conv2
I1028 16:31:39.257793  8487 net.cpp:96] Setting up conv2
I1028 16:31:39.258792  8487 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 16:31:39.258822  8487 net.cpp:67] Creating Layer pool2
I1028 16:31:39.258832  8487 net.cpp:394] pool2 <- conv2
I1028 16:31:39.258844  8487 net.cpp:356] pool2 -> pool2
I1028 16:31:39.258857  8487 net.cpp:96] Setting up pool2
I1028 16:31:39.258867  8487 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 16:31:39.258877  8487 net.cpp:67] Creating Layer relu2
I1028 16:31:39.258885  8487 net.cpp:394] relu2 <- pool2
I1028 16:31:39.258898  8487 net.cpp:345] relu2 -> pool2 (in-place)
I1028 16:31:39.258910  8487 net.cpp:96] Setting up relu2
I1028 16:31:39.258919  8487 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 16:31:39.258931  8487 net.cpp:67] Creating Layer drop2
I1028 16:31:39.258939  8487 net.cpp:394] drop2 <- pool2
I1028 16:31:39.258952  8487 net.cpp:345] drop2 -> pool2 (in-place)
I1028 16:31:39.258965  8487 net.cpp:96] Setting up drop2
I1028 16:31:39.258973  8487 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 16:31:39.258985  8487 net.cpp:67] Creating Layer conv3
I1028 16:31:39.258993  8487 net.cpp:394] conv3 <- pool2
I1028 16:31:39.259006  8487 net.cpp:356] conv3 -> conv3
I1028 16:31:39.259017  8487 net.cpp:96] Setting up conv3
I1028 16:31:39.261692  8487 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 16:31:39.261732  8487 net.cpp:67] Creating Layer pool3
I1028 16:31:39.261741  8487 net.cpp:394] pool3 <- conv3
I1028 16:31:39.261757  8487 net.cpp:356] pool3 -> pool3
I1028 16:31:39.261771  8487 net.cpp:96] Setting up pool3
I1028 16:31:39.261782  8487 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 16:31:39.261793  8487 net.cpp:67] Creating Layer relu3
I1028 16:31:39.261802  8487 net.cpp:394] relu3 <- pool3
I1028 16:31:39.261816  8487 net.cpp:345] relu3 -> pool3 (in-place)
I1028 16:31:39.261826  8487 net.cpp:96] Setting up relu3
I1028 16:31:39.261834  8487 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 16:31:39.261845  8487 net.cpp:67] Creating Layer drop3
I1028 16:31:39.261853  8487 net.cpp:394] drop3 <- pool3
I1028 16:31:39.261864  8487 net.cpp:345] drop3 -> pool3 (in-place)
I1028 16:31:39.261875  8487 net.cpp:96] Setting up drop3
I1028 16:31:39.261884  8487 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 16:31:39.261898  8487 net.cpp:67] Creating Layer ip1
I1028 16:31:39.261906  8487 net.cpp:394] ip1 <- pool3
I1028 16:31:39.261920  8487 net.cpp:356] ip1 -> ip1
I1028 16:31:39.261970  8487 net.cpp:96] Setting up ip1
I1028 16:31:39.800988  8487 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 16:31:39.801054  8487 net.cpp:67] Creating Layer relu4
I1028 16:31:39.801062  8487 net.cpp:394] relu4 <- ip1
I1028 16:31:39.801071  8487 net.cpp:345] relu4 -> ip1 (in-place)
I1028 16:31:39.801080  8487 net.cpp:96] Setting up relu4
I1028 16:31:39.801085  8487 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 16:31:39.801092  8487 net.cpp:67] Creating Layer drop4
I1028 16:31:39.801097  8487 net.cpp:394] drop4 <- ip1
I1028 16:31:39.801105  8487 net.cpp:345] drop4 -> ip1 (in-place)
I1028 16:31:39.801112  8487 net.cpp:96] Setting up drop4
I1028 16:31:39.801117  8487 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 16:31:39.801129  8487 net.cpp:67] Creating Layer ip2
I1028 16:31:39.801134  8487 net.cpp:394] ip2 <- ip1
I1028 16:31:39.801142  8487 net.cpp:356] ip2 -> ip2
I1028 16:31:39.801151  8487 net.cpp:96] Setting up ip2
I1028 16:31:39.809285  8487 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 16:31:39.809345  8487 net.cpp:67] Creating Layer loss
I1028 16:31:39.809352  8487 net.cpp:394] loss <- ip2
I1028 16:31:39.809360  8487 net.cpp:394] loss <- label
I1028 16:31:39.809367  8487 net.cpp:356] loss -> loss
I1028 16:31:39.809376  8487 net.cpp:96] Setting up loss
I1028 16:31:39.809391  8487 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 16:31:39.809398  8487 net.cpp:109]     with loss weight 1
I1028 16:31:39.809450  8487 net.cpp:170] loss needs backward computation.
I1028 16:31:39.809459  8487 net.cpp:170] ip2 needs backward computation.
I1028 16:31:39.809464  8487 net.cpp:170] drop4 needs backward computation.
I1028 16:31:39.809469  8487 net.cpp:170] relu4 needs backward computation.
I1028 16:31:39.809474  8487 net.cpp:170] ip1 needs backward computation.
I1028 16:31:39.809479  8487 net.cpp:170] drop3 needs backward computation.
I1028 16:31:39.809484  8487 net.cpp:170] relu3 needs backward computation.
I1028 16:31:39.809489  8487 net.cpp:170] pool3 needs backward computation.
I1028 16:31:39.809494  8487 net.cpp:170] conv3 needs backward computation.
I1028 16:31:39.809499  8487 net.cpp:170] drop2 needs backward computation.
I1028 16:31:39.809504  8487 net.cpp:170] relu2 needs backward computation.
I1028 16:31:39.809514  8487 net.cpp:170] pool2 needs backward computation.
I1028 16:31:39.809522  8487 net.cpp:170] conv2 needs backward computation.
I1028 16:31:39.809530  8487 net.cpp:170] drop1 needs backward computation.
I1028 16:31:39.809540  8487 net.cpp:170] relu1 needs backward computation.
I1028 16:31:39.809543  8487 net.cpp:170] pool1 needs backward computation.
I1028 16:31:39.809548  8487 net.cpp:170] conv1 needs backward computation.
I1028 16:31:39.809555  8487 net.cpp:172] mnist does not need backward computation.
I1028 16:31:39.809558  8487 net.cpp:208] This network produces output loss
I1028 16:31:39.809571  8487 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 16:31:39.809587  8487 net.cpp:219] Network initialization done.
I1028 16:31:39.809592  8487 net.cpp:220] Memory required for data: 119788292
I1028 16:31:39.809659  8487 solver.cpp:41] Solver scaffolding done.
I1028 16:31:39.809666  8487 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_420000.solverstate
I1028 16:31:39.809671  8487 solver.cpp:160] Solving Captcha
I1028 16:31:39.809690  8487 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_420000.solverstate
I1028 16:31:45.173384  8487 solver.cpp:502] SGDSolver: restoring history
I1028 16:31:45.925223  8487 solver.cpp:191] Iteration 420000, loss = 2.58483
I1028 16:31:45.925285  8487 solver.cpp:206]     Train net output #0: loss = 2.58483 (* 1 = 2.58483 loss)
I1028 16:31:45.925300  8487 solver.cpp:403] Iteration 420000, lr = 0.000595523
I1028 16:35:47.911762  8487 solver.cpp:191] Iteration 421000, loss = 2.41429
I1028 16:35:47.912539  8487 solver.cpp:206]     Train net output #0: loss = 2.41429 (* 1 = 2.41429 loss)
I1028 16:35:47.912575  8487 solver.cpp:403] Iteration 421000, lr = 0.000594487
I1028 16:39:49.292522  8487 solver.cpp:191] Iteration 422000, loss = 2.41094
I1028 16:39:49.293187  8487 solver.cpp:206]     Train net output #0: loss = 2.41094 (* 1 = 2.41094 loss)
I1028 16:39:49.293220  8487 solver.cpp:403] Iteration 422000, lr = 0.000593454
I1028 16:43:50.803293  8487 solver.cpp:191] Iteration 423000, loss = 2.45873
I1028 16:43:50.803869  8487 solver.cpp:206]     Train net output #0: loss = 2.45873 (* 1 = 2.45873 loss)
I1028 16:43:50.803905  8487 solver.cpp:403] Iteration 423000, lr = 0.000592426
I1028 16:47:52.302443  8487 solver.cpp:191] Iteration 424000, loss = 2.34484
I1028 16:47:52.303274  8487 solver.cpp:206]     Train net output #0: loss = 2.34484 (* 1 = 2.34484 loss)
I1028 16:47:52.303311  8487 solver.cpp:403] Iteration 424000, lr = 0.000591402
I1028 16:51:54.363495  8487 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_425000.caffemodel
I1028 16:51:59.008949  8487 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_425000.solverstate
I1028 16:52:03.378139  8487 solver.cpp:228] Iteration 425000, loss = 2.47845
I1028 16:52:03.378775  8487 solver.cpp:233] Optimization Done.
I1028 16:52:03.395380  8487 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 17:14:26.952119 22087 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 17:14:26.952222 22087 net.cpp:358] Input 0 -> data
I1028 17:14:26.952250 22087 net.cpp:67] Creating Layer conv1
I1028 17:14:26.952256 22087 net.cpp:394] conv1 <- data
I1028 17:14:26.952263 22087 net.cpp:356] conv1 -> conv1
I1028 17:14:26.952272 22087 net.cpp:96] Setting up conv1
I1028 17:14:26.952708 22087 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 17:14:26.952733 22087 net.cpp:67] Creating Layer pool1
I1028 17:14:26.952738 22087 net.cpp:394] pool1 <- conv1
I1028 17:14:26.952743 22087 net.cpp:356] pool1 -> pool1
I1028 17:14:26.952750 22087 net.cpp:96] Setting up pool1
I1028 17:14:26.952762 22087 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 17:14:26.952769 22087 net.cpp:67] Creating Layer relu1
I1028 17:14:26.952772 22087 net.cpp:394] relu1 <- pool1
I1028 17:14:26.952777 22087 net.cpp:345] relu1 -> pool1 (in-place)
I1028 17:14:26.952783 22087 net.cpp:96] Setting up relu1
I1028 17:14:26.952787 22087 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 17:14:26.952792 22087 net.cpp:67] Creating Layer drop1
I1028 17:14:26.952796 22087 net.cpp:394] drop1 <- pool1
I1028 17:14:26.952803 22087 net.cpp:345] drop1 -> pool1 (in-place)
I1028 17:14:26.952810 22087 net.cpp:96] Setting up drop1
I1028 17:14:26.952814 22087 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 17:14:26.952821 22087 net.cpp:67] Creating Layer conv2
I1028 17:14:26.952826 22087 net.cpp:394] conv2 <- pool1
I1028 17:14:26.952832 22087 net.cpp:356] conv2 -> conv2
I1028 17:14:26.952839 22087 net.cpp:96] Setting up conv2
I1028 17:14:26.953392 22087 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 17:14:26.953408 22087 net.cpp:67] Creating Layer pool2
I1028 17:14:26.953413 22087 net.cpp:394] pool2 <- conv2
I1028 17:14:26.953418 22087 net.cpp:356] pool2 -> pool2
I1028 17:14:26.953424 22087 net.cpp:96] Setting up pool2
I1028 17:14:26.953430 22087 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 17:14:26.953435 22087 net.cpp:67] Creating Layer relu2
I1028 17:14:26.953438 22087 net.cpp:394] relu2 <- pool2
I1028 17:14:26.953445 22087 net.cpp:345] relu2 -> pool2 (in-place)
I1028 17:14:26.953451 22087 net.cpp:96] Setting up relu2
I1028 17:14:26.953455 22087 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 17:14:26.953460 22087 net.cpp:67] Creating Layer drop2
I1028 17:14:26.953464 22087 net.cpp:394] drop2 <- pool2
I1028 17:14:26.953469 22087 net.cpp:345] drop2 -> pool2 (in-place)
I1028 17:14:26.953476 22087 net.cpp:96] Setting up drop2
I1028 17:14:26.953481 22087 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 17:14:26.953487 22087 net.cpp:67] Creating Layer conv3
I1028 17:14:26.953491 22087 net.cpp:394] conv3 <- pool2
I1028 17:14:26.953497 22087 net.cpp:356] conv3 -> conv3
I1028 17:14:26.953503 22087 net.cpp:96] Setting up conv3
I1028 17:14:26.954962 22087 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 17:14:26.954979 22087 net.cpp:67] Creating Layer pool3
I1028 17:14:26.954983 22087 net.cpp:394] pool3 <- conv3
I1028 17:14:26.954991 22087 net.cpp:356] pool3 -> pool3
I1028 17:14:26.954998 22087 net.cpp:96] Setting up pool3
I1028 17:14:26.955004 22087 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 17:14:26.955009 22087 net.cpp:67] Creating Layer relu3
I1028 17:14:26.955013 22087 net.cpp:394] relu3 <- pool3
I1028 17:14:26.955019 22087 net.cpp:345] relu3 -> pool3 (in-place)
I1028 17:14:26.955025 22087 net.cpp:96] Setting up relu3
I1028 17:14:26.955029 22087 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 17:14:26.955034 22087 net.cpp:67] Creating Layer drop3
I1028 17:14:26.955039 22087 net.cpp:394] drop3 <- pool3
I1028 17:14:26.955044 22087 net.cpp:345] drop3 -> pool3 (in-place)
I1028 17:14:26.955049 22087 net.cpp:96] Setting up drop3
I1028 17:14:26.955054 22087 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 17:14:26.955060 22087 net.cpp:67] Creating Layer ip1
I1028 17:14:26.955063 22087 net.cpp:394] ip1 <- pool3
I1028 17:14:26.955071 22087 net.cpp:356] ip1 -> ip1
I1028 17:14:26.955078 22087 net.cpp:96] Setting up ip1
I1028 17:14:27.448343 22087 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 17:14:27.448403 22087 net.cpp:67] Creating Layer relu4
I1028 17:14:27.448413 22087 net.cpp:394] relu4 <- ip1
I1028 17:14:27.448436 22087 net.cpp:345] relu4 -> ip1 (in-place)
I1028 17:14:27.448448 22087 net.cpp:96] Setting up relu4
I1028 17:14:27.448454 22087 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 17:14:27.448462 22087 net.cpp:67] Creating Layer drop4
I1028 17:14:27.448465 22087 net.cpp:394] drop4 <- ip1
I1028 17:14:27.448472 22087 net.cpp:345] drop4 -> ip1 (in-place)
I1028 17:14:27.448477 22087 net.cpp:96] Setting up drop4
I1028 17:14:27.448482 22087 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 17:14:27.448493 22087 net.cpp:67] Creating Layer ip2
I1028 17:14:27.448498 22087 net.cpp:394] ip2 <- ip1
I1028 17:14:27.448504 22087 net.cpp:356] ip2 -> ip2
I1028 17:14:27.448516 22087 net.cpp:96] Setting up ip2
I1028 17:14:27.457988 22087 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 17:14:27.458055 22087 net.cpp:67] Creating Layer prob
I1028 17:14:27.458062 22087 net.cpp:394] prob <- ip2
I1028 17:14:27.458070 22087 net.cpp:356] prob -> prob
I1028 17:14:27.458081 22087 net.cpp:96] Setting up prob
I1028 17:14:27.458087 22087 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 17:14:27.458091 22087 net.cpp:172] prob does not need backward computation.
I1028 17:14:27.458096 22087 net.cpp:172] ip2 does not need backward computation.
I1028 17:14:27.458098 22087 net.cpp:172] drop4 does not need backward computation.
I1028 17:14:27.458102 22087 net.cpp:172] relu4 does not need backward computation.
I1028 17:14:27.458106 22087 net.cpp:172] ip1 does not need backward computation.
I1028 17:14:27.458109 22087 net.cpp:172] drop3 does not need backward computation.
I1028 17:14:27.458112 22087 net.cpp:172] relu3 does not need backward computation.
I1028 17:14:27.458117 22087 net.cpp:172] pool3 does not need backward computation.
I1028 17:14:27.458119 22087 net.cpp:172] conv3 does not need backward computation.
I1028 17:14:27.458122 22087 net.cpp:172] drop2 does not need backward computation.
I1028 17:14:27.458127 22087 net.cpp:172] relu2 does not need backward computation.
I1028 17:14:27.458129 22087 net.cpp:172] pool2 does not need backward computation.
I1028 17:14:27.458133 22087 net.cpp:172] conv2 does not need backward computation.
I1028 17:14:27.458137 22087 net.cpp:172] drop1 does not need backward computation.
I1028 17:14:27.458139 22087 net.cpp:172] relu1 does not need backward computation.
I1028 17:14:27.458143 22087 net.cpp:172] pool1 does not need backward computation.
I1028 17:14:27.458147 22087 net.cpp:172] conv1 does not need backward computation.
I1028 17:14:27.458149 22087 net.cpp:208] This network produces output prob
I1028 17:14:27.458163 22087 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 17:14:27.458171 22087 net.cpp:219] Network initialization done.
I1028 17:14:27.458181 22087 net.cpp:220] Memory required for data: 1837200
I1028 17:46:46.651111 29914 convert_imageset.cpp:70] Shuffling data
I1028 17:46:47.258159 29914 convert_imageset.cpp:73] A total of 60000 images.
I1028 17:46:47.258245 29914 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 17:46:49.541918 29914 convert_imageset.cpp:177] Processed 1000 files.
E1028 17:46:51.421181 29914 convert_imageset.cpp:177] Processed 2000 files.
E1028 17:46:53.480182 29914 convert_imageset.cpp:177] Processed 3000 files.
E1028 17:46:55.507771 29914 convert_imageset.cpp:177] Processed 4000 files.
E1028 17:46:57.621569 29914 convert_imageset.cpp:177] Processed 5000 files.
E1028 17:46:59.605484 29914 convert_imageset.cpp:177] Processed 6000 files.
E1028 17:47:01.591357 29914 convert_imageset.cpp:177] Processed 7000 files.
E1028 17:47:03.576961 29914 convert_imageset.cpp:177] Processed 8000 files.
E1028 17:47:05.466219 29914 convert_imageset.cpp:177] Processed 9000 files.
E1028 17:47:07.345057 29914 convert_imageset.cpp:177] Processed 10000 files.
E1028 17:47:09.345373 29914 convert_imageset.cpp:177] Processed 11000 files.
E1028 17:47:11.225747 29914 convert_imageset.cpp:177] Processed 12000 files.
E1028 17:47:13.161999 29914 convert_imageset.cpp:177] Processed 13000 files.
E1028 17:47:14.940569 29914 convert_imageset.cpp:177] Processed 14000 files.
E1028 17:47:16.793157 29914 convert_imageset.cpp:177] Processed 15000 files.
E1028 17:47:18.670150 29914 convert_imageset.cpp:177] Processed 16000 files.
E1028 17:47:20.447263 29914 convert_imageset.cpp:177] Processed 17000 files.
E1028 17:47:22.328582 29914 convert_imageset.cpp:177] Processed 18000 files.
E1028 17:47:24.109216 29914 convert_imageset.cpp:177] Processed 19000 files.
E1028 17:47:25.806669 29914 convert_imageset.cpp:177] Processed 20000 files.
E1028 17:47:27.639771 29914 convert_imageset.cpp:177] Processed 21000 files.
E1028 17:47:29.428294 29914 convert_imageset.cpp:177] Processed 22000 files.
E1028 17:47:31.174228 29914 convert_imageset.cpp:177] Processed 23000 files.
E1028 17:47:32.914975 29914 convert_imageset.cpp:177] Processed 24000 files.
E1028 17:47:34.621637 29914 convert_imageset.cpp:177] Processed 25000 files.
E1028 17:47:36.315100 29914 convert_imageset.cpp:177] Processed 26000 files.
E1028 17:47:38.068166 29914 convert_imageset.cpp:177] Processed 27000 files.
E1028 17:47:39.753563 29914 convert_imageset.cpp:177] Processed 28000 files.
E1028 17:47:41.445962 29914 convert_imageset.cpp:177] Processed 29000 files.
E1028 17:47:43.126691 29914 convert_imageset.cpp:177] Processed 30000 files.
E1028 17:47:44.798054 29914 convert_imageset.cpp:177] Processed 31000 files.
E1028 17:47:46.712668 29914 convert_imageset.cpp:177] Processed 32000 files.
E1028 17:47:48.330903 29914 convert_imageset.cpp:177] Processed 33000 files.
E1028 17:47:50.019448 29914 convert_imageset.cpp:177] Processed 34000 files.
E1028 17:47:51.672456 29914 convert_imageset.cpp:177] Processed 35000 files.
E1028 17:47:53.382665 29914 convert_imageset.cpp:177] Processed 36000 files.
E1028 17:47:55.035328 29914 convert_imageset.cpp:177] Processed 37000 files.
E1028 17:47:56.666610 29914 convert_imageset.cpp:177] Processed 38000 files.
E1028 17:47:58.376096 29914 convert_imageset.cpp:177] Processed 39000 files.
E1028 17:48:00.049144 29914 convert_imageset.cpp:177] Processed 40000 files.
E1028 17:48:01.988327 29914 convert_imageset.cpp:177] Processed 41000 files.
E1028 17:48:03.776324 29914 convert_imageset.cpp:177] Processed 42000 files.
E1028 17:48:05.420183 29914 convert_imageset.cpp:177] Processed 43000 files.
E1028 17:48:07.132258 29914 convert_imageset.cpp:177] Processed 44000 files.
E1028 17:48:08.676050 29914 convert_imageset.cpp:177] Processed 45000 files.
E1028 17:48:10.336645 29914 convert_imageset.cpp:177] Processed 46000 files.
E1028 17:48:11.847489 29914 convert_imageset.cpp:177] Processed 47000 files.
E1028 17:48:13.576881 29914 convert_imageset.cpp:177] Processed 48000 files.
E1028 17:48:15.283318 29914 convert_imageset.cpp:177] Processed 49000 files.
E1028 17:48:16.878764 29914 convert_imageset.cpp:177] Processed 50000 files.
E1028 17:48:18.362942 29914 convert_imageset.cpp:177] Processed 51000 files.
E1028 17:48:19.937347 29914 convert_imageset.cpp:177] Processed 52000 files.
E1028 17:48:21.604549 29914 convert_imageset.cpp:177] Processed 53000 files.
E1028 17:48:23.248374 29914 convert_imageset.cpp:177] Processed 54000 files.
E1028 17:48:24.860539 29914 convert_imageset.cpp:177] Processed 55000 files.
E1028 17:48:26.404187 29914 convert_imageset.cpp:177] Processed 56000 files.
E1028 17:48:28.609655 29914 convert_imageset.cpp:177] Processed 57000 files.
E1028 17:48:30.211665 29914 convert_imageset.cpp:177] Processed 58000 files.
E1028 17:48:31.861948 29914 convert_imageset.cpp:177] Processed 59000 files.
E1028 17:48:33.437988 29914 convert_imageset.cpp:177] Processed 60000 files.
I1028 17:48:33.622222 30076 caffe.cpp:99] Use GPU with device ID 0
I1028 17:48:33.961273 30076 caffe.cpp:107] Starting Optimization
I1028 17:48:33.961393 30076 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 430000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 17:48:33.961417 30076 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 17:48:33.964205 30076 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 17:48:33.964458 30076 net.cpp:67] Creating Layer mnist
I1028 17:48:33.964488 30076 net.cpp:356] mnist -> data
I1028 17:48:33.964524 30076 net.cpp:356] mnist -> label
I1028 17:48:33.964556 30076 net.cpp:96] Setting up mnist
I1028 17:48:33.971576 30076 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 17:48:33.971705 30076 data_layer.cpp:128] output data size: 64,1,50,180
I1028 17:48:33.973415 30076 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 17:48:33.973454 30076 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 17:48:33.973481 30076 net.cpp:67] Creating Layer conv1
I1028 17:48:33.973495 30076 net.cpp:394] conv1 <- data
I1028 17:48:33.973527 30076 net.cpp:356] conv1 -> conv1
I1028 17:48:33.973556 30076 net.cpp:96] Setting up conv1
I1028 17:48:33.974462 30076 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 17:48:33.974521 30076 net.cpp:67] Creating Layer pool1
I1028 17:48:33.974536 30076 net.cpp:394] pool1 <- conv1
I1028 17:48:33.974557 30076 net.cpp:356] pool1 -> pool1
I1028 17:48:33.974577 30076 net.cpp:96] Setting up pool1
I1028 17:48:33.974609 30076 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 17:48:33.974627 30076 net.cpp:67] Creating Layer relu1
I1028 17:48:33.974639 30076 net.cpp:394] relu1 <- pool1
I1028 17:48:33.974655 30076 net.cpp:345] relu1 -> pool1 (in-place)
I1028 17:48:33.974673 30076 net.cpp:96] Setting up relu1
I1028 17:48:33.974685 30076 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 17:48:33.974704 30076 net.cpp:67] Creating Layer drop1
I1028 17:48:33.974715 30076 net.cpp:394] drop1 <- pool1
I1028 17:48:33.974736 30076 net.cpp:345] drop1 -> pool1 (in-place)
I1028 17:48:33.974756 30076 net.cpp:96] Setting up drop1
I1028 17:48:33.974769 30076 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 17:48:33.974787 30076 net.cpp:67] Creating Layer conv2
I1028 17:48:33.974799 30076 net.cpp:394] conv2 <- pool1
I1028 17:48:33.974822 30076 net.cpp:356] conv2 -> conv2
I1028 17:48:33.974843 30076 net.cpp:96] Setting up conv2
I1028 17:48:33.976382 30076 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 17:48:33.976472 30076 net.cpp:67] Creating Layer pool2
I1028 17:48:33.976502 30076 net.cpp:394] pool2 <- conv2
I1028 17:48:33.976518 30076 net.cpp:356] pool2 -> pool2
I1028 17:48:33.976534 30076 net.cpp:96] Setting up pool2
I1028 17:48:33.976547 30076 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 17:48:33.976560 30076 net.cpp:67] Creating Layer relu2
I1028 17:48:33.976570 30076 net.cpp:394] relu2 <- pool2
I1028 17:48:33.976588 30076 net.cpp:345] relu2 -> pool2 (in-place)
I1028 17:48:33.976603 30076 net.cpp:96] Setting up relu2
I1028 17:48:33.976613 30076 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 17:48:33.976629 30076 net.cpp:67] Creating Layer drop2
I1028 17:48:33.976639 30076 net.cpp:394] drop2 <- pool2
I1028 17:48:33.976655 30076 net.cpp:345] drop2 -> pool2 (in-place)
I1028 17:48:33.976670 30076 net.cpp:96] Setting up drop2
I1028 17:48:33.976680 30076 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 17:48:33.976696 30076 net.cpp:67] Creating Layer conv3
I1028 17:48:33.976706 30076 net.cpp:394] conv3 <- pool2
I1028 17:48:33.976719 30076 net.cpp:356] conv3 -> conv3
I1028 17:48:33.976735 30076 net.cpp:96] Setting up conv3
I1028 17:48:33.980062 30076 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 17:48:33.980103 30076 net.cpp:67] Creating Layer pool3
I1028 17:48:33.980115 30076 net.cpp:394] pool3 <- conv3
I1028 17:48:33.980141 30076 net.cpp:356] pool3 -> pool3
I1028 17:48:33.980159 30076 net.cpp:96] Setting up pool3
I1028 17:48:33.980172 30076 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 17:48:33.980185 30076 net.cpp:67] Creating Layer relu3
I1028 17:48:33.980195 30076 net.cpp:394] relu3 <- pool3
I1028 17:48:33.980212 30076 net.cpp:345] relu3 -> pool3 (in-place)
I1028 17:48:33.980226 30076 net.cpp:96] Setting up relu3
I1028 17:48:33.980237 30076 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 17:48:33.980249 30076 net.cpp:67] Creating Layer drop3
I1028 17:48:33.980259 30076 net.cpp:394] drop3 <- pool3
I1028 17:48:33.980273 30076 net.cpp:345] drop3 -> pool3 (in-place)
I1028 17:48:33.980286 30076 net.cpp:96] Setting up drop3
I1028 17:48:33.980298 30076 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 17:48:33.980312 30076 net.cpp:67] Creating Layer ip1
I1028 17:48:33.980322 30076 net.cpp:394] ip1 <- pool3
I1028 17:48:33.980340 30076 net.cpp:356] ip1 -> ip1
I1028 17:48:33.980396 30076 net.cpp:96] Setting up ip1
I1028 17:48:34.497375 30076 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 17:48:34.497436 30076 net.cpp:67] Creating Layer relu4
I1028 17:48:34.497443 30076 net.cpp:394] relu4 <- ip1
I1028 17:48:34.497453 30076 net.cpp:345] relu4 -> ip1 (in-place)
I1028 17:48:34.497462 30076 net.cpp:96] Setting up relu4
I1028 17:48:34.497467 30076 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 17:48:34.497474 30076 net.cpp:67] Creating Layer drop4
I1028 17:48:34.497479 30076 net.cpp:394] drop4 <- ip1
I1028 17:48:34.497486 30076 net.cpp:345] drop4 -> ip1 (in-place)
I1028 17:48:34.497493 30076 net.cpp:96] Setting up drop4
I1028 17:48:34.497499 30076 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 17:48:34.497509 30076 net.cpp:67] Creating Layer ip2
I1028 17:48:34.497514 30076 net.cpp:394] ip2 <- ip1
I1028 17:48:34.497521 30076 net.cpp:356] ip2 -> ip2
I1028 17:48:34.497530 30076 net.cpp:96] Setting up ip2
I1028 17:48:34.507984 30076 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 17:48:34.508044 30076 net.cpp:67] Creating Layer loss
I1028 17:48:34.508051 30076 net.cpp:394] loss <- ip2
I1028 17:48:34.508059 30076 net.cpp:394] loss <- label
I1028 17:48:34.508065 30076 net.cpp:356] loss -> loss
I1028 17:48:34.508075 30076 net.cpp:96] Setting up loss
I1028 17:48:34.508088 30076 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 17:48:34.508093 30076 net.cpp:109]     with loss weight 1
I1028 17:48:34.508129 30076 net.cpp:170] loss needs backward computation.
I1028 17:48:34.508134 30076 net.cpp:170] ip2 needs backward computation.
I1028 17:48:34.508138 30076 net.cpp:170] drop4 needs backward computation.
I1028 17:48:34.508142 30076 net.cpp:170] relu4 needs backward computation.
I1028 17:48:34.508147 30076 net.cpp:170] ip1 needs backward computation.
I1028 17:48:34.508152 30076 net.cpp:170] drop3 needs backward computation.
I1028 17:48:34.508155 30076 net.cpp:170] relu3 needs backward computation.
I1028 17:48:34.508160 30076 net.cpp:170] pool3 needs backward computation.
I1028 17:48:34.508164 30076 net.cpp:170] conv3 needs backward computation.
I1028 17:48:34.508169 30076 net.cpp:170] drop2 needs backward computation.
I1028 17:48:34.508173 30076 net.cpp:170] relu2 needs backward computation.
I1028 17:48:34.508177 30076 net.cpp:170] pool2 needs backward computation.
I1028 17:48:34.508183 30076 net.cpp:170] conv2 needs backward computation.
I1028 17:48:34.508186 30076 net.cpp:170] drop1 needs backward computation.
I1028 17:48:34.508190 30076 net.cpp:170] relu1 needs backward computation.
I1028 17:48:34.508194 30076 net.cpp:170] pool1 needs backward computation.
I1028 17:48:34.508199 30076 net.cpp:170] conv1 needs backward computation.
I1028 17:48:34.508203 30076 net.cpp:172] mnist does not need backward computation.
I1028 17:48:34.508208 30076 net.cpp:208] This network produces output loss
I1028 17:48:34.508219 30076 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 17:48:34.508225 30076 net.cpp:219] Network initialization done.
I1028 17:48:34.508229 30076 net.cpp:220] Memory required for data: 119788292
I1028 17:48:34.508301 30076 solver.cpp:41] Solver scaffolding done.
I1028 17:48:34.508308 30076 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_425000.solverstate
I1028 17:48:34.508313 30076 solver.cpp:160] Solving Captcha
I1028 17:48:34.508332 30076 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_425000.solverstate
I1028 17:48:37.382652 30076 solver.cpp:502] SGDSolver: restoring history
I1028 17:48:38.132686 30076 solver.cpp:191] Iteration 425000, loss = 2.49138
I1028 17:48:38.132748 30076 solver.cpp:206]     Train net output #0: loss = 2.49138 (* 1 = 2.49138 loss)
I1028 17:48:38.132764 30076 solver.cpp:403] Iteration 425000, lr = 0.000590382
I1028 17:52:40.168491 30076 solver.cpp:191] Iteration 426000, loss = 2.38293
I1028 17:52:40.169100 30076 solver.cpp:206]     Train net output #0: loss = 2.38293 (* 1 = 2.38293 loss)
I1028 17:52:40.169137 30076 solver.cpp:403] Iteration 426000, lr = 0.000589366
I1028 17:56:41.613873 30076 solver.cpp:191] Iteration 427000, loss = 2.46306
I1028 17:56:41.614490 30076 solver.cpp:206]     Train net output #0: loss = 2.46306 (* 1 = 2.46306 loss)
I1028 17:56:41.614522 30076 solver.cpp:403] Iteration 427000, lr = 0.000588354
I1028 18:00:43.047684 30076 solver.cpp:191] Iteration 428000, loss = 2.45308
I1028 18:00:43.048280 30076 solver.cpp:206]     Train net output #0: loss = 2.45308 (* 1 = 2.45308 loss)
I1028 18:00:43.048318 30076 solver.cpp:403] Iteration 428000, lr = 0.000587347
I1028 18:04:44.568092 30076 solver.cpp:191] Iteration 429000, loss = 2.40659
I1028 18:04:44.568657 30076 solver.cpp:206]     Train net output #0: loss = 2.40659 (* 1 = 2.40659 loss)
I1028 18:04:44.568696 30076 solver.cpp:403] Iteration 429000, lr = 0.000586343
I1028 18:08:46.543210 30076 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_430000.caffemodel
I1028 18:08:51.153439 30076 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_430000.solverstate
I1028 18:08:54.996954 30076 solver.cpp:228] Iteration 430000, loss = 2.52193
I1028 18:08:54.997536 30076 solver.cpp:233] Optimization Done.
I1028 18:08:54.997560 30076 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 18:31:13.936735 11429 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 18:31:13.936836 11429 net.cpp:358] Input 0 -> data
I1028 18:31:13.936861 11429 net.cpp:67] Creating Layer conv1
I1028 18:31:13.936866 11429 net.cpp:394] conv1 <- data
I1028 18:31:13.936872 11429 net.cpp:356] conv1 -> conv1
I1028 18:31:13.936882 11429 net.cpp:96] Setting up conv1
I1028 18:31:13.937202 11429 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 18:31:13.937222 11429 net.cpp:67] Creating Layer pool1
I1028 18:31:13.937227 11429 net.cpp:394] pool1 <- conv1
I1028 18:31:13.937234 11429 net.cpp:356] pool1 -> pool1
I1028 18:31:13.937242 11429 net.cpp:96] Setting up pool1
I1028 18:31:13.937253 11429 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 18:31:13.937260 11429 net.cpp:67] Creating Layer relu1
I1028 18:31:13.937264 11429 net.cpp:394] relu1 <- pool1
I1028 18:31:13.937269 11429 net.cpp:345] relu1 -> pool1 (in-place)
I1028 18:31:13.937275 11429 net.cpp:96] Setting up relu1
I1028 18:31:13.937280 11429 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 18:31:13.937288 11429 net.cpp:67] Creating Layer drop1
I1028 18:31:13.937291 11429 net.cpp:394] drop1 <- pool1
I1028 18:31:13.937297 11429 net.cpp:345] drop1 -> pool1 (in-place)
I1028 18:31:13.937304 11429 net.cpp:96] Setting up drop1
I1028 18:31:13.937309 11429 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 18:31:13.937315 11429 net.cpp:67] Creating Layer conv2
I1028 18:31:13.937319 11429 net.cpp:394] conv2 <- pool1
I1028 18:31:13.937325 11429 net.cpp:356] conv2 -> conv2
I1028 18:31:13.937331 11429 net.cpp:96] Setting up conv2
I1028 18:31:13.937883 11429 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 18:31:13.937898 11429 net.cpp:67] Creating Layer pool2
I1028 18:31:13.937902 11429 net.cpp:394] pool2 <- conv2
I1028 18:31:13.937908 11429 net.cpp:356] pool2 -> pool2
I1028 18:31:13.937916 11429 net.cpp:96] Setting up pool2
I1028 18:31:13.937921 11429 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 18:31:13.937928 11429 net.cpp:67] Creating Layer relu2
I1028 18:31:13.937933 11429 net.cpp:394] relu2 <- pool2
I1028 18:31:13.937938 11429 net.cpp:345] relu2 -> pool2 (in-place)
I1028 18:31:13.937944 11429 net.cpp:96] Setting up relu2
I1028 18:31:13.937948 11429 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 18:31:13.937953 11429 net.cpp:67] Creating Layer drop2
I1028 18:31:13.937958 11429 net.cpp:394] drop2 <- pool2
I1028 18:31:13.937964 11429 net.cpp:345] drop2 -> pool2 (in-place)
I1028 18:31:13.937970 11429 net.cpp:96] Setting up drop2
I1028 18:31:13.937974 11429 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 18:31:13.937981 11429 net.cpp:67] Creating Layer conv3
I1028 18:31:13.937985 11429 net.cpp:394] conv3 <- pool2
I1028 18:31:13.937994 11429 net.cpp:356] conv3 -> conv3
I1028 18:31:13.938001 11429 net.cpp:96] Setting up conv3
I1028 18:31:13.939528 11429 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 18:31:13.939546 11429 net.cpp:67] Creating Layer pool3
I1028 18:31:13.939551 11429 net.cpp:394] pool3 <- conv3
I1028 18:31:13.939558 11429 net.cpp:356] pool3 -> pool3
I1028 18:31:13.939568 11429 net.cpp:96] Setting up pool3
I1028 18:31:13.939573 11429 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 18:31:13.939579 11429 net.cpp:67] Creating Layer relu3
I1028 18:31:13.939584 11429 net.cpp:394] relu3 <- pool3
I1028 18:31:13.939589 11429 net.cpp:345] relu3 -> pool3 (in-place)
I1028 18:31:13.939594 11429 net.cpp:96] Setting up relu3
I1028 18:31:13.939599 11429 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 18:31:13.939604 11429 net.cpp:67] Creating Layer drop3
I1028 18:31:13.939607 11429 net.cpp:394] drop3 <- pool3
I1028 18:31:13.939613 11429 net.cpp:345] drop3 -> pool3 (in-place)
I1028 18:31:13.939618 11429 net.cpp:96] Setting up drop3
I1028 18:31:13.939622 11429 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 18:31:13.939630 11429 net.cpp:67] Creating Layer ip1
I1028 18:31:13.939635 11429 net.cpp:394] ip1 <- pool3
I1028 18:31:13.939640 11429 net.cpp:356] ip1 -> ip1
I1028 18:31:13.939647 11429 net.cpp:96] Setting up ip1
I1028 18:31:14.401152 11429 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 18:31:14.401214 11429 net.cpp:67] Creating Layer relu4
I1028 18:31:14.401221 11429 net.cpp:394] relu4 <- ip1
I1028 18:31:14.401232 11429 net.cpp:345] relu4 -> ip1 (in-place)
I1028 18:31:14.401240 11429 net.cpp:96] Setting up relu4
I1028 18:31:14.401245 11429 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 18:31:14.401252 11429 net.cpp:67] Creating Layer drop4
I1028 18:31:14.401257 11429 net.cpp:394] drop4 <- ip1
I1028 18:31:14.401263 11429 net.cpp:345] drop4 -> ip1 (in-place)
I1028 18:31:14.401270 11429 net.cpp:96] Setting up drop4
I1028 18:31:14.401275 11429 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 18:31:14.401283 11429 net.cpp:67] Creating Layer ip2
I1028 18:31:14.401288 11429 net.cpp:394] ip2 <- ip1
I1028 18:31:14.401295 11429 net.cpp:356] ip2 -> ip2
I1028 18:31:14.401307 11429 net.cpp:96] Setting up ip2
I1028 18:31:14.410303 11429 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 18:31:14.410382 11429 net.cpp:67] Creating Layer prob
I1028 18:31:14.410389 11429 net.cpp:394] prob <- ip2
I1028 18:31:14.410398 11429 net.cpp:356] prob -> prob
I1028 18:31:14.410408 11429 net.cpp:96] Setting up prob
I1028 18:31:14.410414 11429 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 18:31:14.410418 11429 net.cpp:172] prob does not need backward computation.
I1028 18:31:14.410423 11429 net.cpp:172] ip2 does not need backward computation.
I1028 18:31:14.410426 11429 net.cpp:172] drop4 does not need backward computation.
I1028 18:31:14.410429 11429 net.cpp:172] relu4 does not need backward computation.
I1028 18:31:14.410434 11429 net.cpp:172] ip1 does not need backward computation.
I1028 18:31:14.410437 11429 net.cpp:172] drop3 does not need backward computation.
I1028 18:31:14.410440 11429 net.cpp:172] relu3 does not need backward computation.
I1028 18:31:14.410444 11429 net.cpp:172] pool3 does not need backward computation.
I1028 18:31:14.410449 11429 net.cpp:172] conv3 does not need backward computation.
I1028 18:31:14.410451 11429 net.cpp:172] drop2 does not need backward computation.
I1028 18:31:14.410455 11429 net.cpp:172] relu2 does not need backward computation.
I1028 18:31:14.410459 11429 net.cpp:172] pool2 does not need backward computation.
I1028 18:31:14.410462 11429 net.cpp:172] conv2 does not need backward computation.
I1028 18:31:14.410465 11429 net.cpp:172] drop1 does not need backward computation.
I1028 18:31:14.410470 11429 net.cpp:172] relu1 does not need backward computation.
I1028 18:31:14.410472 11429 net.cpp:172] pool1 does not need backward computation.
I1028 18:31:14.410476 11429 net.cpp:172] conv1 does not need backward computation.
I1028 18:31:14.410480 11429 net.cpp:208] This network produces output prob
I1028 18:31:14.410493 11429 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 18:31:14.410501 11429 net.cpp:219] Network initialization done.
I1028 18:31:14.410506 11429 net.cpp:220] Memory required for data: 1837200
I1028 19:03:23.415436 19095 convert_imageset.cpp:70] Shuffling data
I1028 19:03:24.018172 19095 convert_imageset.cpp:73] A total of 60000 images.
I1028 19:03:24.018262 19095 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 19:03:26.319528 19095 convert_imageset.cpp:177] Processed 1000 files.
E1028 19:03:28.339938 19095 convert_imageset.cpp:177] Processed 2000 files.
E1028 19:03:30.348641 19095 convert_imageset.cpp:177] Processed 3000 files.
E1028 19:03:32.416471 19095 convert_imageset.cpp:177] Processed 4000 files.
E1028 19:03:34.277251 19095 convert_imageset.cpp:177] Processed 5000 files.
E1028 19:03:35.988560 19095 convert_imageset.cpp:177] Processed 6000 files.
E1028 19:03:37.805619 19095 convert_imageset.cpp:177] Processed 7000 files.
E1028 19:03:39.699038 19095 convert_imageset.cpp:177] Processed 8000 files.
E1028 19:03:41.625648 19095 convert_imageset.cpp:177] Processed 9000 files.
E1028 19:03:43.455916 19095 convert_imageset.cpp:177] Processed 10000 files.
E1028 19:03:45.299290 19095 convert_imageset.cpp:177] Processed 11000 files.
E1028 19:03:47.131471 19095 convert_imageset.cpp:177] Processed 12000 files.
E1028 19:03:48.849444 19095 convert_imageset.cpp:177] Processed 13000 files.
E1028 19:03:50.608224 19095 convert_imageset.cpp:177] Processed 14000 files.
E1028 19:03:52.376461 19095 convert_imageset.cpp:177] Processed 15000 files.
E1028 19:03:54.193706 19095 convert_imageset.cpp:177] Processed 16000 files.
E1028 19:03:55.925683 19095 convert_imageset.cpp:177] Processed 17000 files.
E1028 19:03:57.654657 19095 convert_imageset.cpp:177] Processed 18000 files.
E1028 19:03:59.396579 19095 convert_imageset.cpp:177] Processed 19000 files.
E1028 19:04:00.978894 19095 convert_imageset.cpp:177] Processed 20000 files.
E1028 19:04:02.637495 19095 convert_imageset.cpp:177] Processed 21000 files.
E1028 19:04:04.360898 19095 convert_imageset.cpp:177] Processed 22000 files.
E1028 19:04:06.096478 19095 convert_imageset.cpp:177] Processed 23000 files.
E1028 19:04:07.835047 19095 convert_imageset.cpp:177] Processed 24000 files.
E1028 19:04:09.488646 19095 convert_imageset.cpp:177] Processed 25000 files.
E1028 19:04:11.097293 19095 convert_imageset.cpp:177] Processed 26000 files.
E1028 19:04:12.649214 19095 convert_imageset.cpp:177] Processed 27000 files.
E1028 19:04:14.833312 19095 convert_imageset.cpp:177] Processed 28000 files.
E1028 19:04:16.482604 19095 convert_imageset.cpp:177] Processed 29000 files.
E1028 19:04:18.111207 19095 convert_imageset.cpp:177] Processed 30000 files.
E1028 19:04:19.937288 19095 convert_imageset.cpp:177] Processed 31000 files.
E1028 19:04:21.557667 19095 convert_imageset.cpp:177] Processed 32000 files.
E1028 19:04:23.143172 19095 convert_imageset.cpp:177] Processed 33000 files.
E1028 19:04:24.712524 19095 convert_imageset.cpp:177] Processed 34000 files.
E1028 19:04:26.443351 19095 convert_imageset.cpp:177] Processed 35000 files.
E1028 19:04:28.091081 19095 convert_imageset.cpp:177] Processed 36000 files.
E1028 19:04:29.784103 19095 convert_imageset.cpp:177] Processed 37000 files.
E1028 19:04:31.452239 19095 convert_imageset.cpp:177] Processed 38000 files.
E1028 19:04:32.956820 19095 convert_imageset.cpp:177] Processed 39000 files.
E1028 19:04:34.607705 19095 convert_imageset.cpp:177] Processed 40000 files.
E1028 19:04:36.265697 19095 convert_imageset.cpp:177] Processed 41000 files.
E1028 19:04:37.824026 19095 convert_imageset.cpp:177] Processed 42000 files.
E1028 19:04:39.434567 19095 convert_imageset.cpp:177] Processed 43000 files.
E1028 19:04:40.998551 19095 convert_imageset.cpp:177] Processed 44000 files.
E1028 19:04:42.452957 19095 convert_imageset.cpp:177] Processed 45000 files.
E1028 19:04:43.916638 19095 convert_imageset.cpp:177] Processed 46000 files.
E1028 19:04:45.398852 19095 convert_imageset.cpp:177] Processed 47000 files.
E1028 19:04:46.917876 19095 convert_imageset.cpp:177] Processed 48000 files.
E1028 19:04:48.402840 19095 convert_imageset.cpp:177] Processed 49000 files.
E1028 19:04:49.877676 19095 convert_imageset.cpp:177] Processed 50000 files.
E1028 19:04:51.323436 19095 convert_imageset.cpp:177] Processed 51000 files.
E1028 19:04:52.907663 19095 convert_imageset.cpp:177] Processed 52000 files.
E1028 19:04:54.365249 19095 convert_imageset.cpp:177] Processed 53000 files.
E1028 19:04:55.829587 19095 convert_imageset.cpp:177] Processed 54000 files.
E1028 19:04:57.417501 19095 convert_imageset.cpp:177] Processed 55000 files.
E1028 19:04:58.978284 19095 convert_imageset.cpp:177] Processed 56000 files.
E1028 19:05:00.593538 19095 convert_imageset.cpp:177] Processed 57000 files.
E1028 19:05:02.060780 19095 convert_imageset.cpp:177] Processed 58000 files.
E1028 19:05:03.453971 19095 convert_imageset.cpp:177] Processed 59000 files.
E1028 19:05:05.215970 19095 convert_imageset.cpp:177] Processed 60000 files.
I1028 19:05:05.392974 19260 caffe.cpp:99] Use GPU with device ID 0
I1028 19:05:05.748112 19260 caffe.cpp:107] Starting Optimization
I1028 19:05:05.748219 19260 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 435000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 19:05:05.748242 19260 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 19:05:05.766564 19260 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 19:05:05.766664 19260 net.cpp:67] Creating Layer mnist
I1028 19:05:05.766674 19260 net.cpp:356] mnist -> data
I1028 19:05:05.766691 19260 net.cpp:356] mnist -> label
I1028 19:05:05.766705 19260 net.cpp:96] Setting up mnist
I1028 19:05:05.774574 19260 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 19:05:05.774703 19260 data_layer.cpp:128] output data size: 64,1,50,180
I1028 19:05:05.775935 19260 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 19:05:05.775988 19260 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 19:05:05.776021 19260 net.cpp:67] Creating Layer conv1
I1028 19:05:05.776036 19260 net.cpp:394] conv1 <- data
I1028 19:05:05.776070 19260 net.cpp:356] conv1 -> conv1
I1028 19:05:05.776085 19260 net.cpp:96] Setting up conv1
I1028 19:05:05.776494 19260 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 19:05:05.776530 19260 net.cpp:67] Creating Layer pool1
I1028 19:05:05.776536 19260 net.cpp:394] pool1 <- conv1
I1028 19:05:05.776543 19260 net.cpp:356] pool1 -> pool1
I1028 19:05:05.776551 19260 net.cpp:96] Setting up pool1
I1028 19:05:05.776567 19260 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 19:05:05.776574 19260 net.cpp:67] Creating Layer relu1
I1028 19:05:05.776579 19260 net.cpp:394] relu1 <- pool1
I1028 19:05:05.776587 19260 net.cpp:345] relu1 -> pool1 (in-place)
I1028 19:05:05.776594 19260 net.cpp:96] Setting up relu1
I1028 19:05:05.776599 19260 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 19:05:05.776607 19260 net.cpp:67] Creating Layer drop1
I1028 19:05:05.776612 19260 net.cpp:394] drop1 <- pool1
I1028 19:05:05.776618 19260 net.cpp:345] drop1 -> pool1 (in-place)
I1028 19:05:05.776623 19260 net.cpp:96] Setting up drop1
I1028 19:05:05.776629 19260 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 19:05:05.776638 19260 net.cpp:67] Creating Layer conv2
I1028 19:05:05.776643 19260 net.cpp:394] conv2 <- pool1
I1028 19:05:05.776650 19260 net.cpp:356] conv2 -> conv2
I1028 19:05:05.776657 19260 net.cpp:96] Setting up conv2
I1028 19:05:05.777245 19260 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 19:05:05.777262 19260 net.cpp:67] Creating Layer pool2
I1028 19:05:05.777267 19260 net.cpp:394] pool2 <- conv2
I1028 19:05:05.777276 19260 net.cpp:356] pool2 -> pool2
I1028 19:05:05.777284 19260 net.cpp:96] Setting up pool2
I1028 19:05:05.777290 19260 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 19:05:05.777297 19260 net.cpp:67] Creating Layer relu2
I1028 19:05:05.777302 19260 net.cpp:394] relu2 <- pool2
I1028 19:05:05.777307 19260 net.cpp:345] relu2 -> pool2 (in-place)
I1028 19:05:05.777312 19260 net.cpp:96] Setting up relu2
I1028 19:05:05.777318 19260 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 19:05:05.777326 19260 net.cpp:67] Creating Layer drop2
I1028 19:05:05.777331 19260 net.cpp:394] drop2 <- pool2
I1028 19:05:05.777338 19260 net.cpp:345] drop2 -> pool2 (in-place)
I1028 19:05:05.777343 19260 net.cpp:96] Setting up drop2
I1028 19:05:05.777348 19260 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 19:05:05.777355 19260 net.cpp:67] Creating Layer conv3
I1028 19:05:05.777360 19260 net.cpp:394] conv3 <- pool2
I1028 19:05:05.777369 19260 net.cpp:356] conv3 -> conv3
I1028 19:05:05.777375 19260 net.cpp:96] Setting up conv3
I1028 19:05:05.780036 19260 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 19:05:05.780098 19260 net.cpp:67] Creating Layer pool3
I1028 19:05:05.780113 19260 net.cpp:394] pool3 <- conv3
I1028 19:05:05.780131 19260 net.cpp:356] pool3 -> pool3
I1028 19:05:05.780151 19260 net.cpp:96] Setting up pool3
I1028 19:05:05.780167 19260 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 19:05:05.780196 19260 net.cpp:67] Creating Layer relu3
I1028 19:05:05.780210 19260 net.cpp:394] relu3 <- pool3
I1028 19:05:05.780226 19260 net.cpp:345] relu3 -> pool3 (in-place)
I1028 19:05:05.780243 19260 net.cpp:96] Setting up relu3
I1028 19:05:05.780256 19260 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 19:05:05.780272 19260 net.cpp:67] Creating Layer drop3
I1028 19:05:05.780284 19260 net.cpp:394] drop3 <- pool3
I1028 19:05:05.780303 19260 net.cpp:345] drop3 -> pool3 (in-place)
I1028 19:05:05.780321 19260 net.cpp:96] Setting up drop3
I1028 19:05:05.780335 19260 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 19:05:05.780354 19260 net.cpp:67] Creating Layer ip1
I1028 19:05:05.780365 19260 net.cpp:394] ip1 <- pool3
I1028 19:05:05.780382 19260 net.cpp:356] ip1 -> ip1
I1028 19:05:05.780485 19260 net.cpp:96] Setting up ip1
I1028 19:05:06.297163 19260 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 19:05:06.297225 19260 net.cpp:67] Creating Layer relu4
I1028 19:05:06.297232 19260 net.cpp:394] relu4 <- ip1
I1028 19:05:06.297241 19260 net.cpp:345] relu4 -> ip1 (in-place)
I1028 19:05:06.297250 19260 net.cpp:96] Setting up relu4
I1028 19:05:06.297255 19260 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 19:05:06.297266 19260 net.cpp:67] Creating Layer drop4
I1028 19:05:06.297271 19260 net.cpp:394] drop4 <- ip1
I1028 19:05:06.297276 19260 net.cpp:345] drop4 -> ip1 (in-place)
I1028 19:05:06.297282 19260 net.cpp:96] Setting up drop4
I1028 19:05:06.297287 19260 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 19:05:06.297299 19260 net.cpp:67] Creating Layer ip2
I1028 19:05:06.297304 19260 net.cpp:394] ip2 <- ip1
I1028 19:05:06.297312 19260 net.cpp:356] ip2 -> ip2
I1028 19:05:06.297319 19260 net.cpp:96] Setting up ip2
I1028 19:05:06.306265 19260 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 19:05:06.306320 19260 net.cpp:67] Creating Layer loss
I1028 19:05:06.306326 19260 net.cpp:394] loss <- ip2
I1028 19:05:06.306334 19260 net.cpp:394] loss <- label
I1028 19:05:06.306341 19260 net.cpp:356] loss -> loss
I1028 19:05:06.306351 19260 net.cpp:96] Setting up loss
I1028 19:05:06.306363 19260 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 19:05:06.306368 19260 net.cpp:109]     with loss weight 1
I1028 19:05:06.306407 19260 net.cpp:170] loss needs backward computation.
I1028 19:05:06.306412 19260 net.cpp:170] ip2 needs backward computation.
I1028 19:05:06.306416 19260 net.cpp:170] drop4 needs backward computation.
I1028 19:05:06.306421 19260 net.cpp:170] relu4 needs backward computation.
I1028 19:05:06.306424 19260 net.cpp:170] ip1 needs backward computation.
I1028 19:05:06.306429 19260 net.cpp:170] drop3 needs backward computation.
I1028 19:05:06.306433 19260 net.cpp:170] relu3 needs backward computation.
I1028 19:05:06.306437 19260 net.cpp:170] pool3 needs backward computation.
I1028 19:05:06.306442 19260 net.cpp:170] conv3 needs backward computation.
I1028 19:05:06.306447 19260 net.cpp:170] drop2 needs backward computation.
I1028 19:05:06.306450 19260 net.cpp:170] relu2 needs backward computation.
I1028 19:05:06.306455 19260 net.cpp:170] pool2 needs backward computation.
I1028 19:05:06.306459 19260 net.cpp:170] conv2 needs backward computation.
I1028 19:05:06.306464 19260 net.cpp:170] drop1 needs backward computation.
I1028 19:05:06.306468 19260 net.cpp:170] relu1 needs backward computation.
I1028 19:05:06.306473 19260 net.cpp:170] pool1 needs backward computation.
I1028 19:05:06.306476 19260 net.cpp:170] conv1 needs backward computation.
I1028 19:05:06.306481 19260 net.cpp:172] mnist does not need backward computation.
I1028 19:05:06.306485 19260 net.cpp:208] This network produces output loss
I1028 19:05:06.306499 19260 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 19:05:06.306506 19260 net.cpp:219] Network initialization done.
I1028 19:05:06.306510 19260 net.cpp:220] Memory required for data: 119788292
I1028 19:05:06.306571 19260 solver.cpp:41] Solver scaffolding done.
I1028 19:05:06.306577 19260 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_430000.solverstate
I1028 19:05:06.306587 19260 solver.cpp:160] Solving Captcha
I1028 19:05:06.306607 19260 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_430000.solverstate
I1028 19:05:09.305541 19260 solver.cpp:502] SGDSolver: restoring history
I1028 19:05:10.040451 19260 solver.cpp:191] Iteration 430000, loss = 2.55807
I1028 19:05:10.040518 19260 solver.cpp:206]     Train net output #0: loss = 2.55807 (* 1 = 2.55807 loss)
I1028 19:05:10.040534 19260 solver.cpp:403] Iteration 430000, lr = 0.000585343
I1028 19:09:11.986567 19260 solver.cpp:191] Iteration 431000, loss = 2.49821
I1028 19:09:11.987298 19260 solver.cpp:206]     Train net output #0: loss = 2.49821 (* 1 = 2.49821 loss)
I1028 19:09:11.987334 19260 solver.cpp:403] Iteration 431000, lr = 0.000584347
I1028 19:13:13.356087 19260 solver.cpp:191] Iteration 432000, loss = 2.60029
I1028 19:13:13.356710 19260 solver.cpp:206]     Train net output #0: loss = 2.60029 (* 1 = 2.60029 loss)
I1028 19:13:13.356747 19260 solver.cpp:403] Iteration 432000, lr = 0.000583355
I1028 19:17:14.634238 19260 solver.cpp:191] Iteration 433000, loss = 2.37648
I1028 19:17:14.635229 19260 solver.cpp:206]     Train net output #0: loss = 2.37648 (* 1 = 2.37648 loss)
I1028 19:17:14.635262 19260 solver.cpp:403] Iteration 433000, lr = 0.000582368
I1028 19:21:16.004492 19260 solver.cpp:191] Iteration 434000, loss = 2.54586
I1028 19:21:16.005131 19260 solver.cpp:206]     Train net output #0: loss = 2.54586 (* 1 = 2.54586 loss)
I1028 19:21:16.005164 19260 solver.cpp:403] Iteration 434000, lr = 0.000581384
I1028 19:25:17.898135 19260 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_435000.caffemodel
I1028 19:25:22.715487 19260 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_435000.solverstate
I1028 19:25:25.951822 19260 solver.cpp:228] Iteration 435000, loss = 2.3729
I1028 19:25:25.952487 19260 solver.cpp:233] Optimization Done.
I1028 19:25:25.952512 19260 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 19:47:23.133728 32683 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 19:47:23.133827 32683 net.cpp:358] Input 0 -> data
I1028 19:47:23.133852 32683 net.cpp:67] Creating Layer conv1
I1028 19:47:23.133857 32683 net.cpp:394] conv1 <- data
I1028 19:47:23.133863 32683 net.cpp:356] conv1 -> conv1
I1028 19:47:23.133874 32683 net.cpp:96] Setting up conv1
I1028 19:47:23.134191 32683 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 19:47:23.134209 32683 net.cpp:67] Creating Layer pool1
I1028 19:47:23.134214 32683 net.cpp:394] pool1 <- conv1
I1028 19:47:23.134219 32683 net.cpp:356] pool1 -> pool1
I1028 19:47:23.134227 32683 net.cpp:96] Setting up pool1
I1028 19:47:23.134238 32683 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 19:47:23.134245 32683 net.cpp:67] Creating Layer relu1
I1028 19:47:23.134249 32683 net.cpp:394] relu1 <- pool1
I1028 19:47:23.134254 32683 net.cpp:345] relu1 -> pool1 (in-place)
I1028 19:47:23.134260 32683 net.cpp:96] Setting up relu1
I1028 19:47:23.134264 32683 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 19:47:23.134269 32683 net.cpp:67] Creating Layer drop1
I1028 19:47:23.134274 32683 net.cpp:394] drop1 <- pool1
I1028 19:47:23.134281 32683 net.cpp:345] drop1 -> pool1 (in-place)
I1028 19:47:23.134289 32683 net.cpp:96] Setting up drop1
I1028 19:47:23.134294 32683 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 19:47:23.134300 32683 net.cpp:67] Creating Layer conv2
I1028 19:47:23.134305 32683 net.cpp:394] conv2 <- pool1
I1028 19:47:23.134311 32683 net.cpp:356] conv2 -> conv2
I1028 19:47:23.134318 32683 net.cpp:96] Setting up conv2
I1028 19:47:23.134871 32683 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 19:47:23.134887 32683 net.cpp:67] Creating Layer pool2
I1028 19:47:23.134892 32683 net.cpp:394] pool2 <- conv2
I1028 19:47:23.134897 32683 net.cpp:356] pool2 -> pool2
I1028 19:47:23.134904 32683 net.cpp:96] Setting up pool2
I1028 19:47:23.134910 32683 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 19:47:23.134915 32683 net.cpp:67] Creating Layer relu2
I1028 19:47:23.134919 32683 net.cpp:394] relu2 <- pool2
I1028 19:47:23.134927 32683 net.cpp:345] relu2 -> pool2 (in-place)
I1028 19:47:23.134932 32683 net.cpp:96] Setting up relu2
I1028 19:47:23.134935 32683 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 19:47:23.134941 32683 net.cpp:67] Creating Layer drop2
I1028 19:47:23.134944 32683 net.cpp:394] drop2 <- pool2
I1028 19:47:23.134951 32683 net.cpp:345] drop2 -> pool2 (in-place)
I1028 19:47:23.134958 32683 net.cpp:96] Setting up drop2
I1028 19:47:23.134961 32683 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 19:47:23.134968 32683 net.cpp:67] Creating Layer conv3
I1028 19:47:23.134973 32683 net.cpp:394] conv3 <- pool2
I1028 19:47:23.134977 32683 net.cpp:356] conv3 -> conv3
I1028 19:47:23.134984 32683 net.cpp:96] Setting up conv3
I1028 19:47:23.136464 32683 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 19:47:23.136482 32683 net.cpp:67] Creating Layer pool3
I1028 19:47:23.136487 32683 net.cpp:394] pool3 <- conv3
I1028 19:47:23.136494 32683 net.cpp:356] pool3 -> pool3
I1028 19:47:23.136507 32683 net.cpp:96] Setting up pool3
I1028 19:47:23.136512 32683 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 19:47:23.136518 32683 net.cpp:67] Creating Layer relu3
I1028 19:47:23.136523 32683 net.cpp:394] relu3 <- pool3
I1028 19:47:23.136531 32683 net.cpp:345] relu3 -> pool3 (in-place)
I1028 19:47:23.136538 32683 net.cpp:96] Setting up relu3
I1028 19:47:23.136544 32683 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 19:47:23.136550 32683 net.cpp:67] Creating Layer drop3
I1028 19:47:23.136554 32683 net.cpp:394] drop3 <- pool3
I1028 19:47:23.136559 32683 net.cpp:345] drop3 -> pool3 (in-place)
I1028 19:47:23.136565 32683 net.cpp:96] Setting up drop3
I1028 19:47:23.136569 32683 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 19:47:23.136575 32683 net.cpp:67] Creating Layer ip1
I1028 19:47:23.136579 32683 net.cpp:394] ip1 <- pool3
I1028 19:47:23.136586 32683 net.cpp:356] ip1 -> ip1
I1028 19:47:23.136593 32683 net.cpp:96] Setting up ip1
I1028 19:47:23.626809 32683 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 19:47:23.626873 32683 net.cpp:67] Creating Layer relu4
I1028 19:47:23.626881 32683 net.cpp:394] relu4 <- ip1
I1028 19:47:23.626890 32683 net.cpp:345] relu4 -> ip1 (in-place)
I1028 19:47:23.626899 32683 net.cpp:96] Setting up relu4
I1028 19:47:23.626904 32683 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 19:47:23.626912 32683 net.cpp:67] Creating Layer drop4
I1028 19:47:23.626916 32683 net.cpp:394] drop4 <- ip1
I1028 19:47:23.626921 32683 net.cpp:345] drop4 -> ip1 (in-place)
I1028 19:47:23.626927 32683 net.cpp:96] Setting up drop4
I1028 19:47:23.626934 32683 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 19:47:23.626943 32683 net.cpp:67] Creating Layer ip2
I1028 19:47:23.626947 32683 net.cpp:394] ip2 <- ip1
I1028 19:47:23.626953 32683 net.cpp:356] ip2 -> ip2
I1028 19:47:23.626967 32683 net.cpp:96] Setting up ip2
I1028 19:47:23.636467 32683 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 19:47:23.636545 32683 net.cpp:67] Creating Layer prob
I1028 19:47:23.636553 32683 net.cpp:394] prob <- ip2
I1028 19:47:23.636561 32683 net.cpp:356] prob -> prob
I1028 19:47:23.636571 32683 net.cpp:96] Setting up prob
I1028 19:47:23.636579 32683 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 19:47:23.636582 32683 net.cpp:172] prob does not need backward computation.
I1028 19:47:23.636586 32683 net.cpp:172] ip2 does not need backward computation.
I1028 19:47:23.636590 32683 net.cpp:172] drop4 does not need backward computation.
I1028 19:47:23.636593 32683 net.cpp:172] relu4 does not need backward computation.
I1028 19:47:23.636597 32683 net.cpp:172] ip1 does not need backward computation.
I1028 19:47:23.636600 32683 net.cpp:172] drop3 does not need backward computation.
I1028 19:47:23.636603 32683 net.cpp:172] relu3 does not need backward computation.
I1028 19:47:23.636607 32683 net.cpp:172] pool3 does not need backward computation.
I1028 19:47:23.636610 32683 net.cpp:172] conv3 does not need backward computation.
I1028 19:47:23.636615 32683 net.cpp:172] drop2 does not need backward computation.
I1028 19:47:23.636617 32683 net.cpp:172] relu2 does not need backward computation.
I1028 19:47:23.636621 32683 net.cpp:172] pool2 does not need backward computation.
I1028 19:47:23.636625 32683 net.cpp:172] conv2 does not need backward computation.
I1028 19:47:23.636628 32683 net.cpp:172] drop1 does not need backward computation.
I1028 19:47:23.636631 32683 net.cpp:172] relu1 does not need backward computation.
I1028 19:47:23.636634 32683 net.cpp:172] pool1 does not need backward computation.
I1028 19:47:23.636638 32683 net.cpp:172] conv1 does not need backward computation.
I1028 19:47:23.636641 32683 net.cpp:208] This network produces output prob
I1028 19:47:23.636656 32683 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 19:47:23.636664 32683 net.cpp:219] Network initialization done.
I1028 19:47:23.636667 32683 net.cpp:220] Memory required for data: 1837200
I1028 20:21:11.141540  8749 convert_imageset.cpp:70] Shuffling data
I1028 20:21:11.752840  8749 convert_imageset.cpp:73] A total of 60000 images.
I1028 20:21:11.752923  8749 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 20:21:14.206882  8749 convert_imageset.cpp:177] Processed 1000 files.
E1028 20:21:16.138170  8749 convert_imageset.cpp:177] Processed 2000 files.
E1028 20:21:18.347851  8749 convert_imageset.cpp:177] Processed 3000 files.
E1028 20:21:20.654983  8749 convert_imageset.cpp:177] Processed 4000 files.
E1028 20:21:22.713150  8749 convert_imageset.cpp:177] Processed 5000 files.
E1028 20:21:24.757405  8749 convert_imageset.cpp:177] Processed 6000 files.
E1028 20:21:26.797130  8749 convert_imageset.cpp:177] Processed 7000 files.
E1028 20:21:28.682663  8749 convert_imageset.cpp:177] Processed 8000 files.
E1028 20:21:30.600589  8749 convert_imageset.cpp:177] Processed 9000 files.
E1028 20:21:32.606081  8749 convert_imageset.cpp:177] Processed 10000 files.
E1028 20:21:34.564661  8749 convert_imageset.cpp:177] Processed 11000 files.
E1028 20:21:36.467128  8749 convert_imageset.cpp:177] Processed 12000 files.
E1028 20:21:38.371281  8749 convert_imageset.cpp:177] Processed 13000 files.
E1028 20:21:40.155747  8749 convert_imageset.cpp:177] Processed 14000 files.
E1028 20:21:42.031246  8749 convert_imageset.cpp:177] Processed 15000 files.
E1028 20:21:43.841338  8749 convert_imageset.cpp:177] Processed 16000 files.
E1028 20:21:45.622467  8749 convert_imageset.cpp:177] Processed 17000 files.
E1028 20:21:47.375751  8749 convert_imageset.cpp:177] Processed 18000 files.
E1028 20:21:49.171728  8749 convert_imageset.cpp:177] Processed 19000 files.
E1028 20:21:50.977355  8749 convert_imageset.cpp:177] Processed 20000 files.
E1028 20:21:52.863733  8749 convert_imageset.cpp:177] Processed 21000 files.
E1028 20:21:54.583699  8749 convert_imageset.cpp:177] Processed 22000 files.
E1028 20:21:56.316547  8749 convert_imageset.cpp:177] Processed 23000 files.
E1028 20:21:58.052911  8749 convert_imageset.cpp:177] Processed 24000 files.
E1028 20:21:59.744859  8749 convert_imageset.cpp:177] Processed 25000 files.
E1028 20:22:01.481637  8749 convert_imageset.cpp:177] Processed 26000 files.
E1028 20:22:03.226019  8749 convert_imageset.cpp:177] Processed 27000 files.
E1028 20:22:04.966024  8749 convert_imageset.cpp:177] Processed 28000 files.
E1028 20:22:06.649626  8749 convert_imageset.cpp:177] Processed 29000 files.
E1028 20:22:08.368330  8749 convert_imageset.cpp:177] Processed 30000 files.
E1028 20:22:10.099254  8749 convert_imageset.cpp:177] Processed 31000 files.
E1028 20:22:11.880791  8749 convert_imageset.cpp:177] Processed 32000 files.
E1028 20:22:13.635869  8749 convert_imageset.cpp:177] Processed 33000 files.
E1028 20:22:15.282796  8749 convert_imageset.cpp:177] Processed 34000 files.
E1028 20:22:16.778199  8749 convert_imageset.cpp:177] Processed 35000 files.
E1028 20:22:18.374271  8749 convert_imageset.cpp:177] Processed 36000 files.
E1028 20:22:19.970237  8749 convert_imageset.cpp:177] Processed 37000 files.
E1028 20:22:21.460111  8749 convert_imageset.cpp:177] Processed 38000 files.
E1028 20:22:23.077723  8749 convert_imageset.cpp:177] Processed 39000 files.
E1028 20:22:24.590304  8749 convert_imageset.cpp:177] Processed 40000 files.
E1028 20:22:26.228241  8749 convert_imageset.cpp:177] Processed 41000 files.
E1028 20:22:27.815984  8749 convert_imageset.cpp:177] Processed 42000 files.
E1028 20:22:29.353441  8749 convert_imageset.cpp:177] Processed 43000 files.
E1028 20:22:30.916952  8749 convert_imageset.cpp:177] Processed 44000 files.
E1028 20:22:32.387085  8749 convert_imageset.cpp:177] Processed 45000 files.
E1028 20:22:33.879257  8749 convert_imageset.cpp:177] Processed 46000 files.
E1028 20:22:35.421156  8749 convert_imageset.cpp:177] Processed 47000 files.
E1028 20:22:36.865727  8749 convert_imageset.cpp:177] Processed 48000 files.
E1028 20:22:38.446562  8749 convert_imageset.cpp:177] Processed 49000 files.
E1028 20:22:39.936107  8749 convert_imageset.cpp:177] Processed 50000 files.
E1028 20:22:41.448858  8749 convert_imageset.cpp:177] Processed 51000 files.
E1028 20:22:43.005885  8749 convert_imageset.cpp:177] Processed 52000 files.
E1028 20:22:44.598484  8749 convert_imageset.cpp:177] Processed 53000 files.
E1028 20:22:46.181470  8749 convert_imageset.cpp:177] Processed 54000 files.
E1028 20:22:47.785873  8749 convert_imageset.cpp:177] Processed 55000 files.
E1028 20:22:49.419037  8749 convert_imageset.cpp:177] Processed 56000 files.
E1028 20:22:51.051023  8749 convert_imageset.cpp:177] Processed 57000 files.
E1028 20:22:52.719327  8749 convert_imageset.cpp:177] Processed 58000 files.
E1028 20:22:54.328546  8749 convert_imageset.cpp:177] Processed 59000 files.
E1028 20:22:55.897048  8749 convert_imageset.cpp:177] Processed 60000 files.
I1028 20:22:56.643641  8902 caffe.cpp:99] Use GPU with device ID 0
I1028 20:22:56.995352  8902 caffe.cpp:107] Starting Optimization
I1028 20:22:56.995466  8902 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 440000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 20:22:56.995491  8902 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 20:22:57.010931  8902 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 20:22:57.011148  8902 net.cpp:67] Creating Layer mnist
I1028 20:22:57.011174  8902 net.cpp:356] mnist -> data
I1028 20:22:57.011210  8902 net.cpp:356] mnist -> label
I1028 20:22:57.011241  8902 net.cpp:96] Setting up mnist
I1028 20:22:57.018625  8902 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 20:22:57.018720  8902 data_layer.cpp:128] output data size: 64,1,50,180
I1028 20:22:57.019657  8902 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 20:22:57.019681  8902 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 20:22:57.019696  8902 net.cpp:67] Creating Layer conv1
I1028 20:22:57.019704  8902 net.cpp:394] conv1 <- data
I1028 20:22:57.019726  8902 net.cpp:356] conv1 -> conv1
I1028 20:22:57.019742  8902 net.cpp:96] Setting up conv1
I1028 20:22:57.020220  8902 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 20:22:57.020258  8902 net.cpp:67] Creating Layer pool1
I1028 20:22:57.020267  8902 net.cpp:394] pool1 <- conv1
I1028 20:22:57.020275  8902 net.cpp:356] pool1 -> pool1
I1028 20:22:57.020284  8902 net.cpp:96] Setting up pool1
I1028 20:22:57.020303  8902 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 20:22:57.020313  8902 net.cpp:67] Creating Layer relu1
I1028 20:22:57.020319  8902 net.cpp:394] relu1 <- pool1
I1028 20:22:57.020329  8902 net.cpp:345] relu1 -> pool1 (in-place)
I1028 20:22:57.020339  8902 net.cpp:96] Setting up relu1
I1028 20:22:57.020344  8902 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 20:22:57.020354  8902 net.cpp:67] Creating Layer drop1
I1028 20:22:57.020360  8902 net.cpp:394] drop1 <- pool1
I1028 20:22:57.020367  8902 net.cpp:345] drop1 -> pool1 (in-place)
I1028 20:22:57.020375  8902 net.cpp:96] Setting up drop1
I1028 20:22:57.020382  8902 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 20:22:57.020395  8902 net.cpp:67] Creating Layer conv2
I1028 20:22:57.020401  8902 net.cpp:394] conv2 <- pool1
I1028 20:22:57.020409  8902 net.cpp:356] conv2 -> conv2
I1028 20:22:57.020437  8902 net.cpp:96] Setting up conv2
I1028 20:22:57.021191  8902 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 20:22:57.021212  8902 net.cpp:67] Creating Layer pool2
I1028 20:22:57.021221  8902 net.cpp:394] pool2 <- conv2
I1028 20:22:57.021230  8902 net.cpp:356] pool2 -> pool2
I1028 20:22:57.021240  8902 net.cpp:96] Setting up pool2
I1028 20:22:57.021247  8902 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 20:22:57.021255  8902 net.cpp:67] Creating Layer relu2
I1028 20:22:57.021261  8902 net.cpp:394] relu2 <- pool2
I1028 20:22:57.021270  8902 net.cpp:345] relu2 -> pool2 (in-place)
I1028 20:22:57.021277  8902 net.cpp:96] Setting up relu2
I1028 20:22:57.021283  8902 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 20:22:57.021296  8902 net.cpp:67] Creating Layer drop2
I1028 20:22:57.021302  8902 net.cpp:394] drop2 <- pool2
I1028 20:22:57.021311  8902 net.cpp:345] drop2 -> pool2 (in-place)
I1028 20:22:57.021318  8902 net.cpp:96] Setting up drop2
I1028 20:22:57.021324  8902 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 20:22:57.021338  8902 net.cpp:67] Creating Layer conv3
I1028 20:22:57.021344  8902 net.cpp:394] conv3 <- pool2
I1028 20:22:57.021353  8902 net.cpp:356] conv3 -> conv3
I1028 20:22:57.021363  8902 net.cpp:96] Setting up conv3
I1028 20:22:57.023344  8902 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 20:22:57.023371  8902 net.cpp:67] Creating Layer pool3
I1028 20:22:57.023378  8902 net.cpp:394] pool3 <- conv3
I1028 20:22:57.023387  8902 net.cpp:356] pool3 -> pool3
I1028 20:22:57.023397  8902 net.cpp:96] Setting up pool3
I1028 20:22:57.023406  8902 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 20:22:57.023416  8902 net.cpp:67] Creating Layer relu3
I1028 20:22:57.023423  8902 net.cpp:394] relu3 <- pool3
I1028 20:22:57.023430  8902 net.cpp:345] relu3 -> pool3 (in-place)
I1028 20:22:57.023438  8902 net.cpp:96] Setting up relu3
I1028 20:22:57.023452  8902 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 20:22:57.023461  8902 net.cpp:67] Creating Layer drop3
I1028 20:22:57.023468  8902 net.cpp:394] drop3 <- pool3
I1028 20:22:57.023478  8902 net.cpp:345] drop3 -> pool3 (in-place)
I1028 20:22:57.023486  8902 net.cpp:96] Setting up drop3
I1028 20:22:57.023493  8902 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 20:22:57.023502  8902 net.cpp:67] Creating Layer ip1
I1028 20:22:57.023509  8902 net.cpp:394] ip1 <- pool3
I1028 20:22:57.023516  8902 net.cpp:356] ip1 -> ip1
I1028 20:22:57.023555  8902 net.cpp:96] Setting up ip1
I1028 20:22:57.549893  8902 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 20:22:57.549952  8902 net.cpp:67] Creating Layer relu4
I1028 20:22:57.549959  8902 net.cpp:394] relu4 <- ip1
I1028 20:22:57.549970  8902 net.cpp:345] relu4 -> ip1 (in-place)
I1028 20:22:57.549980  8902 net.cpp:96] Setting up relu4
I1028 20:22:57.549985  8902 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 20:22:57.549993  8902 net.cpp:67] Creating Layer drop4
I1028 20:22:57.549998  8902 net.cpp:394] drop4 <- ip1
I1028 20:22:57.550003  8902 net.cpp:345] drop4 -> ip1 (in-place)
I1028 20:22:57.550009  8902 net.cpp:96] Setting up drop4
I1028 20:22:57.550014  8902 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 20:22:57.550025  8902 net.cpp:67] Creating Layer ip2
I1028 20:22:57.550030  8902 net.cpp:394] ip2 <- ip1
I1028 20:22:57.550037  8902 net.cpp:356] ip2 -> ip2
I1028 20:22:57.550045  8902 net.cpp:96] Setting up ip2
I1028 20:22:57.559020  8902 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 20:22:57.559078  8902 net.cpp:67] Creating Layer loss
I1028 20:22:57.559085  8902 net.cpp:394] loss <- ip2
I1028 20:22:57.559093  8902 net.cpp:394] loss <- label
I1028 20:22:57.559100  8902 net.cpp:356] loss -> loss
I1028 20:22:57.559109  8902 net.cpp:96] Setting up loss
I1028 20:22:57.559119  8902 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 20:22:57.559124  8902 net.cpp:109]     with loss weight 1
I1028 20:22:57.559161  8902 net.cpp:170] loss needs backward computation.
I1028 20:22:57.559166  8902 net.cpp:170] ip2 needs backward computation.
I1028 20:22:57.559172  8902 net.cpp:170] drop4 needs backward computation.
I1028 20:22:57.559177  8902 net.cpp:170] relu4 needs backward computation.
I1028 20:22:57.559181  8902 net.cpp:170] ip1 needs backward computation.
I1028 20:22:57.559186  8902 net.cpp:170] drop3 needs backward computation.
I1028 20:22:57.559191  8902 net.cpp:170] relu3 needs backward computation.
I1028 20:22:57.559195  8902 net.cpp:170] pool3 needs backward computation.
I1028 20:22:57.559201  8902 net.cpp:170] conv3 needs backward computation.
I1028 20:22:57.559206  8902 net.cpp:170] drop2 needs backward computation.
I1028 20:22:57.559209  8902 net.cpp:170] relu2 needs backward computation.
I1028 20:22:57.559214  8902 net.cpp:170] pool2 needs backward computation.
I1028 20:22:57.559218  8902 net.cpp:170] conv2 needs backward computation.
I1028 20:22:57.559223  8902 net.cpp:170] drop1 needs backward computation.
I1028 20:22:57.559227  8902 net.cpp:170] relu1 needs backward computation.
I1028 20:22:57.559232  8902 net.cpp:170] pool1 needs backward computation.
I1028 20:22:57.559237  8902 net.cpp:170] conv1 needs backward computation.
I1028 20:22:57.559242  8902 net.cpp:172] mnist does not need backward computation.
I1028 20:22:57.559245  8902 net.cpp:208] This network produces output loss
I1028 20:22:57.559257  8902 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 20:22:57.559264  8902 net.cpp:219] Network initialization done.
I1028 20:22:57.559268  8902 net.cpp:220] Memory required for data: 119788292
I1028 20:22:57.559326  8902 solver.cpp:41] Solver scaffolding done.
I1028 20:22:57.559334  8902 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_435000.solverstate
I1028 20:22:57.559337  8902 solver.cpp:160] Solving Captcha
I1028 20:22:57.559356  8902 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_435000.solverstate
I1028 20:23:02.192806  8902 solver.cpp:502] SGDSolver: restoring history
I1028 20:23:02.946878  8902 solver.cpp:191] Iteration 435000, loss = 2.41931
I1028 20:23:02.946938  8902 solver.cpp:206]     Train net output #0: loss = 2.41931 (* 1 = 2.41931 loss)
I1028 20:23:02.946954  8902 solver.cpp:403] Iteration 435000, lr = 0.000580403
I1028 20:27:05.062486  8902 solver.cpp:191] Iteration 436000, loss = 2.65914
I1028 20:27:05.063089  8902 solver.cpp:206]     Train net output #0: loss = 2.65914 (* 1 = 2.65914 loss)
I1028 20:27:05.063102  8902 solver.cpp:403] Iteration 436000, lr = 0.000579427
I1028 20:31:06.655377  8902 solver.cpp:191] Iteration 437000, loss = 2.46379
I1028 20:31:06.656046  8902 solver.cpp:206]     Train net output #0: loss = 2.46379 (* 1 = 2.46379 loss)
I1028 20:31:06.656085  8902 solver.cpp:403] Iteration 437000, lr = 0.000578455
I1028 20:35:08.297991  8902 solver.cpp:191] Iteration 438000, loss = 2.54808
I1028 20:35:08.298555  8902 solver.cpp:206]     Train net output #0: loss = 2.54808 (* 1 = 2.54808 loss)
I1028 20:35:08.298588  8902 solver.cpp:403] Iteration 438000, lr = 0.000577486
I1028 20:39:09.817623  8902 solver.cpp:191] Iteration 439000, loss = 2.34687
I1028 20:39:09.818209  8902 solver.cpp:206]     Train net output #0: loss = 2.34687 (* 1 = 2.34687 loss)
I1028 20:39:09.818224  8902 solver.cpp:403] Iteration 439000, lr = 0.000576521
I1028 20:43:11.533089  8902 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_440000.caffemodel
I1028 20:43:16.496347  8902 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_440000.solverstate
I1028 20:43:20.123275  8902 solver.cpp:228] Iteration 440000, loss = 2.34761
I1028 20:43:20.123877  8902 solver.cpp:233] Optimization Done.
I1028 20:43:20.123900  8902 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 21:05:15.784328 22397 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 21:05:15.784999 22397 net.cpp:358] Input 0 -> data
I1028 21:05:15.785043 22397 net.cpp:67] Creating Layer conv1
I1028 21:05:15.785053 22397 net.cpp:394] conv1 <- data
I1028 21:05:15.785063 22397 net.cpp:356] conv1 -> conv1
I1028 21:05:15.785079 22397 net.cpp:96] Setting up conv1
I1028 21:05:15.785594 22397 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 21:05:15.785624 22397 net.cpp:67] Creating Layer pool1
I1028 21:05:15.785631 22397 net.cpp:394] pool1 <- conv1
I1028 21:05:15.785641 22397 net.cpp:356] pool1 -> pool1
I1028 21:05:15.785652 22397 net.cpp:96] Setting up pool1
I1028 21:05:15.785670 22397 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 21:05:15.785681 22397 net.cpp:67] Creating Layer relu1
I1028 21:05:15.785687 22397 net.cpp:394] relu1 <- pool1
I1028 21:05:15.785696 22397 net.cpp:345] relu1 -> pool1 (in-place)
I1028 21:05:15.785706 22397 net.cpp:96] Setting up relu1
I1028 21:05:15.785712 22397 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 21:05:15.785722 22397 net.cpp:67] Creating Layer drop1
I1028 21:05:15.785728 22397 net.cpp:394] drop1 <- pool1
I1028 21:05:15.785737 22397 net.cpp:345] drop1 -> pool1 (in-place)
I1028 21:05:15.785745 22397 net.cpp:96] Setting up drop1
I1028 21:05:15.785753 22397 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 21:05:15.785768 22397 net.cpp:67] Creating Layer conv2
I1028 21:05:15.785775 22397 net.cpp:394] conv2 <- pool1
I1028 21:05:15.785787 22397 net.cpp:356] conv2 -> conv2
I1028 21:05:15.785799 22397 net.cpp:96] Setting up conv2
I1028 21:05:15.786679 22397 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 21:05:15.786705 22397 net.cpp:67] Creating Layer pool2
I1028 21:05:15.786711 22397 net.cpp:394] pool2 <- conv2
I1028 21:05:15.786721 22397 net.cpp:356] pool2 -> pool2
I1028 21:05:15.786732 22397 net.cpp:96] Setting up pool2
I1028 21:05:15.786741 22397 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 21:05:15.786749 22397 net.cpp:67] Creating Layer relu2
I1028 21:05:15.786756 22397 net.cpp:394] relu2 <- pool2
I1028 21:05:15.786767 22397 net.cpp:345] relu2 -> pool2 (in-place)
I1028 21:05:15.786777 22397 net.cpp:96] Setting up relu2
I1028 21:05:15.786784 22397 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 21:05:15.786792 22397 net.cpp:67] Creating Layer drop2
I1028 21:05:15.786799 22397 net.cpp:394] drop2 <- pool2
I1028 21:05:15.786808 22397 net.cpp:345] drop2 -> pool2 (in-place)
I1028 21:05:15.786818 22397 net.cpp:96] Setting up drop2
I1028 21:05:15.786824 22397 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 21:05:15.786837 22397 net.cpp:67] Creating Layer conv3
I1028 21:05:15.786844 22397 net.cpp:394] conv3 <- pool2
I1028 21:05:15.786854 22397 net.cpp:356] conv3 -> conv3
I1028 21:05:15.786865 22397 net.cpp:96] Setting up conv3
I1028 21:05:15.789208 22397 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 21:05:15.789234 22397 net.cpp:67] Creating Layer pool3
I1028 21:05:15.789242 22397 net.cpp:394] pool3 <- conv3
I1028 21:05:15.789254 22397 net.cpp:356] pool3 -> pool3
I1028 21:05:15.789265 22397 net.cpp:96] Setting up pool3
I1028 21:05:15.789274 22397 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 21:05:15.789283 22397 net.cpp:67] Creating Layer relu3
I1028 21:05:15.789289 22397 net.cpp:394] relu3 <- pool3
I1028 21:05:15.789299 22397 net.cpp:345] relu3 -> pool3 (in-place)
I1028 21:05:15.789306 22397 net.cpp:96] Setting up relu3
I1028 21:05:15.789314 22397 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 21:05:15.789327 22397 net.cpp:67] Creating Layer drop3
I1028 21:05:15.789335 22397 net.cpp:394] drop3 <- pool3
I1028 21:05:15.789345 22397 net.cpp:345] drop3 -> pool3 (in-place)
I1028 21:05:15.789353 22397 net.cpp:96] Setting up drop3
I1028 21:05:15.789361 22397 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 21:05:15.789371 22397 net.cpp:67] Creating Layer ip1
I1028 21:05:15.789377 22397 net.cpp:394] ip1 <- pool3
I1028 21:05:15.789391 22397 net.cpp:356] ip1 -> ip1
I1028 21:05:15.789402 22397 net.cpp:96] Setting up ip1
I1028 21:05:16.300174 22397 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 21:05:16.300240 22397 net.cpp:67] Creating Layer relu4
I1028 21:05:16.300247 22397 net.cpp:394] relu4 <- ip1
I1028 21:05:16.300259 22397 net.cpp:345] relu4 -> ip1 (in-place)
I1028 21:05:16.300268 22397 net.cpp:96] Setting up relu4
I1028 21:05:16.300274 22397 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 21:05:16.300282 22397 net.cpp:67] Creating Layer drop4
I1028 21:05:16.300285 22397 net.cpp:394] drop4 <- ip1
I1028 21:05:16.300292 22397 net.cpp:345] drop4 -> ip1 (in-place)
I1028 21:05:16.300297 22397 net.cpp:96] Setting up drop4
I1028 21:05:16.300302 22397 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 21:05:16.300310 22397 net.cpp:67] Creating Layer ip2
I1028 21:05:16.300314 22397 net.cpp:394] ip2 <- ip1
I1028 21:05:16.300324 22397 net.cpp:356] ip2 -> ip2
I1028 21:05:16.300336 22397 net.cpp:96] Setting up ip2
I1028 21:05:16.312814 22397 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 21:05:16.312882 22397 net.cpp:67] Creating Layer prob
I1028 21:05:16.312891 22397 net.cpp:394] prob <- ip2
I1028 21:05:16.312898 22397 net.cpp:356] prob -> prob
I1028 21:05:16.312909 22397 net.cpp:96] Setting up prob
I1028 21:05:16.312916 22397 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 21:05:16.312919 22397 net.cpp:172] prob does not need backward computation.
I1028 21:05:16.312923 22397 net.cpp:172] ip2 does not need backward computation.
I1028 21:05:16.312927 22397 net.cpp:172] drop4 does not need backward computation.
I1028 21:05:16.312932 22397 net.cpp:172] relu4 does not need backward computation.
I1028 21:05:16.312935 22397 net.cpp:172] ip1 does not need backward computation.
I1028 21:05:16.312939 22397 net.cpp:172] drop3 does not need backward computation.
I1028 21:05:16.312942 22397 net.cpp:172] relu3 does not need backward computation.
I1028 21:05:16.312947 22397 net.cpp:172] pool3 does not need backward computation.
I1028 21:05:16.312950 22397 net.cpp:172] conv3 does not need backward computation.
I1028 21:05:16.312953 22397 net.cpp:172] drop2 does not need backward computation.
I1028 21:05:16.312958 22397 net.cpp:172] relu2 does not need backward computation.
I1028 21:05:16.312961 22397 net.cpp:172] pool2 does not need backward computation.
I1028 21:05:16.312965 22397 net.cpp:172] conv2 does not need backward computation.
I1028 21:05:16.312968 22397 net.cpp:172] drop1 does not need backward computation.
I1028 21:05:16.312973 22397 net.cpp:172] relu1 does not need backward computation.
I1028 21:05:16.312976 22397 net.cpp:172] pool1 does not need backward computation.
I1028 21:05:16.312980 22397 net.cpp:172] conv1 does not need backward computation.
I1028 21:05:16.312983 22397 net.cpp:208] This network produces output prob
I1028 21:05:16.312997 22397 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 21:05:16.313005 22397 net.cpp:219] Network initialization done.
I1028 21:05:16.313009 22397 net.cpp:220] Memory required for data: 1837200
I1028 21:38:41.689693 30639 convert_imageset.cpp:70] Shuffling data
I1028 21:38:42.279916 30639 convert_imageset.cpp:73] A total of 60000 images.
I1028 21:38:42.279994 30639 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 21:38:44.448370 30639 convert_imageset.cpp:177] Processed 1000 files.
E1028 21:38:46.475214 30639 convert_imageset.cpp:177] Processed 2000 files.
E1028 21:38:48.402720 30639 convert_imageset.cpp:177] Processed 3000 files.
E1028 21:38:50.433403 30639 convert_imageset.cpp:177] Processed 4000 files.
E1028 21:38:52.368763 30639 convert_imageset.cpp:177] Processed 5000 files.
E1028 21:38:54.171013 30639 convert_imageset.cpp:177] Processed 6000 files.
E1028 21:38:56.104276 30639 convert_imageset.cpp:177] Processed 7000 files.
E1028 21:38:58.149257 30639 convert_imageset.cpp:177] Processed 8000 files.
E1028 21:39:00.277565 30639 convert_imageset.cpp:177] Processed 9000 files.
E1028 21:39:01.986487 30639 convert_imageset.cpp:177] Processed 10000 files.
E1028 21:39:03.693424 30639 convert_imageset.cpp:177] Processed 11000 files.
E1028 21:39:05.480836 30639 convert_imageset.cpp:177] Processed 12000 files.
E1028 21:39:07.263871 30639 convert_imageset.cpp:177] Processed 13000 files.
E1028 21:39:08.971604 30639 convert_imageset.cpp:177] Processed 14000 files.
E1028 21:39:10.781190 30639 convert_imageset.cpp:177] Processed 15000 files.
E1028 21:39:12.562376 30639 convert_imageset.cpp:177] Processed 16000 files.
E1028 21:39:14.903127 30639 convert_imageset.cpp:177] Processed 17000 files.
E1028 21:39:16.617046 30639 convert_imageset.cpp:177] Processed 18000 files.
E1028 21:39:18.258457 30639 convert_imageset.cpp:177] Processed 19000 files.
E1028 21:39:19.934363 30639 convert_imageset.cpp:177] Processed 20000 files.
E1028 21:39:21.695000 30639 convert_imageset.cpp:177] Processed 21000 files.
E1028 21:39:23.371423 30639 convert_imageset.cpp:177] Processed 22000 files.
E1028 21:39:24.989593 30639 convert_imageset.cpp:177] Processed 23000 files.
E1028 21:39:26.669836 30639 convert_imageset.cpp:177] Processed 24000 files.
E1028 21:39:28.269892 30639 convert_imageset.cpp:177] Processed 25000 files.
E1028 21:39:29.967535 30639 convert_imageset.cpp:177] Processed 26000 files.
E1028 21:39:31.655264 30639 convert_imageset.cpp:177] Processed 27000 files.
E1028 21:39:33.228117 30639 convert_imageset.cpp:177] Processed 28000 files.
E1028 21:39:34.835132 30639 convert_imageset.cpp:177] Processed 29000 files.
E1028 21:39:36.669883 30639 convert_imageset.cpp:177] Processed 30000 files.
E1028 21:39:38.251850 30639 convert_imageset.cpp:177] Processed 31000 files.
E1028 21:39:39.785600 30639 convert_imageset.cpp:177] Processed 32000 files.
E1028 21:39:41.500931 30639 convert_imageset.cpp:177] Processed 33000 files.
E1028 21:39:43.144639 30639 convert_imageset.cpp:177] Processed 34000 files.
E1028 21:39:44.722949 30639 convert_imageset.cpp:177] Processed 35000 files.
E1028 21:39:46.239511 30639 convert_imageset.cpp:177] Processed 36000 files.
E1028 21:39:47.807219 30639 convert_imageset.cpp:177] Processed 37000 files.
E1028 21:39:49.383658 30639 convert_imageset.cpp:177] Processed 38000 files.
E1028 21:39:51.113803 30639 convert_imageset.cpp:177] Processed 39000 files.
E1028 21:39:52.792836 30639 convert_imageset.cpp:177] Processed 40000 files.
E1028 21:39:54.308961 30639 convert_imageset.cpp:177] Processed 41000 files.
E1028 21:39:55.940316 30639 convert_imageset.cpp:177] Processed 42000 files.
E1028 21:39:57.616334 30639 convert_imageset.cpp:177] Processed 43000 files.
E1028 21:39:59.268220 30639 convert_imageset.cpp:177] Processed 44000 files.
E1028 21:40:00.954028 30639 convert_imageset.cpp:177] Processed 45000 files.
E1028 21:40:02.599525 30639 convert_imageset.cpp:177] Processed 46000 files.
E1028 21:40:04.217599 30639 convert_imageset.cpp:177] Processed 47000 files.
E1028 21:40:05.917239 30639 convert_imageset.cpp:177] Processed 48000 files.
E1028 21:40:07.678871 30639 convert_imageset.cpp:177] Processed 49000 files.
E1028 21:40:09.335194 30639 convert_imageset.cpp:177] Processed 50000 files.
E1028 21:40:11.053903 30639 convert_imageset.cpp:177] Processed 51000 files.
E1028 21:40:12.805146 30639 convert_imageset.cpp:177] Processed 52000 files.
E1028 21:40:14.515580 30639 convert_imageset.cpp:177] Processed 53000 files.
E1028 21:40:16.157629 30639 convert_imageset.cpp:177] Processed 54000 files.
E1028 21:40:17.763485 30639 convert_imageset.cpp:177] Processed 55000 files.
E1028 21:40:19.370259 30639 convert_imageset.cpp:177] Processed 56000 files.
E1028 21:40:20.966084 30639 convert_imageset.cpp:177] Processed 57000 files.
E1028 21:40:22.599944 30639 convert_imageset.cpp:177] Processed 58000 files.
E1028 21:40:24.486093 30639 convert_imageset.cpp:177] Processed 59000 files.
E1028 21:40:26.155349 30639 convert_imageset.cpp:177] Processed 60000 files.
I1028 21:40:26.316696 30834 caffe.cpp:99] Use GPU with device ID 0
I1028 21:40:26.681145 30834 caffe.cpp:107] Starting Optimization
I1028 21:40:26.681269 30834 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 445000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 21:40:26.681295 30834 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 21:40:26.684039 30834 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 21:40:26.684195 30834 net.cpp:67] Creating Layer mnist
I1028 21:40:26.684214 30834 net.cpp:356] mnist -> data
I1028 21:40:26.684240 30834 net.cpp:356] mnist -> label
I1028 21:40:26.684263 30834 net.cpp:96] Setting up mnist
I1028 21:40:26.693262 30834 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 21:40:26.693379 30834 data_layer.cpp:128] output data size: 64,1,50,180
I1028 21:40:26.694818 30834 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 21:40:26.694852 30834 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 21:40:26.694878 30834 net.cpp:67] Creating Layer conv1
I1028 21:40:26.694890 30834 net.cpp:394] conv1 <- data
I1028 21:40:26.694921 30834 net.cpp:356] conv1 -> conv1
I1028 21:40:26.694942 30834 net.cpp:96] Setting up conv1
I1028 21:40:26.695703 30834 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 21:40:26.695755 30834 net.cpp:67] Creating Layer pool1
I1028 21:40:26.695767 30834 net.cpp:394] pool1 <- conv1
I1028 21:40:26.695785 30834 net.cpp:356] pool1 -> pool1
I1028 21:40:26.695801 30834 net.cpp:96] Setting up pool1
I1028 21:40:26.695830 30834 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 21:40:26.695845 30834 net.cpp:67] Creating Layer relu1
I1028 21:40:26.695857 30834 net.cpp:394] relu1 <- pool1
I1028 21:40:26.695869 30834 net.cpp:345] relu1 -> pool1 (in-place)
I1028 21:40:26.695883 30834 net.cpp:96] Setting up relu1
I1028 21:40:26.695894 30834 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 21:40:26.695910 30834 net.cpp:67] Creating Layer drop1
I1028 21:40:26.695920 30834 net.cpp:394] drop1 <- pool1
I1028 21:40:26.695938 30834 net.cpp:345] drop1 -> pool1 (in-place)
I1028 21:40:26.695953 30834 net.cpp:96] Setting up drop1
I1028 21:40:26.695966 30834 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 21:40:26.695981 30834 net.cpp:67] Creating Layer conv2
I1028 21:40:26.695991 30834 net.cpp:394] conv2 <- pool1
I1028 21:40:26.696008 30834 net.cpp:356] conv2 -> conv2
I1028 21:40:26.696027 30834 net.cpp:96] Setting up conv2
I1028 21:40:26.697309 30834 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 21:40:26.697346 30834 net.cpp:67] Creating Layer pool2
I1028 21:40:26.697358 30834 net.cpp:394] pool2 <- conv2
I1028 21:40:26.697373 30834 net.cpp:356] pool2 -> pool2
I1028 21:40:26.697388 30834 net.cpp:96] Setting up pool2
I1028 21:40:26.697402 30834 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 21:40:26.697414 30834 net.cpp:67] Creating Layer relu2
I1028 21:40:26.697424 30834 net.cpp:394] relu2 <- pool2
I1028 21:40:26.697441 30834 net.cpp:345] relu2 -> pool2 (in-place)
I1028 21:40:26.697456 30834 net.cpp:96] Setting up relu2
I1028 21:40:26.697468 30834 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 21:40:26.697482 30834 net.cpp:67] Creating Layer drop2
I1028 21:40:26.697494 30834 net.cpp:394] drop2 <- pool2
I1028 21:40:26.697510 30834 net.cpp:345] drop2 -> pool2 (in-place)
I1028 21:40:26.697525 30834 net.cpp:96] Setting up drop2
I1028 21:40:26.697535 30834 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 21:40:26.697551 30834 net.cpp:67] Creating Layer conv3
I1028 21:40:26.697561 30834 net.cpp:394] conv3 <- pool2
I1028 21:40:26.697576 30834 net.cpp:356] conv3 -> conv3
I1028 21:40:26.697592 30834 net.cpp:96] Setting up conv3
I1028 21:40:26.700850 30834 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 21:40:26.700887 30834 net.cpp:67] Creating Layer pool3
I1028 21:40:26.700896 30834 net.cpp:394] pool3 <- conv3
I1028 21:40:26.700911 30834 net.cpp:356] pool3 -> pool3
I1028 21:40:26.700923 30834 net.cpp:96] Setting up pool3
I1028 21:40:26.700933 30834 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 21:40:26.700944 30834 net.cpp:67] Creating Layer relu3
I1028 21:40:26.700952 30834 net.cpp:394] relu3 <- pool3
I1028 21:40:26.700963 30834 net.cpp:345] relu3 -> pool3 (in-place)
I1028 21:40:26.700974 30834 net.cpp:96] Setting up relu3
I1028 21:40:26.700983 30834 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 21:40:26.700992 30834 net.cpp:67] Creating Layer drop3
I1028 21:40:26.701000 30834 net.cpp:394] drop3 <- pool3
I1028 21:40:26.701017 30834 net.cpp:345] drop3 -> pool3 (in-place)
I1028 21:40:26.701028 30834 net.cpp:96] Setting up drop3
I1028 21:40:26.701036 30834 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 21:40:26.701048 30834 net.cpp:67] Creating Layer ip1
I1028 21:40:26.701056 30834 net.cpp:394] ip1 <- pool3
I1028 21:40:26.701069 30834 net.cpp:356] ip1 -> ip1
I1028 21:40:26.701115 30834 net.cpp:96] Setting up ip1
I1028 21:40:27.099958 30834 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 21:40:27.100023 30834 net.cpp:67] Creating Layer relu4
I1028 21:40:27.100030 30834 net.cpp:394] relu4 <- ip1
I1028 21:40:27.100039 30834 net.cpp:345] relu4 -> ip1 (in-place)
I1028 21:40:27.100049 30834 net.cpp:96] Setting up relu4
I1028 21:40:27.100054 30834 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 21:40:27.100061 30834 net.cpp:67] Creating Layer drop4
I1028 21:40:27.100065 30834 net.cpp:394] drop4 <- ip1
I1028 21:40:27.100075 30834 net.cpp:345] drop4 -> ip1 (in-place)
I1028 21:40:27.100080 30834 net.cpp:96] Setting up drop4
I1028 21:40:27.100086 30834 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 21:40:27.100096 30834 net.cpp:67] Creating Layer ip2
I1028 21:40:27.100101 30834 net.cpp:394] ip2 <- ip1
I1028 21:40:27.100109 30834 net.cpp:356] ip2 -> ip2
I1028 21:40:27.100117 30834 net.cpp:96] Setting up ip2
I1028 21:40:27.110024 30834 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 21:40:27.110085 30834 net.cpp:67] Creating Layer loss
I1028 21:40:27.110091 30834 net.cpp:394] loss <- ip2
I1028 21:40:27.110100 30834 net.cpp:394] loss <- label
I1028 21:40:27.110106 30834 net.cpp:356] loss -> loss
I1028 21:40:27.110116 30834 net.cpp:96] Setting up loss
I1028 21:40:27.110129 30834 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 21:40:27.110134 30834 net.cpp:109]     with loss weight 1
I1028 21:40:27.110169 30834 net.cpp:170] loss needs backward computation.
I1028 21:40:27.110175 30834 net.cpp:170] ip2 needs backward computation.
I1028 21:40:27.110179 30834 net.cpp:170] drop4 needs backward computation.
I1028 21:40:27.110183 30834 net.cpp:170] relu4 needs backward computation.
I1028 21:40:27.110188 30834 net.cpp:170] ip1 needs backward computation.
I1028 21:40:27.110193 30834 net.cpp:170] drop3 needs backward computation.
I1028 21:40:27.110198 30834 net.cpp:170] relu3 needs backward computation.
I1028 21:40:27.110201 30834 net.cpp:170] pool3 needs backward computation.
I1028 21:40:27.110206 30834 net.cpp:170] conv3 needs backward computation.
I1028 21:40:27.110211 30834 net.cpp:170] drop2 needs backward computation.
I1028 21:40:27.110215 30834 net.cpp:170] relu2 needs backward computation.
I1028 21:40:27.110220 30834 net.cpp:170] pool2 needs backward computation.
I1028 21:40:27.110224 30834 net.cpp:170] conv2 needs backward computation.
I1028 21:40:27.110229 30834 net.cpp:170] drop1 needs backward computation.
I1028 21:40:27.110234 30834 net.cpp:170] relu1 needs backward computation.
I1028 21:40:27.110237 30834 net.cpp:170] pool1 needs backward computation.
I1028 21:40:27.110242 30834 net.cpp:170] conv1 needs backward computation.
I1028 21:40:27.110247 30834 net.cpp:172] mnist does not need backward computation.
I1028 21:40:27.110251 30834 net.cpp:208] This network produces output loss
I1028 21:40:27.110262 30834 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 21:40:27.110270 30834 net.cpp:219] Network initialization done.
I1028 21:40:27.110273 30834 net.cpp:220] Memory required for data: 119788292
I1028 21:40:27.110332 30834 solver.cpp:41] Solver scaffolding done.
I1028 21:40:27.110338 30834 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_440000.solverstate
I1028 21:40:27.110342 30834 solver.cpp:160] Solving Captcha
I1028 21:40:27.110362 30834 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_440000.solverstate
I1028 21:40:30.609489 30834 solver.cpp:502] SGDSolver: restoring history
I1028 21:40:31.342084 30834 solver.cpp:191] Iteration 440000, loss = 2.46985
I1028 21:40:31.342154 30834 solver.cpp:206]     Train net output #0: loss = 2.46985 (* 1 = 2.46985 loss)
I1028 21:40:31.342170 30834 solver.cpp:403] Iteration 440000, lr = 0.00057556
I1028 21:44:33.574281 30834 solver.cpp:191] Iteration 441000, loss = 2.31459
I1028 21:44:33.575016 30834 solver.cpp:206]     Train net output #0: loss = 2.31459 (* 1 = 2.31459 loss)
I1028 21:44:33.575049 30834 solver.cpp:403] Iteration 441000, lr = 0.000574603
I1028 21:48:35.292312 30834 solver.cpp:191] Iteration 442000, loss = 2.57088
I1028 21:48:35.293040 30834 solver.cpp:206]     Train net output #0: loss = 2.57088 (* 1 = 2.57088 loss)
I1028 21:48:35.293073 30834 solver.cpp:403] Iteration 442000, lr = 0.000573649
I1028 21:52:37.005022 30834 solver.cpp:191] Iteration 443000, loss = 2.44021
I1028 21:52:37.005614 30834 solver.cpp:206]     Train net output #0: loss = 2.44021 (* 1 = 2.44021 loss)
I1028 21:52:37.005648 30834 solver.cpp:403] Iteration 443000, lr = 0.000572699
I1028 21:56:38.679343 30834 solver.cpp:191] Iteration 444000, loss = 2.54311
I1028 21:56:38.679914 30834 solver.cpp:206]     Train net output #0: loss = 2.54311 (* 1 = 2.54311 loss)
I1028 21:56:38.679947 30834 solver.cpp:403] Iteration 444000, lr = 0.000571753
I1028 22:00:40.708118 30834 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_445000.caffemodel
I1028 22:00:45.239188 30834 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_445000.solverstate
I1028 22:00:49.272887 30834 solver.cpp:228] Iteration 445000, loss = 2.30268
I1028 22:00:49.273432 30834 solver.cpp:233] Optimization Done.
I1028 22:00:49.273458 30834 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 22:25:39.971164 13126 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 22:25:39.971273 13126 net.cpp:358] Input 0 -> data
I1028 22:25:39.971303 13126 net.cpp:67] Creating Layer conv1
I1028 22:25:39.971308 13126 net.cpp:394] conv1 <- data
I1028 22:25:39.971315 13126 net.cpp:356] conv1 -> conv1
I1028 22:25:39.971325 13126 net.cpp:96] Setting up conv1
I1028 22:25:39.971642 13126 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 22:25:39.971662 13126 net.cpp:67] Creating Layer pool1
I1028 22:25:39.971665 13126 net.cpp:394] pool1 <- conv1
I1028 22:25:39.971671 13126 net.cpp:356] pool1 -> pool1
I1028 22:25:39.971679 13126 net.cpp:96] Setting up pool1
I1028 22:25:39.971693 13126 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 22:25:39.971699 13126 net.cpp:67] Creating Layer relu1
I1028 22:25:39.971704 13126 net.cpp:394] relu1 <- pool1
I1028 22:25:39.971712 13126 net.cpp:345] relu1 -> pool1 (in-place)
I1028 22:25:39.971719 13126 net.cpp:96] Setting up relu1
I1028 22:25:39.971722 13126 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 22:25:39.971729 13126 net.cpp:67] Creating Layer drop1
I1028 22:25:39.971732 13126 net.cpp:394] drop1 <- pool1
I1028 22:25:39.971737 13126 net.cpp:345] drop1 -> pool1 (in-place)
I1028 22:25:39.971743 13126 net.cpp:96] Setting up drop1
I1028 22:25:39.971748 13126 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 22:25:39.971755 13126 net.cpp:67] Creating Layer conv2
I1028 22:25:39.971760 13126 net.cpp:394] conv2 <- pool1
I1028 22:25:39.971765 13126 net.cpp:356] conv2 -> conv2
I1028 22:25:39.971772 13126 net.cpp:96] Setting up conv2
I1028 22:25:39.972342 13126 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 22:25:39.972364 13126 net.cpp:67] Creating Layer pool2
I1028 22:25:39.972373 13126 net.cpp:394] pool2 <- conv2
I1028 22:25:39.972380 13126 net.cpp:356] pool2 -> pool2
I1028 22:25:39.972394 13126 net.cpp:96] Setting up pool2
I1028 22:25:39.972405 13126 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 22:25:39.972411 13126 net.cpp:67] Creating Layer relu2
I1028 22:25:39.972415 13126 net.cpp:394] relu2 <- pool2
I1028 22:25:39.972522 13126 net.cpp:345] relu2 -> pool2 (in-place)
I1028 22:25:39.972540 13126 net.cpp:96] Setting up relu2
I1028 22:25:39.972545 13126 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 22:25:39.972554 13126 net.cpp:67] Creating Layer drop2
I1028 22:25:39.972558 13126 net.cpp:394] drop2 <- pool2
I1028 22:25:39.972568 13126 net.cpp:345] drop2 -> pool2 (in-place)
I1028 22:25:39.972573 13126 net.cpp:96] Setting up drop2
I1028 22:25:39.972579 13126 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 22:25:39.972585 13126 net.cpp:67] Creating Layer conv3
I1028 22:25:39.972589 13126 net.cpp:394] conv3 <- pool2
I1028 22:25:39.972597 13126 net.cpp:356] conv3 -> conv3
I1028 22:25:39.972604 13126 net.cpp:96] Setting up conv3
I1028 22:25:39.974057 13126 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 22:25:39.974076 13126 net.cpp:67] Creating Layer pool3
I1028 22:25:39.974081 13126 net.cpp:394] pool3 <- conv3
I1028 22:25:39.974086 13126 net.cpp:356] pool3 -> pool3
I1028 22:25:39.974092 13126 net.cpp:96] Setting up pool3
I1028 22:25:39.974098 13126 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 22:25:39.974105 13126 net.cpp:67] Creating Layer relu3
I1028 22:25:39.974109 13126 net.cpp:394] relu3 <- pool3
I1028 22:25:39.974114 13126 net.cpp:345] relu3 -> pool3 (in-place)
I1028 22:25:39.974120 13126 net.cpp:96] Setting up relu3
I1028 22:25:39.974124 13126 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 22:25:39.974129 13126 net.cpp:67] Creating Layer drop3
I1028 22:25:39.974133 13126 net.cpp:394] drop3 <- pool3
I1028 22:25:39.974140 13126 net.cpp:345] drop3 -> pool3 (in-place)
I1028 22:25:39.974149 13126 net.cpp:96] Setting up drop3
I1028 22:25:39.974154 13126 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 22:25:39.974161 13126 net.cpp:67] Creating Layer ip1
I1028 22:25:39.974165 13126 net.cpp:394] ip1 <- pool3
I1028 22:25:39.974171 13126 net.cpp:356] ip1 -> ip1
I1028 22:25:39.974179 13126 net.cpp:96] Setting up ip1
I1028 22:25:40.464473 13126 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 22:25:40.464531 13126 net.cpp:67] Creating Layer relu4
I1028 22:25:40.464539 13126 net.cpp:394] relu4 <- ip1
I1028 22:25:40.464547 13126 net.cpp:345] relu4 -> ip1 (in-place)
I1028 22:25:40.464556 13126 net.cpp:96] Setting up relu4
I1028 22:25:40.464561 13126 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 22:25:40.464568 13126 net.cpp:67] Creating Layer drop4
I1028 22:25:40.464572 13126 net.cpp:394] drop4 <- ip1
I1028 22:25:40.464581 13126 net.cpp:345] drop4 -> ip1 (in-place)
I1028 22:25:40.464587 13126 net.cpp:96] Setting up drop4
I1028 22:25:40.464592 13126 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 22:25:40.464601 13126 net.cpp:67] Creating Layer ip2
I1028 22:25:40.464604 13126 net.cpp:394] ip2 <- ip1
I1028 22:25:40.464612 13126 net.cpp:356] ip2 -> ip2
I1028 22:25:40.464623 13126 net.cpp:96] Setting up ip2
I1028 22:25:40.473436 13126 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 22:25:40.473500 13126 net.cpp:67] Creating Layer prob
I1028 22:25:40.473507 13126 net.cpp:394] prob <- ip2
I1028 22:25:40.473515 13126 net.cpp:356] prob -> prob
I1028 22:25:40.473526 13126 net.cpp:96] Setting up prob
I1028 22:25:40.473532 13126 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 22:25:40.473536 13126 net.cpp:172] prob does not need backward computation.
I1028 22:25:40.473541 13126 net.cpp:172] ip2 does not need backward computation.
I1028 22:25:40.473543 13126 net.cpp:172] drop4 does not need backward computation.
I1028 22:25:40.473547 13126 net.cpp:172] relu4 does not need backward computation.
I1028 22:25:40.473551 13126 net.cpp:172] ip1 does not need backward computation.
I1028 22:25:40.473554 13126 net.cpp:172] drop3 does not need backward computation.
I1028 22:25:40.473558 13126 net.cpp:172] relu3 does not need backward computation.
I1028 22:25:40.473562 13126 net.cpp:172] pool3 does not need backward computation.
I1028 22:25:40.473565 13126 net.cpp:172] conv3 does not need backward computation.
I1028 22:25:40.473569 13126 net.cpp:172] drop2 does not need backward computation.
I1028 22:25:40.473572 13126 net.cpp:172] relu2 does not need backward computation.
I1028 22:25:40.473577 13126 net.cpp:172] pool2 does not need backward computation.
I1028 22:25:40.473579 13126 net.cpp:172] conv2 does not need backward computation.
I1028 22:25:40.473583 13126 net.cpp:172] drop1 does not need backward computation.
I1028 22:25:40.473587 13126 net.cpp:172] relu1 does not need backward computation.
I1028 22:25:40.473590 13126 net.cpp:172] pool1 does not need backward computation.
I1028 22:25:40.473593 13126 net.cpp:172] conv1 does not need backward computation.
I1028 22:25:40.473598 13126 net.cpp:208] This network produces output prob
I1028 22:25:40.473610 13126 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 22:25:40.473619 13126 net.cpp:219] Network initialization done.
I1028 22:25:40.473623 13126 net.cpp:220] Memory required for data: 1837200
I1028 23:01:58.381511 20604 convert_imageset.cpp:70] Shuffling data
I1028 23:01:58.980279 20604 convert_imageset.cpp:73] A total of 60000 images.
I1028 23:01:58.980357 20604 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1028 23:02:01.059870 20604 convert_imageset.cpp:177] Processed 1000 files.
E1028 23:02:03.164054 20604 convert_imageset.cpp:177] Processed 2000 files.
E1028 23:02:05.082093 20604 convert_imageset.cpp:177] Processed 3000 files.
E1028 23:02:07.061063 20604 convert_imageset.cpp:177] Processed 4000 files.
E1028 23:02:09.260020 20604 convert_imageset.cpp:177] Processed 5000 files.
E1028 23:02:11.088626 20604 convert_imageset.cpp:177] Processed 6000 files.
E1028 23:02:12.984746 20604 convert_imageset.cpp:177] Processed 7000 files.
E1028 23:02:14.971379 20604 convert_imageset.cpp:177] Processed 8000 files.
E1028 23:02:16.812178 20604 convert_imageset.cpp:177] Processed 9000 files.
E1028 23:02:18.788995 20604 convert_imageset.cpp:177] Processed 10000 files.
E1028 23:02:20.681674 20604 convert_imageset.cpp:177] Processed 11000 files.
E1028 23:02:22.524649 20604 convert_imageset.cpp:177] Processed 12000 files.
E1028 23:02:24.333083 20604 convert_imageset.cpp:177] Processed 13000 files.
E1028 23:02:26.168735 20604 convert_imageset.cpp:177] Processed 14000 files.
E1028 23:02:28.054855 20604 convert_imageset.cpp:177] Processed 15000 files.
E1028 23:02:29.924051 20604 convert_imageset.cpp:177] Processed 16000 files.
E1028 23:02:31.684478 20604 convert_imageset.cpp:177] Processed 17000 files.
E1028 23:02:33.470998 20604 convert_imageset.cpp:177] Processed 18000 files.
E1028 23:02:35.182472 20604 convert_imageset.cpp:177] Processed 19000 files.
E1028 23:02:36.949300 20604 convert_imageset.cpp:177] Processed 20000 files.
E1028 23:02:38.588862 20604 convert_imageset.cpp:177] Processed 21000 files.
E1028 23:02:40.277171 20604 convert_imageset.cpp:177] Processed 22000 files.
E1028 23:02:41.890040 20604 convert_imageset.cpp:177] Processed 23000 files.
E1028 23:02:43.677817 20604 convert_imageset.cpp:177] Processed 24000 files.
E1028 23:02:45.467262 20604 convert_imageset.cpp:177] Processed 25000 files.
E1028 23:02:47.231969 20604 convert_imageset.cpp:177] Processed 26000 files.
E1028 23:02:48.828884 20604 convert_imageset.cpp:177] Processed 27000 files.
E1028 23:02:50.402696 20604 convert_imageset.cpp:177] Processed 28000 files.
E1028 23:02:52.092831 20604 convert_imageset.cpp:177] Processed 29000 files.
E1028 23:02:53.952270 20604 convert_imageset.cpp:177] Processed 30000 files.
E1028 23:02:55.553510 20604 convert_imageset.cpp:177] Processed 31000 files.
E1028 23:02:57.249589 20604 convert_imageset.cpp:177] Processed 32000 files.
E1028 23:02:58.837172 20604 convert_imageset.cpp:177] Processed 33000 files.
E1028 23:03:00.517513 20604 convert_imageset.cpp:177] Processed 34000 files.
E1028 23:03:02.115538 20604 convert_imageset.cpp:177] Processed 35000 files.
E1028 23:03:03.777971 20604 convert_imageset.cpp:177] Processed 36000 files.
E1028 23:03:05.457427 20604 convert_imageset.cpp:177] Processed 37000 files.
E1028 23:03:07.147258 20604 convert_imageset.cpp:177] Processed 38000 files.
E1028 23:03:08.761729 20604 convert_imageset.cpp:177] Processed 39000 files.
E1028 23:03:10.372490 20604 convert_imageset.cpp:177] Processed 40000 files.
E1028 23:03:12.103935 20604 convert_imageset.cpp:177] Processed 41000 files.
E1028 23:03:13.715853 20604 convert_imageset.cpp:177] Processed 42000 files.
E1028 23:03:15.323797 20604 convert_imageset.cpp:177] Processed 43000 files.
E1028 23:03:17.064332 20604 convert_imageset.cpp:177] Processed 44000 files.
E1028 23:03:18.680789 20604 convert_imageset.cpp:177] Processed 45000 files.
E1028 23:03:20.263627 20604 convert_imageset.cpp:177] Processed 46000 files.
E1028 23:03:21.865087 20604 convert_imageset.cpp:177] Processed 47000 files.
E1028 23:03:23.435895 20604 convert_imageset.cpp:177] Processed 48000 files.
E1028 23:03:25.177453 20604 convert_imageset.cpp:177] Processed 49000 files.
E1028 23:03:26.838703 20604 convert_imageset.cpp:177] Processed 50000 files.
E1028 23:03:28.621317 20604 convert_imageset.cpp:177] Processed 51000 files.
E1028 23:03:30.258816 20604 convert_imageset.cpp:177] Processed 52000 files.
E1028 23:03:31.893115 20604 convert_imageset.cpp:177] Processed 53000 files.
E1028 23:03:33.485622 20604 convert_imageset.cpp:177] Processed 54000 files.
E1028 23:03:35.175151 20604 convert_imageset.cpp:177] Processed 55000 files.
E1028 23:03:36.806156 20604 convert_imageset.cpp:177] Processed 56000 files.
E1028 23:03:38.390029 20604 convert_imageset.cpp:177] Processed 57000 files.
E1028 23:03:39.881713 20604 convert_imageset.cpp:177] Processed 58000 files.
E1028 23:03:41.633154 20604 convert_imageset.cpp:177] Processed 59000 files.
E1028 23:03:43.172662 20604 convert_imageset.cpp:177] Processed 60000 files.
I1028 23:03:43.360210 20766 caffe.cpp:99] Use GPU with device ID 0
I1028 23:03:43.713871 20766 caffe.cpp:107] Starting Optimization
I1028 23:03:43.714004 20766 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 450000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1028 23:03:43.714035 20766 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1028 23:03:43.721477 20766 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1028 23:03:43.721690 20766 net.cpp:67] Creating Layer mnist
I1028 23:03:43.721715 20766 net.cpp:356] mnist -> data
I1028 23:03:43.721752 20766 net.cpp:356] mnist -> label
I1028 23:03:43.721793 20766 net.cpp:96] Setting up mnist
I1028 23:03:43.727941 20766 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1028 23:03:43.728037 20766 data_layer.cpp:128] output data size: 64,1,50,180
I1028 23:03:43.728955 20766 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1028 23:03:43.728978 20766 net.cpp:103] Top shape: 64 1 1 1 (64)
I1028 23:03:43.728994 20766 net.cpp:67] Creating Layer conv1
I1028 23:03:43.729001 20766 net.cpp:394] conv1 <- data
I1028 23:03:43.729017 20766 net.cpp:356] conv1 -> conv1
I1028 23:03:43.729033 20766 net.cpp:96] Setting up conv1
I1028 23:03:43.729480 20766 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1028 23:03:43.729518 20766 net.cpp:67] Creating Layer pool1
I1028 23:03:43.729526 20766 net.cpp:394] pool1 <- conv1
I1028 23:03:43.729535 20766 net.cpp:356] pool1 -> pool1
I1028 23:03:43.729545 20766 net.cpp:96] Setting up pool1
I1028 23:03:43.729563 20766 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 23:03:43.729573 20766 net.cpp:67] Creating Layer relu1
I1028 23:03:43.729578 20766 net.cpp:394] relu1 <- pool1
I1028 23:03:43.729585 20766 net.cpp:345] relu1 -> pool1 (in-place)
I1028 23:03:43.729593 20766 net.cpp:96] Setting up relu1
I1028 23:03:43.729599 20766 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 23:03:43.729609 20766 net.cpp:67] Creating Layer drop1
I1028 23:03:43.729614 20766 net.cpp:394] drop1 <- pool1
I1028 23:03:43.729624 20766 net.cpp:345] drop1 -> pool1 (in-place)
I1028 23:03:43.729632 20766 net.cpp:96] Setting up drop1
I1028 23:03:43.729640 20766 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1028 23:03:43.729647 20766 net.cpp:67] Creating Layer conv2
I1028 23:03:43.729653 20766 net.cpp:394] conv2 <- pool1
I1028 23:03:43.729663 20766 net.cpp:356] conv2 -> conv2
I1028 23:03:43.729673 20766 net.cpp:96] Setting up conv2
I1028 23:03:43.730387 20766 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1028 23:03:43.730411 20766 net.cpp:67] Creating Layer pool2
I1028 23:03:43.730417 20766 net.cpp:394] pool2 <- conv2
I1028 23:03:43.730425 20766 net.cpp:356] pool2 -> pool2
I1028 23:03:43.730434 20766 net.cpp:96] Setting up pool2
I1028 23:03:43.730443 20766 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 23:03:43.730449 20766 net.cpp:67] Creating Layer relu2
I1028 23:03:43.730455 20766 net.cpp:394] relu2 <- pool2
I1028 23:03:43.730465 20766 net.cpp:345] relu2 -> pool2 (in-place)
I1028 23:03:43.730473 20766 net.cpp:96] Setting up relu2
I1028 23:03:43.730479 20766 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 23:03:43.730489 20766 net.cpp:67] Creating Layer drop2
I1028 23:03:43.730494 20766 net.cpp:394] drop2 <- pool2
I1028 23:03:43.730501 20766 net.cpp:345] drop2 -> pool2 (in-place)
I1028 23:03:43.730510 20766 net.cpp:96] Setting up drop2
I1028 23:03:43.730517 20766 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1028 23:03:43.730526 20766 net.cpp:67] Creating Layer conv3
I1028 23:03:43.730531 20766 net.cpp:394] conv3 <- pool2
I1028 23:03:43.730540 20766 net.cpp:356] conv3 -> conv3
I1028 23:03:43.730548 20766 net.cpp:96] Setting up conv3
I1028 23:03:43.732455 20766 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1028 23:03:43.732492 20766 net.cpp:67] Creating Layer pool3
I1028 23:03:43.732501 20766 net.cpp:394] pool3 <- conv3
I1028 23:03:43.732517 20766 net.cpp:356] pool3 -> pool3
I1028 23:03:43.732525 20766 net.cpp:96] Setting up pool3
I1028 23:03:43.732532 20766 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 23:03:43.732543 20766 net.cpp:67] Creating Layer relu3
I1028 23:03:43.732548 20766 net.cpp:394] relu3 <- pool3
I1028 23:03:43.732555 20766 net.cpp:345] relu3 -> pool3 (in-place)
I1028 23:03:43.732563 20766 net.cpp:96] Setting up relu3
I1028 23:03:43.732566 20766 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 23:03:43.732573 20766 net.cpp:67] Creating Layer drop3
I1028 23:03:43.732578 20766 net.cpp:394] drop3 <- pool3
I1028 23:03:43.732583 20766 net.cpp:345] drop3 -> pool3 (in-place)
I1028 23:03:43.732589 20766 net.cpp:96] Setting up drop3
I1028 23:03:43.732594 20766 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1028 23:03:43.732607 20766 net.cpp:67] Creating Layer ip1
I1028 23:03:43.732611 20766 net.cpp:394] ip1 <- pool3
I1028 23:03:43.732622 20766 net.cpp:356] ip1 -> ip1
I1028 23:03:43.732656 20766 net.cpp:96] Setting up ip1
I1028 23:03:44.172454 20766 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 23:03:44.172515 20766 net.cpp:67] Creating Layer relu4
I1028 23:03:44.172523 20766 net.cpp:394] relu4 <- ip1
I1028 23:03:44.172533 20766 net.cpp:345] relu4 -> ip1 (in-place)
I1028 23:03:44.172540 20766 net.cpp:96] Setting up relu4
I1028 23:03:44.172546 20766 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 23:03:44.172552 20766 net.cpp:67] Creating Layer drop4
I1028 23:03:44.172557 20766 net.cpp:394] drop4 <- ip1
I1028 23:03:44.172567 20766 net.cpp:345] drop4 -> ip1 (in-place)
I1028 23:03:44.172574 20766 net.cpp:96] Setting up drop4
I1028 23:03:44.172580 20766 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1028 23:03:44.172590 20766 net.cpp:67] Creating Layer ip2
I1028 23:03:44.172595 20766 net.cpp:394] ip2 <- ip1
I1028 23:03:44.172603 20766 net.cpp:356] ip2 -> ip2
I1028 23:03:44.172611 20766 net.cpp:96] Setting up ip2
I1028 23:03:44.181470 20766 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1028 23:03:44.181527 20766 net.cpp:67] Creating Layer loss
I1028 23:03:44.181535 20766 net.cpp:394] loss <- ip2
I1028 23:03:44.181541 20766 net.cpp:394] loss <- label
I1028 23:03:44.181548 20766 net.cpp:356] loss -> loss
I1028 23:03:44.181558 20766 net.cpp:96] Setting up loss
I1028 23:03:44.181571 20766 net.cpp:103] Top shape: 1 1 1 1 (1)
I1028 23:03:44.181576 20766 net.cpp:109]     with loss weight 1
I1028 23:03:44.181612 20766 net.cpp:170] loss needs backward computation.
I1028 23:03:44.181617 20766 net.cpp:170] ip2 needs backward computation.
I1028 23:03:44.181622 20766 net.cpp:170] drop4 needs backward computation.
I1028 23:03:44.181627 20766 net.cpp:170] relu4 needs backward computation.
I1028 23:03:44.181630 20766 net.cpp:170] ip1 needs backward computation.
I1028 23:03:44.181635 20766 net.cpp:170] drop3 needs backward computation.
I1028 23:03:44.181640 20766 net.cpp:170] relu3 needs backward computation.
I1028 23:03:44.181644 20766 net.cpp:170] pool3 needs backward computation.
I1028 23:03:44.181649 20766 net.cpp:170] conv3 needs backward computation.
I1028 23:03:44.181653 20766 net.cpp:170] drop2 needs backward computation.
I1028 23:03:44.181658 20766 net.cpp:170] relu2 needs backward computation.
I1028 23:03:44.181663 20766 net.cpp:170] pool2 needs backward computation.
I1028 23:03:44.181668 20766 net.cpp:170] conv2 needs backward computation.
I1028 23:03:44.181673 20766 net.cpp:170] drop1 needs backward computation.
I1028 23:03:44.181676 20766 net.cpp:170] relu1 needs backward computation.
I1028 23:03:44.181681 20766 net.cpp:170] pool1 needs backward computation.
I1028 23:03:44.181685 20766 net.cpp:170] conv1 needs backward computation.
I1028 23:03:44.181690 20766 net.cpp:172] mnist does not need backward computation.
I1028 23:03:44.181694 20766 net.cpp:208] This network produces output loss
I1028 23:03:44.181705 20766 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 23:03:44.181712 20766 net.cpp:219] Network initialization done.
I1028 23:03:44.181716 20766 net.cpp:220] Memory required for data: 119788292
I1028 23:03:44.181776 20766 solver.cpp:41] Solver scaffolding done.
I1028 23:03:44.181782 20766 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_445000.solverstate
I1028 23:03:44.181787 20766 solver.cpp:160] Solving Captcha
I1028 23:03:44.181805 20766 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_445000.solverstate
I1028 23:03:49.386623 20766 solver.cpp:502] SGDSolver: restoring history
I1028 23:03:50.213716 20766 solver.cpp:191] Iteration 445000, loss = 2.38877
I1028 23:03:50.213778 20766 solver.cpp:206]     Train net output #0: loss = 2.38877 (* 1 = 2.38877 loss)
I1028 23:03:50.213793 20766 solver.cpp:403] Iteration 445000, lr = 0.00057081
I1028 23:07:51.850126 20766 solver.cpp:191] Iteration 446000, loss = 2.51288
I1028 23:07:51.850775 20766 solver.cpp:206]     Train net output #0: loss = 2.51288 (* 1 = 2.51288 loss)
I1028 23:07:51.850811 20766 solver.cpp:403] Iteration 446000, lr = 0.000569871
I1028 23:11:53.055750 20766 solver.cpp:191] Iteration 447000, loss = 2.346
I1028 23:11:53.056519 20766 solver.cpp:206]     Train net output #0: loss = 2.346 (* 1 = 2.346 loss)
I1028 23:11:53.056551 20766 solver.cpp:403] Iteration 447000, lr = 0.000568935
I1028 23:15:54.297349 20766 solver.cpp:191] Iteration 448000, loss = 2.52486
I1028 23:15:54.297873 20766 solver.cpp:206]     Train net output #0: loss = 2.52486 (* 1 = 2.52486 loss)
I1028 23:15:54.297910 20766 solver.cpp:403] Iteration 448000, lr = 0.000568003
I1028 23:19:55.558657 20766 solver.cpp:191] Iteration 449000, loss = 2.55023
I1028 23:19:55.559340 20766 solver.cpp:206]     Train net output #0: loss = 2.55023 (* 1 = 2.55023 loss)
I1028 23:19:55.559376 20766 solver.cpp:403] Iteration 449000, lr = 0.000567075
I1028 23:23:57.385349 20766 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_450000.caffemodel
I1028 23:24:02.556541 20766 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_450000.solverstate
I1028 23:24:07.000170 20766 solver.cpp:228] Iteration 450000, loss = 2.53509
I1028 23:24:07.000788 20766 solver.cpp:233] Optimization Done.
I1028 23:24:07.000815 20766 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1028 23:48:22.832229  2872 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1028 23:48:22.832329  2872 net.cpp:358] Input 0 -> data
I1028 23:48:22.832356  2872 net.cpp:67] Creating Layer conv1
I1028 23:48:22.832362  2872 net.cpp:394] conv1 <- data
I1028 23:48:22.832368  2872 net.cpp:356] conv1 -> conv1
I1028 23:48:22.832378  2872 net.cpp:96] Setting up conv1
I1028 23:48:22.832731  2872 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1028 23:48:22.832752  2872 net.cpp:67] Creating Layer pool1
I1028 23:48:22.832762  2872 net.cpp:394] pool1 <- conv1
I1028 23:48:22.832768  2872 net.cpp:356] pool1 -> pool1
I1028 23:48:22.832777  2872 net.cpp:96] Setting up pool1
I1028 23:48:22.832788  2872 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 23:48:22.832795  2872 net.cpp:67] Creating Layer relu1
I1028 23:48:22.832803  2872 net.cpp:394] relu1 <- pool1
I1028 23:48:22.832808  2872 net.cpp:345] relu1 -> pool1 (in-place)
I1028 23:48:22.832814  2872 net.cpp:96] Setting up relu1
I1028 23:48:22.832818  2872 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 23:48:22.832826  2872 net.cpp:67] Creating Layer drop1
I1028 23:48:22.832831  2872 net.cpp:394] drop1 <- pool1
I1028 23:48:22.832836  2872 net.cpp:345] drop1 -> pool1 (in-place)
I1028 23:48:22.832842  2872 net.cpp:96] Setting up drop1
I1028 23:48:22.832847  2872 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1028 23:48:22.832854  2872 net.cpp:67] Creating Layer conv2
I1028 23:48:22.832859  2872 net.cpp:394] conv2 <- pool1
I1028 23:48:22.832865  2872 net.cpp:356] conv2 -> conv2
I1028 23:48:22.832872  2872 net.cpp:96] Setting up conv2
I1028 23:48:22.833422  2872 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1028 23:48:22.833436  2872 net.cpp:67] Creating Layer pool2
I1028 23:48:22.833441  2872 net.cpp:394] pool2 <- conv2
I1028 23:48:22.833447  2872 net.cpp:356] pool2 -> pool2
I1028 23:48:22.833453  2872 net.cpp:96] Setting up pool2
I1028 23:48:22.833459  2872 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 23:48:22.833467  2872 net.cpp:67] Creating Layer relu2
I1028 23:48:22.833472  2872 net.cpp:394] relu2 <- pool2
I1028 23:48:22.833477  2872 net.cpp:345] relu2 -> pool2 (in-place)
I1028 23:48:22.833482  2872 net.cpp:96] Setting up relu2
I1028 23:48:22.833487  2872 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 23:48:22.833492  2872 net.cpp:67] Creating Layer drop2
I1028 23:48:22.833495  2872 net.cpp:394] drop2 <- pool2
I1028 23:48:22.833503  2872 net.cpp:345] drop2 -> pool2 (in-place)
I1028 23:48:22.833508  2872 net.cpp:96] Setting up drop2
I1028 23:48:22.833513  2872 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1028 23:48:22.833519  2872 net.cpp:67] Creating Layer conv3
I1028 23:48:22.833523  2872 net.cpp:394] conv3 <- pool2
I1028 23:48:22.833530  2872 net.cpp:356] conv3 -> conv3
I1028 23:48:22.833537  2872 net.cpp:96] Setting up conv3
I1028 23:48:22.835120  2872 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1028 23:48:22.835139  2872 net.cpp:67] Creating Layer pool3
I1028 23:48:22.835145  2872 net.cpp:394] pool3 <- conv3
I1028 23:48:22.835151  2872 net.cpp:356] pool3 -> pool3
I1028 23:48:22.835157  2872 net.cpp:96] Setting up pool3
I1028 23:48:22.835162  2872 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 23:48:22.835170  2872 net.cpp:67] Creating Layer relu3
I1028 23:48:22.835175  2872 net.cpp:394] relu3 <- pool3
I1028 23:48:22.835180  2872 net.cpp:345] relu3 -> pool3 (in-place)
I1028 23:48:22.835185  2872 net.cpp:96] Setting up relu3
I1028 23:48:22.835188  2872 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 23:48:22.835194  2872 net.cpp:67] Creating Layer drop3
I1028 23:48:22.835198  2872 net.cpp:394] drop3 <- pool3
I1028 23:48:22.835203  2872 net.cpp:345] drop3 -> pool3 (in-place)
I1028 23:48:22.835208  2872 net.cpp:96] Setting up drop3
I1028 23:48:22.835213  2872 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1028 23:48:22.835222  2872 net.cpp:67] Creating Layer ip1
I1028 23:48:22.835227  2872 net.cpp:394] ip1 <- pool3
I1028 23:48:22.835234  2872 net.cpp:356] ip1 -> ip1
I1028 23:48:22.835242  2872 net.cpp:96] Setting up ip1
I1028 23:48:23.314524  2872 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 23:48:23.314580  2872 net.cpp:67] Creating Layer relu4
I1028 23:48:23.314589  2872 net.cpp:394] relu4 <- ip1
I1028 23:48:23.314597  2872 net.cpp:345] relu4 -> ip1 (in-place)
I1028 23:48:23.314606  2872 net.cpp:96] Setting up relu4
I1028 23:48:23.314611  2872 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 23:48:23.314618  2872 net.cpp:67] Creating Layer drop4
I1028 23:48:23.314622  2872 net.cpp:394] drop4 <- ip1
I1028 23:48:23.314632  2872 net.cpp:345] drop4 -> ip1 (in-place)
I1028 23:48:23.314638  2872 net.cpp:96] Setting up drop4
I1028 23:48:23.314643  2872 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1028 23:48:23.314652  2872 net.cpp:67] Creating Layer ip2
I1028 23:48:23.314656  2872 net.cpp:394] ip2 <- ip1
I1028 23:48:23.314662  2872 net.cpp:356] ip2 -> ip2
I1028 23:48:23.314676  2872 net.cpp:96] Setting up ip2
I1028 23:48:23.323930  2872 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 23:48:23.324004  2872 net.cpp:67] Creating Layer prob
I1028 23:48:23.324012  2872 net.cpp:394] prob <- ip2
I1028 23:48:23.324020  2872 net.cpp:356] prob -> prob
I1028 23:48:23.324030  2872 net.cpp:96] Setting up prob
I1028 23:48:23.324036  2872 net.cpp:103] Top shape: 1 378 1 1 (378)
I1028 23:48:23.324040  2872 net.cpp:172] prob does not need backward computation.
I1028 23:48:23.324044  2872 net.cpp:172] ip2 does not need backward computation.
I1028 23:48:23.324048  2872 net.cpp:172] drop4 does not need backward computation.
I1028 23:48:23.324053  2872 net.cpp:172] relu4 does not need backward computation.
I1028 23:48:23.324055  2872 net.cpp:172] ip1 does not need backward computation.
I1028 23:48:23.324059  2872 net.cpp:172] drop3 does not need backward computation.
I1028 23:48:23.324064  2872 net.cpp:172] relu3 does not need backward computation.
I1028 23:48:23.324066  2872 net.cpp:172] pool3 does not need backward computation.
I1028 23:48:23.324070  2872 net.cpp:172] conv3 does not need backward computation.
I1028 23:48:23.324074  2872 net.cpp:172] drop2 does not need backward computation.
I1028 23:48:23.324077  2872 net.cpp:172] relu2 does not need backward computation.
I1028 23:48:23.324081  2872 net.cpp:172] pool2 does not need backward computation.
I1028 23:48:23.324085  2872 net.cpp:172] conv2 does not need backward computation.
I1028 23:48:23.324089  2872 net.cpp:172] drop1 does not need backward computation.
I1028 23:48:23.324092  2872 net.cpp:172] relu1 does not need backward computation.
I1028 23:48:23.324095  2872 net.cpp:172] pool1 does not need backward computation.
I1028 23:48:23.324100  2872 net.cpp:172] conv1 does not need backward computation.
I1028 23:48:23.324102  2872 net.cpp:208] This network produces output prob
I1028 23:48:23.324116  2872 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1028 23:48:23.324125  2872 net.cpp:219] Network initialization done.
I1028 23:48:23.324128  2872 net.cpp:220] Memory required for data: 1837200
I1029 00:25:22.030659 10774 convert_imageset.cpp:70] Shuffling data
I1029 00:25:22.723915 10774 convert_imageset.cpp:73] A total of 60000 images.
I1029 00:25:22.723994 10774 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 00:25:25.313865 10774 convert_imageset.cpp:177] Processed 1000 files.
E1029 00:25:27.831581 10774 convert_imageset.cpp:177] Processed 2000 files.
E1029 00:25:30.023461 10774 convert_imageset.cpp:177] Processed 3000 files.
E1029 00:25:32.406440 10774 convert_imageset.cpp:177] Processed 4000 files.
E1029 00:25:34.706687 10774 convert_imageset.cpp:177] Processed 5000 files.
E1029 00:25:36.902781 10774 convert_imageset.cpp:177] Processed 6000 files.
E1029 00:25:38.899335 10774 convert_imageset.cpp:177] Processed 7000 files.
E1029 00:25:40.979972 10774 convert_imageset.cpp:177] Processed 8000 files.
E1029 00:25:43.129112 10774 convert_imageset.cpp:177] Processed 9000 files.
E1029 00:25:45.228075 10774 convert_imageset.cpp:177] Processed 10000 files.
E1029 00:25:47.219534 10774 convert_imageset.cpp:177] Processed 11000 files.
E1029 00:25:49.215483 10774 convert_imageset.cpp:177] Processed 12000 files.
E1029 00:25:51.147047 10774 convert_imageset.cpp:177] Processed 13000 files.
E1029 00:25:53.085295 10774 convert_imageset.cpp:177] Processed 14000 files.
E1029 00:25:55.041775 10774 convert_imageset.cpp:177] Processed 15000 files.
E1029 00:25:56.985736 10774 convert_imageset.cpp:177] Processed 16000 files.
E1029 00:25:59.011641 10774 convert_imageset.cpp:177] Processed 17000 files.
E1029 00:26:00.938447 10774 convert_imageset.cpp:177] Processed 18000 files.
E1029 00:26:02.829666 10774 convert_imageset.cpp:177] Processed 19000 files.
E1029 00:26:04.702889 10774 convert_imageset.cpp:177] Processed 20000 files.
E1029 00:26:06.589012 10774 convert_imageset.cpp:177] Processed 21000 files.
E1029 00:26:08.390367 10774 convert_imageset.cpp:177] Processed 22000 files.
E1029 00:26:10.196599 10774 convert_imageset.cpp:177] Processed 23000 files.
E1029 00:26:11.950407 10774 convert_imageset.cpp:177] Processed 24000 files.
E1029 00:26:13.888834 10774 convert_imageset.cpp:177] Processed 25000 files.
E1029 00:26:15.672564 10774 convert_imageset.cpp:177] Processed 26000 files.
E1029 00:26:17.512850 10774 convert_imageset.cpp:177] Processed 27000 files.
E1029 00:26:19.299579 10774 convert_imageset.cpp:177] Processed 28000 files.
E1029 00:26:21.121464 10774 convert_imageset.cpp:177] Processed 29000 files.
E1029 00:26:22.949410 10774 convert_imageset.cpp:177] Processed 30000 files.
E1029 00:26:24.776927 10774 convert_imageset.cpp:177] Processed 31000 files.
E1029 00:26:26.582787 10774 convert_imageset.cpp:177] Processed 32000 files.
E1029 00:26:28.284699 10774 convert_imageset.cpp:177] Processed 33000 files.
E1029 00:26:30.062405 10774 convert_imageset.cpp:177] Processed 34000 files.
E1029 00:26:31.812650 10774 convert_imageset.cpp:177] Processed 35000 files.
E1029 00:26:33.664798 10774 convert_imageset.cpp:177] Processed 36000 files.
E1029 00:26:35.458806 10774 convert_imageset.cpp:177] Processed 37000 files.
E1029 00:26:37.110841 10774 convert_imageset.cpp:177] Processed 38000 files.
E1029 00:26:38.780807 10774 convert_imageset.cpp:177] Processed 39000 files.
E1029 00:26:40.536301 10774 convert_imageset.cpp:177] Processed 40000 files.
E1029 00:26:42.317924 10774 convert_imageset.cpp:177] Processed 41000 files.
E1029 00:26:44.008479 10774 convert_imageset.cpp:177] Processed 42000 files.
E1029 00:26:45.769424 10774 convert_imageset.cpp:177] Processed 43000 files.
E1029 00:26:47.504554 10774 convert_imageset.cpp:177] Processed 44000 files.
E1029 00:26:49.251446 10774 convert_imageset.cpp:177] Processed 45000 files.
E1029 00:26:50.870868 10774 convert_imageset.cpp:177] Processed 46000 files.
E1029 00:26:52.673318 10774 convert_imageset.cpp:177] Processed 47000 files.
E1029 00:26:54.434479 10774 convert_imageset.cpp:177] Processed 48000 files.
E1029 00:26:56.260360 10774 convert_imageset.cpp:177] Processed 49000 files.
E1029 00:26:57.866367 10774 convert_imageset.cpp:177] Processed 50000 files.
E1029 00:26:59.594019 10774 convert_imageset.cpp:177] Processed 51000 files.
E1029 00:27:01.286767 10774 convert_imageset.cpp:177] Processed 52000 files.
E1029 00:27:03.003999 10774 convert_imageset.cpp:177] Processed 53000 files.
E1029 00:27:04.646646 10774 convert_imageset.cpp:177] Processed 54000 files.
E1029 00:27:06.236064 10774 convert_imageset.cpp:177] Processed 55000 files.
E1029 00:27:07.848320 10774 convert_imageset.cpp:177] Processed 56000 files.
E1029 00:27:09.458521 10774 convert_imageset.cpp:177] Processed 57000 files.
E1029 00:27:11.110040 10774 convert_imageset.cpp:177] Processed 58000 files.
E1029 00:27:12.722594 10774 convert_imageset.cpp:177] Processed 59000 files.
E1029 00:27:14.328893 10774 convert_imageset.cpp:177] Processed 60000 files.
I1029 00:27:14.523239 10879 caffe.cpp:99] Use GPU with device ID 0
I1029 00:27:14.887418 10879 caffe.cpp:107] Starting Optimization
I1029 00:27:14.887531 10879 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 455000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 00:27:14.887560 10879 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 00:27:14.894944 10879 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 00:27:14.895045 10879 net.cpp:67] Creating Layer mnist
I1029 00:27:14.895056 10879 net.cpp:356] mnist -> data
I1029 00:27:14.895074 10879 net.cpp:356] mnist -> label
I1029 00:27:14.895090 10879 net.cpp:96] Setting up mnist
I1029 00:27:14.903112 10879 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 00:27:14.903214 10879 data_layer.cpp:128] output data size: 64,1,50,180
I1029 00:27:14.904662 10879 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 00:27:14.904707 10879 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 00:27:14.904731 10879 net.cpp:67] Creating Layer conv1
I1029 00:27:14.904742 10879 net.cpp:394] conv1 <- data
I1029 00:27:14.904765 10879 net.cpp:356] conv1 -> conv1
I1029 00:27:14.904784 10879 net.cpp:96] Setting up conv1
I1029 00:27:14.905158 10879 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 00:27:14.905190 10879 net.cpp:67] Creating Layer pool1
I1029 00:27:14.905195 10879 net.cpp:394] pool1 <- conv1
I1029 00:27:14.905205 10879 net.cpp:356] pool1 -> pool1
I1029 00:27:14.905212 10879 net.cpp:96] Setting up pool1
I1029 00:27:14.905228 10879 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 00:27:14.905236 10879 net.cpp:67] Creating Layer relu1
I1029 00:27:14.905241 10879 net.cpp:394] relu1 <- pool1
I1029 00:27:14.905247 10879 net.cpp:345] relu1 -> pool1 (in-place)
I1029 00:27:14.905253 10879 net.cpp:96] Setting up relu1
I1029 00:27:14.905258 10879 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 00:27:14.905266 10879 net.cpp:67] Creating Layer drop1
I1029 00:27:14.905269 10879 net.cpp:394] drop1 <- pool1
I1029 00:27:14.905278 10879 net.cpp:345] drop1 -> pool1 (in-place)
I1029 00:27:14.905284 10879 net.cpp:96] Setting up drop1
I1029 00:27:14.905290 10879 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 00:27:14.905298 10879 net.cpp:67] Creating Layer conv2
I1029 00:27:14.905303 10879 net.cpp:394] conv2 <- pool1
I1029 00:27:14.905310 10879 net.cpp:356] conv2 -> conv2
I1029 00:27:14.905319 10879 net.cpp:96] Setting up conv2
I1029 00:27:14.905912 10879 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 00:27:14.905933 10879 net.cpp:67] Creating Layer pool2
I1029 00:27:14.905939 10879 net.cpp:394] pool2 <- conv2
I1029 00:27:14.905946 10879 net.cpp:356] pool2 -> pool2
I1029 00:27:14.905952 10879 net.cpp:96] Setting up pool2
I1029 00:27:14.905958 10879 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 00:27:14.905964 10879 net.cpp:67] Creating Layer relu2
I1029 00:27:14.905969 10879 net.cpp:394] relu2 <- pool2
I1029 00:27:14.905977 10879 net.cpp:345] relu2 -> pool2 (in-place)
I1029 00:27:14.905983 10879 net.cpp:96] Setting up relu2
I1029 00:27:14.905987 10879 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 00:27:14.905995 10879 net.cpp:67] Creating Layer drop2
I1029 00:27:14.905999 10879 net.cpp:394] drop2 <- pool2
I1029 00:27:14.906008 10879 net.cpp:345] drop2 -> pool2 (in-place)
I1029 00:27:14.906013 10879 net.cpp:96] Setting up drop2
I1029 00:27:14.906018 10879 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 00:27:14.906026 10879 net.cpp:67] Creating Layer conv3
I1029 00:27:14.906030 10879 net.cpp:394] conv3 <- pool2
I1029 00:27:14.906036 10879 net.cpp:356] conv3 -> conv3
I1029 00:27:14.906044 10879 net.cpp:96] Setting up conv3
I1029 00:27:14.908037 10879 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 00:27:14.908071 10879 net.cpp:67] Creating Layer pool3
I1029 00:27:14.908079 10879 net.cpp:394] pool3 <- conv3
I1029 00:27:14.908093 10879 net.cpp:356] pool3 -> pool3
I1029 00:27:14.908105 10879 net.cpp:96] Setting up pool3
I1029 00:27:14.908115 10879 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 00:27:14.908125 10879 net.cpp:67] Creating Layer relu3
I1029 00:27:14.908133 10879 net.cpp:394] relu3 <- pool3
I1029 00:27:14.908144 10879 net.cpp:345] relu3 -> pool3 (in-place)
I1029 00:27:14.908155 10879 net.cpp:96] Setting up relu3
I1029 00:27:14.908164 10879 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 00:27:14.908172 10879 net.cpp:67] Creating Layer drop3
I1029 00:27:14.908180 10879 net.cpp:394] drop3 <- pool3
I1029 00:27:14.908190 10879 net.cpp:345] drop3 -> pool3 (in-place)
I1029 00:27:14.908200 10879 net.cpp:96] Setting up drop3
I1029 00:27:14.908207 10879 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 00:27:14.908218 10879 net.cpp:67] Creating Layer ip1
I1029 00:27:14.908226 10879 net.cpp:394] ip1 <- pool3
I1029 00:27:14.908239 10879 net.cpp:356] ip1 -> ip1
I1029 00:27:14.908288 10879 net.cpp:96] Setting up ip1
I1029 00:27:15.386286 10879 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 00:27:15.386343 10879 net.cpp:67] Creating Layer relu4
I1029 00:27:15.386350 10879 net.cpp:394] relu4 <- ip1
I1029 00:27:15.386359 10879 net.cpp:345] relu4 -> ip1 (in-place)
I1029 00:27:15.386369 10879 net.cpp:96] Setting up relu4
I1029 00:27:15.386374 10879 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 00:27:15.386381 10879 net.cpp:67] Creating Layer drop4
I1029 00:27:15.386385 10879 net.cpp:394] drop4 <- ip1
I1029 00:27:15.386395 10879 net.cpp:345] drop4 -> ip1 (in-place)
I1029 00:27:15.386402 10879 net.cpp:96] Setting up drop4
I1029 00:27:15.386407 10879 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 00:27:15.386417 10879 net.cpp:67] Creating Layer ip2
I1029 00:27:15.386422 10879 net.cpp:394] ip2 <- ip1
I1029 00:27:15.386430 10879 net.cpp:356] ip2 -> ip2
I1029 00:27:15.386440 10879 net.cpp:96] Setting up ip2
I1029 00:27:15.395691 10879 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 00:27:15.395750 10879 net.cpp:67] Creating Layer loss
I1029 00:27:15.395757 10879 net.cpp:394] loss <- ip2
I1029 00:27:15.395766 10879 net.cpp:394] loss <- label
I1029 00:27:15.395772 10879 net.cpp:356] loss -> loss
I1029 00:27:15.395782 10879 net.cpp:96] Setting up loss
I1029 00:27:15.395794 10879 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 00:27:15.395799 10879 net.cpp:109]     with loss weight 1
I1029 00:27:15.395835 10879 net.cpp:170] loss needs backward computation.
I1029 00:27:15.395843 10879 net.cpp:170] ip2 needs backward computation.
I1029 00:27:15.395848 10879 net.cpp:170] drop4 needs backward computation.
I1029 00:27:15.395853 10879 net.cpp:170] relu4 needs backward computation.
I1029 00:27:15.395858 10879 net.cpp:170] ip1 needs backward computation.
I1029 00:27:15.395862 10879 net.cpp:170] drop3 needs backward computation.
I1029 00:27:15.395866 10879 net.cpp:170] relu3 needs backward computation.
I1029 00:27:15.395871 10879 net.cpp:170] pool3 needs backward computation.
I1029 00:27:15.395875 10879 net.cpp:170] conv3 needs backward computation.
I1029 00:27:15.395880 10879 net.cpp:170] drop2 needs backward computation.
I1029 00:27:15.395884 10879 net.cpp:170] relu2 needs backward computation.
I1029 00:27:15.395889 10879 net.cpp:170] pool2 needs backward computation.
I1029 00:27:15.395894 10879 net.cpp:170] conv2 needs backward computation.
I1029 00:27:15.395898 10879 net.cpp:170] drop1 needs backward computation.
I1029 00:27:15.395902 10879 net.cpp:170] relu1 needs backward computation.
I1029 00:27:15.395907 10879 net.cpp:170] pool1 needs backward computation.
I1029 00:27:15.395911 10879 net.cpp:170] conv1 needs backward computation.
I1029 00:27:15.395916 10879 net.cpp:172] mnist does not need backward computation.
I1029 00:27:15.395920 10879 net.cpp:208] This network produces output loss
I1029 00:27:15.395932 10879 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 00:27:15.395941 10879 net.cpp:219] Network initialization done.
I1029 00:27:15.395944 10879 net.cpp:220] Memory required for data: 119788292
I1029 00:27:15.396008 10879 solver.cpp:41] Solver scaffolding done.
I1029 00:27:15.396014 10879 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_450000.solverstate
I1029 00:27:15.396019 10879 solver.cpp:160] Solving Captcha
I1029 00:27:15.396044 10879 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_450000.solverstate
I1029 00:27:20.827700 10879 solver.cpp:502] SGDSolver: restoring history
I1029 00:27:21.633194 10879 solver.cpp:191] Iteration 450000, loss = 2.46831
I1029 00:27:21.633249 10879 solver.cpp:206]     Train net output #0: loss = 2.46831 (* 1 = 2.46831 loss)
I1029 00:27:21.633265 10879 solver.cpp:403] Iteration 450000, lr = 0.00056615
I1029 00:31:23.779660 10879 solver.cpp:191] Iteration 451000, loss = 2.37345
I1029 00:31:23.780304 10879 solver.cpp:206]     Train net output #0: loss = 2.37345 (* 1 = 2.37345 loss)
I1029 00:31:23.780342 10879 solver.cpp:403] Iteration 451000, lr = 0.000565229
I1029 00:35:25.337141 10879 solver.cpp:191] Iteration 452000, loss = 2.41882
I1029 00:35:25.337766 10879 solver.cpp:206]     Train net output #0: loss = 2.41882 (* 1 = 2.41882 loss)
I1029 00:35:25.337805 10879 solver.cpp:403] Iteration 452000, lr = 0.000564311
I1029 00:39:27.126909 10879 solver.cpp:191] Iteration 453000, loss = 2.41303
I1029 00:39:27.127493 10879 solver.cpp:206]     Train net output #0: loss = 2.41303 (* 1 = 2.41303 loss)
I1029 00:39:27.127532 10879 solver.cpp:403] Iteration 453000, lr = 0.000563397
I1029 00:43:28.948917 10879 solver.cpp:191] Iteration 454000, loss = 2.44843
I1029 00:43:28.949496 10879 solver.cpp:206]     Train net output #0: loss = 2.44843 (* 1 = 2.44843 loss)
I1029 00:43:28.949528 10879 solver.cpp:403] Iteration 454000, lr = 0.000562486
I1029 00:47:31.299018 10879 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_455000.caffemodel
I1029 00:47:36.260618 10879 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_455000.solverstate
I1029 00:47:40.009578 10879 solver.cpp:228] Iteration 455000, loss = 2.34094
I1029 00:47:40.010113 10879 solver.cpp:233] Optimization Done.
I1029 00:47:40.010138 10879 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 01:12:56.132990 25718 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 01:12:56.133100 25718 net.cpp:358] Input 0 -> data
I1029 01:12:56.133123 25718 net.cpp:67] Creating Layer conv1
I1029 01:12:56.133129 25718 net.cpp:394] conv1 <- data
I1029 01:12:56.133136 25718 net.cpp:356] conv1 -> conv1
I1029 01:12:56.133146 25718 net.cpp:96] Setting up conv1
I1029 01:12:56.133466 25718 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 01:12:56.133487 25718 net.cpp:67] Creating Layer pool1
I1029 01:12:56.133492 25718 net.cpp:394] pool1 <- conv1
I1029 01:12:56.133498 25718 net.cpp:356] pool1 -> pool1
I1029 01:12:56.133505 25718 net.cpp:96] Setting up pool1
I1029 01:12:56.133520 25718 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 01:12:56.133527 25718 net.cpp:67] Creating Layer relu1
I1029 01:12:56.133532 25718 net.cpp:394] relu1 <- pool1
I1029 01:12:56.133538 25718 net.cpp:345] relu1 -> pool1 (in-place)
I1029 01:12:56.133544 25718 net.cpp:96] Setting up relu1
I1029 01:12:56.133549 25718 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 01:12:56.133555 25718 net.cpp:67] Creating Layer drop1
I1029 01:12:56.133559 25718 net.cpp:394] drop1 <- pool1
I1029 01:12:56.133564 25718 net.cpp:345] drop1 -> pool1 (in-place)
I1029 01:12:56.133570 25718 net.cpp:96] Setting up drop1
I1029 01:12:56.133575 25718 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 01:12:56.133582 25718 net.cpp:67] Creating Layer conv2
I1029 01:12:56.133586 25718 net.cpp:394] conv2 <- pool1
I1029 01:12:56.133591 25718 net.cpp:356] conv2 -> conv2
I1029 01:12:56.133599 25718 net.cpp:96] Setting up conv2
I1029 01:12:56.134147 25718 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 01:12:56.134162 25718 net.cpp:67] Creating Layer pool2
I1029 01:12:56.134166 25718 net.cpp:394] pool2 <- conv2
I1029 01:12:56.134174 25718 net.cpp:356] pool2 -> pool2
I1029 01:12:56.134182 25718 net.cpp:96] Setting up pool2
I1029 01:12:56.134187 25718 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 01:12:56.134193 25718 net.cpp:67] Creating Layer relu2
I1029 01:12:56.134197 25718 net.cpp:394] relu2 <- pool2
I1029 01:12:56.134202 25718 net.cpp:345] relu2 -> pool2 (in-place)
I1029 01:12:56.134210 25718 net.cpp:96] Setting up relu2
I1029 01:12:56.134214 25718 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 01:12:56.134220 25718 net.cpp:67] Creating Layer drop2
I1029 01:12:56.134224 25718 net.cpp:394] drop2 <- pool2
I1029 01:12:56.134229 25718 net.cpp:345] drop2 -> pool2 (in-place)
I1029 01:12:56.134235 25718 net.cpp:96] Setting up drop2
I1029 01:12:56.134239 25718 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 01:12:56.134246 25718 net.cpp:67] Creating Layer conv3
I1029 01:12:56.134250 25718 net.cpp:394] conv3 <- pool2
I1029 01:12:56.134258 25718 net.cpp:356] conv3 -> conv3
I1029 01:12:56.134265 25718 net.cpp:96] Setting up conv3
I1029 01:12:56.135864 25718 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 01:12:56.135885 25718 net.cpp:67] Creating Layer pool3
I1029 01:12:56.135890 25718 net.cpp:394] pool3 <- conv3
I1029 01:12:56.135898 25718 net.cpp:356] pool3 -> pool3
I1029 01:12:56.135905 25718 net.cpp:96] Setting up pool3
I1029 01:12:56.135911 25718 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 01:12:56.135916 25718 net.cpp:67] Creating Layer relu3
I1029 01:12:56.135921 25718 net.cpp:394] relu3 <- pool3
I1029 01:12:56.135926 25718 net.cpp:345] relu3 -> pool3 (in-place)
I1029 01:12:56.135931 25718 net.cpp:96] Setting up relu3
I1029 01:12:56.135936 25718 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 01:12:56.135941 25718 net.cpp:67] Creating Layer drop3
I1029 01:12:56.135946 25718 net.cpp:394] drop3 <- pool3
I1029 01:12:56.135952 25718 net.cpp:345] drop3 -> pool3 (in-place)
I1029 01:12:56.135958 25718 net.cpp:96] Setting up drop3
I1029 01:12:56.135962 25718 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 01:12:56.135969 25718 net.cpp:67] Creating Layer ip1
I1029 01:12:56.135973 25718 net.cpp:394] ip1 <- pool3
I1029 01:12:56.135980 25718 net.cpp:356] ip1 -> ip1
I1029 01:12:56.135988 25718 net.cpp:96] Setting up ip1
I1029 01:12:56.646237 25718 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 01:12:56.646299 25718 net.cpp:67] Creating Layer relu4
I1029 01:12:56.646307 25718 net.cpp:394] relu4 <- ip1
I1029 01:12:56.646317 25718 net.cpp:345] relu4 -> ip1 (in-place)
I1029 01:12:56.646325 25718 net.cpp:96] Setting up relu4
I1029 01:12:56.646330 25718 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 01:12:56.646339 25718 net.cpp:67] Creating Layer drop4
I1029 01:12:56.646344 25718 net.cpp:394] drop4 <- ip1
I1029 01:12:56.646350 25718 net.cpp:345] drop4 -> ip1 (in-place)
I1029 01:12:56.646356 25718 net.cpp:96] Setting up drop4
I1029 01:12:56.646361 25718 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 01:12:56.646369 25718 net.cpp:67] Creating Layer ip2
I1029 01:12:56.646373 25718 net.cpp:394] ip2 <- ip1
I1029 01:12:56.646380 25718 net.cpp:356] ip2 -> ip2
I1029 01:12:56.646392 25718 net.cpp:96] Setting up ip2
I1029 01:12:56.655318 25718 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 01:12:56.655380 25718 net.cpp:67] Creating Layer prob
I1029 01:12:56.655387 25718 net.cpp:394] prob <- ip2
I1029 01:12:56.655395 25718 net.cpp:356] prob -> prob
I1029 01:12:56.655405 25718 net.cpp:96] Setting up prob
I1029 01:12:56.655414 25718 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 01:12:56.655419 25718 net.cpp:172] prob does not need backward computation.
I1029 01:12:56.655423 25718 net.cpp:172] ip2 does not need backward computation.
I1029 01:12:56.655427 25718 net.cpp:172] drop4 does not need backward computation.
I1029 01:12:56.655431 25718 net.cpp:172] relu4 does not need backward computation.
I1029 01:12:56.655436 25718 net.cpp:172] ip1 does not need backward computation.
I1029 01:12:56.655438 25718 net.cpp:172] drop3 does not need backward computation.
I1029 01:12:56.655442 25718 net.cpp:172] relu3 does not need backward computation.
I1029 01:12:56.655447 25718 net.cpp:172] pool3 does not need backward computation.
I1029 01:12:56.655449 25718 net.cpp:172] conv3 does not need backward computation.
I1029 01:12:56.655453 25718 net.cpp:172] drop2 does not need backward computation.
I1029 01:12:56.655457 25718 net.cpp:172] relu2 does not need backward computation.
I1029 01:12:56.655460 25718 net.cpp:172] pool2 does not need backward computation.
I1029 01:12:56.655464 25718 net.cpp:172] conv2 does not need backward computation.
I1029 01:12:56.655467 25718 net.cpp:172] drop1 does not need backward computation.
I1029 01:12:56.655472 25718 net.cpp:172] relu1 does not need backward computation.
I1029 01:12:56.655474 25718 net.cpp:172] pool1 does not need backward computation.
I1029 01:12:56.655478 25718 net.cpp:172] conv1 does not need backward computation.
I1029 01:12:56.655482 25718 net.cpp:208] This network produces output prob
I1029 01:12:56.655493 25718 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 01:12:56.655500 25718 net.cpp:219] Network initialization done.
I1029 01:12:56.655503 25718 net.cpp:220] Memory required for data: 1837200
I1029 01:52:03.346287  2034 convert_imageset.cpp:70] Shuffling data
I1029 01:52:04.046330  2034 convert_imageset.cpp:73] A total of 60000 images.
I1029 01:52:04.046407  2034 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 01:52:06.707273  2034 convert_imageset.cpp:177] Processed 1000 files.
E1029 01:52:09.130767  2034 convert_imageset.cpp:177] Processed 2000 files.
E1029 01:52:11.441292  2034 convert_imageset.cpp:177] Processed 3000 files.
E1029 01:52:13.636216  2034 convert_imageset.cpp:177] Processed 4000 files.
E1029 01:52:15.834149  2034 convert_imageset.cpp:177] Processed 5000 files.
E1029 01:52:17.938825  2034 convert_imageset.cpp:177] Processed 6000 files.
E1029 01:52:19.846460  2034 convert_imageset.cpp:177] Processed 7000 files.
E1029 01:52:21.937491  2034 convert_imageset.cpp:177] Processed 8000 files.
E1029 01:52:23.981099  2034 convert_imageset.cpp:177] Processed 9000 files.
E1029 01:52:26.012140  2034 convert_imageset.cpp:177] Processed 10000 files.
E1029 01:52:28.252578  2034 convert_imageset.cpp:177] Processed 11000 files.
E1029 01:52:30.568121  2034 convert_imageset.cpp:177] Processed 12000 files.
E1029 01:52:32.507987  2034 convert_imageset.cpp:177] Processed 13000 files.
E1029 01:52:34.462254  2034 convert_imageset.cpp:177] Processed 14000 files.
E1029 01:52:36.396513  2034 convert_imageset.cpp:177] Processed 15000 files.
E1029 01:52:38.405576  2034 convert_imageset.cpp:177] Processed 16000 files.
E1029 01:52:40.323222  2034 convert_imageset.cpp:177] Processed 17000 files.
E1029 01:52:42.186431  2034 convert_imageset.cpp:177] Processed 18000 files.
E1029 01:52:44.032515  2034 convert_imageset.cpp:177] Processed 19000 files.
E1029 01:52:45.986294  2034 convert_imageset.cpp:177] Processed 20000 files.
E1029 01:52:47.877295  2034 convert_imageset.cpp:177] Processed 21000 files.
E1029 01:52:49.689388  2034 convert_imageset.cpp:177] Processed 22000 files.
E1029 01:52:51.606622  2034 convert_imageset.cpp:177] Processed 23000 files.
E1029 01:52:53.414054  2034 convert_imageset.cpp:177] Processed 24000 files.
E1029 01:52:55.144455  2034 convert_imageset.cpp:177] Processed 25000 files.
E1029 01:52:56.987244  2034 convert_imageset.cpp:177] Processed 26000 files.
E1029 01:52:58.767041  2034 convert_imageset.cpp:177] Processed 27000 files.
E1029 01:53:00.534245  2034 convert_imageset.cpp:177] Processed 28000 files.
E1029 01:53:02.403852  2034 convert_imageset.cpp:177] Processed 29000 files.
E1029 01:53:04.180352  2034 convert_imageset.cpp:177] Processed 30000 files.
E1029 01:53:06.181252  2034 convert_imageset.cpp:177] Processed 31000 files.
E1029 01:53:08.098690  2034 convert_imageset.cpp:177] Processed 32000 files.
E1029 01:53:09.981401  2034 convert_imageset.cpp:177] Processed 33000 files.
E1029 01:53:11.728584  2034 convert_imageset.cpp:177] Processed 34000 files.
E1029 01:53:13.533591  2034 convert_imageset.cpp:177] Processed 35000 files.
E1029 01:53:15.277861  2034 convert_imageset.cpp:177] Processed 36000 files.
E1029 01:53:16.988572  2034 convert_imageset.cpp:177] Processed 37000 files.
E1029 01:53:18.708972  2034 convert_imageset.cpp:177] Processed 38000 files.
E1029 01:53:20.465940  2034 convert_imageset.cpp:177] Processed 39000 files.
E1029 01:53:22.198987  2034 convert_imageset.cpp:177] Processed 40000 files.
E1029 01:53:23.910825  2034 convert_imageset.cpp:177] Processed 41000 files.
E1029 01:53:25.621023  2034 convert_imageset.cpp:177] Processed 42000 files.
E1029 01:53:27.425503  2034 convert_imageset.cpp:177] Processed 43000 files.
E1029 01:53:29.131017  2034 convert_imageset.cpp:177] Processed 44000 files.
E1029 01:53:30.794688  2034 convert_imageset.cpp:177] Processed 45000 files.
E1029 01:53:32.455168  2034 convert_imageset.cpp:177] Processed 46000 files.
E1029 01:53:34.202442  2034 convert_imageset.cpp:177] Processed 47000 files.
E1029 01:53:35.893916  2034 convert_imageset.cpp:177] Processed 48000 files.
E1029 01:53:37.644876  2034 convert_imageset.cpp:177] Processed 49000 files.
E1029 01:53:39.355198  2034 convert_imageset.cpp:177] Processed 50000 files.
E1029 01:53:41.024382  2034 convert_imageset.cpp:177] Processed 51000 files.
E1029 01:53:42.785598  2034 convert_imageset.cpp:177] Processed 52000 files.
E1029 01:53:44.484652  2034 convert_imageset.cpp:177] Processed 53000 files.
E1029 01:53:46.238868  2034 convert_imageset.cpp:177] Processed 54000 files.
E1029 01:53:48.026087  2034 convert_imageset.cpp:177] Processed 55000 files.
E1029 01:53:49.684393  2034 convert_imageset.cpp:177] Processed 56000 files.
E1029 01:53:51.314317  2034 convert_imageset.cpp:177] Processed 57000 files.
E1029 01:53:53.018972  2034 convert_imageset.cpp:177] Processed 58000 files.
E1029 01:53:54.681460  2034 convert_imageset.cpp:177] Processed 59000 files.
E1029 01:53:56.459147  2034 convert_imageset.cpp:177] Processed 60000 files.
I1029 01:53:56.664726  2193 caffe.cpp:99] Use GPU with device ID 0
I1029 01:53:57.014137  2193 caffe.cpp:107] Starting Optimization
I1029 01:53:57.014262  2193 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 460000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 01:53:57.014293  2193 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 01:53:57.023316  2193 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 01:53:57.023408  2193 net.cpp:67] Creating Layer mnist
I1029 01:53:57.023419  2193 net.cpp:356] mnist -> data
I1029 01:53:57.023437  2193 net.cpp:356] mnist -> label
I1029 01:53:57.023450  2193 net.cpp:96] Setting up mnist
I1029 01:53:57.030612  2193 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 01:53:57.030710  2193 data_layer.cpp:128] output data size: 64,1,50,180
I1029 01:53:57.031512  2193 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 01:53:57.031536  2193 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 01:53:57.031549  2193 net.cpp:67] Creating Layer conv1
I1029 01:53:57.031560  2193 net.cpp:394] conv1 <- data
I1029 01:53:57.031575  2193 net.cpp:356] conv1 -> conv1
I1029 01:53:57.031586  2193 net.cpp:96] Setting up conv1
I1029 01:53:57.031955  2193 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 01:53:57.031988  2193 net.cpp:67] Creating Layer pool1
I1029 01:53:57.031994  2193 net.cpp:394] pool1 <- conv1
I1029 01:53:57.032001  2193 net.cpp:356] pool1 -> pool1
I1029 01:53:57.032008  2193 net.cpp:96] Setting up pool1
I1029 01:53:57.032026  2193 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 01:53:57.032034  2193 net.cpp:67] Creating Layer relu1
I1029 01:53:57.032039  2193 net.cpp:394] relu1 <- pool1
I1029 01:53:57.032045  2193 net.cpp:345] relu1 -> pool1 (in-place)
I1029 01:53:57.032052  2193 net.cpp:96] Setting up relu1
I1029 01:53:57.032057  2193 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 01:53:57.032064  2193 net.cpp:67] Creating Layer drop1
I1029 01:53:57.032068  2193 net.cpp:394] drop1 <- pool1
I1029 01:53:57.032076  2193 net.cpp:345] drop1 -> pool1 (in-place)
I1029 01:53:57.032083  2193 net.cpp:96] Setting up drop1
I1029 01:53:57.032089  2193 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 01:53:57.032096  2193 net.cpp:67] Creating Layer conv2
I1029 01:53:57.032100  2193 net.cpp:394] conv2 <- pool1
I1029 01:53:57.032109  2193 net.cpp:356] conv2 -> conv2
I1029 01:53:57.032117  2193 net.cpp:96] Setting up conv2
I1029 01:53:57.032727  2193 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 01:53:57.032747  2193 net.cpp:67] Creating Layer pool2
I1029 01:53:57.032753  2193 net.cpp:394] pool2 <- conv2
I1029 01:53:57.032760  2193 net.cpp:356] pool2 -> pool2
I1029 01:53:57.032768  2193 net.cpp:96] Setting up pool2
I1029 01:53:57.032773  2193 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 01:53:57.032778  2193 net.cpp:67] Creating Layer relu2
I1029 01:53:57.032783  2193 net.cpp:394] relu2 <- pool2
I1029 01:53:57.032790  2193 net.cpp:345] relu2 -> pool2 (in-place)
I1029 01:53:57.032798  2193 net.cpp:96] Setting up relu2
I1029 01:53:57.032802  2193 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 01:53:57.032809  2193 net.cpp:67] Creating Layer drop2
I1029 01:53:57.032814  2193 net.cpp:394] drop2 <- pool2
I1029 01:53:57.032819  2193 net.cpp:345] drop2 -> pool2 (in-place)
I1029 01:53:57.032825  2193 net.cpp:96] Setting up drop2
I1029 01:53:57.032830  2193 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 01:53:57.032841  2193 net.cpp:67] Creating Layer conv3
I1029 01:53:57.032846  2193 net.cpp:394] conv3 <- pool2
I1029 01:53:57.032853  2193 net.cpp:356] conv3 -> conv3
I1029 01:53:57.032860  2193 net.cpp:96] Setting up conv3
I1029 01:53:57.034373  2193 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 01:53:57.034399  2193 net.cpp:67] Creating Layer pool3
I1029 01:53:57.034404  2193 net.cpp:394] pool3 <- conv3
I1029 01:53:57.034412  2193 net.cpp:356] pool3 -> pool3
I1029 01:53:57.034421  2193 net.cpp:96] Setting up pool3
I1029 01:53:57.034427  2193 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 01:53:57.034433  2193 net.cpp:67] Creating Layer relu3
I1029 01:53:57.034438  2193 net.cpp:394] relu3 <- pool3
I1029 01:53:57.034443  2193 net.cpp:345] relu3 -> pool3 (in-place)
I1029 01:53:57.034452  2193 net.cpp:96] Setting up relu3
I1029 01:53:57.034456  2193 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 01:53:57.034463  2193 net.cpp:67] Creating Layer drop3
I1029 01:53:57.034467  2193 net.cpp:394] drop3 <- pool3
I1029 01:53:57.034473  2193 net.cpp:345] drop3 -> pool3 (in-place)
I1029 01:53:57.034479  2193 net.cpp:96] Setting up drop3
I1029 01:53:57.034483  2193 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 01:53:57.034490  2193 net.cpp:67] Creating Layer ip1
I1029 01:53:57.034494  2193 net.cpp:394] ip1 <- pool3
I1029 01:53:57.034502  2193 net.cpp:356] ip1 -> ip1
I1029 01:53:57.034538  2193 net.cpp:96] Setting up ip1
I1029 01:53:57.507858  2193 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 01:53:57.507918  2193 net.cpp:67] Creating Layer relu4
I1029 01:53:57.507925  2193 net.cpp:394] relu4 <- ip1
I1029 01:53:57.507943  2193 net.cpp:345] relu4 -> ip1 (in-place)
I1029 01:53:57.507953  2193 net.cpp:96] Setting up relu4
I1029 01:53:57.507958  2193 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 01:53:57.507966  2193 net.cpp:67] Creating Layer drop4
I1029 01:53:57.507969  2193 net.cpp:394] drop4 <- ip1
I1029 01:53:57.507975  2193 net.cpp:345] drop4 -> ip1 (in-place)
I1029 01:53:57.507982  2193 net.cpp:96] Setting up drop4
I1029 01:53:57.507987  2193 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 01:53:57.507999  2193 net.cpp:67] Creating Layer ip2
I1029 01:53:57.508004  2193 net.cpp:394] ip2 <- ip1
I1029 01:53:57.508013  2193 net.cpp:356] ip2 -> ip2
I1029 01:53:57.508023  2193 net.cpp:96] Setting up ip2
I1029 01:53:57.516271  2193 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 01:53:57.516335  2193 net.cpp:67] Creating Layer loss
I1029 01:53:57.516342  2193 net.cpp:394] loss <- ip2
I1029 01:53:57.516350  2193 net.cpp:394] loss <- label
I1029 01:53:57.516357  2193 net.cpp:356] loss -> loss
I1029 01:53:57.516367  2193 net.cpp:96] Setting up loss
I1029 01:53:57.516378  2193 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 01:53:57.516383  2193 net.cpp:109]     with loss weight 1
I1029 01:53:57.516443  2193 net.cpp:170] loss needs backward computation.
I1029 01:53:57.516453  2193 net.cpp:170] ip2 needs backward computation.
I1029 01:53:57.516458  2193 net.cpp:170] drop4 needs backward computation.
I1029 01:53:57.516463  2193 net.cpp:170] relu4 needs backward computation.
I1029 01:53:57.516468  2193 net.cpp:170] ip1 needs backward computation.
I1029 01:53:57.516472  2193 net.cpp:170] drop3 needs backward computation.
I1029 01:53:57.516477  2193 net.cpp:170] relu3 needs backward computation.
I1029 01:53:57.516481  2193 net.cpp:170] pool3 needs backward computation.
I1029 01:53:57.516486  2193 net.cpp:170] conv3 needs backward computation.
I1029 01:53:57.516491  2193 net.cpp:170] drop2 needs backward computation.
I1029 01:53:57.516496  2193 net.cpp:170] relu2 needs backward computation.
I1029 01:53:57.516500  2193 net.cpp:170] pool2 needs backward computation.
I1029 01:53:57.516505  2193 net.cpp:170] conv2 needs backward computation.
I1029 01:53:57.516510  2193 net.cpp:170] drop1 needs backward computation.
I1029 01:53:57.516515  2193 net.cpp:170] relu1 needs backward computation.
I1029 01:53:57.516520  2193 net.cpp:170] pool1 needs backward computation.
I1029 01:53:57.516525  2193 net.cpp:170] conv1 needs backward computation.
I1029 01:53:57.516530  2193 net.cpp:172] mnist does not need backward computation.
I1029 01:53:57.516533  2193 net.cpp:208] This network produces output loss
I1029 01:53:57.516547  2193 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 01:53:57.516556  2193 net.cpp:219] Network initialization done.
I1029 01:53:57.516559  2193 net.cpp:220] Memory required for data: 119788292
I1029 01:53:57.516621  2193 solver.cpp:41] Solver scaffolding done.
I1029 01:53:57.516628  2193 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_455000.solverstate
I1029 01:53:57.516631  2193 solver.cpp:160] Solving Captcha
I1029 01:53:57.516650  2193 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_455000.solverstate
I1029 01:54:01.264716  2193 solver.cpp:502] SGDSolver: restoring history
I1029 01:54:02.015430  2193 solver.cpp:191] Iteration 455000, loss = 2.33241
I1029 01:54:02.015488  2193 solver.cpp:206]     Train net output #0: loss = 2.33241 (* 1 = 2.33241 loss)
I1029 01:54:02.015503  2193 solver.cpp:403] Iteration 455000, lr = 0.000561578
I1029 01:58:03.884451  2193 solver.cpp:191] Iteration 456000, loss = 2.40854
I1029 01:58:03.885202  2193 solver.cpp:206]     Train net output #0: loss = 2.40854 (* 1 = 2.40854 loss)
I1029 01:58:03.885239  2193 solver.cpp:403] Iteration 456000, lr = 0.000560674
I1029 02:02:05.514859  2193 solver.cpp:191] Iteration 457000, loss = 2.61881
I1029 02:02:05.515444  2193 solver.cpp:206]     Train net output #0: loss = 2.61881 (* 1 = 2.61881 loss)
I1029 02:02:05.515486  2193 solver.cpp:403] Iteration 457000, lr = 0.000559774
I1029 02:06:07.010442  2193 solver.cpp:191] Iteration 458000, loss = 2.45467
I1029 02:06:07.010987  2193 solver.cpp:206]     Train net output #0: loss = 2.45467 (* 1 = 2.45467 loss)
I1029 02:06:07.011021  2193 solver.cpp:403] Iteration 458000, lr = 0.000558876
I1029 02:10:08.653316  2193 solver.cpp:191] Iteration 459000, loss = 2.26103
I1029 02:10:08.654009  2193 solver.cpp:206]     Train net output #0: loss = 2.26103 (* 1 = 2.26103 loss)
I1029 02:10:08.654042  2193 solver.cpp:403] Iteration 459000, lr = 0.000557982
I1029 02:14:10.833066  2193 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_460000.caffemodel
I1029 02:14:16.409039  2193 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_460000.solverstate
I1029 02:14:21.753850  2193 solver.cpp:228] Iteration 460000, loss = 2.29278
I1029 02:14:21.754319  2193 solver.cpp:233] Optimization Done.
I1029 02:14:21.754346  2193 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 02:42:21.100649 18198 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 02:42:21.100759 18198 net.cpp:358] Input 0 -> data
I1029 02:42:21.100783 18198 net.cpp:67] Creating Layer conv1
I1029 02:42:21.100793 18198 net.cpp:394] conv1 <- data
I1029 02:42:21.100800 18198 net.cpp:356] conv1 -> conv1
I1029 02:42:21.100811 18198 net.cpp:96] Setting up conv1
I1029 02:42:21.101135 18198 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 02:42:21.101156 18198 net.cpp:67] Creating Layer pool1
I1029 02:42:21.101161 18198 net.cpp:394] pool1 <- conv1
I1029 02:42:21.101166 18198 net.cpp:356] pool1 -> pool1
I1029 02:42:21.101174 18198 net.cpp:96] Setting up pool1
I1029 02:42:21.101189 18198 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 02:42:21.101197 18198 net.cpp:67] Creating Layer relu1
I1029 02:42:21.101202 18198 net.cpp:394] relu1 <- pool1
I1029 02:42:21.101207 18198 net.cpp:345] relu1 -> pool1 (in-place)
I1029 02:42:21.101212 18198 net.cpp:96] Setting up relu1
I1029 02:42:21.101217 18198 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 02:42:21.101222 18198 net.cpp:67] Creating Layer drop1
I1029 02:42:21.101225 18198 net.cpp:394] drop1 <- pool1
I1029 02:42:21.101233 18198 net.cpp:345] drop1 -> pool1 (in-place)
I1029 02:42:21.101239 18198 net.cpp:96] Setting up drop1
I1029 02:42:21.101244 18198 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 02:42:21.101251 18198 net.cpp:67] Creating Layer conv2
I1029 02:42:21.101255 18198 net.cpp:394] conv2 <- pool1
I1029 02:42:21.101263 18198 net.cpp:356] conv2 -> conv2
I1029 02:42:21.101269 18198 net.cpp:96] Setting up conv2
I1029 02:42:21.101821 18198 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 02:42:21.101837 18198 net.cpp:67] Creating Layer pool2
I1029 02:42:21.101842 18198 net.cpp:394] pool2 <- conv2
I1029 02:42:21.101848 18198 net.cpp:356] pool2 -> pool2
I1029 02:42:21.101855 18198 net.cpp:96] Setting up pool2
I1029 02:42:21.101861 18198 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 02:42:21.101866 18198 net.cpp:67] Creating Layer relu2
I1029 02:42:21.101869 18198 net.cpp:394] relu2 <- pool2
I1029 02:42:21.101876 18198 net.cpp:345] relu2 -> pool2 (in-place)
I1029 02:42:21.101882 18198 net.cpp:96] Setting up relu2
I1029 02:42:21.101886 18198 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 02:42:21.101892 18198 net.cpp:67] Creating Layer drop2
I1029 02:42:21.101897 18198 net.cpp:394] drop2 <- pool2
I1029 02:42:21.101902 18198 net.cpp:345] drop2 -> pool2 (in-place)
I1029 02:42:21.101908 18198 net.cpp:96] Setting up drop2
I1029 02:42:21.101912 18198 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 02:42:21.101919 18198 net.cpp:67] Creating Layer conv3
I1029 02:42:21.101923 18198 net.cpp:394] conv3 <- pool2
I1029 02:42:21.101929 18198 net.cpp:356] conv3 -> conv3
I1029 02:42:21.101935 18198 net.cpp:96] Setting up conv3
I1029 02:42:21.103381 18198 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 02:42:21.103396 18198 net.cpp:67] Creating Layer pool3
I1029 02:42:21.103400 18198 net.cpp:394] pool3 <- conv3
I1029 02:42:21.103409 18198 net.cpp:356] pool3 -> pool3
I1029 02:42:21.103415 18198 net.cpp:96] Setting up pool3
I1029 02:42:21.103420 18198 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 02:42:21.103425 18198 net.cpp:67] Creating Layer relu3
I1029 02:42:21.103430 18198 net.cpp:394] relu3 <- pool3
I1029 02:42:21.103437 18198 net.cpp:345] relu3 -> pool3 (in-place)
I1029 02:42:21.103443 18198 net.cpp:96] Setting up relu3
I1029 02:42:21.103447 18198 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 02:42:21.103452 18198 net.cpp:67] Creating Layer drop3
I1029 02:42:21.103456 18198 net.cpp:394] drop3 <- pool3
I1029 02:42:21.103462 18198 net.cpp:345] drop3 -> pool3 (in-place)
I1029 02:42:21.103467 18198 net.cpp:96] Setting up drop3
I1029 02:42:21.103471 18198 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 02:42:21.103477 18198 net.cpp:67] Creating Layer ip1
I1029 02:42:21.103482 18198 net.cpp:394] ip1 <- pool3
I1029 02:42:21.103489 18198 net.cpp:356] ip1 -> ip1
I1029 02:42:21.103497 18198 net.cpp:96] Setting up ip1
I1029 02:42:21.665190 18198 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 02:42:21.665256 18198 net.cpp:67] Creating Layer relu4
I1029 02:42:21.665264 18198 net.cpp:394] relu4 <- ip1
I1029 02:42:21.665273 18198 net.cpp:345] relu4 -> ip1 (in-place)
I1029 02:42:21.665292 18198 net.cpp:96] Setting up relu4
I1029 02:42:21.665297 18198 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 02:42:21.665305 18198 net.cpp:67] Creating Layer drop4
I1029 02:42:21.665309 18198 net.cpp:394] drop4 <- ip1
I1029 02:42:21.665315 18198 net.cpp:345] drop4 -> ip1 (in-place)
I1029 02:42:21.665321 18198 net.cpp:96] Setting up drop4
I1029 02:42:21.665326 18198 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 02:42:21.665336 18198 net.cpp:67] Creating Layer ip2
I1029 02:42:21.665341 18198 net.cpp:394] ip2 <- ip1
I1029 02:42:21.665348 18198 net.cpp:356] ip2 -> ip2
I1029 02:42:21.665367 18198 net.cpp:96] Setting up ip2
I1029 02:42:21.673676 18198 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 02:42:21.673740 18198 net.cpp:67] Creating Layer prob
I1029 02:42:21.673748 18198 net.cpp:394] prob <- ip2
I1029 02:42:21.673755 18198 net.cpp:356] prob -> prob
I1029 02:42:21.673765 18198 net.cpp:96] Setting up prob
I1029 02:42:21.673773 18198 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 02:42:21.673776 18198 net.cpp:172] prob does not need backward computation.
I1029 02:42:21.673780 18198 net.cpp:172] ip2 does not need backward computation.
I1029 02:42:21.673784 18198 net.cpp:172] drop4 does not need backward computation.
I1029 02:42:21.673787 18198 net.cpp:172] relu4 does not need backward computation.
I1029 02:42:21.673791 18198 net.cpp:172] ip1 does not need backward computation.
I1029 02:42:21.673794 18198 net.cpp:172] drop3 does not need backward computation.
I1029 02:42:21.673799 18198 net.cpp:172] relu3 does not need backward computation.
I1029 02:42:21.673802 18198 net.cpp:172] pool3 does not need backward computation.
I1029 02:42:21.673805 18198 net.cpp:172] conv3 does not need backward computation.
I1029 02:42:21.673810 18198 net.cpp:172] drop2 does not need backward computation.
I1029 02:42:21.673812 18198 net.cpp:172] relu2 does not need backward computation.
I1029 02:42:21.673816 18198 net.cpp:172] pool2 does not need backward computation.
I1029 02:42:21.673820 18198 net.cpp:172] conv2 does not need backward computation.
I1029 02:42:21.673823 18198 net.cpp:172] drop1 does not need backward computation.
I1029 02:42:21.673826 18198 net.cpp:172] relu1 does not need backward computation.
I1029 02:42:21.673830 18198 net.cpp:172] pool1 does not need backward computation.
I1029 02:42:21.673833 18198 net.cpp:172] conv1 does not need backward computation.
I1029 02:42:21.673837 18198 net.cpp:208] This network produces output prob
I1029 02:42:21.673851 18198 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 02:42:21.673858 18198 net.cpp:219] Network initialization done.
I1029 02:42:21.673862 18198 net.cpp:220] Memory required for data: 1837200
I1029 03:18:31.480918 25781 convert_imageset.cpp:70] Shuffling data
I1029 03:18:32.126976 25781 convert_imageset.cpp:73] A total of 60000 images.
I1029 03:18:32.127055 25781 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 03:18:34.820132 25781 convert_imageset.cpp:177] Processed 1000 files.
E1029 03:18:37.296923 25781 convert_imageset.cpp:177] Processed 2000 files.
E1029 03:18:39.610074 25781 convert_imageset.cpp:177] Processed 3000 files.
E1029 03:18:41.966025 25781 convert_imageset.cpp:177] Processed 4000 files.
E1029 03:18:44.139330 25781 convert_imageset.cpp:177] Processed 5000 files.
E1029 03:18:46.196187 25781 convert_imageset.cpp:177] Processed 6000 files.
E1029 03:18:48.516650 25781 convert_imageset.cpp:177] Processed 7000 files.
E1029 03:18:50.717391 25781 convert_imageset.cpp:177] Processed 8000 files.
E1029 03:18:53.005776 25781 convert_imageset.cpp:177] Processed 9000 files.
E1029 03:18:55.162423 25781 convert_imageset.cpp:177] Processed 10000 files.
E1029 03:18:57.134835 25781 convert_imageset.cpp:177] Processed 11000 files.
E1029 03:18:59.171533 25781 convert_imageset.cpp:177] Processed 12000 files.
E1029 03:19:01.113679 25781 convert_imageset.cpp:177] Processed 13000 files.
E1029 03:19:03.179875 25781 convert_imageset.cpp:177] Processed 14000 files.
E1029 03:19:05.108310 25781 convert_imageset.cpp:177] Processed 15000 files.
E1029 03:19:06.969614 25781 convert_imageset.cpp:177] Processed 16000 files.
E1029 03:19:08.867030 25781 convert_imageset.cpp:177] Processed 17000 files.
E1029 03:19:10.799137 25781 convert_imageset.cpp:177] Processed 18000 files.
E1029 03:19:12.631069 25781 convert_imageset.cpp:177] Processed 19000 files.
E1029 03:19:14.576918 25781 convert_imageset.cpp:177] Processed 20000 files.
E1029 03:19:16.362149 25781 convert_imageset.cpp:177] Processed 21000 files.
E1029 03:19:18.195380 25781 convert_imageset.cpp:177] Processed 22000 files.
E1029 03:19:19.983800 25781 convert_imageset.cpp:177] Processed 23000 files.
E1029 03:19:21.810245 25781 convert_imageset.cpp:177] Processed 24000 files.
E1029 03:19:23.645346 25781 convert_imageset.cpp:177] Processed 25000 files.
E1029 03:19:25.413179 25781 convert_imageset.cpp:177] Processed 26000 files.
E1029 03:19:27.147486 25781 convert_imageset.cpp:177] Processed 27000 files.
E1029 03:19:28.902248 25781 convert_imageset.cpp:177] Processed 28000 files.
E1029 03:19:30.652353 25781 convert_imageset.cpp:177] Processed 29000 files.
E1029 03:19:32.370102 25781 convert_imageset.cpp:177] Processed 30000 files.
E1029 03:19:34.215950 25781 convert_imageset.cpp:177] Processed 31000 files.
E1029 03:19:35.950698 25781 convert_imageset.cpp:177] Processed 32000 files.
E1029 03:19:37.683971 25781 convert_imageset.cpp:177] Processed 33000 files.
E1029 03:19:39.368407 25781 convert_imageset.cpp:177] Processed 34000 files.
E1029 03:19:41.086320 25781 convert_imageset.cpp:177] Processed 35000 files.
E1029 03:19:42.795964 25781 convert_imageset.cpp:177] Processed 36000 files.
E1029 03:19:44.636198 25781 convert_imageset.cpp:177] Processed 37000 files.
E1029 03:19:46.474895 25781 convert_imageset.cpp:177] Processed 38000 files.
E1029 03:19:48.136049 25781 convert_imageset.cpp:177] Processed 39000 files.
E1029 03:19:49.796113 25781 convert_imageset.cpp:177] Processed 40000 files.
E1029 03:19:51.741839 25781 convert_imageset.cpp:177] Processed 41000 files.
E1029 03:19:53.502496 25781 convert_imageset.cpp:177] Processed 42000 files.
E1029 03:19:55.196633 25781 convert_imageset.cpp:177] Processed 43000 files.
E1029 03:19:56.918473 25781 convert_imageset.cpp:177] Processed 44000 files.
E1029 03:19:58.640019 25781 convert_imageset.cpp:177] Processed 45000 files.
E1029 03:20:00.309602 25781 convert_imageset.cpp:177] Processed 46000 files.
E1029 03:20:01.983595 25781 convert_imageset.cpp:177] Processed 47000 files.
E1029 03:20:03.674715 25781 convert_imageset.cpp:177] Processed 48000 files.
E1029 03:20:05.415043 25781 convert_imageset.cpp:177] Processed 49000 files.
E1029 03:20:07.117432 25781 convert_imageset.cpp:177] Processed 50000 files.
E1029 03:20:08.785987 25781 convert_imageset.cpp:177] Processed 51000 files.
E1029 03:20:10.523890 25781 convert_imageset.cpp:177] Processed 52000 files.
E1029 03:20:12.211316 25781 convert_imageset.cpp:177] Processed 53000 files.
E1029 03:20:13.878082 25781 convert_imageset.cpp:177] Processed 54000 files.
E1029 03:20:15.559334 25781 convert_imageset.cpp:177] Processed 55000 files.
E1029 03:20:17.233330 25781 convert_imageset.cpp:177] Processed 56000 files.
E1029 03:20:18.897472 25781 convert_imageset.cpp:177] Processed 57000 files.
E1029 03:20:20.504168 25781 convert_imageset.cpp:177] Processed 58000 files.
E1029 03:20:22.169512 25781 convert_imageset.cpp:177] Processed 59000 files.
E1029 03:20:23.850500 25781 convert_imageset.cpp:177] Processed 60000 files.
I1029 03:20:24.197355 25955 caffe.cpp:99] Use GPU with device ID 0
I1029 03:20:24.548018 25955 caffe.cpp:107] Starting Optimization
I1029 03:20:24.548135 25955 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 465000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 03:20:24.548166 25955 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 03:20:24.557026 25955 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 03:20:24.557240 25955 net.cpp:67] Creating Layer mnist
I1029 03:20:24.557265 25955 net.cpp:356] mnist -> data
I1029 03:20:24.557302 25955 net.cpp:356] mnist -> label
I1029 03:20:24.557335 25955 net.cpp:96] Setting up mnist
I1029 03:20:24.565438 25955 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 03:20:24.565572 25955 data_layer.cpp:128] output data size: 64,1,50,180
I1029 03:20:24.567297 25955 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 03:20:24.567337 25955 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 03:20:24.567363 25955 net.cpp:67] Creating Layer conv1
I1029 03:20:24.567378 25955 net.cpp:394] conv1 <- data
I1029 03:20:24.567412 25955 net.cpp:356] conv1 -> conv1
I1029 03:20:24.567440 25955 net.cpp:96] Setting up conv1
I1029 03:20:24.568366 25955 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 03:20:24.568475 25955 net.cpp:67] Creating Layer pool1
I1029 03:20:24.568495 25955 net.cpp:394] pool1 <- conv1
I1029 03:20:24.568513 25955 net.cpp:356] pool1 -> pool1
I1029 03:20:24.568533 25955 net.cpp:96] Setting up pool1
I1029 03:20:24.568567 25955 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 03:20:24.568590 25955 net.cpp:67] Creating Layer relu1
I1029 03:20:24.568604 25955 net.cpp:394] relu1 <- pool1
I1029 03:20:24.568620 25955 net.cpp:345] relu1 -> pool1 (in-place)
I1029 03:20:24.568637 25955 net.cpp:96] Setting up relu1
I1029 03:20:24.568651 25955 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 03:20:24.568670 25955 net.cpp:67] Creating Layer drop1
I1029 03:20:24.568683 25955 net.cpp:394] drop1 <- pool1
I1029 03:20:24.568704 25955 net.cpp:345] drop1 -> pool1 (in-place)
I1029 03:20:24.568722 25955 net.cpp:96] Setting up drop1
I1029 03:20:24.568737 25955 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 03:20:24.568755 25955 net.cpp:67] Creating Layer conv2
I1029 03:20:24.568768 25955 net.cpp:394] conv2 <- pool1
I1029 03:20:24.568790 25955 net.cpp:356] conv2 -> conv2
I1029 03:20:24.568812 25955 net.cpp:96] Setting up conv2
I1029 03:20:24.570349 25955 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 03:20:24.570392 25955 net.cpp:67] Creating Layer pool2
I1029 03:20:24.570408 25955 net.cpp:394] pool2 <- conv2
I1029 03:20:24.570426 25955 net.cpp:356] pool2 -> pool2
I1029 03:20:24.570446 25955 net.cpp:96] Setting up pool2
I1029 03:20:24.570461 25955 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 03:20:24.570477 25955 net.cpp:67] Creating Layer relu2
I1029 03:20:24.570492 25955 net.cpp:394] relu2 <- pool2
I1029 03:20:24.570511 25955 net.cpp:345] relu2 -> pool2 (in-place)
I1029 03:20:24.570529 25955 net.cpp:96] Setting up relu2
I1029 03:20:24.570543 25955 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 03:20:24.570561 25955 net.cpp:67] Creating Layer drop2
I1029 03:20:24.570574 25955 net.cpp:394] drop2 <- pool2
I1029 03:20:24.570591 25955 net.cpp:345] drop2 -> pool2 (in-place)
I1029 03:20:24.570607 25955 net.cpp:96] Setting up drop2
I1029 03:20:24.570621 25955 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 03:20:24.570643 25955 net.cpp:67] Creating Layer conv3
I1029 03:20:24.570657 25955 net.cpp:394] conv3 <- pool2
I1029 03:20:24.570674 25955 net.cpp:356] conv3 -> conv3
I1029 03:20:24.570694 25955 net.cpp:96] Setting up conv3
I1029 03:20:24.574815 25955 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 03:20:24.574870 25955 net.cpp:67] Creating Layer pool3
I1029 03:20:24.574884 25955 net.cpp:394] pool3 <- conv3
I1029 03:20:24.574908 25955 net.cpp:356] pool3 -> pool3
I1029 03:20:24.574928 25955 net.cpp:96] Setting up pool3
I1029 03:20:24.574944 25955 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 03:20:24.574961 25955 net.cpp:67] Creating Layer relu3
I1029 03:20:24.574973 25955 net.cpp:394] relu3 <- pool3
I1029 03:20:24.574990 25955 net.cpp:345] relu3 -> pool3 (in-place)
I1029 03:20:24.575006 25955 net.cpp:96] Setting up relu3
I1029 03:20:24.575018 25955 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 03:20:24.575038 25955 net.cpp:67] Creating Layer drop3
I1029 03:20:24.575052 25955 net.cpp:394] drop3 <- pool3
I1029 03:20:24.575067 25955 net.cpp:345] drop3 -> pool3 (in-place)
I1029 03:20:24.575084 25955 net.cpp:96] Setting up drop3
I1029 03:20:24.575098 25955 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 03:20:24.575116 25955 net.cpp:67] Creating Layer ip1
I1029 03:20:24.575129 25955 net.cpp:394] ip1 <- pool3
I1029 03:20:24.575150 25955 net.cpp:356] ip1 -> ip1
I1029 03:20:24.575213 25955 net.cpp:96] Setting up ip1
I1029 03:20:25.002883 25955 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 03:20:25.002939 25955 net.cpp:67] Creating Layer relu4
I1029 03:20:25.002948 25955 net.cpp:394] relu4 <- ip1
I1029 03:20:25.002956 25955 net.cpp:345] relu4 -> ip1 (in-place)
I1029 03:20:25.002966 25955 net.cpp:96] Setting up relu4
I1029 03:20:25.002971 25955 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 03:20:25.002985 25955 net.cpp:67] Creating Layer drop4
I1029 03:20:25.002990 25955 net.cpp:394] drop4 <- ip1
I1029 03:20:25.002995 25955 net.cpp:345] drop4 -> ip1 (in-place)
I1029 03:20:25.003002 25955 net.cpp:96] Setting up drop4
I1029 03:20:25.003007 25955 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 03:20:25.003021 25955 net.cpp:67] Creating Layer ip2
I1029 03:20:25.003026 25955 net.cpp:394] ip2 <- ip1
I1029 03:20:25.003034 25955 net.cpp:356] ip2 -> ip2
I1029 03:20:25.003042 25955 net.cpp:96] Setting up ip2
I1029 03:20:25.012222 25955 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 03:20:25.012292 25955 net.cpp:67] Creating Layer loss
I1029 03:20:25.012300 25955 net.cpp:394] loss <- ip2
I1029 03:20:25.012307 25955 net.cpp:394] loss <- label
I1029 03:20:25.012315 25955 net.cpp:356] loss -> loss
I1029 03:20:25.012325 25955 net.cpp:96] Setting up loss
I1029 03:20:25.012336 25955 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 03:20:25.012341 25955 net.cpp:109]     with loss weight 1
I1029 03:20:25.012377 25955 net.cpp:170] loss needs backward computation.
I1029 03:20:25.012382 25955 net.cpp:170] ip2 needs backward computation.
I1029 03:20:25.012387 25955 net.cpp:170] drop4 needs backward computation.
I1029 03:20:25.012390 25955 net.cpp:170] relu4 needs backward computation.
I1029 03:20:25.012394 25955 net.cpp:170] ip1 needs backward computation.
I1029 03:20:25.012399 25955 net.cpp:170] drop3 needs backward computation.
I1029 03:20:25.012403 25955 net.cpp:170] relu3 needs backward computation.
I1029 03:20:25.012408 25955 net.cpp:170] pool3 needs backward computation.
I1029 03:20:25.012413 25955 net.cpp:170] conv3 needs backward computation.
I1029 03:20:25.012426 25955 net.cpp:170] drop2 needs backward computation.
I1029 03:20:25.012431 25955 net.cpp:170] relu2 needs backward computation.
I1029 03:20:25.012436 25955 net.cpp:170] pool2 needs backward computation.
I1029 03:20:25.012441 25955 net.cpp:170] conv2 needs backward computation.
I1029 03:20:25.012445 25955 net.cpp:170] drop1 needs backward computation.
I1029 03:20:25.012450 25955 net.cpp:170] relu1 needs backward computation.
I1029 03:20:25.012454 25955 net.cpp:170] pool1 needs backward computation.
I1029 03:20:25.012459 25955 net.cpp:170] conv1 needs backward computation.
I1029 03:20:25.012464 25955 net.cpp:172] mnist does not need backward computation.
I1029 03:20:25.012469 25955 net.cpp:208] This network produces output loss
I1029 03:20:25.012478 25955 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 03:20:25.012486 25955 net.cpp:219] Network initialization done.
I1029 03:20:25.012490 25955 net.cpp:220] Memory required for data: 119788292
I1029 03:20:25.012550 25955 solver.cpp:41] Solver scaffolding done.
I1029 03:20:25.012557 25955 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_460000.solverstate
I1029 03:20:25.012562 25955 solver.cpp:160] Solving Captcha
I1029 03:20:25.012581 25955 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_460000.solverstate
I1029 03:20:29.074661 25955 solver.cpp:502] SGDSolver: restoring history
I1029 03:20:29.829036 25955 solver.cpp:191] Iteration 460000, loss = 2.44785
I1029 03:20:29.829095 25955 solver.cpp:206]     Train net output #0: loss = 2.44785 (* 1 = 2.44785 loss)
I1029 03:20:29.829112 25955 solver.cpp:403] Iteration 460000, lr = 0.000557092
I1029 03:24:34.888219 25955 solver.cpp:191] Iteration 461000, loss = 2.68374
I1029 03:24:34.888949 25955 solver.cpp:206]     Train net output #0: loss = 2.68374 (* 1 = 2.68374 loss)
I1029 03:24:34.888983 25955 solver.cpp:403] Iteration 461000, lr = 0.000556204
I1029 03:28:36.232547 25955 solver.cpp:191] Iteration 462000, loss = 2.32618
I1029 03:28:36.233157 25955 solver.cpp:206]     Train net output #0: loss = 2.32618 (* 1 = 2.32618 loss)
I1029 03:28:36.233189 25955 solver.cpp:403] Iteration 462000, lr = 0.00055532
I1029 03:32:37.660135 25955 solver.cpp:191] Iteration 463000, loss = 2.60364
I1029 03:32:37.660745 25955 solver.cpp:206]     Train net output #0: loss = 2.60364 (* 1 = 2.60364 loss)
I1029 03:32:37.660789 25955 solver.cpp:403] Iteration 463000, lr = 0.000554439
I1029 03:36:39.234760 25955 solver.cpp:191] Iteration 464000, loss = 2.39639
I1029 03:36:39.235533 25955 solver.cpp:206]     Train net output #0: loss = 2.39639 (* 1 = 2.39639 loss)
I1029 03:36:39.235568 25955 solver.cpp:403] Iteration 464000, lr = 0.000553562
I1029 03:40:41.251523 25955 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_465000.caffemodel
I1029 03:40:46.321473 25955 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_465000.solverstate
I1029 03:40:49.901906 25955 solver.cpp:228] Iteration 465000, loss = 2.58844
I1029 03:40:49.902406 25955 solver.cpp:233] Optimization Done.
I1029 03:40:49.902431 25955 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 04:06:03.239475  8395 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 04:06:03.240247  8395 net.cpp:358] Input 0 -> data
I1029 04:06:03.240280  8395 net.cpp:67] Creating Layer conv1
I1029 04:06:03.240286  8395 net.cpp:394] conv1 <- data
I1029 04:06:03.240293  8395 net.cpp:356] conv1 -> conv1
I1029 04:06:03.240303  8395 net.cpp:96] Setting up conv1
I1029 04:06:03.240689  8395 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 04:06:03.240721  8395 net.cpp:67] Creating Layer pool1
I1029 04:06:03.240728  8395 net.cpp:394] pool1 <- conv1
I1029 04:06:03.240734  8395 net.cpp:356] pool1 -> pool1
I1029 04:06:03.240742  8395 net.cpp:96] Setting up pool1
I1029 04:06:03.240757  8395 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 04:06:03.240767  8395 net.cpp:67] Creating Layer relu1
I1029 04:06:03.240772  8395 net.cpp:394] relu1 <- pool1
I1029 04:06:03.240778  8395 net.cpp:345] relu1 -> pool1 (in-place)
I1029 04:06:03.240784  8395 net.cpp:96] Setting up relu1
I1029 04:06:03.240789  8395 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 04:06:03.240795  8395 net.cpp:67] Creating Layer drop1
I1029 04:06:03.240800  8395 net.cpp:394] drop1 <- pool1
I1029 04:06:03.240806  8395 net.cpp:345] drop1 -> pool1 (in-place)
I1029 04:06:03.240813  8395 net.cpp:96] Setting up drop1
I1029 04:06:03.240819  8395 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 04:06:03.240826  8395 net.cpp:67] Creating Layer conv2
I1029 04:06:03.240831  8395 net.cpp:394] conv2 <- pool1
I1029 04:06:03.240838  8395 net.cpp:356] conv2 -> conv2
I1029 04:06:03.240844  8395 net.cpp:96] Setting up conv2
I1029 04:06:03.241461  8395 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 04:06:03.241480  8395 net.cpp:67] Creating Layer pool2
I1029 04:06:03.241487  8395 net.cpp:394] pool2 <- conv2
I1029 04:06:03.241492  8395 net.cpp:356] pool2 -> pool2
I1029 04:06:03.241500  8395 net.cpp:96] Setting up pool2
I1029 04:06:03.241507  8395 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 04:06:03.241513  8395 net.cpp:67] Creating Layer relu2
I1029 04:06:03.241518  8395 net.cpp:394] relu2 <- pool2
I1029 04:06:03.241524  8395 net.cpp:345] relu2 -> pool2 (in-place)
I1029 04:06:03.241531  8395 net.cpp:96] Setting up relu2
I1029 04:06:03.241535  8395 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 04:06:03.241541  8395 net.cpp:67] Creating Layer drop2
I1029 04:06:03.241546  8395 net.cpp:394] drop2 <- pool2
I1029 04:06:03.241552  8395 net.cpp:345] drop2 -> pool2 (in-place)
I1029 04:06:03.241559  8395 net.cpp:96] Setting up drop2
I1029 04:06:03.241564  8395 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 04:06:03.241572  8395 net.cpp:67] Creating Layer conv3
I1029 04:06:03.241577  8395 net.cpp:394] conv3 <- pool2
I1029 04:06:03.241585  8395 net.cpp:356] conv3 -> conv3
I1029 04:06:03.241591  8395 net.cpp:96] Setting up conv3
I1029 04:06:03.243217  8395 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 04:06:03.243234  8395 net.cpp:67] Creating Layer pool3
I1029 04:06:03.243240  8395 net.cpp:394] pool3 <- conv3
I1029 04:06:03.243248  8395 net.cpp:356] pool3 -> pool3
I1029 04:06:03.243257  8395 net.cpp:96] Setting up pool3
I1029 04:06:03.243263  8395 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 04:06:03.243268  8395 net.cpp:67] Creating Layer relu3
I1029 04:06:03.243273  8395 net.cpp:394] relu3 <- pool3
I1029 04:06:03.243278  8395 net.cpp:345] relu3 -> pool3 (in-place)
I1029 04:06:03.243284  8395 net.cpp:96] Setting up relu3
I1029 04:06:03.243289  8395 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 04:06:03.243298  8395 net.cpp:67] Creating Layer drop3
I1029 04:06:03.243302  8395 net.cpp:394] drop3 <- pool3
I1029 04:06:03.243309  8395 net.cpp:345] drop3 -> pool3 (in-place)
I1029 04:06:03.243314  8395 net.cpp:96] Setting up drop3
I1029 04:06:03.243319  8395 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 04:06:03.243326  8395 net.cpp:67] Creating Layer ip1
I1029 04:06:03.243330  8395 net.cpp:394] ip1 <- pool3
I1029 04:06:03.243338  8395 net.cpp:356] ip1 -> ip1
I1029 04:06:03.243347  8395 net.cpp:96] Setting up ip1
I1029 04:06:03.750175  8395 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 04:06:03.750234  8395 net.cpp:67] Creating Layer relu4
I1029 04:06:03.750241  8395 net.cpp:394] relu4 <- ip1
I1029 04:06:03.750253  8395 net.cpp:345] relu4 -> ip1 (in-place)
I1029 04:06:03.750263  8395 net.cpp:96] Setting up relu4
I1029 04:06:03.750268  8395 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 04:06:03.750275  8395 net.cpp:67] Creating Layer drop4
I1029 04:06:03.750286  8395 net.cpp:394] drop4 <- ip1
I1029 04:06:03.750293  8395 net.cpp:345] drop4 -> ip1 (in-place)
I1029 04:06:03.750299  8395 net.cpp:96] Setting up drop4
I1029 04:06:03.750304  8395 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 04:06:03.750314  8395 net.cpp:67] Creating Layer ip2
I1029 04:06:03.750319  8395 net.cpp:394] ip2 <- ip1
I1029 04:06:03.750324  8395 net.cpp:356] ip2 -> ip2
I1029 04:06:03.750336  8395 net.cpp:96] Setting up ip2
I1029 04:06:03.758468  8395 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 04:06:03.758533  8395 net.cpp:67] Creating Layer prob
I1029 04:06:03.758539  8395 net.cpp:394] prob <- ip2
I1029 04:06:03.758548  8395 net.cpp:356] prob -> prob
I1029 04:06:03.758558  8395 net.cpp:96] Setting up prob
I1029 04:06:03.758566  8395 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 04:06:03.758571  8395 net.cpp:172] prob does not need backward computation.
I1029 04:06:03.758575  8395 net.cpp:172] ip2 does not need backward computation.
I1029 04:06:03.758579  8395 net.cpp:172] drop4 does not need backward computation.
I1029 04:06:03.758582  8395 net.cpp:172] relu4 does not need backward computation.
I1029 04:06:03.758586  8395 net.cpp:172] ip1 does not need backward computation.
I1029 04:06:03.758589  8395 net.cpp:172] drop3 does not need backward computation.
I1029 04:06:03.758594  8395 net.cpp:172] relu3 does not need backward computation.
I1029 04:06:03.758596  8395 net.cpp:172] pool3 does not need backward computation.
I1029 04:06:03.758600  8395 net.cpp:172] conv3 does not need backward computation.
I1029 04:06:03.758604  8395 net.cpp:172] drop2 does not need backward computation.
I1029 04:06:03.758607  8395 net.cpp:172] relu2 does not need backward computation.
I1029 04:06:03.758610  8395 net.cpp:172] pool2 does not need backward computation.
I1029 04:06:03.758615  8395 net.cpp:172] conv2 does not need backward computation.
I1029 04:06:03.758617  8395 net.cpp:172] drop1 does not need backward computation.
I1029 04:06:03.758621  8395 net.cpp:172] relu1 does not need backward computation.
I1029 04:06:03.758625  8395 net.cpp:172] pool1 does not need backward computation.
I1029 04:06:03.758628  8395 net.cpp:172] conv1 does not need backward computation.
I1029 04:06:03.758631  8395 net.cpp:208] This network produces output prob
I1029 04:06:03.758642  8395 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 04:06:03.758651  8395 net.cpp:219] Network initialization done.
I1029 04:06:03.758653  8395 net.cpp:220] Memory required for data: 1837200
I1029 04:40:05.118510 17177 convert_imageset.cpp:70] Shuffling data
I1029 04:40:05.881135 17177 convert_imageset.cpp:73] A total of 60000 images.
I1029 04:40:05.881209 17177 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 04:40:08.493590 17177 convert_imageset.cpp:177] Processed 1000 files.
E1029 04:40:11.059311 17177 convert_imageset.cpp:177] Processed 2000 files.
E1029 04:40:13.498961 17177 convert_imageset.cpp:177] Processed 3000 files.
E1029 04:40:15.786260 17177 convert_imageset.cpp:177] Processed 4000 files.
E1029 04:40:18.052242 17177 convert_imageset.cpp:177] Processed 5000 files.
E1029 04:40:20.274142 17177 convert_imageset.cpp:177] Processed 6000 files.
E1029 04:40:22.525027 17177 convert_imageset.cpp:177] Processed 7000 files.
E1029 04:40:24.697577 17177 convert_imageset.cpp:177] Processed 8000 files.
E1029 04:40:26.822167 17177 convert_imageset.cpp:177] Processed 9000 files.
E1029 04:40:28.915992 17177 convert_imageset.cpp:177] Processed 10000 files.
E1029 04:40:31.139673 17177 convert_imageset.cpp:177] Processed 11000 files.
E1029 04:40:33.515244 17177 convert_imageset.cpp:177] Processed 12000 files.
E1029 04:40:35.480629 17177 convert_imageset.cpp:177] Processed 13000 files.
E1029 04:40:37.458968 17177 convert_imageset.cpp:177] Processed 14000 files.
E1029 04:40:39.439709 17177 convert_imageset.cpp:177] Processed 15000 files.
E1029 04:40:41.328901 17177 convert_imageset.cpp:177] Processed 16000 files.
E1029 04:40:43.219017 17177 convert_imageset.cpp:177] Processed 17000 files.
E1029 04:40:45.107375 17177 convert_imageset.cpp:177] Processed 18000 files.
E1029 04:40:46.968557 17177 convert_imageset.cpp:177] Processed 19000 files.
E1029 04:40:48.814990 17177 convert_imageset.cpp:177] Processed 20000 files.
E1029 04:40:50.621242 17177 convert_imageset.cpp:177] Processed 21000 files.
E1029 04:40:52.400374 17177 convert_imageset.cpp:177] Processed 22000 files.
E1029 04:40:54.178107 17177 convert_imageset.cpp:177] Processed 23000 files.
E1029 04:40:55.945780 17177 convert_imageset.cpp:177] Processed 24000 files.
E1029 04:40:57.713078 17177 convert_imageset.cpp:177] Processed 25000 files.
E1029 04:40:59.447736 17177 convert_imageset.cpp:177] Processed 26000 files.
E1029 04:41:01.259898 17177 convert_imageset.cpp:177] Processed 27000 files.
E1029 04:41:02.968214 17177 convert_imageset.cpp:177] Processed 28000 files.
E1029 04:41:05.545809 17177 convert_imageset.cpp:177] Processed 29000 files.
E1029 04:41:07.291244 17177 convert_imageset.cpp:177] Processed 30000 files.
E1029 04:41:09.042625 17177 convert_imageset.cpp:177] Processed 31000 files.
E1029 04:41:10.746388 17177 convert_imageset.cpp:177] Processed 32000 files.
E1029 04:41:12.444679 17177 convert_imageset.cpp:177] Processed 33000 files.
E1029 04:41:14.113085 17177 convert_imageset.cpp:177] Processed 34000 files.
E1029 04:41:15.763164 17177 convert_imageset.cpp:177] Processed 35000 files.
E1029 04:41:17.435672 17177 convert_imageset.cpp:177] Processed 36000 files.
E1029 04:41:19.090692 17177 convert_imageset.cpp:177] Processed 37000 files.
E1029 04:41:20.739405 17177 convert_imageset.cpp:177] Processed 38000 files.
E1029 04:41:22.415639 17177 convert_imageset.cpp:177] Processed 39000 files.
E1029 04:41:24.102192 17177 convert_imageset.cpp:177] Processed 40000 files.
E1029 04:41:25.816573 17177 convert_imageset.cpp:177] Processed 41000 files.
E1029 04:41:27.445735 17177 convert_imageset.cpp:177] Processed 42000 files.
E1029 04:41:29.090332 17177 convert_imageset.cpp:177] Processed 43000 files.
E1029 04:41:30.700306 17177 convert_imageset.cpp:177] Processed 44000 files.
E1029 04:41:32.347921 17177 convert_imageset.cpp:177] Processed 45000 files.
E1029 04:41:34.001910 17177 convert_imageset.cpp:177] Processed 46000 files.
E1029 04:41:35.613031 17177 convert_imageset.cpp:177] Processed 47000 files.
E1029 04:41:37.204351 17177 convert_imageset.cpp:177] Processed 48000 files.
E1029 04:41:38.897462 17177 convert_imageset.cpp:177] Processed 49000 files.
E1029 04:41:40.636698 17177 convert_imageset.cpp:177] Processed 50000 files.
E1029 04:41:42.271813 17177 convert_imageset.cpp:177] Processed 51000 files.
E1029 04:41:43.889446 17177 convert_imageset.cpp:177] Processed 52000 files.
E1029 04:41:45.490322 17177 convert_imageset.cpp:177] Processed 53000 files.
E1029 04:41:47.090397 17177 convert_imageset.cpp:177] Processed 54000 files.
E1029 04:41:48.709710 17177 convert_imageset.cpp:177] Processed 55000 files.
E1029 04:41:50.282238 17177 convert_imageset.cpp:177] Processed 56000 files.
E1029 04:41:51.854456 17177 convert_imageset.cpp:177] Processed 57000 files.
E1029 04:41:53.496873 17177 convert_imageset.cpp:177] Processed 58000 files.
E1029 04:41:55.116569 17177 convert_imageset.cpp:177] Processed 59000 files.
E1029 04:41:56.723546 17177 convert_imageset.cpp:177] Processed 60000 files.
I1029 04:41:56.938206 17275 caffe.cpp:99] Use GPU with device ID 0
I1029 04:41:57.287971 17275 caffe.cpp:107] Starting Optimization
I1029 04:41:57.288099 17275 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 470000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 04:41:57.288125 17275 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 04:41:57.305349 17275 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 04:41:57.305563 17275 net.cpp:67] Creating Layer mnist
I1029 04:41:57.305590 17275 net.cpp:356] mnist -> data
I1029 04:41:57.305626 17275 net.cpp:356] mnist -> label
I1029 04:41:57.305658 17275 net.cpp:96] Setting up mnist
I1029 04:41:57.313338 17275 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 04:41:57.313467 17275 data_layer.cpp:128] output data size: 64,1,50,180
I1029 04:41:57.314409 17275 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 04:41:57.314450 17275 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 04:41:57.314482 17275 net.cpp:67] Creating Layer conv1
I1029 04:41:57.314497 17275 net.cpp:394] conv1 <- data
I1029 04:41:57.314532 17275 net.cpp:356] conv1 -> conv1
I1029 04:41:57.314560 17275 net.cpp:96] Setting up conv1
I1029 04:41:57.315501 17275 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 04:41:57.315567 17275 net.cpp:67] Creating Layer pool1
I1029 04:41:57.315582 17275 net.cpp:394] pool1 <- conv1
I1029 04:41:57.315605 17275 net.cpp:356] pool1 -> pool1
I1029 04:41:57.315634 17275 net.cpp:96] Setting up pool1
I1029 04:41:57.315667 17275 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 04:41:57.315686 17275 net.cpp:67] Creating Layer relu1
I1029 04:41:57.315701 17275 net.cpp:394] relu1 <- pool1
I1029 04:41:57.315716 17275 net.cpp:345] relu1 -> pool1 (in-place)
I1029 04:41:57.315734 17275 net.cpp:96] Setting up relu1
I1029 04:41:57.315747 17275 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 04:41:57.315765 17275 net.cpp:67] Creating Layer drop1
I1029 04:41:57.315778 17275 net.cpp:394] drop1 <- pool1
I1029 04:41:57.315803 17275 net.cpp:345] drop1 -> pool1 (in-place)
I1029 04:41:57.315821 17275 net.cpp:96] Setting up drop1
I1029 04:41:57.315835 17275 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 04:41:57.315855 17275 net.cpp:67] Creating Layer conv2
I1029 04:41:57.315867 17275 net.cpp:394] conv2 <- pool1
I1029 04:41:57.315889 17275 net.cpp:356] conv2 -> conv2
I1029 04:41:57.315912 17275 net.cpp:96] Setting up conv2
I1029 04:41:57.317476 17275 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 04:41:57.317519 17275 net.cpp:67] Creating Layer pool2
I1029 04:41:57.317535 17275 net.cpp:394] pool2 <- conv2
I1029 04:41:57.317553 17275 net.cpp:356] pool2 -> pool2
I1029 04:41:57.317571 17275 net.cpp:96] Setting up pool2
I1029 04:41:57.317587 17275 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 04:41:57.317603 17275 net.cpp:67] Creating Layer relu2
I1029 04:41:57.317615 17275 net.cpp:394] relu2 <- pool2
I1029 04:41:57.317646 17275 net.cpp:345] relu2 -> pool2 (in-place)
I1029 04:41:57.317664 17275 net.cpp:96] Setting up relu2
I1029 04:41:57.317677 17275 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 04:41:57.317697 17275 net.cpp:67] Creating Layer drop2
I1029 04:41:57.317709 17275 net.cpp:394] drop2 <- pool2
I1029 04:41:57.317734 17275 net.cpp:345] drop2 -> pool2 (in-place)
I1029 04:41:57.317751 17275 net.cpp:96] Setting up drop2
I1029 04:41:57.317765 17275 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 04:41:57.317785 17275 net.cpp:67] Creating Layer conv3
I1029 04:41:57.317797 17275 net.cpp:394] conv3 <- pool2
I1029 04:41:57.317816 17275 net.cpp:356] conv3 -> conv3
I1029 04:41:57.317834 17275 net.cpp:96] Setting up conv3
I1029 04:41:57.321880 17275 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 04:41:57.321930 17275 net.cpp:67] Creating Layer pool3
I1029 04:41:57.321944 17275 net.cpp:394] pool3 <- conv3
I1029 04:41:57.321970 17275 net.cpp:356] pool3 -> pool3
I1029 04:41:57.321990 17275 net.cpp:96] Setting up pool3
I1029 04:41:57.322006 17275 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 04:41:57.322023 17275 net.cpp:67] Creating Layer relu3
I1029 04:41:57.322036 17275 net.cpp:394] relu3 <- pool3
I1029 04:41:57.322055 17275 net.cpp:345] relu3 -> pool3 (in-place)
I1029 04:41:57.322073 17275 net.cpp:96] Setting up relu3
I1029 04:41:57.322085 17275 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 04:41:57.322103 17275 net.cpp:67] Creating Layer drop3
I1029 04:41:57.322114 17275 net.cpp:394] drop3 <- pool3
I1029 04:41:57.322130 17275 net.cpp:345] drop3 -> pool3 (in-place)
I1029 04:41:57.322147 17275 net.cpp:96] Setting up drop3
I1029 04:41:57.322161 17275 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 04:41:57.322180 17275 net.cpp:67] Creating Layer ip1
I1029 04:41:57.322192 17275 net.cpp:394] ip1 <- pool3
I1029 04:41:57.322216 17275 net.cpp:356] ip1 -> ip1
I1029 04:41:57.322281 17275 net.cpp:96] Setting up ip1
I1029 04:41:57.796169 17275 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 04:41:57.796231 17275 net.cpp:67] Creating Layer relu4
I1029 04:41:57.796239 17275 net.cpp:394] relu4 <- ip1
I1029 04:41:57.796249 17275 net.cpp:345] relu4 -> ip1 (in-place)
I1029 04:41:57.796258 17275 net.cpp:96] Setting up relu4
I1029 04:41:57.796263 17275 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 04:41:57.796270 17275 net.cpp:67] Creating Layer drop4
I1029 04:41:57.796274 17275 net.cpp:394] drop4 <- ip1
I1029 04:41:57.796284 17275 net.cpp:345] drop4 -> ip1 (in-place)
I1029 04:41:57.796291 17275 net.cpp:96] Setting up drop4
I1029 04:41:57.796303 17275 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 04:41:57.796315 17275 net.cpp:67] Creating Layer ip2
I1029 04:41:57.796320 17275 net.cpp:394] ip2 <- ip1
I1029 04:41:57.796329 17275 net.cpp:356] ip2 -> ip2
I1029 04:41:57.796337 17275 net.cpp:96] Setting up ip2
I1029 04:41:57.806990 17275 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 04:41:57.807047 17275 net.cpp:67] Creating Layer loss
I1029 04:41:57.807055 17275 net.cpp:394] loss <- ip2
I1029 04:41:57.807063 17275 net.cpp:394] loss <- label
I1029 04:41:57.807070 17275 net.cpp:356] loss -> loss
I1029 04:41:57.807080 17275 net.cpp:96] Setting up loss
I1029 04:41:57.807092 17275 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 04:41:57.807097 17275 net.cpp:109]     with loss weight 1
I1029 04:41:57.807138 17275 net.cpp:170] loss needs backward computation.
I1029 04:41:57.807143 17275 net.cpp:170] ip2 needs backward computation.
I1029 04:41:57.807148 17275 net.cpp:170] drop4 needs backward computation.
I1029 04:41:57.807152 17275 net.cpp:170] relu4 needs backward computation.
I1029 04:41:57.807157 17275 net.cpp:170] ip1 needs backward computation.
I1029 04:41:57.807163 17275 net.cpp:170] drop3 needs backward computation.
I1029 04:41:57.807167 17275 net.cpp:170] relu3 needs backward computation.
I1029 04:41:57.807170 17275 net.cpp:170] pool3 needs backward computation.
I1029 04:41:57.807175 17275 net.cpp:170] conv3 needs backward computation.
I1029 04:41:57.807180 17275 net.cpp:170] drop2 needs backward computation.
I1029 04:41:57.807184 17275 net.cpp:170] relu2 needs backward computation.
I1029 04:41:57.807189 17275 net.cpp:170] pool2 needs backward computation.
I1029 04:41:57.807194 17275 net.cpp:170] conv2 needs backward computation.
I1029 04:41:57.807199 17275 net.cpp:170] drop1 needs backward computation.
I1029 04:41:57.807204 17275 net.cpp:170] relu1 needs backward computation.
I1029 04:41:57.807207 17275 net.cpp:170] pool1 needs backward computation.
I1029 04:41:57.807212 17275 net.cpp:170] conv1 needs backward computation.
I1029 04:41:57.807217 17275 net.cpp:172] mnist does not need backward computation.
I1029 04:41:57.807221 17275 net.cpp:208] This network produces output loss
I1029 04:41:57.807234 17275 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 04:41:57.807240 17275 net.cpp:219] Network initialization done.
I1029 04:41:57.807245 17275 net.cpp:220] Memory required for data: 119788292
I1029 04:41:57.807306 17275 solver.cpp:41] Solver scaffolding done.
I1029 04:41:57.807312 17275 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_465000.solverstate
I1029 04:41:57.807317 17275 solver.cpp:160] Solving Captcha
I1029 04:41:57.807335 17275 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_465000.solverstate
I1029 04:42:03.689466 17275 solver.cpp:502] SGDSolver: restoring history
I1029 04:42:04.561144 17275 solver.cpp:191] Iteration 465000, loss = 2.49077
I1029 04:42:04.561199 17275 solver.cpp:206]     Train net output #0: loss = 2.49077 (* 1 = 2.49077 loss)
I1029 04:42:04.561214 17275 solver.cpp:403] Iteration 465000, lr = 0.000552688
I1029 04:46:06.862928 17275 solver.cpp:191] Iteration 466000, loss = 2.37148
I1029 04:46:06.863723 17275 solver.cpp:206]     Train net output #0: loss = 2.37148 (* 1 = 2.37148 loss)
I1029 04:46:06.863759 17275 solver.cpp:403] Iteration 466000, lr = 0.000551817
I1029 04:50:08.529538 17275 solver.cpp:191] Iteration 467000, loss = 2.33906
I1029 04:50:08.530441 17275 solver.cpp:206]     Train net output #0: loss = 2.33906 (* 1 = 2.33906 loss)
I1029 04:50:08.530478 17275 solver.cpp:403] Iteration 467000, lr = 0.000550949
I1029 04:54:10.069036 17275 solver.cpp:191] Iteration 468000, loss = 2.37874
I1029 04:54:10.069629 17275 solver.cpp:206]     Train net output #0: loss = 2.37874 (* 1 = 2.37874 loss)
I1029 04:54:10.069664 17275 solver.cpp:403] Iteration 468000, lr = 0.000550084
I1029 04:58:11.663982 17275 solver.cpp:191] Iteration 469000, loss = 2.63799
I1029 04:58:11.664652 17275 solver.cpp:206]     Train net output #0: loss = 2.63799 (* 1 = 2.63799 loss)
I1029 04:58:11.664685 17275 solver.cpp:403] Iteration 469000, lr = 0.000549223
I1029 05:02:13.966866 17275 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_470000.caffemodel
I1029 05:02:18.647900 17275 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_470000.solverstate
I1029 05:02:22.777534 17275 solver.cpp:228] Iteration 470000, loss = 2.33428
I1029 05:02:22.778251 17275 solver.cpp:233] Optimization Done.
I1029 05:02:22.778275 17275 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 05:24:10.298169 30782 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 05:24:10.298281 30782 net.cpp:358] Input 0 -> data
I1029 05:24:10.298315 30782 net.cpp:67] Creating Layer conv1
I1029 05:24:10.298321 30782 net.cpp:394] conv1 <- data
I1029 05:24:10.298329 30782 net.cpp:356] conv1 -> conv1
I1029 05:24:10.298341 30782 net.cpp:96] Setting up conv1
I1029 05:24:10.298728 30782 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 05:24:10.298750 30782 net.cpp:67] Creating Layer pool1
I1029 05:24:10.298756 30782 net.cpp:394] pool1 <- conv1
I1029 05:24:10.298763 30782 net.cpp:356] pool1 -> pool1
I1029 05:24:10.298777 30782 net.cpp:96] Setting up pool1
I1029 05:24:10.298794 30782 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 05:24:10.298801 30782 net.cpp:67] Creating Layer relu1
I1029 05:24:10.298806 30782 net.cpp:394] relu1 <- pool1
I1029 05:24:10.298813 30782 net.cpp:345] relu1 -> pool1 (in-place)
I1029 05:24:10.298820 30782 net.cpp:96] Setting up relu1
I1029 05:24:10.298825 30782 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 05:24:10.298835 30782 net.cpp:67] Creating Layer drop1
I1029 05:24:10.298841 30782 net.cpp:394] drop1 <- pool1
I1029 05:24:10.298847 30782 net.cpp:345] drop1 -> pool1 (in-place)
I1029 05:24:10.298856 30782 net.cpp:96] Setting up drop1
I1029 05:24:10.298861 30782 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 05:24:10.298869 30782 net.cpp:67] Creating Layer conv2
I1029 05:24:10.298874 30782 net.cpp:394] conv2 <- pool1
I1029 05:24:10.298882 30782 net.cpp:356] conv2 -> conv2
I1029 05:24:10.298889 30782 net.cpp:96] Setting up conv2
I1029 05:24:10.299569 30782 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 05:24:10.299587 30782 net.cpp:67] Creating Layer pool2
I1029 05:24:10.299593 30782 net.cpp:394] pool2 <- conv2
I1029 05:24:10.299599 30782 net.cpp:356] pool2 -> pool2
I1029 05:24:10.299608 30782 net.cpp:96] Setting up pool2
I1029 05:24:10.299615 30782 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 05:24:10.299624 30782 net.cpp:67] Creating Layer relu2
I1029 05:24:10.299630 30782 net.cpp:394] relu2 <- pool2
I1029 05:24:10.299636 30782 net.cpp:345] relu2 -> pool2 (in-place)
I1029 05:24:10.299643 30782 net.cpp:96] Setting up relu2
I1029 05:24:10.299649 30782 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 05:24:10.299654 30782 net.cpp:67] Creating Layer drop2
I1029 05:24:10.299659 30782 net.cpp:394] drop2 <- pool2
I1029 05:24:10.299667 30782 net.cpp:345] drop2 -> pool2 (in-place)
I1029 05:24:10.299675 30782 net.cpp:96] Setting up drop2
I1029 05:24:10.299680 30782 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 05:24:10.299688 30782 net.cpp:67] Creating Layer conv3
I1029 05:24:10.299693 30782 net.cpp:394] conv3 <- pool2
I1029 05:24:10.299703 30782 net.cpp:356] conv3 -> conv3
I1029 05:24:10.299711 30782 net.cpp:96] Setting up conv3
I1029 05:24:10.301519 30782 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 05:24:10.301540 30782 net.cpp:67] Creating Layer pool3
I1029 05:24:10.301548 30782 net.cpp:394] pool3 <- conv3
I1029 05:24:10.301554 30782 net.cpp:356] pool3 -> pool3
I1029 05:24:10.301563 30782 net.cpp:96] Setting up pool3
I1029 05:24:10.301568 30782 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 05:24:10.301578 30782 net.cpp:67] Creating Layer relu3
I1029 05:24:10.301583 30782 net.cpp:394] relu3 <- pool3
I1029 05:24:10.301589 30782 net.cpp:345] relu3 -> pool3 (in-place)
I1029 05:24:10.301595 30782 net.cpp:96] Setting up relu3
I1029 05:24:10.301600 30782 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 05:24:10.301607 30782 net.cpp:67] Creating Layer drop3
I1029 05:24:10.301612 30782 net.cpp:394] drop3 <- pool3
I1029 05:24:10.301619 30782 net.cpp:345] drop3 -> pool3 (in-place)
I1029 05:24:10.301625 30782 net.cpp:96] Setting up drop3
I1029 05:24:10.301630 30782 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 05:24:10.301640 30782 net.cpp:67] Creating Layer ip1
I1029 05:24:10.301645 30782 net.cpp:394] ip1 <- pool3
I1029 05:24:10.301652 30782 net.cpp:356] ip1 -> ip1
I1029 05:24:10.301661 30782 net.cpp:96] Setting up ip1
I1029 05:24:10.807677 30782 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 05:24:10.807740 30782 net.cpp:67] Creating Layer relu4
I1029 05:24:10.807749 30782 net.cpp:394] relu4 <- ip1
I1029 05:24:10.807759 30782 net.cpp:345] relu4 -> ip1 (in-place)
I1029 05:24:10.807766 30782 net.cpp:96] Setting up relu4
I1029 05:24:10.807771 30782 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 05:24:10.807780 30782 net.cpp:67] Creating Layer drop4
I1029 05:24:10.807783 30782 net.cpp:394] drop4 <- ip1
I1029 05:24:10.807791 30782 net.cpp:345] drop4 -> ip1 (in-place)
I1029 05:24:10.807797 30782 net.cpp:96] Setting up drop4
I1029 05:24:10.807803 30782 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 05:24:10.807818 30782 net.cpp:67] Creating Layer ip2
I1029 05:24:10.807822 30782 net.cpp:394] ip2 <- ip1
I1029 05:24:10.807832 30782 net.cpp:356] ip2 -> ip2
I1029 05:24:10.807844 30782 net.cpp:96] Setting up ip2
I1029 05:24:10.818822 30782 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 05:24:10.818894 30782 net.cpp:67] Creating Layer prob
I1029 05:24:10.818902 30782 net.cpp:394] prob <- ip2
I1029 05:24:10.818910 30782 net.cpp:356] prob -> prob
I1029 05:24:10.818920 30782 net.cpp:96] Setting up prob
I1029 05:24:10.818927 30782 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 05:24:10.818931 30782 net.cpp:172] prob does not need backward computation.
I1029 05:24:10.818935 30782 net.cpp:172] ip2 does not need backward computation.
I1029 05:24:10.818938 30782 net.cpp:172] drop4 does not need backward computation.
I1029 05:24:10.818943 30782 net.cpp:172] relu4 does not need backward computation.
I1029 05:24:10.818946 30782 net.cpp:172] ip1 does not need backward computation.
I1029 05:24:10.818949 30782 net.cpp:172] drop3 does not need backward computation.
I1029 05:24:10.818953 30782 net.cpp:172] relu3 does not need backward computation.
I1029 05:24:10.818956 30782 net.cpp:172] pool3 does not need backward computation.
I1029 05:24:10.818960 30782 net.cpp:172] conv3 does not need backward computation.
I1029 05:24:10.818964 30782 net.cpp:172] drop2 does not need backward computation.
I1029 05:24:10.818967 30782 net.cpp:172] relu2 does not need backward computation.
I1029 05:24:10.818971 30782 net.cpp:172] pool2 does not need backward computation.
I1029 05:24:10.818975 30782 net.cpp:172] conv2 does not need backward computation.
I1029 05:24:10.818979 30782 net.cpp:172] drop1 does not need backward computation.
I1029 05:24:10.818982 30782 net.cpp:172] relu1 does not need backward computation.
I1029 05:24:10.818986 30782 net.cpp:172] pool1 does not need backward computation.
I1029 05:24:10.818989 30782 net.cpp:172] conv1 does not need backward computation.
I1029 05:24:10.818994 30782 net.cpp:208] This network produces output prob
I1029 05:24:10.819006 30782 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 05:24:10.819013 30782 net.cpp:219] Network initialization done.
I1029 05:24:10.819017 30782 net.cpp:220] Memory required for data: 1837200
I1029 05:57:40.051271  6624 convert_imageset.cpp:70] Shuffling data
I1029 05:57:40.638782  6624 convert_imageset.cpp:73] A total of 60000 images.
I1029 05:57:40.638859  6624 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 05:57:43.097218  6624 convert_imageset.cpp:177] Processed 1000 files.
E1029 05:57:45.260697  6624 convert_imageset.cpp:177] Processed 2000 files.
E1029 05:57:47.675592  6624 convert_imageset.cpp:177] Processed 3000 files.
E1029 05:57:49.844156  6624 convert_imageset.cpp:177] Processed 4000 files.
E1029 05:57:52.186089  6624 convert_imageset.cpp:177] Processed 5000 files.
E1029 05:57:54.683676  6624 convert_imageset.cpp:177] Processed 6000 files.
E1029 05:57:56.700556  6624 convert_imageset.cpp:177] Processed 7000 files.
E1029 05:57:58.780887  6624 convert_imageset.cpp:177] Processed 8000 files.
E1029 05:58:00.856266  6624 convert_imageset.cpp:177] Processed 9000 files.
E1029 05:58:03.028228  6624 convert_imageset.cpp:177] Processed 10000 files.
E1029 05:58:04.931762  6624 convert_imageset.cpp:177] Processed 11000 files.
E1029 05:58:06.830245  6624 convert_imageset.cpp:177] Processed 12000 files.
E1029 05:58:08.694103  6624 convert_imageset.cpp:177] Processed 13000 files.
E1029 05:58:10.592433  6624 convert_imageset.cpp:177] Processed 14000 files.
E1029 05:58:12.508018  6624 convert_imageset.cpp:177] Processed 15000 files.
E1029 05:58:14.429363  6624 convert_imageset.cpp:177] Processed 16000 files.
E1029 05:58:16.417335  6624 convert_imageset.cpp:177] Processed 17000 files.
E1029 05:58:18.219686  6624 convert_imageset.cpp:177] Processed 18000 files.
E1029 05:58:19.917395  6624 convert_imageset.cpp:177] Processed 19000 files.
E1029 05:58:21.692127  6624 convert_imageset.cpp:177] Processed 20000 files.
E1029 05:58:23.415220  6624 convert_imageset.cpp:177] Processed 21000 files.
E1029 05:58:25.170152  6624 convert_imageset.cpp:177] Processed 22000 files.
E1029 05:58:26.975991  6624 convert_imageset.cpp:177] Processed 23000 files.
E1029 05:58:28.689585  6624 convert_imageset.cpp:177] Processed 24000 files.
E1029 05:58:30.403760  6624 convert_imageset.cpp:177] Processed 25000 files.
E1029 05:58:32.089710  6624 convert_imageset.cpp:177] Processed 26000 files.
E1029 05:58:33.785595  6624 convert_imageset.cpp:177] Processed 27000 files.
E1029 05:58:35.477432  6624 convert_imageset.cpp:177] Processed 28000 files.
E1029 05:58:37.275243  6624 convert_imageset.cpp:177] Processed 29000 files.
E1029 05:58:38.953742  6624 convert_imageset.cpp:177] Processed 30000 files.
E1029 05:58:40.621754  6624 convert_imageset.cpp:177] Processed 31000 files.
E1029 05:58:42.403017  6624 convert_imageset.cpp:177] Processed 32000 files.
E1029 05:58:44.115530  6624 convert_imageset.cpp:177] Processed 33000 files.
E1029 05:58:45.815268  6624 convert_imageset.cpp:177] Processed 34000 files.
E1029 05:58:47.482872  6624 convert_imageset.cpp:177] Processed 35000 files.
E1029 05:58:49.141893  6624 convert_imageset.cpp:177] Processed 36000 files.
E1029 05:58:50.790597  6624 convert_imageset.cpp:177] Processed 37000 files.
E1029 05:58:52.432471  6624 convert_imageset.cpp:177] Processed 38000 files.
E1029 05:58:54.074257  6624 convert_imageset.cpp:177] Processed 39000 files.
E1029 05:58:55.751014  6624 convert_imageset.cpp:177] Processed 40000 files.
E1029 05:58:57.367117  6624 convert_imageset.cpp:177] Processed 41000 files.
E1029 05:58:59.003756  6624 convert_imageset.cpp:177] Processed 42000 files.
E1029 05:59:00.637063  6624 convert_imageset.cpp:177] Processed 43000 files.
E1029 05:59:02.257902  6624 convert_imageset.cpp:177] Processed 44000 files.
E1029 05:59:03.872864  6624 convert_imageset.cpp:177] Processed 45000 files.
E1029 05:59:05.569448  6624 convert_imageset.cpp:177] Processed 46000 files.
E1029 05:59:07.187700  6624 convert_imageset.cpp:177] Processed 47000 files.
E1029 05:59:08.880077  6624 convert_imageset.cpp:177] Processed 48000 files.
E1029 05:59:10.529849  6624 convert_imageset.cpp:177] Processed 49000 files.
E1029 05:59:12.127400  6624 convert_imageset.cpp:177] Processed 50000 files.
E1029 05:59:13.856887  6624 convert_imageset.cpp:177] Processed 51000 files.
E1029 05:59:15.720566  6624 convert_imageset.cpp:177] Processed 52000 files.
E1029 05:59:17.412508  6624 convert_imageset.cpp:177] Processed 53000 files.
E1029 05:59:19.006335  6624 convert_imageset.cpp:177] Processed 54000 files.
E1029 05:59:20.630509  6624 convert_imageset.cpp:177] Processed 55000 files.
E1029 05:59:22.208628  6624 convert_imageset.cpp:177] Processed 56000 files.
E1029 05:59:23.786444  6624 convert_imageset.cpp:177] Processed 57000 files.
E1029 05:59:25.385869  6624 convert_imageset.cpp:177] Processed 58000 files.
E1029 05:59:26.982295  6624 convert_imageset.cpp:177] Processed 59000 files.
E1029 05:59:28.577563  6624 convert_imageset.cpp:177] Processed 60000 files.
I1029 05:59:28.743695  6786 caffe.cpp:99] Use GPU with device ID 0
I1029 05:59:29.082356  6786 caffe.cpp:107] Starting Optimization
I1029 05:59:29.082463  6786 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 475000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 05:59:29.082487  6786 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 05:59:29.085651  6786 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 05:59:29.085870  6786 net.cpp:67] Creating Layer mnist
I1029 05:59:29.085896  6786 net.cpp:356] mnist -> data
I1029 05:59:29.085932  6786 net.cpp:356] mnist -> label
I1029 05:59:29.085963  6786 net.cpp:96] Setting up mnist
I1029 05:59:29.094398  6786 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 05:59:29.094533  6786 data_layer.cpp:128] output data size: 64,1,50,180
I1029 05:59:29.096248  6786 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 05:59:29.096284  6786 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 05:59:29.096312  6786 net.cpp:67] Creating Layer conv1
I1029 05:59:29.096331  6786 net.cpp:394] conv1 <- data
I1029 05:59:29.096365  6786 net.cpp:356] conv1 -> conv1
I1029 05:59:29.096391  6786 net.cpp:96] Setting up conv1
I1029 05:59:29.097337  6786 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 05:59:29.097405  6786 net.cpp:67] Creating Layer pool1
I1029 05:59:29.097422  6786 net.cpp:394] pool1 <- conv1
I1029 05:59:29.097440  6786 net.cpp:356] pool1 -> pool1
I1029 05:59:29.097458  6786 net.cpp:96] Setting up pool1
I1029 05:59:29.097491  6786 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 05:59:29.097513  6786 net.cpp:67] Creating Layer relu1
I1029 05:59:29.097534  6786 net.cpp:394] relu1 <- pool1
I1029 05:59:29.097551  6786 net.cpp:345] relu1 -> pool1 (in-place)
I1029 05:59:29.097568  6786 net.cpp:96] Setting up relu1
I1029 05:59:29.097581  6786 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 05:59:29.097599  6786 net.cpp:67] Creating Layer drop1
I1029 05:59:29.097611  6786 net.cpp:394] drop1 <- pool1
I1029 05:59:29.097627  6786 net.cpp:345] drop1 -> pool1 (in-place)
I1029 05:59:29.097645  6786 net.cpp:96] Setting up drop1
I1029 05:59:29.097658  6786 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 05:59:29.097681  6786 net.cpp:67] Creating Layer conv2
I1029 05:59:29.097694  6786 net.cpp:394] conv2 <- pool1
I1029 05:59:29.097712  6786 net.cpp:356] conv2 -> conv2
I1029 05:59:29.097736  6786 net.cpp:96] Setting up conv2
I1029 05:59:29.099273  6786 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 05:59:29.099316  6786 net.cpp:67] Creating Layer pool2
I1029 05:59:29.099331  6786 net.cpp:394] pool2 <- conv2
I1029 05:59:29.099349  6786 net.cpp:356] pool2 -> pool2
I1029 05:59:29.099367  6786 net.cpp:96] Setting up pool2
I1029 05:59:29.099383  6786 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 05:59:29.099400  6786 net.cpp:67] Creating Layer relu2
I1029 05:59:29.099411  6786 net.cpp:394] relu2 <- pool2
I1029 05:59:29.099426  6786 net.cpp:345] relu2 -> pool2 (in-place)
I1029 05:59:29.099442  6786 net.cpp:96] Setting up relu2
I1029 05:59:29.099454  6786 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 05:59:29.099478  6786 net.cpp:67] Creating Layer drop2
I1029 05:59:29.099491  6786 net.cpp:394] drop2 <- pool2
I1029 05:59:29.099508  6786 net.cpp:345] drop2 -> pool2 (in-place)
I1029 05:59:29.099524  6786 net.cpp:96] Setting up drop2
I1029 05:59:29.099537  6786 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 05:59:29.099560  6786 net.cpp:67] Creating Layer conv3
I1029 05:59:29.099575  6786 net.cpp:394] conv3 <- pool2
I1029 05:59:29.099591  6786 net.cpp:356] conv3 -> conv3
I1029 05:59:29.099611  6786 net.cpp:96] Setting up conv3
I1029 05:59:29.101657  6786 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 05:59:29.101683  6786 net.cpp:67] Creating Layer pool3
I1029 05:59:29.101688  6786 net.cpp:394] pool3 <- conv3
I1029 05:59:29.101696  6786 net.cpp:356] pool3 -> pool3
I1029 05:59:29.101703  6786 net.cpp:96] Setting up pool3
I1029 05:59:29.101709  6786 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 05:59:29.101717  6786 net.cpp:67] Creating Layer relu3
I1029 05:59:29.101722  6786 net.cpp:394] relu3 <- pool3
I1029 05:59:29.101727  6786 net.cpp:345] relu3 -> pool3 (in-place)
I1029 05:59:29.101733  6786 net.cpp:96] Setting up relu3
I1029 05:59:29.101738  6786 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 05:59:29.101745  6786 net.cpp:67] Creating Layer drop3
I1029 05:59:29.101750  6786 net.cpp:394] drop3 <- pool3
I1029 05:59:29.101757  6786 net.cpp:345] drop3 -> pool3 (in-place)
I1029 05:59:29.101763  6786 net.cpp:96] Setting up drop3
I1029 05:59:29.101768  6786 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 05:59:29.101774  6786 net.cpp:67] Creating Layer ip1
I1029 05:59:29.101778  6786 net.cpp:394] ip1 <- pool3
I1029 05:59:29.101784  6786 net.cpp:356] ip1 -> ip1
I1029 05:59:29.101820  6786 net.cpp:96] Setting up ip1
I1029 05:59:29.616981  6786 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 05:59:29.617039  6786 net.cpp:67] Creating Layer relu4
I1029 05:59:29.617046  6786 net.cpp:394] relu4 <- ip1
I1029 05:59:29.617058  6786 net.cpp:345] relu4 -> ip1 (in-place)
I1029 05:59:29.617068  6786 net.cpp:96] Setting up relu4
I1029 05:59:29.617072  6786 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 05:59:29.617079  6786 net.cpp:67] Creating Layer drop4
I1029 05:59:29.617084  6786 net.cpp:394] drop4 <- ip1
I1029 05:59:29.617089  6786 net.cpp:345] drop4 -> ip1 (in-place)
I1029 05:59:29.617095  6786 net.cpp:96] Setting up drop4
I1029 05:59:29.617100  6786 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 05:59:29.617113  6786 net.cpp:67] Creating Layer ip2
I1029 05:59:29.617118  6786 net.cpp:394] ip2 <- ip1
I1029 05:59:29.617131  6786 net.cpp:356] ip2 -> ip2
I1029 05:59:29.617141  6786 net.cpp:96] Setting up ip2
I1029 05:59:29.627605  6786 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 05:59:29.627670  6786 net.cpp:67] Creating Layer loss
I1029 05:59:29.627676  6786 net.cpp:394] loss <- ip2
I1029 05:59:29.627686  6786 net.cpp:394] loss <- label
I1029 05:59:29.627691  6786 net.cpp:356] loss -> loss
I1029 05:59:29.627701  6786 net.cpp:96] Setting up loss
I1029 05:59:29.627713  6786 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 05:59:29.627718  6786 net.cpp:109]     with loss weight 1
I1029 05:59:29.627754  6786 net.cpp:170] loss needs backward computation.
I1029 05:59:29.627759  6786 net.cpp:170] ip2 needs backward computation.
I1029 05:59:29.627764  6786 net.cpp:170] drop4 needs backward computation.
I1029 05:59:29.627768  6786 net.cpp:170] relu4 needs backward computation.
I1029 05:59:29.627773  6786 net.cpp:170] ip1 needs backward computation.
I1029 05:59:29.627776  6786 net.cpp:170] drop3 needs backward computation.
I1029 05:59:29.627780  6786 net.cpp:170] relu3 needs backward computation.
I1029 05:59:29.627785  6786 net.cpp:170] pool3 needs backward computation.
I1029 05:59:29.627789  6786 net.cpp:170] conv3 needs backward computation.
I1029 05:59:29.627794  6786 net.cpp:170] drop2 needs backward computation.
I1029 05:59:29.627799  6786 net.cpp:170] relu2 needs backward computation.
I1029 05:59:29.627802  6786 net.cpp:170] pool2 needs backward computation.
I1029 05:59:29.627806  6786 net.cpp:170] conv2 needs backward computation.
I1029 05:59:29.627811  6786 net.cpp:170] drop1 needs backward computation.
I1029 05:59:29.627815  6786 net.cpp:170] relu1 needs backward computation.
I1029 05:59:29.627820  6786 net.cpp:170] pool1 needs backward computation.
I1029 05:59:29.627823  6786 net.cpp:170] conv1 needs backward computation.
I1029 05:59:29.627828  6786 net.cpp:172] mnist does not need backward computation.
I1029 05:59:29.627832  6786 net.cpp:208] This network produces output loss
I1029 05:59:29.627842  6786 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 05:59:29.627851  6786 net.cpp:219] Network initialization done.
I1029 05:59:29.627854  6786 net.cpp:220] Memory required for data: 119788292
I1029 05:59:29.627912  6786 solver.cpp:41] Solver scaffolding done.
I1029 05:59:29.627919  6786 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_470000.solverstate
I1029 05:59:29.627924  6786 solver.cpp:160] Solving Captcha
I1029 05:59:29.627943  6786 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_470000.solverstate
I1029 05:59:32.609585  6786 solver.cpp:502] SGDSolver: restoring history
I1029 05:59:33.441421  6786 solver.cpp:191] Iteration 470000, loss = 2.46229
I1029 05:59:33.441476  6786 solver.cpp:206]     Train net output #0: loss = 2.46229 (* 1 = 2.46229 loss)
I1029 05:59:33.441491  6786 solver.cpp:403] Iteration 470000, lr = 0.000548364
I1029 06:03:35.624009  6786 solver.cpp:191] Iteration 471000, loss = 2.53794
I1029 06:03:35.624809  6786 solver.cpp:206]     Train net output #0: loss = 2.53794 (* 1 = 2.53794 loss)
I1029 06:03:35.624847  6786 solver.cpp:403] Iteration 471000, lr = 0.000547509
I1029 06:07:37.204089  6786 solver.cpp:191] Iteration 472000, loss = 2.39769
I1029 06:07:37.204782  6786 solver.cpp:206]     Train net output #0: loss = 2.39769 (* 1 = 2.39769 loss)
I1029 06:07:37.204819  6786 solver.cpp:403] Iteration 472000, lr = 0.000546657
I1029 06:11:38.839314  6786 solver.cpp:191] Iteration 473000, loss = 2.58085
I1029 06:11:38.839867  6786 solver.cpp:206]     Train net output #0: loss = 2.58085 (* 1 = 2.58085 loss)
I1029 06:11:38.839901  6786 solver.cpp:403] Iteration 473000, lr = 0.000545808
I1029 06:15:40.400002  6786 solver.cpp:191] Iteration 474000, loss = 2.34115
I1029 06:15:40.400599  6786 solver.cpp:206]     Train net output #0: loss = 2.34115 (* 1 = 2.34115 loss)
I1029 06:15:40.400632  6786 solver.cpp:403] Iteration 474000, lr = 0.000544962
I1029 06:19:42.484477  6786 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_475000.caffemodel
I1029 06:19:46.940026  6786 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_475000.solverstate
I1029 06:19:50.880018  6786 solver.cpp:228] Iteration 475000, loss = 2.4305
I1029 06:19:50.880475  6786 solver.cpp:233] Optimization Done.
I1029 06:19:50.880497  6786 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 06:41:48.684211 20349 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 06:41:48.684314 20349 net.cpp:358] Input 0 -> data
I1029 06:41:48.684345 20349 net.cpp:67] Creating Layer conv1
I1029 06:41:48.684350 20349 net.cpp:394] conv1 <- data
I1029 06:41:48.684356 20349 net.cpp:356] conv1 -> conv1
I1029 06:41:48.684366 20349 net.cpp:96] Setting up conv1
I1029 06:41:48.684819 20349 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 06:41:48.684844 20349 net.cpp:67] Creating Layer pool1
I1029 06:41:48.684849 20349 net.cpp:394] pool1 <- conv1
I1029 06:41:48.684854 20349 net.cpp:356] pool1 -> pool1
I1029 06:41:48.684861 20349 net.cpp:96] Setting up pool1
I1029 06:41:48.684875 20349 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 06:41:48.684881 20349 net.cpp:67] Creating Layer relu1
I1029 06:41:48.684890 20349 net.cpp:394] relu1 <- pool1
I1029 06:41:48.684896 20349 net.cpp:345] relu1 -> pool1 (in-place)
I1029 06:41:48.684902 20349 net.cpp:96] Setting up relu1
I1029 06:41:48.684907 20349 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 06:41:48.684913 20349 net.cpp:67] Creating Layer drop1
I1029 06:41:48.684917 20349 net.cpp:394] drop1 <- pool1
I1029 06:41:48.684922 20349 net.cpp:345] drop1 -> pool1 (in-place)
I1029 06:41:48.684928 20349 net.cpp:96] Setting up drop1
I1029 06:41:48.684933 20349 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 06:41:48.684939 20349 net.cpp:67] Creating Layer conv2
I1029 06:41:48.684944 20349 net.cpp:394] conv2 <- pool1
I1029 06:41:48.684949 20349 net.cpp:356] conv2 -> conv2
I1029 06:41:48.684957 20349 net.cpp:96] Setting up conv2
I1029 06:41:48.685506 20349 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 06:41:48.685521 20349 net.cpp:67] Creating Layer pool2
I1029 06:41:48.685526 20349 net.cpp:394] pool2 <- conv2
I1029 06:41:48.685534 20349 net.cpp:356] pool2 -> pool2
I1029 06:41:48.685541 20349 net.cpp:96] Setting up pool2
I1029 06:41:48.685547 20349 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 06:41:48.685552 20349 net.cpp:67] Creating Layer relu2
I1029 06:41:48.685556 20349 net.cpp:394] relu2 <- pool2
I1029 06:41:48.685564 20349 net.cpp:345] relu2 -> pool2 (in-place)
I1029 06:41:48.685570 20349 net.cpp:96] Setting up relu2
I1029 06:41:48.685573 20349 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 06:41:48.685578 20349 net.cpp:67] Creating Layer drop2
I1029 06:41:48.685582 20349 net.cpp:394] drop2 <- pool2
I1029 06:41:48.685587 20349 net.cpp:345] drop2 -> pool2 (in-place)
I1029 06:41:48.685592 20349 net.cpp:96] Setting up drop2
I1029 06:41:48.685597 20349 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 06:41:48.685605 20349 net.cpp:67] Creating Layer conv3
I1029 06:41:48.685608 20349 net.cpp:394] conv3 <- pool2
I1029 06:41:48.685616 20349 net.cpp:356] conv3 -> conv3
I1029 06:41:48.685622 20349 net.cpp:96] Setting up conv3
I1029 06:41:48.687067 20349 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 06:41:48.687084 20349 net.cpp:67] Creating Layer pool3
I1029 06:41:48.687089 20349 net.cpp:394] pool3 <- conv3
I1029 06:41:48.687096 20349 net.cpp:356] pool3 -> pool3
I1029 06:41:48.687103 20349 net.cpp:96] Setting up pool3
I1029 06:41:48.687108 20349 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 06:41:48.687114 20349 net.cpp:67] Creating Layer relu3
I1029 06:41:48.687118 20349 net.cpp:394] relu3 <- pool3
I1029 06:41:48.687122 20349 net.cpp:345] relu3 -> pool3 (in-place)
I1029 06:41:48.687127 20349 net.cpp:96] Setting up relu3
I1029 06:41:48.687131 20349 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 06:41:48.687137 20349 net.cpp:67] Creating Layer drop3
I1029 06:41:48.687140 20349 net.cpp:394] drop3 <- pool3
I1029 06:41:48.687149 20349 net.cpp:345] drop3 -> pool3 (in-place)
I1029 06:41:48.687153 20349 net.cpp:96] Setting up drop3
I1029 06:41:48.687158 20349 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 06:41:48.687165 20349 net.cpp:67] Creating Layer ip1
I1029 06:41:48.687168 20349 net.cpp:394] ip1 <- pool3
I1029 06:41:48.687175 20349 net.cpp:356] ip1 -> ip1
I1029 06:41:48.687182 20349 net.cpp:96] Setting up ip1
I1029 06:41:49.189518 20349 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 06:41:49.189581 20349 net.cpp:67] Creating Layer relu4
I1029 06:41:49.189589 20349 net.cpp:394] relu4 <- ip1
I1029 06:41:49.189597 20349 net.cpp:345] relu4 -> ip1 (in-place)
I1029 06:41:49.189607 20349 net.cpp:96] Setting up relu4
I1029 06:41:49.189612 20349 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 06:41:49.189622 20349 net.cpp:67] Creating Layer drop4
I1029 06:41:49.189626 20349 net.cpp:394] drop4 <- ip1
I1029 06:41:49.189632 20349 net.cpp:345] drop4 -> ip1 (in-place)
I1029 06:41:49.189638 20349 net.cpp:96] Setting up drop4
I1029 06:41:49.189643 20349 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 06:41:49.189651 20349 net.cpp:67] Creating Layer ip2
I1029 06:41:49.189656 20349 net.cpp:394] ip2 <- ip1
I1029 06:41:49.189663 20349 net.cpp:356] ip2 -> ip2
I1029 06:41:49.189682 20349 net.cpp:96] Setting up ip2
I1029 06:41:49.199486 20349 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 06:41:49.199548 20349 net.cpp:67] Creating Layer prob
I1029 06:41:49.199554 20349 net.cpp:394] prob <- ip2
I1029 06:41:49.199563 20349 net.cpp:356] prob -> prob
I1029 06:41:49.199573 20349 net.cpp:96] Setting up prob
I1029 06:41:49.199584 20349 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 06:41:49.199589 20349 net.cpp:172] prob does not need backward computation.
I1029 06:41:49.199592 20349 net.cpp:172] ip2 does not need backward computation.
I1029 06:41:49.199596 20349 net.cpp:172] drop4 does not need backward computation.
I1029 06:41:49.199600 20349 net.cpp:172] relu4 does not need backward computation.
I1029 06:41:49.199604 20349 net.cpp:172] ip1 does not need backward computation.
I1029 06:41:49.199607 20349 net.cpp:172] drop3 does not need backward computation.
I1029 06:41:49.199610 20349 net.cpp:172] relu3 does not need backward computation.
I1029 06:41:49.199615 20349 net.cpp:172] pool3 does not need backward computation.
I1029 06:41:49.199617 20349 net.cpp:172] conv3 does not need backward computation.
I1029 06:41:49.199621 20349 net.cpp:172] drop2 does not need backward computation.
I1029 06:41:49.199625 20349 net.cpp:172] relu2 does not need backward computation.
I1029 06:41:49.199628 20349 net.cpp:172] pool2 does not need backward computation.
I1029 06:41:49.199631 20349 net.cpp:172] conv2 does not need backward computation.
I1029 06:41:49.199635 20349 net.cpp:172] drop1 does not need backward computation.
I1029 06:41:49.199638 20349 net.cpp:172] relu1 does not need backward computation.
I1029 06:41:49.199642 20349 net.cpp:172] pool1 does not need backward computation.
I1029 06:41:49.199645 20349 net.cpp:172] conv1 does not need backward computation.
I1029 06:41:49.199650 20349 net.cpp:208] This network produces output prob
I1029 06:41:49.199659 20349 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 06:41:49.199667 20349 net.cpp:219] Network initialization done.
I1029 06:41:49.199671 20349 net.cpp:220] Memory required for data: 1837200
I1029 07:16:49.965469 27475 convert_imageset.cpp:70] Shuffling data
I1029 07:16:50.495296 27475 convert_imageset.cpp:73] A total of 60000 images.
I1029 07:16:50.495376 27475 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 07:16:52.809428 27475 convert_imageset.cpp:177] Processed 1000 files.
E1029 07:16:54.745980 27475 convert_imageset.cpp:177] Processed 2000 files.
E1029 07:16:56.778187 27475 convert_imageset.cpp:177] Processed 3000 files.
E1029 07:16:58.714366 27475 convert_imageset.cpp:177] Processed 4000 files.
E1029 07:17:00.518443 27475 convert_imageset.cpp:177] Processed 5000 files.
E1029 07:17:02.296083 27475 convert_imageset.cpp:177] Processed 6000 files.
E1029 07:17:04.101060 27475 convert_imageset.cpp:177] Processed 7000 files.
E1029 07:17:05.948273 27475 convert_imageset.cpp:177] Processed 8000 files.
E1029 07:17:07.709133 27475 convert_imageset.cpp:177] Processed 9000 files.
E1029 07:17:09.418342 27475 convert_imageset.cpp:177] Processed 10000 files.
E1029 07:17:11.140079 27475 convert_imageset.cpp:177] Processed 11000 files.
E1029 07:17:12.931582 27475 convert_imageset.cpp:177] Processed 12000 files.
E1029 07:17:14.713466 27475 convert_imageset.cpp:177] Processed 13000 files.
E1029 07:17:16.467777 27475 convert_imageset.cpp:177] Processed 14000 files.
E1029 07:17:18.138908 27475 convert_imageset.cpp:177] Processed 15000 files.
E1029 07:17:20.001058 27475 convert_imageset.cpp:177] Processed 16000 files.
E1029 07:17:21.699128 27475 convert_imageset.cpp:177] Processed 17000 files.
E1029 07:17:23.428202 27475 convert_imageset.cpp:177] Processed 18000 files.
E1029 07:17:25.150501 27475 convert_imageset.cpp:177] Processed 19000 files.
E1029 07:17:26.922821 27475 convert_imageset.cpp:177] Processed 20000 files.
E1029 07:17:28.557540 27475 convert_imageset.cpp:177] Processed 21000 files.
E1029 07:17:30.311347 27475 convert_imageset.cpp:177] Processed 22000 files.
E1029 07:17:31.879195 27475 convert_imageset.cpp:177] Processed 23000 files.
E1029 07:17:33.565807 27475 convert_imageset.cpp:177] Processed 24000 files.
E1029 07:17:35.192371 27475 convert_imageset.cpp:177] Processed 25000 files.
E1029 07:17:36.904476 27475 convert_imageset.cpp:177] Processed 26000 files.
E1029 07:17:38.467228 27475 convert_imageset.cpp:177] Processed 27000 files.
E1029 07:17:40.033984 27475 convert_imageset.cpp:177] Processed 28000 files.
E1029 07:17:41.730304 27475 convert_imageset.cpp:177] Processed 29000 files.
E1029 07:17:43.271699 27475 convert_imageset.cpp:177] Processed 30000 files.
E1029 07:17:44.829073 27475 convert_imageset.cpp:177] Processed 31000 files.
E1029 07:17:46.341887 27475 convert_imageset.cpp:177] Processed 32000 files.
E1029 07:17:47.838912 27475 convert_imageset.cpp:177] Processed 33000 files.
E1029 07:17:49.552441 27475 convert_imageset.cpp:177] Processed 34000 files.
E1029 07:17:51.175510 27475 convert_imageset.cpp:177] Processed 35000 files.
E1029 07:17:52.865416 27475 convert_imageset.cpp:177] Processed 36000 files.
E1029 07:17:54.522137 27475 convert_imageset.cpp:177] Processed 37000 files.
E1029 07:17:56.206447 27475 convert_imageset.cpp:177] Processed 38000 files.
E1029 07:17:57.707540 27475 convert_imageset.cpp:177] Processed 39000 files.
E1029 07:17:59.278712 27475 convert_imageset.cpp:177] Processed 40000 files.
E1029 07:18:00.866209 27475 convert_imageset.cpp:177] Processed 41000 files.
E1029 07:18:02.480193 27475 convert_imageset.cpp:177] Processed 42000 files.
E1029 07:18:04.098505 27475 convert_imageset.cpp:177] Processed 43000 files.
E1029 07:18:05.612468 27475 convert_imageset.cpp:177] Processed 44000 files.
E1029 07:18:07.142627 27475 convert_imageset.cpp:177] Processed 45000 files.
E1029 07:18:08.653600 27475 convert_imageset.cpp:177] Processed 46000 files.
E1029 07:18:10.105568 27475 convert_imageset.cpp:177] Processed 47000 files.
E1029 07:18:11.723943 27475 convert_imageset.cpp:177] Processed 48000 files.
E1029 07:18:13.203418 27475 convert_imageset.cpp:177] Processed 49000 files.
E1029 07:18:14.745399 27475 convert_imageset.cpp:177] Processed 50000 files.
E1029 07:18:16.308900 27475 convert_imageset.cpp:177] Processed 51000 files.
E1029 07:18:17.807013 27475 convert_imageset.cpp:177] Processed 52000 files.
E1029 07:18:19.358501 27475 convert_imageset.cpp:177] Processed 53000 files.
E1029 07:18:20.847157 27475 convert_imageset.cpp:177] Processed 54000 files.
E1029 07:18:22.521167 27475 convert_imageset.cpp:177] Processed 55000 files.
E1029 07:18:24.168339 27475 convert_imageset.cpp:177] Processed 56000 files.
E1029 07:18:25.738083 27475 convert_imageset.cpp:177] Processed 57000 files.
E1029 07:18:27.080116 27475 convert_imageset.cpp:177] Processed 58000 files.
E1029 07:18:28.504055 27475 convert_imageset.cpp:177] Processed 59000 files.
E1029 07:18:30.040902 27475 convert_imageset.cpp:177] Processed 60000 files.
I1029 07:18:30.255843 27587 caffe.cpp:99] Use GPU with device ID 0
I1029 07:18:30.666347 27587 caffe.cpp:107] Starting Optimization
I1029 07:18:30.666472 27587 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 480000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 07:18:30.666498 27587 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 07:18:30.678185 27587 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 07:18:30.678279 27587 net.cpp:67] Creating Layer mnist
I1029 07:18:30.678290 27587 net.cpp:356] mnist -> data
I1029 07:18:30.678308 27587 net.cpp:356] mnist -> label
I1029 07:18:30.678320 27587 net.cpp:96] Setting up mnist
I1029 07:18:30.686303 27587 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 07:18:30.686399 27587 data_layer.cpp:128] output data size: 64,1,50,180
I1029 07:18:30.687237 27587 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 07:18:30.687264 27587 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 07:18:30.687276 27587 net.cpp:67] Creating Layer conv1
I1029 07:18:30.687281 27587 net.cpp:394] conv1 <- data
I1029 07:18:30.687296 27587 net.cpp:356] conv1 -> conv1
I1029 07:18:30.687307 27587 net.cpp:96] Setting up conv1
I1029 07:18:30.687685 27587 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 07:18:30.687716 27587 net.cpp:67] Creating Layer pool1
I1029 07:18:30.687723 27587 net.cpp:394] pool1 <- conv1
I1029 07:18:30.687729 27587 net.cpp:356] pool1 -> pool1
I1029 07:18:30.687737 27587 net.cpp:96] Setting up pool1
I1029 07:18:30.687753 27587 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 07:18:30.687763 27587 net.cpp:67] Creating Layer relu1
I1029 07:18:30.687768 27587 net.cpp:394] relu1 <- pool1
I1029 07:18:30.687773 27587 net.cpp:345] relu1 -> pool1 (in-place)
I1029 07:18:30.687780 27587 net.cpp:96] Setting up relu1
I1029 07:18:30.687789 27587 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 07:18:30.687798 27587 net.cpp:67] Creating Layer drop1
I1029 07:18:30.687801 27587 net.cpp:394] drop1 <- pool1
I1029 07:18:30.687809 27587 net.cpp:345] drop1 -> pool1 (in-place)
I1029 07:18:30.687816 27587 net.cpp:96] Setting up drop1
I1029 07:18:30.687821 27587 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 07:18:30.687829 27587 net.cpp:67] Creating Layer conv2
I1029 07:18:30.687832 27587 net.cpp:394] conv2 <- pool1
I1029 07:18:30.687840 27587 net.cpp:356] conv2 -> conv2
I1029 07:18:30.687849 27587 net.cpp:96] Setting up conv2
I1029 07:18:30.688457 27587 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 07:18:30.688477 27587 net.cpp:67] Creating Layer pool2
I1029 07:18:30.688483 27587 net.cpp:394] pool2 <- conv2
I1029 07:18:30.688490 27587 net.cpp:356] pool2 -> pool2
I1029 07:18:30.688498 27587 net.cpp:96] Setting up pool2
I1029 07:18:30.688503 27587 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 07:18:30.688509 27587 net.cpp:67] Creating Layer relu2
I1029 07:18:30.688513 27587 net.cpp:394] relu2 <- pool2
I1029 07:18:30.688521 27587 net.cpp:345] relu2 -> pool2 (in-place)
I1029 07:18:30.688529 27587 net.cpp:96] Setting up relu2
I1029 07:18:30.688532 27587 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 07:18:30.688540 27587 net.cpp:67] Creating Layer drop2
I1029 07:18:30.688544 27587 net.cpp:394] drop2 <- pool2
I1029 07:18:30.688550 27587 net.cpp:345] drop2 -> pool2 (in-place)
I1029 07:18:30.688556 27587 net.cpp:96] Setting up drop2
I1029 07:18:30.688561 27587 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 07:18:30.688571 27587 net.cpp:67] Creating Layer conv3
I1029 07:18:30.688576 27587 net.cpp:394] conv3 <- pool2
I1029 07:18:30.688583 27587 net.cpp:356] conv3 -> conv3
I1029 07:18:30.688591 27587 net.cpp:96] Setting up conv3
I1029 07:18:30.690100 27587 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 07:18:30.690127 27587 net.cpp:67] Creating Layer pool3
I1029 07:18:30.690134 27587 net.cpp:394] pool3 <- conv3
I1029 07:18:30.690142 27587 net.cpp:356] pool3 -> pool3
I1029 07:18:30.690150 27587 net.cpp:96] Setting up pool3
I1029 07:18:30.690155 27587 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 07:18:30.690162 27587 net.cpp:67] Creating Layer relu3
I1029 07:18:30.690166 27587 net.cpp:394] relu3 <- pool3
I1029 07:18:30.690172 27587 net.cpp:345] relu3 -> pool3 (in-place)
I1029 07:18:30.690177 27587 net.cpp:96] Setting up relu3
I1029 07:18:30.690182 27587 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 07:18:30.690191 27587 net.cpp:67] Creating Layer drop3
I1029 07:18:30.690194 27587 net.cpp:394] drop3 <- pool3
I1029 07:18:30.690201 27587 net.cpp:345] drop3 -> pool3 (in-place)
I1029 07:18:30.690207 27587 net.cpp:96] Setting up drop3
I1029 07:18:30.690212 27587 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 07:18:30.690218 27587 net.cpp:67] Creating Layer ip1
I1029 07:18:30.690222 27587 net.cpp:394] ip1 <- pool3
I1029 07:18:30.690230 27587 net.cpp:356] ip1 -> ip1
I1029 07:18:30.690268 27587 net.cpp:96] Setting up ip1
I1029 07:18:31.222125 27587 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 07:18:31.222187 27587 net.cpp:67] Creating Layer relu4
I1029 07:18:31.222195 27587 net.cpp:394] relu4 <- ip1
I1029 07:18:31.222205 27587 net.cpp:345] relu4 -> ip1 (in-place)
I1029 07:18:31.222214 27587 net.cpp:96] Setting up relu4
I1029 07:18:31.222220 27587 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 07:18:31.222228 27587 net.cpp:67] Creating Layer drop4
I1029 07:18:31.222231 27587 net.cpp:394] drop4 <- ip1
I1029 07:18:31.222236 27587 net.cpp:345] drop4 -> ip1 (in-place)
I1029 07:18:31.222244 27587 net.cpp:96] Setting up drop4
I1029 07:18:31.222249 27587 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 07:18:31.222261 27587 net.cpp:67] Creating Layer ip2
I1029 07:18:31.222266 27587 net.cpp:394] ip2 <- ip1
I1029 07:18:31.222275 27587 net.cpp:356] ip2 -> ip2
I1029 07:18:31.222283 27587 net.cpp:96] Setting up ip2
I1029 07:18:31.231416 27587 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 07:18:31.231479 27587 net.cpp:67] Creating Layer loss
I1029 07:18:31.231493 27587 net.cpp:394] loss <- ip2
I1029 07:18:31.231501 27587 net.cpp:394] loss <- label
I1029 07:18:31.231508 27587 net.cpp:356] loss -> loss
I1029 07:18:31.231518 27587 net.cpp:96] Setting up loss
I1029 07:18:31.231529 27587 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 07:18:31.231534 27587 net.cpp:109]     with loss weight 1
I1029 07:18:31.231569 27587 net.cpp:170] loss needs backward computation.
I1029 07:18:31.231575 27587 net.cpp:170] ip2 needs backward computation.
I1029 07:18:31.231578 27587 net.cpp:170] drop4 needs backward computation.
I1029 07:18:31.231582 27587 net.cpp:170] relu4 needs backward computation.
I1029 07:18:31.231587 27587 net.cpp:170] ip1 needs backward computation.
I1029 07:18:31.231591 27587 net.cpp:170] drop3 needs backward computation.
I1029 07:18:31.231595 27587 net.cpp:170] relu3 needs backward computation.
I1029 07:18:31.231600 27587 net.cpp:170] pool3 needs backward computation.
I1029 07:18:31.231605 27587 net.cpp:170] conv3 needs backward computation.
I1029 07:18:31.231609 27587 net.cpp:170] drop2 needs backward computation.
I1029 07:18:31.231613 27587 net.cpp:170] relu2 needs backward computation.
I1029 07:18:31.231617 27587 net.cpp:170] pool2 needs backward computation.
I1029 07:18:31.231622 27587 net.cpp:170] conv2 needs backward computation.
I1029 07:18:31.231626 27587 net.cpp:170] drop1 needs backward computation.
I1029 07:18:31.231631 27587 net.cpp:170] relu1 needs backward computation.
I1029 07:18:31.231634 27587 net.cpp:170] pool1 needs backward computation.
I1029 07:18:31.231639 27587 net.cpp:170] conv1 needs backward computation.
I1029 07:18:31.231643 27587 net.cpp:172] mnist does not need backward computation.
I1029 07:18:31.231648 27587 net.cpp:208] This network produces output loss
I1029 07:18:31.231658 27587 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 07:18:31.231665 27587 net.cpp:219] Network initialization done.
I1029 07:18:31.231669 27587 net.cpp:220] Memory required for data: 119788292
I1029 07:18:31.231732 27587 solver.cpp:41] Solver scaffolding done.
I1029 07:18:31.231739 27587 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_475000.solverstate
I1029 07:18:31.231744 27587 solver.cpp:160] Solving Captcha
I1029 07:18:31.231761 27587 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_475000.solverstate
I1029 07:18:36.959463 27587 solver.cpp:502] SGDSolver: restoring history
I1029 07:18:37.788874 27587 solver.cpp:191] Iteration 475000, loss = 2.39546
I1029 07:18:37.788929 27587 solver.cpp:206]     Train net output #0: loss = 2.39546 (* 1 = 2.39546 loss)
I1029 07:18:37.788944 27587 solver.cpp:403] Iteration 475000, lr = 0.000544119
I1029 07:22:39.496593 27587 solver.cpp:191] Iteration 476000, loss = 2.40246
I1029 07:22:39.498142 27587 solver.cpp:206]     Train net output #0: loss = 2.40246 (* 1 = 2.40246 loss)
I1029 07:22:39.498154 27587 solver.cpp:403] Iteration 476000, lr = 0.000543279
I1029 07:26:40.731467 27587 solver.cpp:191] Iteration 477000, loss = 2.59397
I1029 07:26:40.732053 27587 solver.cpp:206]     Train net output #0: loss = 2.59397 (* 1 = 2.59397 loss)
I1029 07:26:40.732085 27587 solver.cpp:403] Iteration 477000, lr = 0.000542442
I1029 07:30:41.871842 27587 solver.cpp:191] Iteration 478000, loss = 2.3948
I1029 07:30:41.872720 27587 solver.cpp:206]     Train net output #0: loss = 2.3948 (* 1 = 2.3948 loss)
I1029 07:30:41.872752 27587 solver.cpp:403] Iteration 478000, lr = 0.000541608
I1029 07:34:43.083518 27587 solver.cpp:191] Iteration 479000, loss = 2.54743
I1029 07:34:43.084172 27587 solver.cpp:206]     Train net output #0: loss = 2.54743 (* 1 = 2.54743 loss)
I1029 07:34:43.084208 27587 solver.cpp:403] Iteration 479000, lr = 0.000540777
I1029 07:38:45.073279 27587 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_480000.caffemodel
I1029 07:38:49.502928 27587 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_480000.solverstate
I1029 07:38:53.108114 27587 solver.cpp:228] Iteration 480000, loss = 2.53861
I1029 07:38:53.108700 27587 solver.cpp:233] Optimization Done.
I1029 07:38:53.108724 27587 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 08:00:20.434454  6940 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 08:00:20.434556  6940 net.cpp:358] Input 0 -> data
I1029 08:00:20.434584  6940 net.cpp:67] Creating Layer conv1
I1029 08:00:20.434590  6940 net.cpp:394] conv1 <- data
I1029 08:00:20.434597  6940 net.cpp:356] conv1 -> conv1
I1029 08:00:20.434607  6940 net.cpp:96] Setting up conv1
I1029 08:00:20.434932  6940 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 08:00:20.434952  6940 net.cpp:67] Creating Layer pool1
I1029 08:00:20.434958  6940 net.cpp:394] pool1 <- conv1
I1029 08:00:20.434964  6940 net.cpp:356] pool1 -> pool1
I1029 08:00:20.434972  6940 net.cpp:96] Setting up pool1
I1029 08:00:20.434988  6940 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 08:00:20.434995  6940 net.cpp:67] Creating Layer relu1
I1029 08:00:20.434999  6940 net.cpp:394] relu1 <- pool1
I1029 08:00:20.435012  6940 net.cpp:345] relu1 -> pool1 (in-place)
I1029 08:00:20.435019  6940 net.cpp:96] Setting up relu1
I1029 08:00:20.435024  6940 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 08:00:20.435035  6940 net.cpp:67] Creating Layer drop1
I1029 08:00:20.435039  6940 net.cpp:394] drop1 <- pool1
I1029 08:00:20.435045  6940 net.cpp:345] drop1 -> pool1 (in-place)
I1029 08:00:20.435052  6940 net.cpp:96] Setting up drop1
I1029 08:00:20.435057  6940 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 08:00:20.435065  6940 net.cpp:67] Creating Layer conv2
I1029 08:00:20.435068  6940 net.cpp:394] conv2 <- pool1
I1029 08:00:20.435075  6940 net.cpp:356] conv2 -> conv2
I1029 08:00:20.435081  6940 net.cpp:96] Setting up conv2
I1029 08:00:20.435659  6940 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 08:00:20.435674  6940 net.cpp:67] Creating Layer pool2
I1029 08:00:20.435679  6940 net.cpp:394] pool2 <- conv2
I1029 08:00:20.435688  6940 net.cpp:356] pool2 -> pool2
I1029 08:00:20.435695  6940 net.cpp:96] Setting up pool2
I1029 08:00:20.435701  6940 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 08:00:20.435706  6940 net.cpp:67] Creating Layer relu2
I1029 08:00:20.435710  6940 net.cpp:394] relu2 <- pool2
I1029 08:00:20.435715  6940 net.cpp:345] relu2 -> pool2 (in-place)
I1029 08:00:20.435721  6940 net.cpp:96] Setting up relu2
I1029 08:00:20.435725  6940 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 08:00:20.435734  6940 net.cpp:67] Creating Layer drop2
I1029 08:00:20.435737  6940 net.cpp:394] drop2 <- pool2
I1029 08:00:20.435742  6940 net.cpp:345] drop2 -> pool2 (in-place)
I1029 08:00:20.435748  6940 net.cpp:96] Setting up drop2
I1029 08:00:20.435753  6940 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 08:00:20.435760  6940 net.cpp:67] Creating Layer conv3
I1029 08:00:20.435765  6940 net.cpp:394] conv3 <- pool2
I1029 08:00:20.435772  6940 net.cpp:356] conv3 -> conv3
I1029 08:00:20.435780  6940 net.cpp:96] Setting up conv3
I1029 08:00:20.437367  6940 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 08:00:20.437388  6940 net.cpp:67] Creating Layer pool3
I1029 08:00:20.437394  6940 net.cpp:394] pool3 <- conv3
I1029 08:00:20.437400  6940 net.cpp:356] pool3 -> pool3
I1029 08:00:20.437409  6940 net.cpp:96] Setting up pool3
I1029 08:00:20.437415  6940 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 08:00:20.437422  6940 net.cpp:67] Creating Layer relu3
I1029 08:00:20.437425  6940 net.cpp:394] relu3 <- pool3
I1029 08:00:20.437430  6940 net.cpp:345] relu3 -> pool3 (in-place)
I1029 08:00:20.437436  6940 net.cpp:96] Setting up relu3
I1029 08:00:20.437440  6940 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 08:00:20.437446  6940 net.cpp:67] Creating Layer drop3
I1029 08:00:20.437450  6940 net.cpp:394] drop3 <- pool3
I1029 08:00:20.437458  6940 net.cpp:345] drop3 -> pool3 (in-place)
I1029 08:00:20.437463  6940 net.cpp:96] Setting up drop3
I1029 08:00:20.437469  6940 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 08:00:20.437475  6940 net.cpp:67] Creating Layer ip1
I1029 08:00:20.437479  6940 net.cpp:394] ip1 <- pool3
I1029 08:00:20.437484  6940 net.cpp:356] ip1 -> ip1
I1029 08:00:20.437494  6940 net.cpp:96] Setting up ip1
I1029 08:00:21.035482  6940 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 08:00:21.035543  6940 net.cpp:67] Creating Layer relu4
I1029 08:00:21.035550  6940 net.cpp:394] relu4 <- ip1
I1029 08:00:21.035559  6940 net.cpp:345] relu4 -> ip1 (in-place)
I1029 08:00:21.035568  6940 net.cpp:96] Setting up relu4
I1029 08:00:21.035573  6940 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 08:00:21.035583  6940 net.cpp:67] Creating Layer drop4
I1029 08:00:21.035588  6940 net.cpp:394] drop4 <- ip1
I1029 08:00:21.035593  6940 net.cpp:345] drop4 -> ip1 (in-place)
I1029 08:00:21.035600  6940 net.cpp:96] Setting up drop4
I1029 08:00:21.035605  6940 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 08:00:21.035614  6940 net.cpp:67] Creating Layer ip2
I1029 08:00:21.035617  6940 net.cpp:394] ip2 <- ip1
I1029 08:00:21.035629  6940 net.cpp:356] ip2 -> ip2
I1029 08:00:21.035641  6940 net.cpp:96] Setting up ip2
I1029 08:00:21.043920  6940 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 08:00:21.043982  6940 net.cpp:67] Creating Layer prob
I1029 08:00:21.043988  6940 net.cpp:394] prob <- ip2
I1029 08:00:21.044003  6940 net.cpp:356] prob -> prob
I1029 08:00:21.044014  6940 net.cpp:96] Setting up prob
I1029 08:00:21.044023  6940 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 08:00:21.044028  6940 net.cpp:172] prob does not need backward computation.
I1029 08:00:21.044033  6940 net.cpp:172] ip2 does not need backward computation.
I1029 08:00:21.044035  6940 net.cpp:172] drop4 does not need backward computation.
I1029 08:00:21.044039  6940 net.cpp:172] relu4 does not need backward computation.
I1029 08:00:21.044044  6940 net.cpp:172] ip1 does not need backward computation.
I1029 08:00:21.044046  6940 net.cpp:172] drop3 does not need backward computation.
I1029 08:00:21.044050  6940 net.cpp:172] relu3 does not need backward computation.
I1029 08:00:21.044054  6940 net.cpp:172] pool3 does not need backward computation.
I1029 08:00:21.044057  6940 net.cpp:172] conv3 does not need backward computation.
I1029 08:00:21.044061  6940 net.cpp:172] drop2 does not need backward computation.
I1029 08:00:21.044064  6940 net.cpp:172] relu2 does not need backward computation.
I1029 08:00:21.044069  6940 net.cpp:172] pool2 does not need backward computation.
I1029 08:00:21.044071  6940 net.cpp:172] conv2 does not need backward computation.
I1029 08:00:21.044075  6940 net.cpp:172] drop1 does not need backward computation.
I1029 08:00:21.044078  6940 net.cpp:172] relu1 does not need backward computation.
I1029 08:00:21.044082  6940 net.cpp:172] pool1 does not need backward computation.
I1029 08:00:21.044085  6940 net.cpp:172] conv1 does not need backward computation.
I1029 08:00:21.044090  6940 net.cpp:208] This network produces output prob
I1029 08:00:21.044100  6940 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 08:00:21.044107  6940 net.cpp:219] Network initialization done.
I1029 08:00:21.044111  6940 net.cpp:220] Memory required for data: 1837200
I1029 08:33:42.895047 15214 convert_imageset.cpp:70] Shuffling data
I1029 08:33:43.533267 15214 convert_imageset.cpp:73] A total of 60000 images.
I1029 08:33:43.533345 15214 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 08:33:46.161742 15214 convert_imageset.cpp:177] Processed 1000 files.
E1029 08:33:48.531533 15214 convert_imageset.cpp:177] Processed 2000 files.
E1029 08:33:50.821984 15214 convert_imageset.cpp:177] Processed 3000 files.
E1029 08:33:52.993332 15214 convert_imageset.cpp:177] Processed 4000 files.
E1029 08:33:55.530261 15214 convert_imageset.cpp:177] Processed 5000 files.
E1029 08:33:57.611778 15214 convert_imageset.cpp:177] Processed 6000 files.
E1029 08:33:59.716537 15214 convert_imageset.cpp:177] Processed 7000 files.
E1029 08:34:01.800720 15214 convert_imageset.cpp:177] Processed 8000 files.
E1029 08:34:03.850556 15214 convert_imageset.cpp:177] Processed 9000 files.
E1029 08:34:05.837820 15214 convert_imageset.cpp:177] Processed 10000 files.
E1029 08:34:07.783814 15214 convert_imageset.cpp:177] Processed 11000 files.
E1029 08:34:09.713928 15214 convert_imageset.cpp:177] Processed 12000 files.
E1029 08:34:11.727900 15214 convert_imageset.cpp:177] Processed 13000 files.
E1029 08:34:13.838371 15214 convert_imageset.cpp:177] Processed 14000 files.
E1029 08:34:16.050336 15214 convert_imageset.cpp:177] Processed 15000 files.
E1029 08:34:17.952849 15214 convert_imageset.cpp:177] Processed 16000 files.
E1029 08:34:19.789923 15214 convert_imageset.cpp:177] Processed 17000 files.
E1029 08:34:21.696203 15214 convert_imageset.cpp:177] Processed 18000 files.
E1029 08:34:23.538040 15214 convert_imageset.cpp:177] Processed 19000 files.
E1029 08:34:25.402474 15214 convert_imageset.cpp:177] Processed 20000 files.
E1029 08:34:27.202448 15214 convert_imageset.cpp:177] Processed 21000 files.
E1029 08:34:28.960181 15214 convert_imageset.cpp:177] Processed 22000 files.
E1029 08:34:30.733355 15214 convert_imageset.cpp:177] Processed 23000 files.
E1029 08:34:32.495935 15214 convert_imageset.cpp:177] Processed 24000 files.
E1029 08:34:34.213385 15214 convert_imageset.cpp:177] Processed 25000 files.
E1029 08:34:35.921756 15214 convert_imageset.cpp:177] Processed 26000 files.
E1029 08:34:37.620466 15214 convert_imageset.cpp:177] Processed 27000 files.
E1029 08:34:39.347654 15214 convert_imageset.cpp:177] Processed 28000 files.
E1029 08:34:41.074779 15214 convert_imageset.cpp:177] Processed 29000 files.
E1029 08:34:42.766892 15214 convert_imageset.cpp:177] Processed 30000 files.
E1029 08:34:44.443043 15214 convert_imageset.cpp:177] Processed 31000 files.
E1029 08:34:46.169698 15214 convert_imageset.cpp:177] Processed 32000 files.
E1029 08:34:47.841233 15214 convert_imageset.cpp:177] Processed 33000 files.
E1029 08:34:49.410630 15214 convert_imageset.cpp:177] Processed 34000 files.
E1029 08:34:50.982507 15214 convert_imageset.cpp:177] Processed 35000 files.
E1029 08:34:52.642927 15214 convert_imageset.cpp:177] Processed 36000 files.
E1029 08:34:54.271337 15214 convert_imageset.cpp:177] Processed 37000 files.
E1029 08:34:55.902242 15214 convert_imageset.cpp:177] Processed 38000 files.
E1029 08:34:57.597023 15214 convert_imageset.cpp:177] Processed 39000 files.
E1029 08:34:59.255779 15214 convert_imageset.cpp:177] Processed 40000 files.
E1029 08:35:01.122344 15214 convert_imageset.cpp:177] Processed 41000 files.
E1029 08:35:02.701130 15214 convert_imageset.cpp:177] Processed 42000 files.
E1029 08:35:04.321142 15214 convert_imageset.cpp:177] Processed 43000 files.
E1029 08:35:06.012578 15214 convert_imageset.cpp:177] Processed 44000 files.
E1029 08:35:07.670063 15214 convert_imageset.cpp:177] Processed 45000 files.
E1029 08:35:09.386267 15214 convert_imageset.cpp:177] Processed 46000 files.
E1029 08:35:10.996440 15214 convert_imageset.cpp:177] Processed 47000 files.
E1029 08:35:12.618451 15214 convert_imageset.cpp:177] Processed 48000 files.
E1029 08:35:14.245668 15214 convert_imageset.cpp:177] Processed 49000 files.
E1029 08:35:15.833402 15214 convert_imageset.cpp:177] Processed 50000 files.
E1029 08:35:17.425411 15214 convert_imageset.cpp:177] Processed 51000 files.
E1029 08:35:19.016957 15214 convert_imageset.cpp:177] Processed 52000 files.
E1029 08:35:20.600097 15214 convert_imageset.cpp:177] Processed 53000 files.
E1029 08:35:22.238657 15214 convert_imageset.cpp:177] Processed 54000 files.
E1029 08:35:23.879057 15214 convert_imageset.cpp:177] Processed 55000 files.
E1029 08:35:25.451534 15214 convert_imageset.cpp:177] Processed 56000 files.
E1029 08:35:27.024632 15214 convert_imageset.cpp:177] Processed 57000 files.
E1029 08:35:28.576594 15214 convert_imageset.cpp:177] Processed 58000 files.
E1029 08:35:30.137622 15214 convert_imageset.cpp:177] Processed 59000 files.
E1029 08:35:31.749141 15214 convert_imageset.cpp:177] Processed 60000 files.
I1029 08:35:32.045079 15381 caffe.cpp:99] Use GPU with device ID 0
I1029 08:35:32.387855 15381 caffe.cpp:107] Starting Optimization
I1029 08:35:32.387980 15381 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 485000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 08:35:32.388005 15381 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 08:35:32.391466 15381 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 08:35:32.391690 15381 net.cpp:67] Creating Layer mnist
I1029 08:35:32.391716 15381 net.cpp:356] mnist -> data
I1029 08:35:32.391752 15381 net.cpp:356] mnist -> label
I1029 08:35:32.391785 15381 net.cpp:96] Setting up mnist
I1029 08:35:32.400796 15381 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 08:35:32.400909 15381 data_layer.cpp:128] output data size: 64,1,50,180
I1029 08:35:32.402429 15381 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 08:35:32.402461 15381 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 08:35:32.402485 15381 net.cpp:67] Creating Layer conv1
I1029 08:35:32.402498 15381 net.cpp:394] conv1 <- data
I1029 08:35:32.402523 15381 net.cpp:356] conv1 -> conv1
I1029 08:35:32.402549 15381 net.cpp:96] Setting up conv1
I1029 08:35:32.403305 15381 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 08:35:32.403362 15381 net.cpp:67] Creating Layer pool1
I1029 08:35:32.403375 15381 net.cpp:394] pool1 <- conv1
I1029 08:35:32.403393 15381 net.cpp:356] pool1 -> pool1
I1029 08:35:32.403411 15381 net.cpp:96] Setting up pool1
I1029 08:35:32.403439 15381 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 08:35:32.403455 15381 net.cpp:67] Creating Layer relu1
I1029 08:35:32.403465 15381 net.cpp:394] relu1 <- pool1
I1029 08:35:32.403480 15381 net.cpp:345] relu1 -> pool1 (in-place)
I1029 08:35:32.403493 15381 net.cpp:96] Setting up relu1
I1029 08:35:32.403504 15381 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 08:35:32.403520 15381 net.cpp:67] Creating Layer drop1
I1029 08:35:32.403532 15381 net.cpp:394] drop1 <- pool1
I1029 08:35:32.403548 15381 net.cpp:345] drop1 -> pool1 (in-place)
I1029 08:35:32.403571 15381 net.cpp:96] Setting up drop1
I1029 08:35:32.403584 15381 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 08:35:32.403599 15381 net.cpp:67] Creating Layer conv2
I1029 08:35:32.403610 15381 net.cpp:394] conv2 <- pool1
I1029 08:35:32.403628 15381 net.cpp:356] conv2 -> conv2
I1029 08:35:32.403646 15381 net.cpp:96] Setting up conv2
I1029 08:35:32.404778 15381 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 08:35:32.404804 15381 net.cpp:67] Creating Layer pool2
I1029 08:35:32.404813 15381 net.cpp:394] pool2 <- conv2
I1029 08:35:32.404822 15381 net.cpp:356] pool2 -> pool2
I1029 08:35:32.404832 15381 net.cpp:96] Setting up pool2
I1029 08:35:32.404840 15381 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 08:35:32.404850 15381 net.cpp:67] Creating Layer relu2
I1029 08:35:32.404855 15381 net.cpp:394] relu2 <- pool2
I1029 08:35:32.404866 15381 net.cpp:345] relu2 -> pool2 (in-place)
I1029 08:35:32.404875 15381 net.cpp:96] Setting up relu2
I1029 08:35:32.404881 15381 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 08:35:32.404891 15381 net.cpp:67] Creating Layer drop2
I1029 08:35:32.404897 15381 net.cpp:394] drop2 <- pool2
I1029 08:35:32.404909 15381 net.cpp:345] drop2 -> pool2 (in-place)
I1029 08:35:32.404918 15381 net.cpp:96] Setting up drop2
I1029 08:35:32.404925 15381 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 08:35:32.404935 15381 net.cpp:67] Creating Layer conv3
I1029 08:35:32.404942 15381 net.cpp:394] conv3 <- pool2
I1029 08:35:32.404950 15381 net.cpp:356] conv3 -> conv3
I1029 08:35:32.404960 15381 net.cpp:96] Setting up conv3
I1029 08:35:32.407043 15381 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 08:35:32.407071 15381 net.cpp:67] Creating Layer pool3
I1029 08:35:32.407078 15381 net.cpp:394] pool3 <- conv3
I1029 08:35:32.407090 15381 net.cpp:356] pool3 -> pool3
I1029 08:35:32.407101 15381 net.cpp:96] Setting up pool3
I1029 08:35:32.407109 15381 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 08:35:32.407119 15381 net.cpp:67] Creating Layer relu3
I1029 08:35:32.407124 15381 net.cpp:394] relu3 <- pool3
I1029 08:35:32.407135 15381 net.cpp:345] relu3 -> pool3 (in-place)
I1029 08:35:32.407143 15381 net.cpp:96] Setting up relu3
I1029 08:35:32.407150 15381 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 08:35:32.407158 15381 net.cpp:67] Creating Layer drop3
I1029 08:35:32.407165 15381 net.cpp:394] drop3 <- pool3
I1029 08:35:32.407172 15381 net.cpp:345] drop3 -> pool3 (in-place)
I1029 08:35:32.407181 15381 net.cpp:96] Setting up drop3
I1029 08:35:32.407187 15381 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 08:35:32.407197 15381 net.cpp:67] Creating Layer ip1
I1029 08:35:32.407203 15381 net.cpp:394] ip1 <- pool3
I1029 08:35:32.407215 15381 net.cpp:356] ip1 -> ip1
I1029 08:35:32.407258 15381 net.cpp:96] Setting up ip1
I1029 08:35:33.001723 15381 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 08:35:33.001780 15381 net.cpp:67] Creating Layer relu4
I1029 08:35:33.001788 15381 net.cpp:394] relu4 <- ip1
I1029 08:35:33.001797 15381 net.cpp:345] relu4 -> ip1 (in-place)
I1029 08:35:33.001806 15381 net.cpp:96] Setting up relu4
I1029 08:35:33.001812 15381 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 08:35:33.001819 15381 net.cpp:67] Creating Layer drop4
I1029 08:35:33.001824 15381 net.cpp:394] drop4 <- ip1
I1029 08:35:33.001832 15381 net.cpp:345] drop4 -> ip1 (in-place)
I1029 08:35:33.001838 15381 net.cpp:96] Setting up drop4
I1029 08:35:33.001843 15381 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 08:35:33.001854 15381 net.cpp:67] Creating Layer ip2
I1029 08:35:33.001859 15381 net.cpp:394] ip2 <- ip1
I1029 08:35:33.001868 15381 net.cpp:356] ip2 -> ip2
I1029 08:35:33.001876 15381 net.cpp:96] Setting up ip2
I1029 08:35:33.014963 15381 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 08:35:33.015025 15381 net.cpp:67] Creating Layer loss
I1029 08:35:33.015033 15381 net.cpp:394] loss <- ip2
I1029 08:35:33.015040 15381 net.cpp:394] loss <- label
I1029 08:35:33.015048 15381 net.cpp:356] loss -> loss
I1029 08:35:33.015058 15381 net.cpp:96] Setting up loss
I1029 08:35:33.015076 15381 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 08:35:33.015082 15381 net.cpp:109]     with loss weight 1
I1029 08:35:33.015128 15381 net.cpp:170] loss needs backward computation.
I1029 08:35:33.015133 15381 net.cpp:170] ip2 needs backward computation.
I1029 08:35:33.015138 15381 net.cpp:170] drop4 needs backward computation.
I1029 08:35:33.015142 15381 net.cpp:170] relu4 needs backward computation.
I1029 08:35:33.015147 15381 net.cpp:170] ip1 needs backward computation.
I1029 08:35:33.015151 15381 net.cpp:170] drop3 needs backward computation.
I1029 08:35:33.015156 15381 net.cpp:170] relu3 needs backward computation.
I1029 08:35:33.015161 15381 net.cpp:170] pool3 needs backward computation.
I1029 08:35:33.015164 15381 net.cpp:170] conv3 needs backward computation.
I1029 08:35:33.015169 15381 net.cpp:170] drop2 needs backward computation.
I1029 08:35:33.015173 15381 net.cpp:170] relu2 needs backward computation.
I1029 08:35:33.015178 15381 net.cpp:170] pool2 needs backward computation.
I1029 08:35:33.015182 15381 net.cpp:170] conv2 needs backward computation.
I1029 08:35:33.015187 15381 net.cpp:170] drop1 needs backward computation.
I1029 08:35:33.015192 15381 net.cpp:170] relu1 needs backward computation.
I1029 08:35:33.015195 15381 net.cpp:170] pool1 needs backward computation.
I1029 08:35:33.015200 15381 net.cpp:170] conv1 needs backward computation.
I1029 08:35:33.015204 15381 net.cpp:172] mnist does not need backward computation.
I1029 08:35:33.015209 15381 net.cpp:208] This network produces output loss
I1029 08:35:33.015219 15381 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 08:35:33.015226 15381 net.cpp:219] Network initialization done.
I1029 08:35:33.015230 15381 net.cpp:220] Memory required for data: 119788292
I1029 08:35:33.015291 15381 solver.cpp:41] Solver scaffolding done.
I1029 08:35:33.015297 15381 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_480000.solverstate
I1029 08:35:33.015302 15381 solver.cpp:160] Solving Captcha
I1029 08:35:33.015321 15381 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_480000.solverstate
I1029 08:35:39.880686 15381 solver.cpp:502] SGDSolver: restoring history
I1029 08:35:40.638392 15381 solver.cpp:191] Iteration 480000, loss = 2.4482
I1029 08:35:40.638445 15381 solver.cpp:206]     Train net output #0: loss = 2.4482 (* 1 = 2.4482 loss)
I1029 08:35:40.638460 15381 solver.cpp:403] Iteration 480000, lr = 0.000539949
I1029 08:39:43.077870 15381 solver.cpp:191] Iteration 481000, loss = 2.43475
I1029 08:39:43.078691 15381 solver.cpp:206]     Train net output #0: loss = 2.43475 (* 1 = 2.43475 loss)
I1029 08:39:43.078723 15381 solver.cpp:403] Iteration 481000, lr = 0.000539124
I1029 08:43:44.531633 15381 solver.cpp:191] Iteration 482000, loss = 2.27351
I1029 08:43:44.532222 15381 solver.cpp:206]     Train net output #0: loss = 2.27351 (* 1 = 2.27351 loss)
I1029 08:43:44.532258 15381 solver.cpp:403] Iteration 482000, lr = 0.000538302
I1029 08:47:46.071440 15381 solver.cpp:191] Iteration 483000, loss = 2.39206
I1029 08:47:46.072206 15381 solver.cpp:206]     Train net output #0: loss = 2.39206 (* 1 = 2.39206 loss)
I1029 08:47:46.072239 15381 solver.cpp:403] Iteration 483000, lr = 0.000537483
I1029 08:51:47.624699 15381 solver.cpp:191] Iteration 484000, loss = 2.45619
I1029 08:51:47.625260 15381 solver.cpp:206]     Train net output #0: loss = 2.45619 (* 1 = 2.45619 loss)
I1029 08:51:47.625296 15381 solver.cpp:403] Iteration 484000, lr = 0.000536667
I1029 08:55:49.789899 15381 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_485000.caffemodel
I1029 08:55:54.192764 15381 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_485000.solverstate
I1029 08:55:58.111958 15381 solver.cpp:228] Iteration 485000, loss = 2.34783
I1029 08:55:58.112457 15381 solver.cpp:233] Optimization Done.
I1029 08:55:58.112491 15381 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 09:17:53.818169 28850 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 09:17:53.818271 28850 net.cpp:358] Input 0 -> data
I1029 09:17:53.818300 28850 net.cpp:67] Creating Layer conv1
I1029 09:17:53.818305 28850 net.cpp:394] conv1 <- data
I1029 09:17:53.818312 28850 net.cpp:356] conv1 -> conv1
I1029 09:17:53.818322 28850 net.cpp:96] Setting up conv1
I1029 09:17:53.818650 28850 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 09:17:53.818670 28850 net.cpp:67] Creating Layer pool1
I1029 09:17:53.818675 28850 net.cpp:394] pool1 <- conv1
I1029 09:17:53.818681 28850 net.cpp:356] pool1 -> pool1
I1029 09:17:53.818688 28850 net.cpp:96] Setting up pool1
I1029 09:17:53.818699 28850 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 09:17:53.818706 28850 net.cpp:67] Creating Layer relu1
I1029 09:17:53.818711 28850 net.cpp:394] relu1 <- pool1
I1029 09:17:53.818716 28850 net.cpp:345] relu1 -> pool1 (in-place)
I1029 09:17:53.818722 28850 net.cpp:96] Setting up relu1
I1029 09:17:53.818725 28850 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 09:17:53.818732 28850 net.cpp:67] Creating Layer drop1
I1029 09:17:53.818735 28850 net.cpp:394] drop1 <- pool1
I1029 09:17:53.818743 28850 net.cpp:345] drop1 -> pool1 (in-place)
I1029 09:17:53.818753 28850 net.cpp:96] Setting up drop1
I1029 09:17:53.818759 28850 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 09:17:53.818765 28850 net.cpp:67] Creating Layer conv2
I1029 09:17:53.818769 28850 net.cpp:394] conv2 <- pool1
I1029 09:17:53.818778 28850 net.cpp:356] conv2 -> conv2
I1029 09:17:53.818784 28850 net.cpp:96] Setting up conv2
I1029 09:17:53.819361 28850 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 09:17:53.819376 28850 net.cpp:67] Creating Layer pool2
I1029 09:17:53.819381 28850 net.cpp:394] pool2 <- conv2
I1029 09:17:53.819386 28850 net.cpp:356] pool2 -> pool2
I1029 09:17:53.819393 28850 net.cpp:96] Setting up pool2
I1029 09:17:53.819399 28850 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 09:17:53.819407 28850 net.cpp:67] Creating Layer relu2
I1029 09:17:53.819411 28850 net.cpp:394] relu2 <- pool2
I1029 09:17:53.819416 28850 net.cpp:345] relu2 -> pool2 (in-place)
I1029 09:17:53.819422 28850 net.cpp:96] Setting up relu2
I1029 09:17:53.819427 28850 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 09:17:53.819432 28850 net.cpp:67] Creating Layer drop2
I1029 09:17:53.819435 28850 net.cpp:394] drop2 <- pool2
I1029 09:17:53.819442 28850 net.cpp:345] drop2 -> pool2 (in-place)
I1029 09:17:53.819448 28850 net.cpp:96] Setting up drop2
I1029 09:17:53.819453 28850 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 09:17:53.819459 28850 net.cpp:67] Creating Layer conv3
I1029 09:17:53.819463 28850 net.cpp:394] conv3 <- pool2
I1029 09:17:53.819469 28850 net.cpp:356] conv3 -> conv3
I1029 09:17:53.819475 28850 net.cpp:96] Setting up conv3
I1029 09:17:53.821033 28850 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 09:17:53.821056 28850 net.cpp:67] Creating Layer pool3
I1029 09:17:53.821063 28850 net.cpp:394] pool3 <- conv3
I1029 09:17:53.821069 28850 net.cpp:356] pool3 -> pool3
I1029 09:17:53.821075 28850 net.cpp:96] Setting up pool3
I1029 09:17:53.821080 28850 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 09:17:53.821086 28850 net.cpp:67] Creating Layer relu3
I1029 09:17:53.821090 28850 net.cpp:394] relu3 <- pool3
I1029 09:17:53.821097 28850 net.cpp:345] relu3 -> pool3 (in-place)
I1029 09:17:53.821102 28850 net.cpp:96] Setting up relu3
I1029 09:17:53.821107 28850 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 09:17:53.821112 28850 net.cpp:67] Creating Layer drop3
I1029 09:17:53.821116 28850 net.cpp:394] drop3 <- pool3
I1029 09:17:53.821121 28850 net.cpp:345] drop3 -> pool3 (in-place)
I1029 09:17:53.821126 28850 net.cpp:96] Setting up drop3
I1029 09:17:53.821131 28850 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 09:17:53.821137 28850 net.cpp:67] Creating Layer ip1
I1029 09:17:53.821141 28850 net.cpp:394] ip1 <- pool3
I1029 09:17:53.821149 28850 net.cpp:356] ip1 -> ip1
I1029 09:17:53.821156 28850 net.cpp:96] Setting up ip1
I1029 09:17:54.464107 28850 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 09:17:54.464169 28850 net.cpp:67] Creating Layer relu4
I1029 09:17:54.464176 28850 net.cpp:394] relu4 <- ip1
I1029 09:17:54.464185 28850 net.cpp:345] relu4 -> ip1 (in-place)
I1029 09:17:54.464195 28850 net.cpp:96] Setting up relu4
I1029 09:17:54.464200 28850 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 09:17:54.464206 28850 net.cpp:67] Creating Layer drop4
I1029 09:17:54.464210 28850 net.cpp:394] drop4 <- ip1
I1029 09:17:54.464215 28850 net.cpp:345] drop4 -> ip1 (in-place)
I1029 09:17:54.464222 28850 net.cpp:96] Setting up drop4
I1029 09:17:54.464227 28850 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 09:17:54.464237 28850 net.cpp:67] Creating Layer ip2
I1029 09:17:54.464241 28850 net.cpp:394] ip2 <- ip1
I1029 09:17:54.464247 28850 net.cpp:356] ip2 -> ip2
I1029 09:17:54.464262 28850 net.cpp:96] Setting up ip2
I1029 09:17:54.479985 28850 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 09:17:54.480062 28850 net.cpp:67] Creating Layer prob
I1029 09:17:54.480069 28850 net.cpp:394] prob <- ip2
I1029 09:17:54.480077 28850 net.cpp:356] prob -> prob
I1029 09:17:54.480087 28850 net.cpp:96] Setting up prob
I1029 09:17:54.480093 28850 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 09:17:54.480104 28850 net.cpp:172] prob does not need backward computation.
I1029 09:17:54.480109 28850 net.cpp:172] ip2 does not need backward computation.
I1029 09:17:54.480113 28850 net.cpp:172] drop4 does not need backward computation.
I1029 09:17:54.480116 28850 net.cpp:172] relu4 does not need backward computation.
I1029 09:17:54.480119 28850 net.cpp:172] ip1 does not need backward computation.
I1029 09:17:54.480123 28850 net.cpp:172] drop3 does not need backward computation.
I1029 09:17:54.480126 28850 net.cpp:172] relu3 does not need backward computation.
I1029 09:17:54.480130 28850 net.cpp:172] pool3 does not need backward computation.
I1029 09:17:54.480134 28850 net.cpp:172] conv3 does not need backward computation.
I1029 09:17:54.480137 28850 net.cpp:172] drop2 does not need backward computation.
I1029 09:17:54.480140 28850 net.cpp:172] relu2 does not need backward computation.
I1029 09:17:54.480144 28850 net.cpp:172] pool2 does not need backward computation.
I1029 09:17:54.480147 28850 net.cpp:172] conv2 does not need backward computation.
I1029 09:17:54.480151 28850 net.cpp:172] drop1 does not need backward computation.
I1029 09:17:54.480154 28850 net.cpp:172] relu1 does not need backward computation.
I1029 09:17:54.480159 28850 net.cpp:172] pool1 does not need backward computation.
I1029 09:17:54.480161 28850 net.cpp:172] conv1 does not need backward computation.
I1029 09:17:54.480165 28850 net.cpp:208] This network produces output prob
I1029 09:17:54.480178 28850 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 09:17:54.480186 28850 net.cpp:219] Network initialization done.
I1029 09:17:54.480190 28850 net.cpp:220] Memory required for data: 1837200
I1029 09:50:11.152309  4285 convert_imageset.cpp:70] Shuffling data
I1029 09:50:11.797611  4285 convert_imageset.cpp:73] A total of 60000 images.
I1029 09:50:11.802896  4285 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 09:50:13.922441  4285 convert_imageset.cpp:177] Processed 1000 files.
E1029 09:50:15.970079  4285 convert_imageset.cpp:177] Processed 2000 files.
E1029 09:50:17.923341  4285 convert_imageset.cpp:177] Processed 3000 files.
E1029 09:50:19.874954  4285 convert_imageset.cpp:177] Processed 4000 files.
E1029 09:50:21.849534  4285 convert_imageset.cpp:177] Processed 5000 files.
E1029 09:50:23.745916  4285 convert_imageset.cpp:177] Processed 6000 files.
E1029 09:50:25.709964  4285 convert_imageset.cpp:177] Processed 7000 files.
E1029 09:50:27.527192  4285 convert_imageset.cpp:177] Processed 8000 files.
E1029 09:50:29.314265  4285 convert_imageset.cpp:177] Processed 9000 files.
E1029 09:50:31.259778  4285 convert_imageset.cpp:177] Processed 10000 files.
E1029 09:50:33.164510  4285 convert_imageset.cpp:177] Processed 11000 files.
E1029 09:50:34.935817  4285 convert_imageset.cpp:177] Processed 12000 files.
E1029 09:50:36.649003  4285 convert_imageset.cpp:177] Processed 13000 files.
E1029 09:50:38.748570  4285 convert_imageset.cpp:177] Processed 14000 files.
E1029 09:50:40.435117  4285 convert_imageset.cpp:177] Processed 15000 files.
E1029 09:50:42.361671  4285 convert_imageset.cpp:177] Processed 16000 files.
E1029 09:50:44.071692  4285 convert_imageset.cpp:177] Processed 17000 files.
E1029 09:50:45.681752  4285 convert_imageset.cpp:177] Processed 18000 files.
E1029 09:50:47.352932  4285 convert_imageset.cpp:177] Processed 19000 files.
E1029 09:50:49.030216  4285 convert_imageset.cpp:177] Processed 20000 files.
E1029 09:50:50.639266  4285 convert_imageset.cpp:177] Processed 21000 files.
E1029 09:50:52.271107  4285 convert_imageset.cpp:177] Processed 22000 files.
E1029 09:50:53.993121  4285 convert_imageset.cpp:177] Processed 23000 files.
E1029 09:50:55.641922  4285 convert_imageset.cpp:177] Processed 24000 files.
E1029 09:50:57.196581  4285 convert_imageset.cpp:177] Processed 25000 files.
E1029 09:50:58.778540  4285 convert_imageset.cpp:177] Processed 26000 files.
E1029 09:51:00.515348  4285 convert_imageset.cpp:177] Processed 27000 files.
E1029 09:51:02.172687  4285 convert_imageset.cpp:177] Processed 28000 files.
E1029 09:51:03.808810  4285 convert_imageset.cpp:177] Processed 29000 files.
E1029 09:51:05.389646  4285 convert_imageset.cpp:177] Processed 30000 files.
E1029 09:51:06.899619  4285 convert_imageset.cpp:177] Processed 31000 files.
E1029 09:51:08.458307  4285 convert_imageset.cpp:177] Processed 32000 files.
E1029 09:51:10.049715  4285 convert_imageset.cpp:177] Processed 33000 files.
E1029 09:51:11.616701  4285 convert_imageset.cpp:177] Processed 34000 files.
E1029 09:51:13.218112  4285 convert_imageset.cpp:177] Processed 35000 files.
E1029 09:51:14.748234  4285 convert_imageset.cpp:177] Processed 36000 files.
E1029 09:51:16.360199  4285 convert_imageset.cpp:177] Processed 37000 files.
E1029 09:51:17.945754  4285 convert_imageset.cpp:177] Processed 38000 files.
E1029 09:51:19.455785  4285 convert_imageset.cpp:177] Processed 39000 files.
E1029 09:51:21.074632  4285 convert_imageset.cpp:177] Processed 40000 files.
E1029 09:51:22.678720  4285 convert_imageset.cpp:177] Processed 41000 files.
E1029 09:51:24.187093  4285 convert_imageset.cpp:177] Processed 42000 files.
E1029 09:51:25.776886  4285 convert_imageset.cpp:177] Processed 43000 files.
E1029 09:51:27.243049  4285 convert_imageset.cpp:177] Processed 44000 files.
E1029 09:51:28.681915  4285 convert_imageset.cpp:177] Processed 45000 files.
E1029 09:51:30.201978  4285 convert_imageset.cpp:177] Processed 46000 files.
E1029 09:51:31.679608  4285 convert_imageset.cpp:177] Processed 47000 files.
E1029 09:51:33.162621  4285 convert_imageset.cpp:177] Processed 48000 files.
E1029 09:51:34.671856  4285 convert_imageset.cpp:177] Processed 49000 files.
E1029 09:51:36.210139  4285 convert_imageset.cpp:177] Processed 50000 files.
E1029 09:51:37.733747  4285 convert_imageset.cpp:177] Processed 51000 files.
E1029 09:51:39.254292  4285 convert_imageset.cpp:177] Processed 52000 files.
E1029 09:51:40.789129  4285 convert_imageset.cpp:177] Processed 53000 files.
E1029 09:51:42.337854  4285 convert_imageset.cpp:177] Processed 54000 files.
E1029 09:51:43.783581  4285 convert_imageset.cpp:177] Processed 55000 files.
E1029 09:51:45.192106  4285 convert_imageset.cpp:177] Processed 56000 files.
E1029 09:51:46.621103  4285 convert_imageset.cpp:177] Processed 57000 files.
E1029 09:51:48.156008  4285 convert_imageset.cpp:177] Processed 58000 files.
E1029 09:51:49.658998  4285 convert_imageset.cpp:177] Processed 59000 files.
E1029 09:51:51.183629  4285 convert_imageset.cpp:177] Processed 60000 files.
I1029 09:51:51.373282  4385 caffe.cpp:99] Use GPU with device ID 0
I1029 09:51:51.756939  4385 caffe.cpp:107] Starting Optimization
I1029 09:51:51.757058  4385 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 490000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 09:51:51.757083  4385 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 09:51:51.771327  4385 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 09:51:51.771425  4385 net.cpp:67] Creating Layer mnist
I1029 09:51:51.771436  4385 net.cpp:356] mnist -> data
I1029 09:51:51.771455  4385 net.cpp:356] mnist -> label
I1029 09:51:51.771468  4385 net.cpp:96] Setting up mnist
I1029 09:51:51.778470  4385 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 09:51:51.778599  4385 data_layer.cpp:128] output data size: 64,1,50,180
I1029 09:51:51.779528  4385 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 09:51:51.779564  4385 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 09:51:51.779592  4385 net.cpp:67] Creating Layer conv1
I1029 09:51:51.779606  4385 net.cpp:394] conv1 <- data
I1029 09:51:51.779638  4385 net.cpp:356] conv1 -> conv1
I1029 09:51:51.779664  4385 net.cpp:96] Setting up conv1
I1029 09:51:51.780082  4385 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 09:51:51.780118  4385 net.cpp:67] Creating Layer pool1
I1029 09:51:51.780125  4385 net.cpp:394] pool1 <- conv1
I1029 09:51:51.780133  4385 net.cpp:356] pool1 -> pool1
I1029 09:51:51.780143  4385 net.cpp:96] Setting up pool1
I1029 09:51:51.780160  4385 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 09:51:51.780169  4385 net.cpp:67] Creating Layer relu1
I1029 09:51:51.780174  4385 net.cpp:394] relu1 <- pool1
I1029 09:51:51.780180  4385 net.cpp:345] relu1 -> pool1 (in-place)
I1029 09:51:51.780189  4385 net.cpp:96] Setting up relu1
I1029 09:51:51.780194  4385 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 09:51:51.780202  4385 net.cpp:67] Creating Layer drop1
I1029 09:51:51.780207  4385 net.cpp:394] drop1 <- pool1
I1029 09:51:51.780216  4385 net.cpp:345] drop1 -> pool1 (in-place)
I1029 09:51:51.780225  4385 net.cpp:96] Setting up drop1
I1029 09:51:51.780231  4385 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 09:51:51.780239  4385 net.cpp:67] Creating Layer conv2
I1029 09:51:51.780249  4385 net.cpp:394] conv2 <- pool1
I1029 09:51:51.780257  4385 net.cpp:356] conv2 -> conv2
I1029 09:51:51.780267  4385 net.cpp:96] Setting up conv2
I1029 09:51:51.781654  4385 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 09:51:51.781700  4385 net.cpp:67] Creating Layer pool2
I1029 09:51:51.781716  4385 net.cpp:394] pool2 <- conv2
I1029 09:51:51.781733  4385 net.cpp:356] pool2 -> pool2
I1029 09:51:51.781754  4385 net.cpp:96] Setting up pool2
I1029 09:51:51.781769  4385 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 09:51:51.781785  4385 net.cpp:67] Creating Layer relu2
I1029 09:51:51.781796  4385 net.cpp:394] relu2 <- pool2
I1029 09:51:51.781821  4385 net.cpp:345] relu2 -> pool2 (in-place)
I1029 09:51:51.781837  4385 net.cpp:96] Setting up relu2
I1029 09:51:51.781851  4385 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 09:51:51.781869  4385 net.cpp:67] Creating Layer drop2
I1029 09:51:51.781882  4385 net.cpp:394] drop2 <- pool2
I1029 09:51:51.781898  4385 net.cpp:345] drop2 -> pool2 (in-place)
I1029 09:51:51.781914  4385 net.cpp:96] Setting up drop2
I1029 09:51:51.781929  4385 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 09:51:51.781950  4385 net.cpp:67] Creating Layer conv3
I1029 09:51:51.781963  4385 net.cpp:394] conv3 <- pool2
I1029 09:51:51.781981  4385 net.cpp:356] conv3 -> conv3
I1029 09:51:51.782001  4385 net.cpp:96] Setting up conv3
I1029 09:51:51.786079  4385 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 09:51:51.786128  4385 net.cpp:67] Creating Layer pool3
I1029 09:51:51.786142  4385 net.cpp:394] pool3 <- conv3
I1029 09:51:51.786164  4385 net.cpp:356] pool3 -> pool3
I1029 09:51:51.786185  4385 net.cpp:96] Setting up pool3
I1029 09:51:51.786202  4385 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 09:51:51.786218  4385 net.cpp:67] Creating Layer relu3
I1029 09:51:51.786231  4385 net.cpp:394] relu3 <- pool3
I1029 09:51:51.786252  4385 net.cpp:345] relu3 -> pool3 (in-place)
I1029 09:51:51.786270  4385 net.cpp:96] Setting up relu3
I1029 09:51:51.786283  4385 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 09:51:51.786300  4385 net.cpp:67] Creating Layer drop3
I1029 09:51:51.786312  4385 net.cpp:394] drop3 <- pool3
I1029 09:51:51.786329  4385 net.cpp:345] drop3 -> pool3 (in-place)
I1029 09:51:51.786346  4385 net.cpp:96] Setting up drop3
I1029 09:51:51.786360  4385 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 09:51:51.786377  4385 net.cpp:67] Creating Layer ip1
I1029 09:51:51.786389  4385 net.cpp:394] ip1 <- pool3
I1029 09:51:51.786412  4385 net.cpp:356] ip1 -> ip1
I1029 09:51:51.786476  4385 net.cpp:96] Setting up ip1
I1029 09:51:52.240568  4385 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 09:51:52.240630  4385 net.cpp:67] Creating Layer relu4
I1029 09:51:52.240638  4385 net.cpp:394] relu4 <- ip1
I1029 09:51:52.240648  4385 net.cpp:345] relu4 -> ip1 (in-place)
I1029 09:51:52.240658  4385 net.cpp:96] Setting up relu4
I1029 09:51:52.240663  4385 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 09:51:52.240670  4385 net.cpp:67] Creating Layer drop4
I1029 09:51:52.240674  4385 net.cpp:394] drop4 <- ip1
I1029 09:51:52.240680  4385 net.cpp:345] drop4 -> ip1 (in-place)
I1029 09:51:52.240687  4385 net.cpp:96] Setting up drop4
I1029 09:51:52.240692  4385 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 09:51:52.240705  4385 net.cpp:67] Creating Layer ip2
I1029 09:51:52.240710  4385 net.cpp:394] ip2 <- ip1
I1029 09:51:52.240718  4385 net.cpp:356] ip2 -> ip2
I1029 09:51:52.240726  4385 net.cpp:96] Setting up ip2
I1029 09:51:52.251066  4385 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 09:51:52.251139  4385 net.cpp:67] Creating Layer loss
I1029 09:51:52.251147  4385 net.cpp:394] loss <- ip2
I1029 09:51:52.251155  4385 net.cpp:394] loss <- label
I1029 09:51:52.251163  4385 net.cpp:356] loss -> loss
I1029 09:51:52.251173  4385 net.cpp:96] Setting up loss
I1029 09:51:52.251185  4385 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 09:51:52.251261  4385 net.cpp:109]     with loss weight 1
I1029 09:51:52.251324  4385 net.cpp:170] loss needs backward computation.
I1029 09:51:52.251343  4385 net.cpp:170] ip2 needs backward computation.
I1029 09:51:52.251349  4385 net.cpp:170] drop4 needs backward computation.
I1029 09:51:52.251356  4385 net.cpp:170] relu4 needs backward computation.
I1029 09:51:52.251363  4385 net.cpp:170] ip1 needs backward computation.
I1029 09:51:52.251368  4385 net.cpp:170] drop3 needs backward computation.
I1029 09:51:52.251375  4385 net.cpp:170] relu3 needs backward computation.
I1029 09:51:52.251381  4385 net.cpp:170] pool3 needs backward computation.
I1029 09:51:52.251389  4385 net.cpp:170] conv3 needs backward computation.
I1029 09:51:52.251396  4385 net.cpp:170] drop2 needs backward computation.
I1029 09:51:52.251405  4385 net.cpp:170] relu2 needs backward computation.
I1029 09:51:52.251411  4385 net.cpp:170] pool2 needs backward computation.
I1029 09:51:52.251422  4385 net.cpp:170] conv2 needs backward computation.
I1029 09:51:52.251431  4385 net.cpp:170] drop1 needs backward computation.
I1029 09:51:52.251441  4385 net.cpp:170] relu1 needs backward computation.
I1029 09:51:52.251448  4385 net.cpp:170] pool1 needs backward computation.
I1029 09:51:52.251456  4385 net.cpp:170] conv1 needs backward computation.
I1029 09:51:52.251464  4385 net.cpp:172] mnist does not need backward computation.
I1029 09:51:52.251480  4385 net.cpp:208] This network produces output loss
I1029 09:51:52.251497  4385 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 09:51:52.251509  4385 net.cpp:219] Network initialization done.
I1029 09:51:52.251519  4385 net.cpp:220] Memory required for data: 119788292
I1029 09:51:52.251595  4385 solver.cpp:41] Solver scaffolding done.
I1029 09:51:52.251603  4385 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_485000.solverstate
I1029 09:51:52.251608  4385 solver.cpp:160] Solving Captcha
I1029 09:51:52.251627  4385 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_485000.solverstate
I1029 09:51:54.618276  4385 solver.cpp:502] SGDSolver: restoring history
I1029 09:51:55.363126  4385 solver.cpp:191] Iteration 485000, loss = 2.53128
I1029 09:51:55.363183  4385 solver.cpp:206]     Train net output #0: loss = 2.53128 (* 1 = 2.53128 loss)
I1029 09:51:55.363198  4385 solver.cpp:403] Iteration 485000, lr = 0.000535853
I1029 09:55:57.248170  4385 solver.cpp:191] Iteration 486000, loss = 2.52077
I1029 09:55:57.248822  4385 solver.cpp:206]     Train net output #0: loss = 2.52077 (* 1 = 2.52077 loss)
I1029 09:55:57.248857  4385 solver.cpp:403] Iteration 486000, lr = 0.000535043
I1029 09:59:58.480808  4385 solver.cpp:191] Iteration 487000, loss = 2.22565
I1029 09:59:58.481431  4385 solver.cpp:206]     Train net output #0: loss = 2.22565 (* 1 = 2.22565 loss)
I1029 09:59:58.481467  4385 solver.cpp:403] Iteration 487000, lr = 0.000534235
I1029 10:03:59.739186  4385 solver.cpp:191] Iteration 488000, loss = 2.53139
I1029 10:03:59.739773  4385 solver.cpp:206]     Train net output #0: loss = 2.53139 (* 1 = 2.53139 loss)
I1029 10:03:59.739809  4385 solver.cpp:403] Iteration 488000, lr = 0.000533431
I1029 10:08:01.151901  4385 solver.cpp:191] Iteration 489000, loss = 2.5333
I1029 10:08:01.152607  4385 solver.cpp:206]     Train net output #0: loss = 2.5333 (* 1 = 2.5333 loss)
I1029 10:08:01.152640  4385 solver.cpp:403] Iteration 489000, lr = 0.000532629
I1029 10:12:03.113885  4385 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_490000.caffemodel
I1029 10:12:08.530707  4385 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_490000.solverstate
I1029 10:12:13.415840  4385 solver.cpp:228] Iteration 490000, loss = 2.24384
I1029 10:12:13.416491  4385 solver.cpp:233] Optimization Done.
I1029 10:12:13.416517  4385 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 10:34:42.103785 18056 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 10:34:42.104480 18056 net.cpp:358] Input 0 -> data
I1029 10:34:42.104533 18056 net.cpp:67] Creating Layer conv1
I1029 10:34:42.104547 18056 net.cpp:394] conv1 <- data
I1029 10:34:42.104565 18056 net.cpp:356] conv1 -> conv1
I1029 10:34:42.104589 18056 net.cpp:96] Setting up conv1
I1029 10:34:42.105466 18056 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 10:34:42.105510 18056 net.cpp:67] Creating Layer pool1
I1029 10:34:42.105523 18056 net.cpp:394] pool1 <- conv1
I1029 10:34:42.105540 18056 net.cpp:356] pool1 -> pool1
I1029 10:34:42.105559 18056 net.cpp:96] Setting up pool1
I1029 10:34:42.105590 18056 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 10:34:42.105609 18056 net.cpp:67] Creating Layer relu1
I1029 10:34:42.105620 18056 net.cpp:394] relu1 <- pool1
I1029 10:34:42.105635 18056 net.cpp:345] relu1 -> pool1 (in-place)
I1029 10:34:42.105650 18056 net.cpp:96] Setting up relu1
I1029 10:34:42.105664 18056 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 10:34:42.105685 18056 net.cpp:67] Creating Layer drop1
I1029 10:34:42.105697 18056 net.cpp:394] drop1 <- pool1
I1029 10:34:42.105713 18056 net.cpp:345] drop1 -> pool1 (in-place)
I1029 10:34:42.105729 18056 net.cpp:96] Setting up drop1
I1029 10:34:42.105742 18056 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 10:34:42.105761 18056 net.cpp:67] Creating Layer conv2
I1029 10:34:42.105778 18056 net.cpp:394] conv2 <- pool1
I1029 10:34:42.105799 18056 net.cpp:356] conv2 -> conv2
I1029 10:34:42.105819 18056 net.cpp:96] Setting up conv2
I1029 10:34:42.107354 18056 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 10:34:42.107390 18056 net.cpp:67] Creating Layer pool2
I1029 10:34:42.107403 18056 net.cpp:394] pool2 <- conv2
I1029 10:34:42.107419 18056 net.cpp:356] pool2 -> pool2
I1029 10:34:42.107436 18056 net.cpp:96] Setting up pool2
I1029 10:34:42.107452 18056 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 10:34:42.107472 18056 net.cpp:67] Creating Layer relu2
I1029 10:34:42.107484 18056 net.cpp:394] relu2 <- pool2
I1029 10:34:42.107499 18056 net.cpp:345] relu2 -> pool2 (in-place)
I1029 10:34:42.107514 18056 net.cpp:96] Setting up relu2
I1029 10:34:42.107527 18056 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 10:34:42.107540 18056 net.cpp:67] Creating Layer drop2
I1029 10:34:42.107552 18056 net.cpp:394] drop2 <- pool2
I1029 10:34:42.107570 18056 net.cpp:345] drop2 -> pool2 (in-place)
I1029 10:34:42.107586 18056 net.cpp:96] Setting up drop2
I1029 10:34:42.107599 18056 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 10:34:42.107617 18056 net.cpp:67] Creating Layer conv3
I1029 10:34:42.107628 18056 net.cpp:394] conv3 <- pool2
I1029 10:34:42.107645 18056 net.cpp:356] conv3 -> conv3
I1029 10:34:42.107662 18056 net.cpp:96] Setting up conv3
I1029 10:34:42.109668 18056 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 10:34:42.109688 18056 net.cpp:67] Creating Layer pool3
I1029 10:34:42.109693 18056 net.cpp:394] pool3 <- conv3
I1029 10:34:42.109699 18056 net.cpp:356] pool3 -> pool3
I1029 10:34:42.109705 18056 net.cpp:96] Setting up pool3
I1029 10:34:42.109710 18056 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 10:34:42.109715 18056 net.cpp:67] Creating Layer relu3
I1029 10:34:42.109719 18056 net.cpp:394] relu3 <- pool3
I1029 10:34:42.109726 18056 net.cpp:345] relu3 -> pool3 (in-place)
I1029 10:34:42.109731 18056 net.cpp:96] Setting up relu3
I1029 10:34:42.109735 18056 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 10:34:42.109741 18056 net.cpp:67] Creating Layer drop3
I1029 10:34:42.109745 18056 net.cpp:394] drop3 <- pool3
I1029 10:34:42.109750 18056 net.cpp:345] drop3 -> pool3 (in-place)
I1029 10:34:42.109755 18056 net.cpp:96] Setting up drop3
I1029 10:34:42.109760 18056 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 10:34:42.109766 18056 net.cpp:67] Creating Layer ip1
I1029 10:34:42.109769 18056 net.cpp:394] ip1 <- pool3
I1029 10:34:42.109777 18056 net.cpp:356] ip1 -> ip1
I1029 10:34:42.109784 18056 net.cpp:96] Setting up ip1
I1029 10:34:42.546440 18056 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 10:34:42.546502 18056 net.cpp:67] Creating Layer relu4
I1029 10:34:42.546509 18056 net.cpp:394] relu4 <- ip1
I1029 10:34:42.546519 18056 net.cpp:345] relu4 -> ip1 (in-place)
I1029 10:34:42.546527 18056 net.cpp:96] Setting up relu4
I1029 10:34:42.546532 18056 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 10:34:42.546540 18056 net.cpp:67] Creating Layer drop4
I1029 10:34:42.546543 18056 net.cpp:394] drop4 <- ip1
I1029 10:34:42.546551 18056 net.cpp:345] drop4 -> ip1 (in-place)
I1029 10:34:42.546557 18056 net.cpp:96] Setting up drop4
I1029 10:34:42.546562 18056 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 10:34:42.546571 18056 net.cpp:67] Creating Layer ip2
I1029 10:34:42.546574 18056 net.cpp:394] ip2 <- ip1
I1029 10:34:42.546581 18056 net.cpp:356] ip2 -> ip2
I1029 10:34:42.546594 18056 net.cpp:96] Setting up ip2
I1029 10:34:42.555758 18056 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 10:34:42.555832 18056 net.cpp:67] Creating Layer prob
I1029 10:34:42.555840 18056 net.cpp:394] prob <- ip2
I1029 10:34:42.555848 18056 net.cpp:356] prob -> prob
I1029 10:34:42.555857 18056 net.cpp:96] Setting up prob
I1029 10:34:42.555863 18056 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 10:34:42.555868 18056 net.cpp:172] prob does not need backward computation.
I1029 10:34:42.555872 18056 net.cpp:172] ip2 does not need backward computation.
I1029 10:34:42.555876 18056 net.cpp:172] drop4 does not need backward computation.
I1029 10:34:42.555886 18056 net.cpp:172] relu4 does not need backward computation.
I1029 10:34:42.555889 18056 net.cpp:172] ip1 does not need backward computation.
I1029 10:34:42.555893 18056 net.cpp:172] drop3 does not need backward computation.
I1029 10:34:42.555896 18056 net.cpp:172] relu3 does not need backward computation.
I1029 10:34:42.555901 18056 net.cpp:172] pool3 does not need backward computation.
I1029 10:34:42.555904 18056 net.cpp:172] conv3 does not need backward computation.
I1029 10:34:42.555907 18056 net.cpp:172] drop2 does not need backward computation.
I1029 10:34:42.555912 18056 net.cpp:172] relu2 does not need backward computation.
I1029 10:34:42.555914 18056 net.cpp:172] pool2 does not need backward computation.
I1029 10:34:42.555918 18056 net.cpp:172] conv2 does not need backward computation.
I1029 10:34:42.555922 18056 net.cpp:172] drop1 does not need backward computation.
I1029 10:34:42.555925 18056 net.cpp:172] relu1 does not need backward computation.
I1029 10:34:42.555929 18056 net.cpp:172] pool1 does not need backward computation.
I1029 10:34:42.555932 18056 net.cpp:172] conv1 does not need backward computation.
I1029 10:34:42.555937 18056 net.cpp:208] This network produces output prob
I1029 10:34:42.555948 18056 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 10:34:42.555956 18056 net.cpp:219] Network initialization done.
I1029 10:34:42.555960 18056 net.cpp:220] Memory required for data: 1837200
I1029 11:08:26.117326 26445 convert_imageset.cpp:70] Shuffling data
I1029 11:08:26.790524 26445 convert_imageset.cpp:73] A total of 60000 images.
I1029 11:08:26.790601 26445 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 11:08:29.045037 26445 convert_imageset.cpp:177] Processed 1000 files.
E1029 11:08:31.403432 26445 convert_imageset.cpp:177] Processed 2000 files.
E1029 11:08:33.655418 26445 convert_imageset.cpp:177] Processed 3000 files.
E1029 11:08:35.839740 26445 convert_imageset.cpp:177] Processed 4000 files.
E1029 11:08:37.898200 26445 convert_imageset.cpp:177] Processed 5000 files.
E1029 11:08:39.933152 26445 convert_imageset.cpp:177] Processed 6000 files.
E1029 11:08:42.015959 26445 convert_imageset.cpp:177] Processed 7000 files.
E1029 11:08:44.274963 26445 convert_imageset.cpp:177] Processed 8000 files.
E1029 11:08:46.311892 26445 convert_imageset.cpp:177] Processed 9000 files.
E1029 11:08:48.638213 26445 convert_imageset.cpp:177] Processed 10000 files.
E1029 11:08:50.502846 26445 convert_imageset.cpp:177] Processed 11000 files.
E1029 11:08:52.503182 26445 convert_imageset.cpp:177] Processed 12000 files.
E1029 11:08:54.424473 26445 convert_imageset.cpp:177] Processed 13000 files.
E1029 11:08:56.341784 26445 convert_imageset.cpp:177] Processed 14000 files.
E1029 11:08:58.225000 26445 convert_imageset.cpp:177] Processed 15000 files.
E1029 11:09:00.165272 26445 convert_imageset.cpp:177] Processed 16000 files.
E1029 11:09:02.011976 26445 convert_imageset.cpp:177] Processed 17000 files.
E1029 11:09:03.885911 26445 convert_imageset.cpp:177] Processed 18000 files.
E1029 11:09:05.663601 26445 convert_imageset.cpp:177] Processed 19000 files.
E1029 11:09:07.427971 26445 convert_imageset.cpp:177] Processed 20000 files.
E1029 11:09:09.174520 26445 convert_imageset.cpp:177] Processed 21000 files.
E1029 11:09:10.983898 26445 convert_imageset.cpp:177] Processed 22000 files.
E1029 11:09:12.744086 26445 convert_imageset.cpp:177] Processed 23000 files.
E1029 11:09:14.646370 26445 convert_imageset.cpp:177] Processed 24000 files.
E1029 11:09:16.367143 26445 convert_imageset.cpp:177] Processed 25000 files.
E1029 11:09:18.015985 26445 convert_imageset.cpp:177] Processed 26000 files.
E1029 11:09:19.636281 26445 convert_imageset.cpp:177] Processed 27000 files.
E1029 11:09:21.223774 26445 convert_imageset.cpp:177] Processed 28000 files.
E1029 11:09:22.834214 26445 convert_imageset.cpp:177] Processed 29000 files.
E1029 11:09:24.399217 26445 convert_imageset.cpp:177] Processed 30000 files.
E1029 11:09:26.134716 26445 convert_imageset.cpp:177] Processed 31000 files.
E1029 11:09:27.885468 26445 convert_imageset.cpp:177] Processed 32000 files.
E1029 11:09:29.553009 26445 convert_imageset.cpp:177] Processed 33000 files.
E1029 11:09:31.075462 26445 convert_imageset.cpp:177] Processed 34000 files.
E1029 11:09:32.582340 26445 convert_imageset.cpp:177] Processed 35000 files.
E1029 11:09:34.241874 26445 convert_imageset.cpp:177] Processed 36000 files.
E1029 11:09:35.802996 26445 convert_imageset.cpp:177] Processed 37000 files.
E1029 11:09:37.397131 26445 convert_imageset.cpp:177] Processed 38000 files.
E1029 11:09:39.014286 26445 convert_imageset.cpp:177] Processed 39000 files.
E1029 11:09:40.578147 26445 convert_imageset.cpp:177] Processed 40000 files.
E1029 11:09:42.188997 26445 convert_imageset.cpp:177] Processed 41000 files.
E1029 11:09:43.708123 26445 convert_imageset.cpp:177] Processed 42000 files.
E1029 11:09:45.328182 26445 convert_imageset.cpp:177] Processed 43000 files.
E1029 11:09:46.879863 26445 convert_imageset.cpp:177] Processed 44000 files.
E1029 11:09:48.403990 26445 convert_imageset.cpp:177] Processed 45000 files.
E1029 11:09:49.861599 26445 convert_imageset.cpp:177] Processed 46000 files.
E1029 11:09:51.347724 26445 convert_imageset.cpp:177] Processed 47000 files.
E1029 11:09:52.986645 26445 convert_imageset.cpp:177] Processed 48000 files.
E1029 11:09:54.491581 26445 convert_imageset.cpp:177] Processed 49000 files.
E1029 11:09:56.024344 26445 convert_imageset.cpp:177] Processed 50000 files.
E1029 11:09:57.548622 26445 convert_imageset.cpp:177] Processed 51000 files.
E1029 11:09:58.999085 26445 convert_imageset.cpp:177] Processed 52000 files.
E1029 11:10:00.517894 26445 convert_imageset.cpp:177] Processed 53000 files.
E1029 11:10:01.980506 26445 convert_imageset.cpp:177] Processed 54000 files.
E1029 11:10:03.568117 26445 convert_imageset.cpp:177] Processed 55000 files.
E1029 11:10:05.134554 26445 convert_imageset.cpp:177] Processed 56000 files.
E1029 11:10:06.611651 26445 convert_imageset.cpp:177] Processed 57000 files.
E1029 11:10:08.086155 26445 convert_imageset.cpp:177] Processed 58000 files.
E1029 11:10:09.525413 26445 convert_imageset.cpp:177] Processed 59000 files.
E1029 11:10:10.988603 26445 convert_imageset.cpp:177] Processed 60000 files.
I1029 11:10:11.217293 26690 caffe.cpp:99] Use GPU with device ID 0
I1029 11:10:11.564383 26690 caffe.cpp:107] Starting Optimization
I1029 11:10:11.564554 26690 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 495000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 11:10:11.564584 26690 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 11:10:11.574693 26690 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 11:10:11.574915 26690 net.cpp:67] Creating Layer mnist
I1029 11:10:11.574940 26690 net.cpp:356] mnist -> data
I1029 11:10:11.574977 26690 net.cpp:356] mnist -> label
I1029 11:10:11.575009 26690 net.cpp:96] Setting up mnist
I1029 11:10:11.581518 26690 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 11:10:11.581647 26690 data_layer.cpp:128] output data size: 64,1,50,180
I1029 11:10:11.583350 26690 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 11:10:11.583384 26690 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 11:10:11.583415 26690 net.cpp:67] Creating Layer conv1
I1029 11:10:11.583430 26690 net.cpp:394] conv1 <- data
I1029 11:10:11.583461 26690 net.cpp:356] conv1 -> conv1
I1029 11:10:11.583487 26690 net.cpp:96] Setting up conv1
I1029 11:10:11.584245 26690 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 11:10:11.584298 26690 net.cpp:67] Creating Layer pool1
I1029 11:10:11.584311 26690 net.cpp:394] pool1 <- conv1
I1029 11:10:11.584329 26690 net.cpp:356] pool1 -> pool1
I1029 11:10:11.584347 26690 net.cpp:96] Setting up pool1
I1029 11:10:11.584374 26690 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 11:10:11.584389 26690 net.cpp:67] Creating Layer relu1
I1029 11:10:11.584400 26690 net.cpp:394] relu1 <- pool1
I1029 11:10:11.584414 26690 net.cpp:345] relu1 -> pool1 (in-place)
I1029 11:10:11.584477 26690 net.cpp:96] Setting up relu1
I1029 11:10:11.584491 26690 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 11:10:11.584507 26690 net.cpp:67] Creating Layer drop1
I1029 11:10:11.584517 26690 net.cpp:394] drop1 <- pool1
I1029 11:10:11.584537 26690 net.cpp:345] drop1 -> pool1 (in-place)
I1029 11:10:11.584553 26690 net.cpp:96] Setting up drop1
I1029 11:10:11.584564 26690 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 11:10:11.584580 26690 net.cpp:67] Creating Layer conv2
I1029 11:10:11.584590 26690 net.cpp:394] conv2 <- pool1
I1029 11:10:11.584609 26690 net.cpp:356] conv2 -> conv2
I1029 11:10:11.584626 26690 net.cpp:96] Setting up conv2
I1029 11:10:11.585887 26690 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 11:10:11.585925 26690 net.cpp:67] Creating Layer pool2
I1029 11:10:11.585938 26690 net.cpp:394] pool2 <- conv2
I1029 11:10:11.585952 26690 net.cpp:356] pool2 -> pool2
I1029 11:10:11.585968 26690 net.cpp:96] Setting up pool2
I1029 11:10:11.585980 26690 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 11:10:11.585994 26690 net.cpp:67] Creating Layer relu2
I1029 11:10:11.586004 26690 net.cpp:394] relu2 <- pool2
I1029 11:10:11.586021 26690 net.cpp:345] relu2 -> pool2 (in-place)
I1029 11:10:11.586036 26690 net.cpp:96] Setting up relu2
I1029 11:10:11.586046 26690 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 11:10:11.586062 26690 net.cpp:67] Creating Layer drop2
I1029 11:10:11.586072 26690 net.cpp:394] drop2 <- pool2
I1029 11:10:11.586089 26690 net.cpp:345] drop2 -> pool2 (in-place)
I1029 11:10:11.586104 26690 net.cpp:96] Setting up drop2
I1029 11:10:11.586115 26690 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 11:10:11.586132 26690 net.cpp:67] Creating Layer conv3
I1029 11:10:11.586141 26690 net.cpp:394] conv3 <- pool2
I1029 11:10:11.586156 26690 net.cpp:356] conv3 -> conv3
I1029 11:10:11.586172 26690 net.cpp:96] Setting up conv3
I1029 11:10:11.589687 26690 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 11:10:11.589733 26690 net.cpp:67] Creating Layer pool3
I1029 11:10:11.589746 26690 net.cpp:394] pool3 <- conv3
I1029 11:10:11.589766 26690 net.cpp:356] pool3 -> pool3
I1029 11:10:11.589786 26690 net.cpp:96] Setting up pool3
I1029 11:10:11.589800 26690 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 11:10:11.589815 26690 net.cpp:67] Creating Layer relu3
I1029 11:10:11.589828 26690 net.cpp:394] relu3 <- pool3
I1029 11:10:11.589846 26690 net.cpp:345] relu3 -> pool3 (in-place)
I1029 11:10:11.589862 26690 net.cpp:96] Setting up relu3
I1029 11:10:11.589874 26690 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 11:10:11.589890 26690 net.cpp:67] Creating Layer drop3
I1029 11:10:11.589900 26690 net.cpp:394] drop3 <- pool3
I1029 11:10:11.589915 26690 net.cpp:345] drop3 -> pool3 (in-place)
I1029 11:10:11.589931 26690 net.cpp:96] Setting up drop3
I1029 11:10:11.589943 26690 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 11:10:11.589959 26690 net.cpp:67] Creating Layer ip1
I1029 11:10:11.589970 26690 net.cpp:394] ip1 <- pool3
I1029 11:10:11.589990 26690 net.cpp:356] ip1 -> ip1
I1029 11:10:11.590051 26690 net.cpp:96] Setting up ip1
I1029 11:10:12.016793 26690 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 11:10:12.016862 26690 net.cpp:67] Creating Layer relu4
I1029 11:10:12.016870 26690 net.cpp:394] relu4 <- ip1
I1029 11:10:12.016880 26690 net.cpp:345] relu4 -> ip1 (in-place)
I1029 11:10:12.016890 26690 net.cpp:96] Setting up relu4
I1029 11:10:12.016896 26690 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 11:10:12.016902 26690 net.cpp:67] Creating Layer drop4
I1029 11:10:12.016907 26690 net.cpp:394] drop4 <- ip1
I1029 11:10:12.016916 26690 net.cpp:345] drop4 -> ip1 (in-place)
I1029 11:10:12.016922 26690 net.cpp:96] Setting up drop4
I1029 11:10:12.016928 26690 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 11:10:12.016939 26690 net.cpp:67] Creating Layer ip2
I1029 11:10:12.016943 26690 net.cpp:394] ip2 <- ip1
I1029 11:10:12.016952 26690 net.cpp:356] ip2 -> ip2
I1029 11:10:12.016960 26690 net.cpp:96] Setting up ip2
I1029 11:10:12.027096 26690 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 11:10:12.027155 26690 net.cpp:67] Creating Layer loss
I1029 11:10:12.027163 26690 net.cpp:394] loss <- ip2
I1029 11:10:12.027171 26690 net.cpp:394] loss <- label
I1029 11:10:12.027179 26690 net.cpp:356] loss -> loss
I1029 11:10:12.027189 26690 net.cpp:96] Setting up loss
I1029 11:10:12.027201 26690 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 11:10:12.027206 26690 net.cpp:109]     with loss weight 1
I1029 11:10:12.027242 26690 net.cpp:170] loss needs backward computation.
I1029 11:10:12.027247 26690 net.cpp:170] ip2 needs backward computation.
I1029 11:10:12.027252 26690 net.cpp:170] drop4 needs backward computation.
I1029 11:10:12.027257 26690 net.cpp:170] relu4 needs backward computation.
I1029 11:10:12.027268 26690 net.cpp:170] ip1 needs backward computation.
I1029 11:10:12.027273 26690 net.cpp:170] drop3 needs backward computation.
I1029 11:10:12.027278 26690 net.cpp:170] relu3 needs backward computation.
I1029 11:10:12.027282 26690 net.cpp:170] pool3 needs backward computation.
I1029 11:10:12.027287 26690 net.cpp:170] conv3 needs backward computation.
I1029 11:10:12.027292 26690 net.cpp:170] drop2 needs backward computation.
I1029 11:10:12.027297 26690 net.cpp:170] relu2 needs backward computation.
I1029 11:10:12.027302 26690 net.cpp:170] pool2 needs backward computation.
I1029 11:10:12.027307 26690 net.cpp:170] conv2 needs backward computation.
I1029 11:10:12.027312 26690 net.cpp:170] drop1 needs backward computation.
I1029 11:10:12.027315 26690 net.cpp:170] relu1 needs backward computation.
I1029 11:10:12.027320 26690 net.cpp:170] pool1 needs backward computation.
I1029 11:10:12.027324 26690 net.cpp:170] conv1 needs backward computation.
I1029 11:10:12.027329 26690 net.cpp:172] mnist does not need backward computation.
I1029 11:10:12.027334 26690 net.cpp:208] This network produces output loss
I1029 11:10:12.027344 26690 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 11:10:12.027353 26690 net.cpp:219] Network initialization done.
I1029 11:10:12.027356 26690 net.cpp:220] Memory required for data: 119788292
I1029 11:10:12.027416 26690 solver.cpp:41] Solver scaffolding done.
I1029 11:10:12.027422 26690 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_490000.solverstate
I1029 11:10:12.027427 26690 solver.cpp:160] Solving Captcha
I1029 11:10:12.027446 26690 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_490000.solverstate
I1029 11:10:15.162446 26690 solver.cpp:502] SGDSolver: restoring history
I1029 11:10:15.971994 26690 solver.cpp:191] Iteration 490000, loss = 2.54416
I1029 11:10:15.972059 26690 solver.cpp:206]     Train net output #0: loss = 2.54416 (* 1 = 2.54416 loss)
I1029 11:10:15.972074 26690 solver.cpp:403] Iteration 490000, lr = 0.00053183
I1029 11:14:17.656275 26690 solver.cpp:191] Iteration 491000, loss = 2.46151
I1029 11:14:17.656960 26690 solver.cpp:206]     Train net output #0: loss = 2.46151 (* 1 = 2.46151 loss)
I1029 11:14:17.656991 26690 solver.cpp:403] Iteration 491000, lr = 0.000531033
I1029 11:18:18.784859 26690 solver.cpp:191] Iteration 492000, loss = 2.49981
I1029 11:18:18.785500 26690 solver.cpp:206]     Train net output #0: loss = 2.49981 (* 1 = 2.49981 loss)
I1029 11:18:18.785537 26690 solver.cpp:403] Iteration 492000, lr = 0.00053024
I1029 11:22:19.944022 26690 solver.cpp:191] Iteration 493000, loss = 2.59461
I1029 11:22:19.944939 26690 solver.cpp:206]     Train net output #0: loss = 2.59461 (* 1 = 2.59461 loss)
I1029 11:22:19.944972 26690 solver.cpp:403] Iteration 493000, lr = 0.000529449
I1029 11:26:21.102952 26690 solver.cpp:191] Iteration 494000, loss = 2.19903
I1029 11:26:21.103767 26690 solver.cpp:206]     Train net output #0: loss = 2.19903 (* 1 = 2.19903 loss)
I1029 11:26:21.103788 26690 solver.cpp:403] Iteration 494000, lr = 0.000528661
I1029 11:30:22.879773 26690 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_495000.caffemodel
I1029 11:30:28.000783 26690 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_495000.solverstate
I1029 11:30:33.893925 26690 solver.cpp:228] Iteration 495000, loss = 2.51936
I1029 11:30:33.894428 26690 solver.cpp:233] Optimization Done.
I1029 11:30:33.894450 26690 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 11:54:06.057291  8345 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 11:54:06.057410  8345 net.cpp:358] Input 0 -> data
I1029 11:54:06.057436  8345 net.cpp:67] Creating Layer conv1
I1029 11:54:06.057443  8345 net.cpp:394] conv1 <- data
I1029 11:54:06.057451  8345 net.cpp:356] conv1 -> conv1
I1029 11:54:06.057462  8345 net.cpp:96] Setting up conv1
I1029 11:54:06.057845  8345 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 11:54:06.057867  8345 net.cpp:67] Creating Layer pool1
I1029 11:54:06.057873  8345 net.cpp:394] pool1 <- conv1
I1029 11:54:06.057879  8345 net.cpp:356] pool1 -> pool1
I1029 11:54:06.057888  8345 net.cpp:96] Setting up pool1
I1029 11:54:06.057905  8345 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 11:54:06.057914  8345 net.cpp:67] Creating Layer relu1
I1029 11:54:06.057919  8345 net.cpp:394] relu1 <- pool1
I1029 11:54:06.057925  8345 net.cpp:345] relu1 -> pool1 (in-place)
I1029 11:54:06.057932  8345 net.cpp:96] Setting up relu1
I1029 11:54:06.057937  8345 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 11:54:06.057945  8345 net.cpp:67] Creating Layer drop1
I1029 11:54:06.057948  8345 net.cpp:394] drop1 <- pool1
I1029 11:54:06.057958  8345 net.cpp:345] drop1 -> pool1 (in-place)
I1029 11:54:06.057966  8345 net.cpp:96] Setting up drop1
I1029 11:54:06.057972  8345 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 11:54:06.057981  8345 net.cpp:67] Creating Layer conv2
I1029 11:54:06.057986  8345 net.cpp:394] conv2 <- pool1
I1029 11:54:06.057994  8345 net.cpp:356] conv2 -> conv2
I1029 11:54:06.058002  8345 net.cpp:96] Setting up conv2
I1029 11:54:06.058686  8345 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 11:54:06.058707  8345 net.cpp:67] Creating Layer pool2
I1029 11:54:06.058713  8345 net.cpp:394] pool2 <- conv2
I1029 11:54:06.058720  8345 net.cpp:356] pool2 -> pool2
I1029 11:54:06.058729  8345 net.cpp:96] Setting up pool2
I1029 11:54:06.058737  8345 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 11:54:06.058743  8345 net.cpp:67] Creating Layer relu2
I1029 11:54:06.058748  8345 net.cpp:394] relu2 <- pool2
I1029 11:54:06.058756  8345 net.cpp:345] relu2 -> pool2 (in-place)
I1029 11:54:06.058763  8345 net.cpp:96] Setting up relu2
I1029 11:54:06.058768  8345 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 11:54:06.058775  8345 net.cpp:67] Creating Layer drop2
I1029 11:54:06.058780  8345 net.cpp:394] drop2 <- pool2
I1029 11:54:06.058786  8345 net.cpp:345] drop2 -> pool2 (in-place)
I1029 11:54:06.058796  8345 net.cpp:96] Setting up drop2
I1029 11:54:06.058802  8345 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 11:54:06.058811  8345 net.cpp:67] Creating Layer conv3
I1029 11:54:06.058815  8345 net.cpp:394] conv3 <- pool2
I1029 11:54:06.058822  8345 net.cpp:356] conv3 -> conv3
I1029 11:54:06.058830  8345 net.cpp:96] Setting up conv3
I1029 11:54:06.060634  8345 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 11:54:06.060650  8345 net.cpp:67] Creating Layer pool3
I1029 11:54:06.060655  8345 net.cpp:394] pool3 <- conv3
I1029 11:54:06.060663  8345 net.cpp:356] pool3 -> pool3
I1029 11:54:06.060670  8345 net.cpp:96] Setting up pool3
I1029 11:54:06.060675  8345 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 11:54:06.060680  8345 net.cpp:67] Creating Layer relu3
I1029 11:54:06.060684  8345 net.cpp:394] relu3 <- pool3
I1029 11:54:06.060691  8345 net.cpp:345] relu3 -> pool3 (in-place)
I1029 11:54:06.060696  8345 net.cpp:96] Setting up relu3
I1029 11:54:06.060700  8345 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 11:54:06.060706  8345 net.cpp:67] Creating Layer drop3
I1029 11:54:06.060710  8345 net.cpp:394] drop3 <- pool3
I1029 11:54:06.060715  8345 net.cpp:345] drop3 -> pool3 (in-place)
I1029 11:54:06.060720  8345 net.cpp:96] Setting up drop3
I1029 11:54:06.060724  8345 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 11:54:06.060731  8345 net.cpp:67] Creating Layer ip1
I1029 11:54:06.060734  8345 net.cpp:394] ip1 <- pool3
I1029 11:54:06.060742  8345 net.cpp:356] ip1 -> ip1
I1029 11:54:06.060750  8345 net.cpp:96] Setting up ip1
I1029 11:54:06.519716  8345 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 11:54:06.519778  8345 net.cpp:67] Creating Layer relu4
I1029 11:54:06.519784  8345 net.cpp:394] relu4 <- ip1
I1029 11:54:06.519793  8345 net.cpp:345] relu4 -> ip1 (in-place)
I1029 11:54:06.519804  8345 net.cpp:96] Setting up relu4
I1029 11:54:06.519809  8345 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 11:54:06.519815  8345 net.cpp:67] Creating Layer drop4
I1029 11:54:06.519819  8345 net.cpp:394] drop4 <- ip1
I1029 11:54:06.519825  8345 net.cpp:345] drop4 -> ip1 (in-place)
I1029 11:54:06.519831  8345 net.cpp:96] Setting up drop4
I1029 11:54:06.519836  8345 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 11:54:06.519846  8345 net.cpp:67] Creating Layer ip2
I1029 11:54:06.519851  8345 net.cpp:394] ip2 <- ip1
I1029 11:54:06.519856  8345 net.cpp:356] ip2 -> ip2
I1029 11:54:06.519868  8345 net.cpp:96] Setting up ip2
I1029 11:54:06.529314  8345 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 11:54:06.529371  8345 net.cpp:67] Creating Layer prob
I1029 11:54:06.529378  8345 net.cpp:394] prob <- ip2
I1029 11:54:06.529386  8345 net.cpp:356] prob -> prob
I1029 11:54:06.529395  8345 net.cpp:96] Setting up prob
I1029 11:54:06.529402  8345 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 11:54:06.529407  8345 net.cpp:172] prob does not need backward computation.
I1029 11:54:06.529410  8345 net.cpp:172] ip2 does not need backward computation.
I1029 11:54:06.529413  8345 net.cpp:172] drop4 does not need backward computation.
I1029 11:54:06.529417  8345 net.cpp:172] relu4 does not need backward computation.
I1029 11:54:06.529420  8345 net.cpp:172] ip1 does not need backward computation.
I1029 11:54:06.529424  8345 net.cpp:172] drop3 does not need backward computation.
I1029 11:54:06.529434  8345 net.cpp:172] relu3 does not need backward computation.
I1029 11:54:06.529438  8345 net.cpp:172] pool3 does not need backward computation.
I1029 11:54:06.529441  8345 net.cpp:172] conv3 does not need backward computation.
I1029 11:54:06.529445  8345 net.cpp:172] drop2 does not need backward computation.
I1029 11:54:06.529449  8345 net.cpp:172] relu2 does not need backward computation.
I1029 11:54:06.529453  8345 net.cpp:172] pool2 does not need backward computation.
I1029 11:54:06.529456  8345 net.cpp:172] conv2 does not need backward computation.
I1029 11:54:06.529459  8345 net.cpp:172] drop1 does not need backward computation.
I1029 11:54:06.529464  8345 net.cpp:172] relu1 does not need backward computation.
I1029 11:54:06.529466  8345 net.cpp:172] pool1 does not need backward computation.
I1029 11:54:06.529470  8345 net.cpp:172] conv1 does not need backward computation.
I1029 11:54:06.529474  8345 net.cpp:208] This network produces output prob
I1029 11:54:06.529486  8345 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 11:54:06.529495  8345 net.cpp:219] Network initialization done.
I1029 11:54:06.529498  8345 net.cpp:220] Memory required for data: 1837200
I1029 12:28:12.739648 16870 convert_imageset.cpp:70] Shuffling data
I1029 12:28:13.432301 16870 convert_imageset.cpp:73] A total of 60000 images.
I1029 12:28:13.432382 16870 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 12:28:15.849308 16870 convert_imageset.cpp:177] Processed 1000 files.
E1029 12:28:17.897868 16870 convert_imageset.cpp:177] Processed 2000 files.
E1029 12:28:20.174319 16870 convert_imageset.cpp:177] Processed 3000 files.
E1029 12:28:22.126538 16870 convert_imageset.cpp:177] Processed 4000 files.
E1029 12:28:23.957186 16870 convert_imageset.cpp:177] Processed 5000 files.
E1029 12:28:25.815198 16870 convert_imageset.cpp:177] Processed 6000 files.
E1029 12:28:27.668588 16870 convert_imageset.cpp:177] Processed 7000 files.
E1029 12:28:29.513828 16870 convert_imageset.cpp:177] Processed 8000 files.
E1029 12:28:31.247331 16870 convert_imageset.cpp:177] Processed 9000 files.
E1029 12:28:32.947945 16870 convert_imageset.cpp:177] Processed 10000 files.
E1029 12:28:34.764873 16870 convert_imageset.cpp:177] Processed 11000 files.
E1029 12:28:36.571130 16870 convert_imageset.cpp:177] Processed 12000 files.
E1029 12:28:38.283629 16870 convert_imageset.cpp:177] Processed 13000 files.
E1029 12:28:40.029806 16870 convert_imageset.cpp:177] Processed 14000 files.
E1029 12:28:41.752975 16870 convert_imageset.cpp:177] Processed 15000 files.
E1029 12:28:43.430915 16870 convert_imageset.cpp:177] Processed 16000 files.
E1029 12:28:45.051029 16870 convert_imageset.cpp:177] Processed 17000 files.
E1029 12:28:46.736588 16870 convert_imageset.cpp:177] Processed 18000 files.
E1029 12:28:48.474674 16870 convert_imageset.cpp:177] Processed 19000 files.
E1029 12:28:50.164006 16870 convert_imageset.cpp:177] Processed 20000 files.
E1029 12:28:51.777523 16870 convert_imageset.cpp:177] Processed 21000 files.
E1029 12:28:53.345537 16870 convert_imageset.cpp:177] Processed 22000 files.
E1029 12:28:54.912317 16870 convert_imageset.cpp:177] Processed 23000 files.
E1029 12:28:56.764192 16870 convert_imageset.cpp:177] Processed 24000 files.
E1029 12:28:58.477298 16870 convert_imageset.cpp:177] Processed 25000 files.
E1029 12:28:59.992488 16870 convert_imageset.cpp:177] Processed 26000 files.
E1029 12:29:01.555814 16870 convert_imageset.cpp:177] Processed 27000 files.
E1029 12:29:03.408303 16870 convert_imageset.cpp:177] Processed 28000 files.
E1029 12:29:05.041419 16870 convert_imageset.cpp:177] Processed 29000 files.
E1029 12:29:06.722820 16870 convert_imageset.cpp:177] Processed 30000 files.
E1029 12:29:08.283491 16870 convert_imageset.cpp:177] Processed 31000 files.
E1029 12:29:09.815531 16870 convert_imageset.cpp:177] Processed 32000 files.
E1029 12:29:11.427175 16870 convert_imageset.cpp:177] Processed 33000 files.
E1029 12:29:12.993955 16870 convert_imageset.cpp:177] Processed 34000 files.
E1029 12:29:14.863291 16870 convert_imageset.cpp:177] Processed 35000 files.
E1029 12:29:16.410400 16870 convert_imageset.cpp:177] Processed 36000 files.
E1029 12:29:17.951899 16870 convert_imageset.cpp:177] Processed 37000 files.
E1029 12:29:19.489941 16870 convert_imageset.cpp:177] Processed 38000 files.
E1029 12:29:21.181629 16870 convert_imageset.cpp:177] Processed 39000 files.
E1029 12:29:22.763968 16870 convert_imageset.cpp:177] Processed 40000 files.
E1029 12:29:24.360611 16870 convert_imageset.cpp:177] Processed 41000 files.
E1029 12:29:26.137338 16870 convert_imageset.cpp:177] Processed 42000 files.
E1029 12:29:27.747254 16870 convert_imageset.cpp:177] Processed 43000 files.
E1029 12:29:29.262703 16870 convert_imageset.cpp:177] Processed 44000 files.
E1029 12:29:30.737262 16870 convert_imageset.cpp:177] Processed 45000 files.
E1029 12:29:32.225534 16870 convert_imageset.cpp:177] Processed 46000 files.
E1029 12:29:33.765118 16870 convert_imageset.cpp:177] Processed 47000 files.
E1029 12:29:35.320740 16870 convert_imageset.cpp:177] Processed 48000 files.
E1029 12:29:36.877041 16870 convert_imageset.cpp:177] Processed 49000 files.
E1029 12:29:38.390841 16870 convert_imageset.cpp:177] Processed 50000 files.
E1029 12:29:40.002372 16870 convert_imageset.cpp:177] Processed 51000 files.
E1029 12:29:41.477533 16870 convert_imageset.cpp:177] Processed 52000 files.
E1029 12:29:43.062605 16870 convert_imageset.cpp:177] Processed 53000 files.
E1029 12:29:44.635025 16870 convert_imageset.cpp:177] Processed 54000 files.
E1029 12:29:46.079617 16870 convert_imageset.cpp:177] Processed 55000 files.
E1029 12:29:47.649734 16870 convert_imageset.cpp:177] Processed 56000 files.
E1029 12:29:49.096052 16870 convert_imageset.cpp:177] Processed 57000 files.
E1029 12:29:50.638536 16870 convert_imageset.cpp:177] Processed 58000 files.
E1029 12:29:52.112414 16870 convert_imageset.cpp:177] Processed 59000 files.
E1029 12:29:53.652408 16870 convert_imageset.cpp:177] Processed 60000 files.
I1029 12:29:53.876127 17027 caffe.cpp:99] Use GPU with device ID 0
I1029 12:29:54.259579 17027 caffe.cpp:107] Starting Optimization
I1029 12:29:54.259711 17027 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 500000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 12:29:54.259738 17027 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 12:29:54.270047 17027 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 12:29:54.270144 17027 net.cpp:67] Creating Layer mnist
I1029 12:29:54.270155 17027 net.cpp:356] mnist -> data
I1029 12:29:54.270172 17027 net.cpp:356] mnist -> label
I1029 12:29:54.270185 17027 net.cpp:96] Setting up mnist
I1029 12:29:54.279048 17027 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 12:29:54.279182 17027 data_layer.cpp:128] output data size: 64,1,50,180
I1029 12:29:54.280125 17027 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 12:29:54.280164 17027 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 12:29:54.280191 17027 net.cpp:67] Creating Layer conv1
I1029 12:29:54.280205 17027 net.cpp:394] conv1 <- data
I1029 12:29:54.280239 17027 net.cpp:356] conv1 -> conv1
I1029 12:29:54.280267 17027 net.cpp:96] Setting up conv1
I1029 12:29:54.281215 17027 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 12:29:54.281285 17027 net.cpp:67] Creating Layer pool1
I1029 12:29:54.281301 17027 net.cpp:394] pool1 <- conv1
I1029 12:29:54.281319 17027 net.cpp:356] pool1 -> pool1
I1029 12:29:54.281339 17027 net.cpp:96] Setting up pool1
I1029 12:29:54.281371 17027 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 12:29:54.281390 17027 net.cpp:67] Creating Layer relu1
I1029 12:29:54.281404 17027 net.cpp:394] relu1 <- pool1
I1029 12:29:54.281420 17027 net.cpp:345] relu1 -> pool1 (in-place)
I1029 12:29:54.281437 17027 net.cpp:96] Setting up relu1
I1029 12:29:54.281450 17027 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 12:29:54.281469 17027 net.cpp:67] Creating Layer drop1
I1029 12:29:54.281482 17027 net.cpp:394] drop1 <- pool1
I1029 12:29:54.281504 17027 net.cpp:345] drop1 -> pool1 (in-place)
I1029 12:29:54.281523 17027 net.cpp:96] Setting up drop1
I1029 12:29:54.281538 17027 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 12:29:54.281555 17027 net.cpp:67] Creating Layer conv2
I1029 12:29:54.281569 17027 net.cpp:394] conv2 <- pool1
I1029 12:29:54.281590 17027 net.cpp:356] conv2 -> conv2
I1029 12:29:54.281612 17027 net.cpp:96] Setting up conv2
I1029 12:29:54.283149 17027 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 12:29:54.283190 17027 net.cpp:67] Creating Layer pool2
I1029 12:29:54.283205 17027 net.cpp:394] pool2 <- conv2
I1029 12:29:54.283223 17027 net.cpp:356] pool2 -> pool2
I1029 12:29:54.283252 17027 net.cpp:96] Setting up pool2
I1029 12:29:54.283268 17027 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 12:29:54.283284 17027 net.cpp:67] Creating Layer relu2
I1029 12:29:54.283298 17027 net.cpp:394] relu2 <- pool2
I1029 12:29:54.283318 17027 net.cpp:345] relu2 -> pool2 (in-place)
I1029 12:29:54.283336 17027 net.cpp:96] Setting up relu2
I1029 12:29:54.283349 17027 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 12:29:54.283368 17027 net.cpp:67] Creating Layer drop2
I1029 12:29:54.283381 17027 net.cpp:394] drop2 <- pool2
I1029 12:29:54.283401 17027 net.cpp:345] drop2 -> pool2 (in-place)
I1029 12:29:54.283421 17027 net.cpp:96] Setting up drop2
I1029 12:29:54.283434 17027 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 12:29:54.283453 17027 net.cpp:67] Creating Layer conv3
I1029 12:29:54.283465 17027 net.cpp:394] conv3 <- pool2
I1029 12:29:54.283483 17027 net.cpp:356] conv3 -> conv3
I1029 12:29:54.283502 17027 net.cpp:96] Setting up conv3
I1029 12:29:54.287585 17027 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 12:29:54.287632 17027 net.cpp:67] Creating Layer pool3
I1029 12:29:54.287647 17027 net.cpp:394] pool3 <- conv3
I1029 12:29:54.287669 17027 net.cpp:356] pool3 -> pool3
I1029 12:29:54.287691 17027 net.cpp:96] Setting up pool3
I1029 12:29:54.287708 17027 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 12:29:54.287725 17027 net.cpp:67] Creating Layer relu3
I1029 12:29:54.287737 17027 net.cpp:394] relu3 <- pool3
I1029 12:29:54.287758 17027 net.cpp:345] relu3 -> pool3 (in-place)
I1029 12:29:54.287776 17027 net.cpp:96] Setting up relu3
I1029 12:29:54.287788 17027 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 12:29:54.287806 17027 net.cpp:67] Creating Layer drop3
I1029 12:29:54.287818 17027 net.cpp:394] drop3 <- pool3
I1029 12:29:54.287834 17027 net.cpp:345] drop3 -> pool3 (in-place)
I1029 12:29:54.287852 17027 net.cpp:96] Setting up drop3
I1029 12:29:54.287865 17027 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 12:29:54.287883 17027 net.cpp:67] Creating Layer ip1
I1029 12:29:54.287896 17027 net.cpp:394] ip1 <- pool3
I1029 12:29:54.287919 17027 net.cpp:356] ip1 -> ip1
I1029 12:29:54.287981 17027 net.cpp:96] Setting up ip1
I1029 12:29:54.747061 17027 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 12:29:54.747123 17027 net.cpp:67] Creating Layer relu4
I1029 12:29:54.747129 17027 net.cpp:394] relu4 <- ip1
I1029 12:29:54.747139 17027 net.cpp:345] relu4 -> ip1 (in-place)
I1029 12:29:54.747148 17027 net.cpp:96] Setting up relu4
I1029 12:29:54.747153 17027 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 12:29:54.747160 17027 net.cpp:67] Creating Layer drop4
I1029 12:29:54.747164 17027 net.cpp:394] drop4 <- ip1
I1029 12:29:54.747174 17027 net.cpp:345] drop4 -> ip1 (in-place)
I1029 12:29:54.747179 17027 net.cpp:96] Setting up drop4
I1029 12:29:54.747185 17027 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 12:29:54.747196 17027 net.cpp:67] Creating Layer ip2
I1029 12:29:54.747200 17027 net.cpp:394] ip2 <- ip1
I1029 12:29:54.747210 17027 net.cpp:356] ip2 -> ip2
I1029 12:29:54.747218 17027 net.cpp:96] Setting up ip2
I1029 12:29:54.755344 17027 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 12:29:54.755400 17027 net.cpp:67] Creating Layer loss
I1029 12:29:54.755408 17027 net.cpp:394] loss <- ip2
I1029 12:29:54.755415 17027 net.cpp:394] loss <- label
I1029 12:29:54.755422 17027 net.cpp:356] loss -> loss
I1029 12:29:54.755432 17027 net.cpp:96] Setting up loss
I1029 12:29:54.755445 17027 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 12:29:54.755450 17027 net.cpp:109]     with loss weight 1
I1029 12:29:54.755491 17027 net.cpp:170] loss needs backward computation.
I1029 12:29:54.755497 17027 net.cpp:170] ip2 needs backward computation.
I1029 12:29:54.755501 17027 net.cpp:170] drop4 needs backward computation.
I1029 12:29:54.755506 17027 net.cpp:170] relu4 needs backward computation.
I1029 12:29:54.755511 17027 net.cpp:170] ip1 needs backward computation.
I1029 12:29:54.755516 17027 net.cpp:170] drop3 needs backward computation.
I1029 12:29:54.755527 17027 net.cpp:170] relu3 needs backward computation.
I1029 12:29:54.755532 17027 net.cpp:170] pool3 needs backward computation.
I1029 12:29:54.755537 17027 net.cpp:170] conv3 needs backward computation.
I1029 12:29:54.755542 17027 net.cpp:170] drop2 needs backward computation.
I1029 12:29:54.755545 17027 net.cpp:170] relu2 needs backward computation.
I1029 12:29:54.755550 17027 net.cpp:170] pool2 needs backward computation.
I1029 12:29:54.755555 17027 net.cpp:170] conv2 needs backward computation.
I1029 12:29:54.755560 17027 net.cpp:170] drop1 needs backward computation.
I1029 12:29:54.755564 17027 net.cpp:170] relu1 needs backward computation.
I1029 12:29:54.755569 17027 net.cpp:170] pool1 needs backward computation.
I1029 12:29:54.755574 17027 net.cpp:170] conv1 needs backward computation.
I1029 12:29:54.755578 17027 net.cpp:172] mnist does not need backward computation.
I1029 12:29:54.755584 17027 net.cpp:208] This network produces output loss
I1029 12:29:54.755594 17027 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 12:29:54.755601 17027 net.cpp:219] Network initialization done.
I1029 12:29:54.755605 17027 net.cpp:220] Memory required for data: 119788292
I1029 12:29:54.755666 17027 solver.cpp:41] Solver scaffolding done.
I1029 12:29:54.755672 17027 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_495000.solverstate
I1029 12:29:54.755677 17027 solver.cpp:160] Solving Captcha
I1029 12:29:54.755697 17027 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_495000.solverstate
I1029 12:30:00.067724 17027 solver.cpp:502] SGDSolver: restoring history
I1029 12:30:00.812677 17027 solver.cpp:191] Iteration 495000, loss = 2.41619
I1029 12:30:00.812741 17027 solver.cpp:206]     Train net output #0: loss = 2.41619 (* 1 = 2.41619 loss)
I1029 12:30:00.812755 17027 solver.cpp:403] Iteration 495000, lr = 0.000527875
I1029 12:34:02.783493 17027 solver.cpp:191] Iteration 496000, loss = 2.43839
I1029 12:34:02.796728 17027 solver.cpp:206]     Train net output #0: loss = 2.43839 (* 1 = 2.43839 loss)
I1029 12:34:02.796764 17027 solver.cpp:403] Iteration 496000, lr = 0.000527093
I1029 12:38:04.324568 17027 solver.cpp:191] Iteration 497000, loss = 2.53296
I1029 12:38:04.325206 17027 solver.cpp:206]     Train net output #0: loss = 2.53296 (* 1 = 2.53296 loss)
I1029 12:38:04.325239 17027 solver.cpp:403] Iteration 497000, lr = 0.000526313
I1029 12:42:05.924348 17027 solver.cpp:191] Iteration 498000, loss = 2.39016
I1029 12:42:05.925214 17027 solver.cpp:206]     Train net output #0: loss = 2.39016 (* 1 = 2.39016 loss)
I1029 12:42:05.925246 17027 solver.cpp:403] Iteration 498000, lr = 0.000525536
I1029 12:46:07.494694 17027 solver.cpp:191] Iteration 499000, loss = 2.36103
I1029 12:46:07.495352 17027 solver.cpp:206]     Train net output #0: loss = 2.36103 (* 1 = 2.36103 loss)
I1029 12:46:07.495365 17027 solver.cpp:403] Iteration 499000, lr = 0.000524761
I1029 12:50:09.691443 17027 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_500000.caffemodel
I1029 12:50:14.609966 17027 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_500000.solverstate
I1029 12:50:18.348958 17027 solver.cpp:228] Iteration 500000, loss = 2.20929
I1029 12:50:18.349606 17027 solver.cpp:233] Optimization Done.
I1029 12:50:18.349629 17027 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 13:13:25.339424 30919 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 13:13:25.339525 30919 net.cpp:358] Input 0 -> data
I1029 13:13:25.339555 30919 net.cpp:67] Creating Layer conv1
I1029 13:13:25.339560 30919 net.cpp:394] conv1 <- data
I1029 13:13:25.339566 30919 net.cpp:356] conv1 -> conv1
I1029 13:13:25.339576 30919 net.cpp:96] Setting up conv1
I1029 13:13:25.339887 30919 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 13:13:25.339906 30919 net.cpp:67] Creating Layer pool1
I1029 13:13:25.339911 30919 net.cpp:394] pool1 <- conv1
I1029 13:13:25.339917 30919 net.cpp:356] pool1 -> pool1
I1029 13:13:25.339925 30919 net.cpp:96] Setting up pool1
I1029 13:13:25.339939 30919 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 13:13:25.339947 30919 net.cpp:67] Creating Layer relu1
I1029 13:13:25.339951 30919 net.cpp:394] relu1 <- pool1
I1029 13:13:25.339959 30919 net.cpp:345] relu1 -> pool1 (in-place)
I1029 13:13:25.339965 30919 net.cpp:96] Setting up relu1
I1029 13:13:25.339969 30919 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 13:13:25.339975 30919 net.cpp:67] Creating Layer drop1
I1029 13:13:25.339979 30919 net.cpp:394] drop1 <- pool1
I1029 13:13:25.339984 30919 net.cpp:345] drop1 -> pool1 (in-place)
I1029 13:13:25.339990 30919 net.cpp:96] Setting up drop1
I1029 13:13:25.339995 30919 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 13:13:25.340003 30919 net.cpp:67] Creating Layer conv2
I1029 13:13:25.340006 30919 net.cpp:394] conv2 <- pool1
I1029 13:13:25.340013 30919 net.cpp:356] conv2 -> conv2
I1029 13:13:25.340018 30919 net.cpp:96] Setting up conv2
I1029 13:13:25.340605 30919 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 13:13:25.340620 30919 net.cpp:67] Creating Layer pool2
I1029 13:13:25.340626 30919 net.cpp:394] pool2 <- conv2
I1029 13:13:25.340634 30919 net.cpp:356] pool2 -> pool2
I1029 13:13:25.340646 30919 net.cpp:96] Setting up pool2
I1029 13:13:25.340651 30919 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 13:13:25.340657 30919 net.cpp:67] Creating Layer relu2
I1029 13:13:25.340662 30919 net.cpp:394] relu2 <- pool2
I1029 13:13:25.340667 30919 net.cpp:345] relu2 -> pool2 (in-place)
I1029 13:13:25.340672 30919 net.cpp:96] Setting up relu2
I1029 13:13:25.340677 30919 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 13:13:25.340683 30919 net.cpp:67] Creating Layer drop2
I1029 13:13:25.340687 30919 net.cpp:394] drop2 <- pool2
I1029 13:13:25.340693 30919 net.cpp:345] drop2 -> pool2 (in-place)
I1029 13:13:25.340698 30919 net.cpp:96] Setting up drop2
I1029 13:13:25.340703 30919 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 13:13:25.340710 30919 net.cpp:67] Creating Layer conv3
I1029 13:13:25.340714 30919 net.cpp:394] conv3 <- pool2
I1029 13:13:25.340723 30919 net.cpp:356] conv3 -> conv3
I1029 13:13:25.340729 30919 net.cpp:96] Setting up conv3
I1029 13:13:25.342190 30919 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 13:13:25.342207 30919 net.cpp:67] Creating Layer pool3
I1029 13:13:25.342212 30919 net.cpp:394] pool3 <- conv3
I1029 13:13:25.342218 30919 net.cpp:356] pool3 -> pool3
I1029 13:13:25.342226 30919 net.cpp:96] Setting up pool3
I1029 13:13:25.342231 30919 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 13:13:25.342237 30919 net.cpp:67] Creating Layer relu3
I1029 13:13:25.342242 30919 net.cpp:394] relu3 <- pool3
I1029 13:13:25.342247 30919 net.cpp:345] relu3 -> pool3 (in-place)
I1029 13:13:25.342252 30919 net.cpp:96] Setting up relu3
I1029 13:13:25.342255 30919 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 13:13:25.342262 30919 net.cpp:67] Creating Layer drop3
I1029 13:13:25.342264 30919 net.cpp:394] drop3 <- pool3
I1029 13:13:25.342272 30919 net.cpp:345] drop3 -> pool3 (in-place)
I1029 13:13:25.342278 30919 net.cpp:96] Setting up drop3
I1029 13:13:25.342283 30919 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 13:13:25.342288 30919 net.cpp:67] Creating Layer ip1
I1029 13:13:25.342293 30919 net.cpp:394] ip1 <- pool3
I1029 13:13:25.342300 30919 net.cpp:356] ip1 -> ip1
I1029 13:13:25.342308 30919 net.cpp:96] Setting up ip1
I1029 13:13:25.749140 30919 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 13:13:25.749197 30919 net.cpp:67] Creating Layer relu4
I1029 13:13:25.749205 30919 net.cpp:394] relu4 <- ip1
I1029 13:13:25.749213 30919 net.cpp:345] relu4 -> ip1 (in-place)
I1029 13:13:25.749222 30919 net.cpp:96] Setting up relu4
I1029 13:13:25.749228 30919 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 13:13:25.749243 30919 net.cpp:67] Creating Layer drop4
I1029 13:13:25.749248 30919 net.cpp:394] drop4 <- ip1
I1029 13:13:25.749253 30919 net.cpp:345] drop4 -> ip1 (in-place)
I1029 13:13:25.749259 30919 net.cpp:96] Setting up drop4
I1029 13:13:25.749265 30919 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 13:13:25.749272 30919 net.cpp:67] Creating Layer ip2
I1029 13:13:25.749277 30919 net.cpp:394] ip2 <- ip1
I1029 13:13:25.749285 30919 net.cpp:356] ip2 -> ip2
I1029 13:13:25.749297 30919 net.cpp:96] Setting up ip2
I1029 13:13:25.758636 30919 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 13:13:25.758689 30919 net.cpp:67] Creating Layer prob
I1029 13:13:25.758695 30919 net.cpp:394] prob <- ip2
I1029 13:13:25.758703 30919 net.cpp:356] prob -> prob
I1029 13:13:25.758713 30919 net.cpp:96] Setting up prob
I1029 13:13:25.758721 30919 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 13:13:25.758726 30919 net.cpp:172] prob does not need backward computation.
I1029 13:13:25.758730 30919 net.cpp:172] ip2 does not need backward computation.
I1029 13:13:25.758734 30919 net.cpp:172] drop4 does not need backward computation.
I1029 13:13:25.758738 30919 net.cpp:172] relu4 does not need backward computation.
I1029 13:13:25.758741 30919 net.cpp:172] ip1 does not need backward computation.
I1029 13:13:25.758744 30919 net.cpp:172] drop3 does not need backward computation.
I1029 13:13:25.758749 30919 net.cpp:172] relu3 does not need backward computation.
I1029 13:13:25.758752 30919 net.cpp:172] pool3 does not need backward computation.
I1029 13:13:25.758762 30919 net.cpp:172] conv3 does not need backward computation.
I1029 13:13:25.758767 30919 net.cpp:172] drop2 does not need backward computation.
I1029 13:13:25.758770 30919 net.cpp:172] relu2 does not need backward computation.
I1029 13:13:25.758774 30919 net.cpp:172] pool2 does not need backward computation.
I1029 13:13:25.758779 30919 net.cpp:172] conv2 does not need backward computation.
I1029 13:13:25.758781 30919 net.cpp:172] drop1 does not need backward computation.
I1029 13:13:25.758785 30919 net.cpp:172] relu1 does not need backward computation.
I1029 13:13:25.758790 30919 net.cpp:172] pool1 does not need backward computation.
I1029 13:13:25.758792 30919 net.cpp:172] conv1 does not need backward computation.
I1029 13:13:25.758796 30919 net.cpp:208] This network produces output prob
I1029 13:13:25.758811 30919 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 13:13:25.758819 30919 net.cpp:219] Network initialization done.
I1029 13:13:25.758823 30919 net.cpp:220] Memory required for data: 1837200
I1029 13:45:53.970826  6502 convert_imageset.cpp:70] Shuffling data
I1029 13:45:54.571681  6502 convert_imageset.cpp:73] A total of 60000 images.
I1029 13:45:54.571758  6502 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 13:45:56.634492  6502 convert_imageset.cpp:177] Processed 1000 files.
E1029 13:45:58.957939  6502 convert_imageset.cpp:177] Processed 2000 files.
E1029 13:46:01.172610  6502 convert_imageset.cpp:177] Processed 3000 files.
E1029 13:46:03.315807  6502 convert_imageset.cpp:177] Processed 4000 files.
E1029 13:46:05.449265  6502 convert_imageset.cpp:177] Processed 5000 files.
E1029 13:46:07.557139  6502 convert_imageset.cpp:177] Processed 6000 files.
E1029 13:46:09.525315  6502 convert_imageset.cpp:177] Processed 7000 files.
E1029 13:46:11.381059  6502 convert_imageset.cpp:177] Processed 8000 files.
E1029 13:46:13.146116  6502 convert_imageset.cpp:177] Processed 9000 files.
E1029 13:46:14.924674  6502 convert_imageset.cpp:177] Processed 10000 files.
E1029 13:46:16.854529  6502 convert_imageset.cpp:177] Processed 11000 files.
E1029 13:46:18.841508  6502 convert_imageset.cpp:177] Processed 12000 files.
E1029 13:46:20.788583  6502 convert_imageset.cpp:177] Processed 13000 files.
E1029 13:46:22.719200  6502 convert_imageset.cpp:177] Processed 14000 files.
E1029 13:46:24.629403  6502 convert_imageset.cpp:177] Processed 15000 files.
E1029 13:46:26.514567  6502 convert_imageset.cpp:177] Processed 16000 files.
E1029 13:46:28.189095  6502 convert_imageset.cpp:177] Processed 17000 files.
E1029 13:46:30.020972  6502 convert_imageset.cpp:177] Processed 18000 files.
E1029 13:46:31.925153  6502 convert_imageset.cpp:177] Processed 19000 files.
E1029 13:46:33.622803  6502 convert_imageset.cpp:177] Processed 20000 files.
E1029 13:46:35.330596  6502 convert_imageset.cpp:177] Processed 21000 files.
E1029 13:46:36.906256  6502 convert_imageset.cpp:177] Processed 22000 files.
E1029 13:46:38.707520  6502 convert_imageset.cpp:177] Processed 23000 files.
E1029 13:46:40.421084  6502 convert_imageset.cpp:177] Processed 24000 files.
E1029 13:46:42.139545  6502 convert_imageset.cpp:177] Processed 25000 files.
E1029 13:46:43.860718  6502 convert_imageset.cpp:177] Processed 26000 files.
E1029 13:46:45.542376  6502 convert_imageset.cpp:177] Processed 27000 files.
E1029 13:46:47.203198  6502 convert_imageset.cpp:177] Processed 28000 files.
E1029 13:46:48.921764  6502 convert_imageset.cpp:177] Processed 29000 files.
E1029 13:46:50.529603  6502 convert_imageset.cpp:177] Processed 30000 files.
E1029 13:46:52.161253  6502 convert_imageset.cpp:177] Processed 31000 files.
E1029 13:46:53.689431  6502 convert_imageset.cpp:177] Processed 32000 files.
E1029 13:46:55.274049  6502 convert_imageset.cpp:177] Processed 33000 files.
E1029 13:46:56.895529  6502 convert_imageset.cpp:177] Processed 34000 files.
E1029 13:46:58.397135  6502 convert_imageset.cpp:177] Processed 35000 files.
E1029 13:46:59.915748  6502 convert_imageset.cpp:177] Processed 36000 files.
E1029 13:47:02.545035  6502 convert_imageset.cpp:177] Processed 37000 files.
E1029 13:47:04.065955  6502 convert_imageset.cpp:177] Processed 38000 files.
E1029 13:47:05.620246  6502 convert_imageset.cpp:177] Processed 39000 files.
E1029 13:47:07.135036  6502 convert_imageset.cpp:177] Processed 40000 files.
E1029 13:47:08.823909  6502 convert_imageset.cpp:177] Processed 41000 files.
E1029 13:47:10.550585  6502 convert_imageset.cpp:177] Processed 42000 files.
E1029 13:47:12.139183  6502 convert_imageset.cpp:177] Processed 43000 files.
E1029 13:47:13.699051  6502 convert_imageset.cpp:177] Processed 44000 files.
E1029 13:47:15.218119  6502 convert_imageset.cpp:177] Processed 45000 files.
E1029 13:47:16.795857  6502 convert_imageset.cpp:177] Processed 46000 files.
E1029 13:47:18.333498  6502 convert_imageset.cpp:177] Processed 47000 files.
E1029 13:47:19.850380  6502 convert_imageset.cpp:177] Processed 48000 files.
E1029 13:47:21.438928  6502 convert_imageset.cpp:177] Processed 49000 files.
E1029 13:47:22.942361  6502 convert_imageset.cpp:177] Processed 50000 files.
E1029 13:47:24.624243  6502 convert_imageset.cpp:177] Processed 51000 files.
E1029 13:47:26.094378  6502 convert_imageset.cpp:177] Processed 52000 files.
E1029 13:47:27.573375  6502 convert_imageset.cpp:177] Processed 53000 files.
E1029 13:47:29.079150  6502 convert_imageset.cpp:177] Processed 54000 files.
E1029 13:47:30.749199  6502 convert_imageset.cpp:177] Processed 55000 files.
E1029 13:47:32.468199  6502 convert_imageset.cpp:177] Processed 56000 files.
E1029 13:47:33.983433  6502 convert_imageset.cpp:177] Processed 57000 files.
E1029 13:47:35.613086  6502 convert_imageset.cpp:177] Processed 58000 files.
E1029 13:47:37.397236  6502 convert_imageset.cpp:177] Processed 59000 files.
E1029 13:47:38.953053  6502 convert_imageset.cpp:177] Processed 60000 files.
I1029 13:47:39.175196  6603 caffe.cpp:99] Use GPU with device ID 0
I1029 13:47:39.511291  6603 caffe.cpp:107] Starting Optimization
I1029 13:47:39.511425  6603 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 505000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 13:47:39.511453  6603 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 13:47:39.523768  6603 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 13:47:39.523864  6603 net.cpp:67] Creating Layer mnist
I1029 13:47:39.523874  6603 net.cpp:356] mnist -> data
I1029 13:47:39.523890  6603 net.cpp:356] mnist -> label
I1029 13:47:39.523905  6603 net.cpp:96] Setting up mnist
I1029 13:47:39.536607  6603 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 13:47:39.536695  6603 data_layer.cpp:128] output data size: 64,1,50,180
I1029 13:47:39.537456  6603 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 13:47:39.537482  6603 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 13:47:39.537495  6603 net.cpp:67] Creating Layer conv1
I1029 13:47:39.537502  6603 net.cpp:394] conv1 <- data
I1029 13:47:39.537515  6603 net.cpp:356] conv1 -> conv1
I1029 13:47:39.537529  6603 net.cpp:96] Setting up conv1
I1029 13:47:39.537917  6603 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 13:47:39.537952  6603 net.cpp:67] Creating Layer pool1
I1029 13:47:39.537960  6603 net.cpp:394] pool1 <- conv1
I1029 13:47:39.537966  6603 net.cpp:356] pool1 -> pool1
I1029 13:47:39.537973  6603 net.cpp:96] Setting up pool1
I1029 13:47:39.537992  6603 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 13:47:39.538000  6603 net.cpp:67] Creating Layer relu1
I1029 13:47:39.538005  6603 net.cpp:394] relu1 <- pool1
I1029 13:47:39.538012  6603 net.cpp:345] relu1 -> pool1 (in-place)
I1029 13:47:39.538017  6603 net.cpp:96] Setting up relu1
I1029 13:47:39.538022  6603 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 13:47:39.538029  6603 net.cpp:67] Creating Layer drop1
I1029 13:47:39.538034  6603 net.cpp:394] drop1 <- pool1
I1029 13:47:39.538043  6603 net.cpp:345] drop1 -> pool1 (in-place)
I1029 13:47:39.538049  6603 net.cpp:96] Setting up drop1
I1029 13:47:39.538054  6603 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 13:47:39.538063  6603 net.cpp:67] Creating Layer conv2
I1029 13:47:39.538066  6603 net.cpp:394] conv2 <- pool1
I1029 13:47:39.538075  6603 net.cpp:356] conv2 -> conv2
I1029 13:47:39.538084  6603 net.cpp:96] Setting up conv2
I1029 13:47:39.538660  6603 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 13:47:39.538678  6603 net.cpp:67] Creating Layer pool2
I1029 13:47:39.538684  6603 net.cpp:394] pool2 <- conv2
I1029 13:47:39.538691  6603 net.cpp:356] pool2 -> pool2
I1029 13:47:39.538698  6603 net.cpp:96] Setting up pool2
I1029 13:47:39.538704  6603 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 13:47:39.538710  6603 net.cpp:67] Creating Layer relu2
I1029 13:47:39.538720  6603 net.cpp:394] relu2 <- pool2
I1029 13:47:39.538728  6603 net.cpp:345] relu2 -> pool2 (in-place)
I1029 13:47:39.538735  6603 net.cpp:96] Setting up relu2
I1029 13:47:39.538740  6603 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 13:47:39.538748  6603 net.cpp:67] Creating Layer drop2
I1029 13:47:39.538753  6603 net.cpp:394] drop2 <- pool2
I1029 13:47:39.538758  6603 net.cpp:345] drop2 -> pool2 (in-place)
I1029 13:47:39.538764  6603 net.cpp:96] Setting up drop2
I1029 13:47:39.538769  6603 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 13:47:39.538779  6603 net.cpp:67] Creating Layer conv3
I1029 13:47:39.538784  6603 net.cpp:394] conv3 <- pool2
I1029 13:47:39.538790  6603 net.cpp:356] conv3 -> conv3
I1029 13:47:39.538797  6603 net.cpp:96] Setting up conv3
I1029 13:47:39.540304  6603 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 13:47:39.540331  6603 net.cpp:67] Creating Layer pool3
I1029 13:47:39.540336  6603 net.cpp:394] pool3 <- conv3
I1029 13:47:39.540345  6603 net.cpp:356] pool3 -> pool3
I1029 13:47:39.540354  6603 net.cpp:96] Setting up pool3
I1029 13:47:39.540360  6603 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 13:47:39.540366  6603 net.cpp:67] Creating Layer relu3
I1029 13:47:39.540370  6603 net.cpp:394] relu3 <- pool3
I1029 13:47:39.540376  6603 net.cpp:345] relu3 -> pool3 (in-place)
I1029 13:47:39.540383  6603 net.cpp:96] Setting up relu3
I1029 13:47:39.540388  6603 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 13:47:39.540395  6603 net.cpp:67] Creating Layer drop3
I1029 13:47:39.540400  6603 net.cpp:394] drop3 <- pool3
I1029 13:47:39.540405  6603 net.cpp:345] drop3 -> pool3 (in-place)
I1029 13:47:39.540410  6603 net.cpp:96] Setting up drop3
I1029 13:47:39.540416  6603 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 13:47:39.540462  6603 net.cpp:67] Creating Layer ip1
I1029 13:47:39.540467  6603 net.cpp:394] ip1 <- pool3
I1029 13:47:39.540478  6603 net.cpp:356] ip1 -> ip1
I1029 13:47:39.540518  6603 net.cpp:96] Setting up ip1
I1029 13:47:39.957705  6603 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 13:47:39.957767  6603 net.cpp:67] Creating Layer relu4
I1029 13:47:39.957774  6603 net.cpp:394] relu4 <- ip1
I1029 13:47:39.957783  6603 net.cpp:345] relu4 -> ip1 (in-place)
I1029 13:47:39.957792  6603 net.cpp:96] Setting up relu4
I1029 13:47:39.957798  6603 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 13:47:39.957805  6603 net.cpp:67] Creating Layer drop4
I1029 13:47:39.957809  6603 net.cpp:394] drop4 <- ip1
I1029 13:47:39.957815  6603 net.cpp:345] drop4 -> ip1 (in-place)
I1029 13:47:39.957821  6603 net.cpp:96] Setting up drop4
I1029 13:47:39.957826  6603 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 13:47:39.957839  6603 net.cpp:67] Creating Layer ip2
I1029 13:47:39.957844  6603 net.cpp:394] ip2 <- ip1
I1029 13:47:39.957852  6603 net.cpp:356] ip2 -> ip2
I1029 13:47:39.957860  6603 net.cpp:96] Setting up ip2
I1029 13:47:39.968365  6603 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 13:47:39.968443  6603 net.cpp:67] Creating Layer loss
I1029 13:47:39.968451  6603 net.cpp:394] loss <- ip2
I1029 13:47:39.968459  6603 net.cpp:394] loss <- label
I1029 13:47:39.968466  6603 net.cpp:356] loss -> loss
I1029 13:47:39.968477  6603 net.cpp:96] Setting up loss
I1029 13:47:39.968488  6603 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 13:47:39.968493  6603 net.cpp:109]     with loss weight 1
I1029 13:47:39.968535  6603 net.cpp:170] loss needs backward computation.
I1029 13:47:39.968540  6603 net.cpp:170] ip2 needs backward computation.
I1029 13:47:39.968545  6603 net.cpp:170] drop4 needs backward computation.
I1029 13:47:39.968549  6603 net.cpp:170] relu4 needs backward computation.
I1029 13:47:39.968554  6603 net.cpp:170] ip1 needs backward computation.
I1029 13:47:39.968559  6603 net.cpp:170] drop3 needs backward computation.
I1029 13:47:39.968562  6603 net.cpp:170] relu3 needs backward computation.
I1029 13:47:39.968567  6603 net.cpp:170] pool3 needs backward computation.
I1029 13:47:39.968571  6603 net.cpp:170] conv3 needs backward computation.
I1029 13:47:39.968583  6603 net.cpp:170] drop2 needs backward computation.
I1029 13:47:39.968587  6603 net.cpp:170] relu2 needs backward computation.
I1029 13:47:39.968592  6603 net.cpp:170] pool2 needs backward computation.
I1029 13:47:39.968597  6603 net.cpp:170] conv2 needs backward computation.
I1029 13:47:39.968601  6603 net.cpp:170] drop1 needs backward computation.
I1029 13:47:39.968606  6603 net.cpp:170] relu1 needs backward computation.
I1029 13:47:39.968611  6603 net.cpp:170] pool1 needs backward computation.
I1029 13:47:39.968616  6603 net.cpp:170] conv1 needs backward computation.
I1029 13:47:39.968619  6603 net.cpp:172] mnist does not need backward computation.
I1029 13:47:39.968624  6603 net.cpp:208] This network produces output loss
I1029 13:47:39.968634  6603 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 13:47:39.968642  6603 net.cpp:219] Network initialization done.
I1029 13:47:39.968646  6603 net.cpp:220] Memory required for data: 119788292
I1029 13:47:39.968708  6603 solver.cpp:41] Solver scaffolding done.
I1029 13:47:39.968713  6603 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_500000.solverstate
I1029 13:47:39.968719  6603 solver.cpp:160] Solving Captcha
I1029 13:47:39.968737  6603 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_500000.solverstate
I1029 13:47:51.407805  6603 solver.cpp:502] SGDSolver: restoring history
I1029 13:47:52.162993  6603 solver.cpp:191] Iteration 500000, loss = 2.42928
I1029 13:47:52.163056  6603 solver.cpp:206]     Train net output #0: loss = 2.42928 (* 1 = 2.42928 loss)
I1029 13:47:52.163070  6603 solver.cpp:403] Iteration 500000, lr = 0.000523989
I1029 13:51:54.278578  6603 solver.cpp:191] Iteration 501000, loss = 2.3297
I1029 13:51:54.279147  6603 solver.cpp:206]     Train net output #0: loss = 2.3297 (* 1 = 2.3297 loss)
I1029 13:51:54.279167  6603 solver.cpp:403] Iteration 501000, lr = 0.00052322
I1029 13:55:55.449782  6603 solver.cpp:191] Iteration 502000, loss = 2.45278
I1029 13:55:55.450417  6603 solver.cpp:206]     Train net output #0: loss = 2.45278 (* 1 = 2.45278 loss)
I1029 13:55:55.450450  6603 solver.cpp:403] Iteration 502000, lr = 0.000522453
I1029 13:59:56.710613  6603 solver.cpp:191] Iteration 503000, loss = 2.33955
I1029 13:59:56.711159  6603 solver.cpp:206]     Train net output #0: loss = 2.33955 (* 1 = 2.33955 loss)
I1029 13:59:56.711197  6603 solver.cpp:403] Iteration 503000, lr = 0.000521689
I1029 14:03:57.969470  6603 solver.cpp:191] Iteration 504000, loss = 2.32193
I1029 14:03:57.970054  6603 solver.cpp:206]     Train net output #0: loss = 2.32193 (* 1 = 2.32193 loss)
I1029 14:03:57.970072  6603 solver.cpp:403] Iteration 504000, lr = 0.000520928
I1029 14:07:59.724673  6603 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_505000.caffemodel
I1029 14:08:04.298637  6603 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_505000.solverstate
I1029 14:08:08.323933  6603 solver.cpp:228] Iteration 505000, loss = 2.24065
I1029 14:08:08.324478  6603 solver.cpp:233] Optimization Done.
I1029 14:08:08.324506  6603 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 14:31:07.775377 20618 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 14:31:07.775480 20618 net.cpp:358] Input 0 -> data
I1029 14:31:07.775503 20618 net.cpp:67] Creating Layer conv1
I1029 14:31:07.775508 20618 net.cpp:394] conv1 <- data
I1029 14:31:07.775516 20618 net.cpp:356] conv1 -> conv1
I1029 14:31:07.775526 20618 net.cpp:96] Setting up conv1
I1029 14:31:07.775838 20618 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 14:31:07.775857 20618 net.cpp:67] Creating Layer pool1
I1029 14:31:07.775862 20618 net.cpp:394] pool1 <- conv1
I1029 14:31:07.775867 20618 net.cpp:356] pool1 -> pool1
I1029 14:31:07.775876 20618 net.cpp:96] Setting up pool1
I1029 14:31:07.775888 20618 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 14:31:07.775895 20618 net.cpp:67] Creating Layer relu1
I1029 14:31:07.775899 20618 net.cpp:394] relu1 <- pool1
I1029 14:31:07.775904 20618 net.cpp:345] relu1 -> pool1 (in-place)
I1029 14:31:07.775910 20618 net.cpp:96] Setting up relu1
I1029 14:31:07.775915 20618 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 14:31:07.775920 20618 net.cpp:67] Creating Layer drop1
I1029 14:31:07.775924 20618 net.cpp:394] drop1 <- pool1
I1029 14:31:07.775933 20618 net.cpp:345] drop1 -> pool1 (in-place)
I1029 14:31:07.775939 20618 net.cpp:96] Setting up drop1
I1029 14:31:07.775944 20618 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 14:31:07.775951 20618 net.cpp:67] Creating Layer conv2
I1029 14:31:07.775955 20618 net.cpp:394] conv2 <- pool1
I1029 14:31:07.775962 20618 net.cpp:356] conv2 -> conv2
I1029 14:31:07.775969 20618 net.cpp:96] Setting up conv2
I1029 14:31:07.776562 20618 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 14:31:07.776581 20618 net.cpp:67] Creating Layer pool2
I1029 14:31:07.776585 20618 net.cpp:394] pool2 <- conv2
I1029 14:31:07.776590 20618 net.cpp:356] pool2 -> pool2
I1029 14:31:07.776597 20618 net.cpp:96] Setting up pool2
I1029 14:31:07.776603 20618 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 14:31:07.776612 20618 net.cpp:67] Creating Layer relu2
I1029 14:31:07.776615 20618 net.cpp:394] relu2 <- pool2
I1029 14:31:07.776623 20618 net.cpp:345] relu2 -> pool2 (in-place)
I1029 14:31:07.776629 20618 net.cpp:96] Setting up relu2
I1029 14:31:07.776633 20618 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 14:31:07.776638 20618 net.cpp:67] Creating Layer drop2
I1029 14:31:07.776643 20618 net.cpp:394] drop2 <- pool2
I1029 14:31:07.776649 20618 net.cpp:345] drop2 -> pool2 (in-place)
I1029 14:31:07.776655 20618 net.cpp:96] Setting up drop2
I1029 14:31:07.776660 20618 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 14:31:07.776667 20618 net.cpp:67] Creating Layer conv3
I1029 14:31:07.776671 20618 net.cpp:394] conv3 <- pool2
I1029 14:31:07.776676 20618 net.cpp:356] conv3 -> conv3
I1029 14:31:07.776684 20618 net.cpp:96] Setting up conv3
I1029 14:31:07.778144 20618 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 14:31:07.778159 20618 net.cpp:67] Creating Layer pool3
I1029 14:31:07.778164 20618 net.cpp:394] pool3 <- conv3
I1029 14:31:07.778172 20618 net.cpp:356] pool3 -> pool3
I1029 14:31:07.778179 20618 net.cpp:96] Setting up pool3
I1029 14:31:07.778184 20618 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 14:31:07.778189 20618 net.cpp:67] Creating Layer relu3
I1029 14:31:07.778193 20618 net.cpp:394] relu3 <- pool3
I1029 14:31:07.778200 20618 net.cpp:345] relu3 -> pool3 (in-place)
I1029 14:31:07.778205 20618 net.cpp:96] Setting up relu3
I1029 14:31:07.778210 20618 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 14:31:07.778215 20618 net.cpp:67] Creating Layer drop3
I1029 14:31:07.778219 20618 net.cpp:394] drop3 <- pool3
I1029 14:31:07.778224 20618 net.cpp:345] drop3 -> pool3 (in-place)
I1029 14:31:07.778229 20618 net.cpp:96] Setting up drop3
I1029 14:31:07.778234 20618 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 14:31:07.778240 20618 net.cpp:67] Creating Layer ip1
I1029 14:31:07.778244 20618 net.cpp:394] ip1 <- pool3
I1029 14:31:07.778251 20618 net.cpp:356] ip1 -> ip1
I1029 14:31:07.778259 20618 net.cpp:96] Setting up ip1
I1029 14:31:08.196483 20618 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 14:31:08.196543 20618 net.cpp:67] Creating Layer relu4
I1029 14:31:08.196552 20618 net.cpp:394] relu4 <- ip1
I1029 14:31:08.196560 20618 net.cpp:345] relu4 -> ip1 (in-place)
I1029 14:31:08.196569 20618 net.cpp:96] Setting up relu4
I1029 14:31:08.196574 20618 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 14:31:08.196581 20618 net.cpp:67] Creating Layer drop4
I1029 14:31:08.196585 20618 net.cpp:394] drop4 <- ip1
I1029 14:31:08.196591 20618 net.cpp:345] drop4 -> ip1 (in-place)
I1029 14:31:08.196598 20618 net.cpp:96] Setting up drop4
I1029 14:31:08.196602 20618 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 14:31:08.196612 20618 net.cpp:67] Creating Layer ip2
I1029 14:31:08.196617 20618 net.cpp:394] ip2 <- ip1
I1029 14:31:08.196624 20618 net.cpp:356] ip2 -> ip2
I1029 14:31:08.196636 20618 net.cpp:96] Setting up ip2
I1029 14:31:08.205430 20618 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 14:31:08.205502 20618 net.cpp:67] Creating Layer prob
I1029 14:31:08.205509 20618 net.cpp:394] prob <- ip2
I1029 14:31:08.205518 20618 net.cpp:356] prob -> prob
I1029 14:31:08.205528 20618 net.cpp:96] Setting up prob
I1029 14:31:08.205533 20618 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 14:31:08.205538 20618 net.cpp:172] prob does not need backward computation.
I1029 14:31:08.205543 20618 net.cpp:172] ip2 does not need backward computation.
I1029 14:31:08.205545 20618 net.cpp:172] drop4 does not need backward computation.
I1029 14:31:08.205549 20618 net.cpp:172] relu4 does not need backward computation.
I1029 14:31:08.205554 20618 net.cpp:172] ip1 does not need backward computation.
I1029 14:31:08.205556 20618 net.cpp:172] drop3 does not need backward computation.
I1029 14:31:08.205560 20618 net.cpp:172] relu3 does not need backward computation.
I1029 14:31:08.205564 20618 net.cpp:172] pool3 does not need backward computation.
I1029 14:31:08.205567 20618 net.cpp:172] conv3 does not need backward computation.
I1029 14:31:08.205570 20618 net.cpp:172] drop2 does not need backward computation.
I1029 14:31:08.205574 20618 net.cpp:172] relu2 does not need backward computation.
I1029 14:31:08.205585 20618 net.cpp:172] pool2 does not need backward computation.
I1029 14:31:08.205588 20618 net.cpp:172] conv2 does not need backward computation.
I1029 14:31:08.205592 20618 net.cpp:172] drop1 does not need backward computation.
I1029 14:31:08.205595 20618 net.cpp:172] relu1 does not need backward computation.
I1029 14:31:08.205600 20618 net.cpp:172] pool1 does not need backward computation.
I1029 14:31:08.205602 20618 net.cpp:172] conv1 does not need backward computation.
I1029 14:31:08.205606 20618 net.cpp:208] This network produces output prob
I1029 14:31:08.205620 20618 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 14:31:08.205627 20618 net.cpp:219] Network initialization done.
I1029 14:31:08.205631 20618 net.cpp:220] Memory required for data: 1837200
I1029 15:03:31.081615 28375 convert_imageset.cpp:70] Shuffling data
I1029 15:03:31.712821 28375 convert_imageset.cpp:73] A total of 60000 images.
I1029 15:03:31.725935 28375 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 15:03:34.292374 28375 convert_imageset.cpp:177] Processed 1000 files.
E1029 15:03:36.650601 28375 convert_imageset.cpp:177] Processed 2000 files.
E1029 15:03:38.934545 28375 convert_imageset.cpp:177] Processed 3000 files.
E1029 15:03:41.114087 28375 convert_imageset.cpp:177] Processed 4000 files.
E1029 15:03:43.273008 28375 convert_imageset.cpp:177] Processed 5000 files.
E1029 15:03:45.581138 28375 convert_imageset.cpp:177] Processed 6000 files.
E1029 15:03:47.637755 28375 convert_imageset.cpp:177] Processed 7000 files.
E1029 15:03:49.819545 28375 convert_imageset.cpp:177] Processed 8000 files.
E1029 15:03:51.898566 28375 convert_imageset.cpp:177] Processed 9000 files.
E1029 15:03:53.910904 28375 convert_imageset.cpp:177] Processed 10000 files.
E1029 15:03:56.005870 28375 convert_imageset.cpp:177] Processed 11000 files.
E1029 15:03:57.958130 28375 convert_imageset.cpp:177] Processed 12000 files.
E1029 15:03:59.866914 28375 convert_imageset.cpp:177] Processed 13000 files.
E1029 15:04:01.814676 28375 convert_imageset.cpp:177] Processed 14000 files.
E1029 15:04:03.648916 28375 convert_imageset.cpp:177] Processed 15000 files.
E1029 15:04:05.494401 28375 convert_imageset.cpp:177] Processed 16000 files.
E1029 15:04:07.319772 28375 convert_imageset.cpp:177] Processed 17000 files.
E1029 15:04:09.141635 28375 convert_imageset.cpp:177] Processed 18000 files.
E1029 15:04:11.031436 28375 convert_imageset.cpp:177] Processed 19000 files.
E1029 15:04:12.889188 28375 convert_imageset.cpp:177] Processed 20000 files.
E1029 15:04:14.638798 28375 convert_imageset.cpp:177] Processed 21000 files.
E1029 15:04:16.377095 28375 convert_imageset.cpp:177] Processed 22000 files.
E1029 15:04:18.494050 28375 convert_imageset.cpp:177] Processed 23000 files.
E1029 15:04:20.247493 28375 convert_imageset.cpp:177] Processed 24000 files.
E1029 15:04:22.009016 28375 convert_imageset.cpp:177] Processed 25000 files.
E1029 15:04:23.742903 28375 convert_imageset.cpp:177] Processed 26000 files.
E1029 15:04:25.302271 28375 convert_imageset.cpp:177] Processed 27000 files.
E1029 15:04:26.978564 28375 convert_imageset.cpp:177] Processed 28000 files.
E1029 15:04:28.486896 28375 convert_imageset.cpp:177] Processed 29000 files.
E1029 15:04:30.175840 28375 convert_imageset.cpp:177] Processed 30000 files.
E1029 15:04:31.847013 28375 convert_imageset.cpp:177] Processed 31000 files.
E1029 15:04:33.513134 28375 convert_imageset.cpp:177] Processed 32000 files.
E1029 15:04:35.128057 28375 convert_imageset.cpp:177] Processed 33000 files.
E1029 15:04:36.616196 28375 convert_imageset.cpp:177] Processed 34000 files.
E1029 15:04:38.154259 28375 convert_imageset.cpp:177] Processed 35000 files.
E1029 15:04:39.788226 28375 convert_imageset.cpp:177] Processed 36000 files.
E1029 15:04:41.438333 28375 convert_imageset.cpp:177] Processed 37000 files.
E1029 15:04:43.054960 28375 convert_imageset.cpp:177] Processed 38000 files.
E1029 15:04:44.591279 28375 convert_imageset.cpp:177] Processed 39000 files.
E1029 15:04:46.123366 28375 convert_imageset.cpp:177] Processed 40000 files.
E1029 15:04:47.664194 28375 convert_imageset.cpp:177] Processed 41000 files.
E1029 15:04:49.194392 28375 convert_imageset.cpp:177] Processed 42000 files.
E1029 15:04:50.726021 28375 convert_imageset.cpp:177] Processed 43000 files.
E1029 15:04:52.287184 28375 convert_imageset.cpp:177] Processed 44000 files.
E1029 15:04:53.879864 28375 convert_imageset.cpp:177] Processed 45000 files.
E1029 15:04:55.411912 28375 convert_imageset.cpp:177] Processed 46000 files.
E1029 15:04:57.031837 28375 convert_imageset.cpp:177] Processed 47000 files.
E1029 15:04:58.613556 28375 convert_imageset.cpp:177] Processed 48000 files.
E1029 15:05:00.151087 28375 convert_imageset.cpp:177] Processed 49000 files.
E1029 15:05:01.705956 28375 convert_imageset.cpp:177] Processed 50000 files.
E1029 15:05:03.303542 28375 convert_imageset.cpp:177] Processed 51000 files.
E1029 15:05:05.008056 28375 convert_imageset.cpp:177] Processed 52000 files.
E1029 15:05:06.689818 28375 convert_imageset.cpp:177] Processed 53000 files.
E1029 15:05:08.160187 28375 convert_imageset.cpp:177] Processed 54000 files.
E1029 15:05:09.735538 28375 convert_imageset.cpp:177] Processed 55000 files.
E1029 15:05:11.223786 28375 convert_imageset.cpp:177] Processed 56000 files.
E1029 15:05:12.678773 28375 convert_imageset.cpp:177] Processed 57000 files.
E1029 15:05:14.310892 28375 convert_imageset.cpp:177] Processed 58000 files.
E1029 15:05:15.787272 28375 convert_imageset.cpp:177] Processed 59000 files.
E1029 15:05:17.223319 28375 convert_imageset.cpp:177] Processed 60000 files.
I1029 15:05:17.420522 28595 caffe.cpp:99] Use GPU with device ID 0
I1029 15:05:17.757639 28595 caffe.cpp:107] Starting Optimization
I1029 15:05:17.757767 28595 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 510000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 15:05:17.757792 28595 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 15:05:17.771381 28595 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 15:05:17.771486 28595 net.cpp:67] Creating Layer mnist
I1029 15:05:17.771497 28595 net.cpp:356] mnist -> data
I1029 15:05:17.771517 28595 net.cpp:356] mnist -> label
I1029 15:05:17.771530 28595 net.cpp:96] Setting up mnist
I1029 15:05:17.778301 28595 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 15:05:17.778395 28595 data_layer.cpp:128] output data size: 64,1,50,180
I1029 15:05:17.779276 28595 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 15:05:17.779301 28595 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 15:05:17.779314 28595 net.cpp:67] Creating Layer conv1
I1029 15:05:17.779321 28595 net.cpp:394] conv1 <- data
I1029 15:05:17.779336 28595 net.cpp:356] conv1 -> conv1
I1029 15:05:17.779348 28595 net.cpp:96] Setting up conv1
I1029 15:05:17.779742 28595 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 15:05:17.779777 28595 net.cpp:67] Creating Layer pool1
I1029 15:05:17.779783 28595 net.cpp:394] pool1 <- conv1
I1029 15:05:17.779790 28595 net.cpp:356] pool1 -> pool1
I1029 15:05:17.779800 28595 net.cpp:96] Setting up pool1
I1029 15:05:17.779817 28595 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 15:05:17.779825 28595 net.cpp:67] Creating Layer relu1
I1029 15:05:17.779830 28595 net.cpp:394] relu1 <- pool1
I1029 15:05:17.779837 28595 net.cpp:345] relu1 -> pool1 (in-place)
I1029 15:05:17.779844 28595 net.cpp:96] Setting up relu1
I1029 15:05:17.779850 28595 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 15:05:17.779857 28595 net.cpp:67] Creating Layer drop1
I1029 15:05:17.779862 28595 net.cpp:394] drop1 <- pool1
I1029 15:05:17.779870 28595 net.cpp:345] drop1 -> pool1 (in-place)
I1029 15:05:17.779878 28595 net.cpp:96] Setting up drop1
I1029 15:05:17.779885 28595 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 15:05:17.779891 28595 net.cpp:67] Creating Layer conv2
I1029 15:05:17.779896 28595 net.cpp:394] conv2 <- pool1
I1029 15:05:17.779906 28595 net.cpp:356] conv2 -> conv2
I1029 15:05:17.779913 28595 net.cpp:96] Setting up conv2
I1029 15:05:17.780560 28595 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 15:05:17.780580 28595 net.cpp:67] Creating Layer pool2
I1029 15:05:17.780586 28595 net.cpp:394] pool2 <- conv2
I1029 15:05:17.780593 28595 net.cpp:356] pool2 -> pool2
I1029 15:05:17.780601 28595 net.cpp:96] Setting up pool2
I1029 15:05:17.780607 28595 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 15:05:17.780614 28595 net.cpp:67] Creating Layer relu2
I1029 15:05:17.780618 28595 net.cpp:394] relu2 <- pool2
I1029 15:05:17.780627 28595 net.cpp:345] relu2 -> pool2 (in-place)
I1029 15:05:17.780634 28595 net.cpp:96] Setting up relu2
I1029 15:05:17.780645 28595 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 15:05:17.780653 28595 net.cpp:67] Creating Layer drop2
I1029 15:05:17.780658 28595 net.cpp:394] drop2 <- pool2
I1029 15:05:17.780665 28595 net.cpp:345] drop2 -> pool2 (in-place)
I1029 15:05:17.780673 28595 net.cpp:96] Setting up drop2
I1029 15:05:17.780678 28595 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 15:05:17.780689 28595 net.cpp:67] Creating Layer conv3
I1029 15:05:17.780694 28595 net.cpp:394] conv3 <- pool2
I1029 15:05:17.780700 28595 net.cpp:356] conv3 -> conv3
I1029 15:05:17.780709 28595 net.cpp:96] Setting up conv3
I1029 15:05:17.782361 28595 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 15:05:17.782387 28595 net.cpp:67] Creating Layer pool3
I1029 15:05:17.782393 28595 net.cpp:394] pool3 <- conv3
I1029 15:05:17.782402 28595 net.cpp:356] pool3 -> pool3
I1029 15:05:17.782412 28595 net.cpp:96] Setting up pool3
I1029 15:05:17.782418 28595 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 15:05:17.782425 28595 net.cpp:67] Creating Layer relu3
I1029 15:05:17.782430 28595 net.cpp:394] relu3 <- pool3
I1029 15:05:17.782438 28595 net.cpp:345] relu3 -> pool3 (in-place)
I1029 15:05:17.782445 28595 net.cpp:96] Setting up relu3
I1029 15:05:17.782450 28595 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 15:05:17.782457 28595 net.cpp:67] Creating Layer drop3
I1029 15:05:17.782462 28595 net.cpp:394] drop3 <- pool3
I1029 15:05:17.782469 28595 net.cpp:345] drop3 -> pool3 (in-place)
I1029 15:05:17.782475 28595 net.cpp:96] Setting up drop3
I1029 15:05:17.782481 28595 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 15:05:17.782490 28595 net.cpp:67] Creating Layer ip1
I1029 15:05:17.782495 28595 net.cpp:394] ip1 <- pool3
I1029 15:05:17.782505 28595 net.cpp:356] ip1 -> ip1
I1029 15:05:17.782541 28595 net.cpp:96] Setting up ip1
I1029 15:05:18.306422 28595 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 15:05:18.306484 28595 net.cpp:67] Creating Layer relu4
I1029 15:05:18.306493 28595 net.cpp:394] relu4 <- ip1
I1029 15:05:18.306501 28595 net.cpp:345] relu4 -> ip1 (in-place)
I1029 15:05:18.306510 28595 net.cpp:96] Setting up relu4
I1029 15:05:18.306515 28595 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 15:05:18.306522 28595 net.cpp:67] Creating Layer drop4
I1029 15:05:18.306527 28595 net.cpp:394] drop4 <- ip1
I1029 15:05:18.306534 28595 net.cpp:345] drop4 -> ip1 (in-place)
I1029 15:05:18.306540 28595 net.cpp:96] Setting up drop4
I1029 15:05:18.306545 28595 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 15:05:18.306557 28595 net.cpp:67] Creating Layer ip2
I1029 15:05:18.306562 28595 net.cpp:394] ip2 <- ip1
I1029 15:05:18.306571 28595 net.cpp:356] ip2 -> ip2
I1029 15:05:18.306579 28595 net.cpp:96] Setting up ip2
I1029 15:05:18.315482 28595 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 15:05:18.315542 28595 net.cpp:67] Creating Layer loss
I1029 15:05:18.315549 28595 net.cpp:394] loss <- ip2
I1029 15:05:18.315557 28595 net.cpp:394] loss <- label
I1029 15:05:18.315564 28595 net.cpp:356] loss -> loss
I1029 15:05:18.315573 28595 net.cpp:96] Setting up loss
I1029 15:05:18.315584 28595 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 15:05:18.315590 28595 net.cpp:109]     with loss weight 1
I1029 15:05:18.315626 28595 net.cpp:170] loss needs backward computation.
I1029 15:05:18.315631 28595 net.cpp:170] ip2 needs backward computation.
I1029 15:05:18.315635 28595 net.cpp:170] drop4 needs backward computation.
I1029 15:05:18.315640 28595 net.cpp:170] relu4 needs backward computation.
I1029 15:05:18.315644 28595 net.cpp:170] ip1 needs backward computation.
I1029 15:05:18.315649 28595 net.cpp:170] drop3 needs backward computation.
I1029 15:05:18.315654 28595 net.cpp:170] relu3 needs backward computation.
I1029 15:05:18.315657 28595 net.cpp:170] pool3 needs backward computation.
I1029 15:05:18.315661 28595 net.cpp:170] conv3 needs backward computation.
I1029 15:05:18.315666 28595 net.cpp:170] drop2 needs backward computation.
I1029 15:05:18.315671 28595 net.cpp:170] relu2 needs backward computation.
I1029 15:05:18.315675 28595 net.cpp:170] pool2 needs backward computation.
I1029 15:05:18.315686 28595 net.cpp:170] conv2 needs backward computation.
I1029 15:05:18.315691 28595 net.cpp:170] drop1 needs backward computation.
I1029 15:05:18.315696 28595 net.cpp:170] relu1 needs backward computation.
I1029 15:05:18.315701 28595 net.cpp:170] pool1 needs backward computation.
I1029 15:05:18.315704 28595 net.cpp:170] conv1 needs backward computation.
I1029 15:05:18.315709 28595 net.cpp:172] mnist does not need backward computation.
I1029 15:05:18.315713 28595 net.cpp:208] This network produces output loss
I1029 15:05:18.315723 28595 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 15:05:18.315732 28595 net.cpp:219] Network initialization done.
I1029 15:05:18.315735 28595 net.cpp:220] Memory required for data: 119788292
I1029 15:05:18.315793 28595 solver.cpp:41] Solver scaffolding done.
I1029 15:05:18.315799 28595 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_505000.solverstate
I1029 15:05:18.315804 28595 solver.cpp:160] Solving Captcha
I1029 15:05:18.315822 28595 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_505000.solverstate
I1029 15:05:21.510866 28595 solver.cpp:502] SGDSolver: restoring history
I1029 15:05:22.240885 28595 solver.cpp:191] Iteration 505000, loss = 2.34954
I1029 15:05:22.240949 28595 solver.cpp:206]     Train net output #0: loss = 2.34954 (* 1 = 2.34954 loss)
I1029 15:05:22.240965 28595 solver.cpp:403] Iteration 505000, lr = 0.000520169
I1029 15:09:23.872450 28595 solver.cpp:191] Iteration 506000, loss = 2.24756
I1029 15:09:23.881378 28595 solver.cpp:206]     Train net output #0: loss = 2.24756 (* 1 = 2.24756 loss)
I1029 15:09:23.881410 28595 solver.cpp:403] Iteration 506000, lr = 0.000519413
I1029 15:13:25.091384 28595 solver.cpp:191] Iteration 507000, loss = 2.3626
I1029 15:13:25.091967 28595 solver.cpp:206]     Train net output #0: loss = 2.3626 (* 1 = 2.3626 loss)
I1029 15:13:25.092001 28595 solver.cpp:403] Iteration 507000, lr = 0.000518659
I1029 15:17:26.429426 28595 solver.cpp:191] Iteration 508000, loss = 2.36374
I1029 15:17:26.430024 28595 solver.cpp:206]     Train net output #0: loss = 2.36374 (* 1 = 2.36374 loss)
I1029 15:17:26.430057 28595 solver.cpp:403] Iteration 508000, lr = 0.000517908
I1029 15:21:27.620496 28595 solver.cpp:191] Iteration 509000, loss = 2.36066
I1029 15:21:27.621124 28595 solver.cpp:206]     Train net output #0: loss = 2.36066 (* 1 = 2.36066 loss)
I1029 15:21:27.621155 28595 solver.cpp:403] Iteration 509000, lr = 0.000517159
I1029 15:25:29.347323 28595 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_510000.caffemodel
I1029 15:25:33.645316 28595 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_510000.solverstate
I1029 15:25:37.426687 28595 solver.cpp:228] Iteration 510000, loss = 2.55264
I1029 15:25:37.427211 28595 solver.cpp:233] Optimization Done.
I1029 15:25:37.427233 28595 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 15:48:03.828565  9867 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 15:48:03.828672  9867 net.cpp:358] Input 0 -> data
I1029 15:48:03.828698  9867 net.cpp:67] Creating Layer conv1
I1029 15:48:03.828703  9867 net.cpp:394] conv1 <- data
I1029 15:48:03.828711  9867 net.cpp:356] conv1 -> conv1
I1029 15:48:03.828721  9867 net.cpp:96] Setting up conv1
I1029 15:48:03.829047  9867 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 15:48:03.829074  9867 net.cpp:67] Creating Layer pool1
I1029 15:48:03.829080  9867 net.cpp:394] pool1 <- conv1
I1029 15:48:03.829087  9867 net.cpp:356] pool1 -> pool1
I1029 15:48:03.829095  9867 net.cpp:96] Setting up pool1
I1029 15:48:03.829108  9867 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 15:48:03.829118  9867 net.cpp:67] Creating Layer relu1
I1029 15:48:03.829123  9867 net.cpp:394] relu1 <- pool1
I1029 15:48:03.829128  9867 net.cpp:345] relu1 -> pool1 (in-place)
I1029 15:48:03.829134  9867 net.cpp:96] Setting up relu1
I1029 15:48:03.829138  9867 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 15:48:03.829146  9867 net.cpp:67] Creating Layer drop1
I1029 15:48:03.829151  9867 net.cpp:394] drop1 <- pool1
I1029 15:48:03.829156  9867 net.cpp:345] drop1 -> pool1 (in-place)
I1029 15:48:03.829162  9867 net.cpp:96] Setting up drop1
I1029 15:48:03.829167  9867 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 15:48:03.829174  9867 net.cpp:67] Creating Layer conv2
I1029 15:48:03.829179  9867 net.cpp:394] conv2 <- pool1
I1029 15:48:03.829185  9867 net.cpp:356] conv2 -> conv2
I1029 15:48:03.829190  9867 net.cpp:96] Setting up conv2
I1029 15:48:03.829743  9867 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 15:48:03.829758  9867 net.cpp:67] Creating Layer pool2
I1029 15:48:03.829762  9867 net.cpp:394] pool2 <- conv2
I1029 15:48:03.829768  9867 net.cpp:356] pool2 -> pool2
I1029 15:48:03.829776  9867 net.cpp:96] Setting up pool2
I1029 15:48:03.829782  9867 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 15:48:03.829789  9867 net.cpp:67] Creating Layer relu2
I1029 15:48:03.829793  9867 net.cpp:394] relu2 <- pool2
I1029 15:48:03.829798  9867 net.cpp:345] relu2 -> pool2 (in-place)
I1029 15:48:03.829804  9867 net.cpp:96] Setting up relu2
I1029 15:48:03.829808  9867 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 15:48:03.829818  9867 net.cpp:67] Creating Layer drop2
I1029 15:48:03.829823  9867 net.cpp:394] drop2 <- pool2
I1029 15:48:03.829828  9867 net.cpp:345] drop2 -> pool2 (in-place)
I1029 15:48:03.829833  9867 net.cpp:96] Setting up drop2
I1029 15:48:03.829838  9867 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 15:48:03.829845  9867 net.cpp:67] Creating Layer conv3
I1029 15:48:03.829849  9867 net.cpp:394] conv3 <- pool2
I1029 15:48:03.829856  9867 net.cpp:356] conv3 -> conv3
I1029 15:48:03.829864  9867 net.cpp:96] Setting up conv3
I1029 15:48:03.831300  9867 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 15:48:03.831317  9867 net.cpp:67] Creating Layer pool3
I1029 15:48:03.831323  9867 net.cpp:394] pool3 <- conv3
I1029 15:48:03.831328  9867 net.cpp:356] pool3 -> pool3
I1029 15:48:03.831336  9867 net.cpp:96] Setting up pool3
I1029 15:48:03.831341  9867 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 15:48:03.831347  9867 net.cpp:67] Creating Layer relu3
I1029 15:48:03.831352  9867 net.cpp:394] relu3 <- pool3
I1029 15:48:03.831357  9867 net.cpp:345] relu3 -> pool3 (in-place)
I1029 15:48:03.831362  9867 net.cpp:96] Setting up relu3
I1029 15:48:03.831367  9867 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 15:48:03.831372  9867 net.cpp:67] Creating Layer drop3
I1029 15:48:03.831377  9867 net.cpp:394] drop3 <- pool3
I1029 15:48:03.831382  9867 net.cpp:345] drop3 -> pool3 (in-place)
I1029 15:48:03.831387  9867 net.cpp:96] Setting up drop3
I1029 15:48:03.831392  9867 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 15:48:03.831399  9867 net.cpp:67] Creating Layer ip1
I1029 15:48:03.831404  9867 net.cpp:394] ip1 <- pool3
I1029 15:48:03.831410  9867 net.cpp:356] ip1 -> ip1
I1029 15:48:03.831418  9867 net.cpp:96] Setting up ip1
I1029 15:48:04.250794  9867 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 15:48:04.250854  9867 net.cpp:67] Creating Layer relu4
I1029 15:48:04.250861  9867 net.cpp:394] relu4 <- ip1
I1029 15:48:04.250870  9867 net.cpp:345] relu4 -> ip1 (in-place)
I1029 15:48:04.250880  9867 net.cpp:96] Setting up relu4
I1029 15:48:04.250885  9867 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 15:48:04.250891  9867 net.cpp:67] Creating Layer drop4
I1029 15:48:04.250895  9867 net.cpp:394] drop4 <- ip1
I1029 15:48:04.250903  9867 net.cpp:345] drop4 -> ip1 (in-place)
I1029 15:48:04.250910  9867 net.cpp:96] Setting up drop4
I1029 15:48:04.250916  9867 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 15:48:04.250923  9867 net.cpp:67] Creating Layer ip2
I1029 15:48:04.250927  9867 net.cpp:394] ip2 <- ip1
I1029 15:48:04.250934  9867 net.cpp:356] ip2 -> ip2
I1029 15:48:04.250947  9867 net.cpp:96] Setting up ip2
I1029 15:48:04.261414  9867 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 15:48:04.261477  9867 net.cpp:67] Creating Layer prob
I1029 15:48:04.261484  9867 net.cpp:394] prob <- ip2
I1029 15:48:04.261492  9867 net.cpp:356] prob -> prob
I1029 15:48:04.261503  9867 net.cpp:96] Setting up prob
I1029 15:48:04.261509  9867 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 15:48:04.261514  9867 net.cpp:172] prob does not need backward computation.
I1029 15:48:04.261518  9867 net.cpp:172] ip2 does not need backward computation.
I1029 15:48:04.261521  9867 net.cpp:172] drop4 does not need backward computation.
I1029 15:48:04.261525  9867 net.cpp:172] relu4 does not need backward computation.
I1029 15:48:04.261528  9867 net.cpp:172] ip1 does not need backward computation.
I1029 15:48:04.261533  9867 net.cpp:172] drop3 does not need backward computation.
I1029 15:48:04.261536  9867 net.cpp:172] relu3 does not need backward computation.
I1029 15:48:04.261539  9867 net.cpp:172] pool3 does not need backward computation.
I1029 15:48:04.261543  9867 net.cpp:172] conv3 does not need backward computation.
I1029 15:48:04.261546  9867 net.cpp:172] drop2 does not need backward computation.
I1029 15:48:04.261550  9867 net.cpp:172] relu2 does not need backward computation.
I1029 15:48:04.261554  9867 net.cpp:172] pool2 does not need backward computation.
I1029 15:48:04.261557  9867 net.cpp:172] conv2 does not need backward computation.
I1029 15:48:04.261571  9867 net.cpp:172] drop1 does not need backward computation.
I1029 15:48:04.261575  9867 net.cpp:172] relu1 does not need backward computation.
I1029 15:48:04.261579  9867 net.cpp:172] pool1 does not need backward computation.
I1029 15:48:04.261582  9867 net.cpp:172] conv1 does not need backward computation.
I1029 15:48:04.261586  9867 net.cpp:208] This network produces output prob
I1029 15:48:04.261600  9867 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 15:48:04.261610  9867 net.cpp:219] Network initialization done.
I1029 15:48:04.261612  9867 net.cpp:220] Memory required for data: 1837200
I1029 16:21:58.253406 18277 convert_imageset.cpp:70] Shuffling data
I1029 16:21:58.866956 18277 convert_imageset.cpp:73] A total of 60000 images.
I1029 16:21:58.878340 18277 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 16:22:01.306996 18277 convert_imageset.cpp:177] Processed 1000 files.
E1029 16:22:03.855882 18277 convert_imageset.cpp:177] Processed 2000 files.
E1029 16:22:06.396096 18277 convert_imageset.cpp:177] Processed 3000 files.
E1029 16:22:08.917636 18277 convert_imageset.cpp:177] Processed 4000 files.
E1029 16:22:11.204881 18277 convert_imageset.cpp:177] Processed 5000 files.
E1029 16:22:13.452396 18277 convert_imageset.cpp:177] Processed 6000 files.
E1029 16:22:15.797343 18277 convert_imageset.cpp:177] Processed 7000 files.
E1029 16:22:18.072268 18277 convert_imageset.cpp:177] Processed 8000 files.
E1029 16:22:20.296385 18277 convert_imageset.cpp:177] Processed 9000 files.
E1029 16:22:22.422308 18277 convert_imageset.cpp:177] Processed 10000 files.
E1029 16:22:24.560461 18277 convert_imageset.cpp:177] Processed 11000 files.
E1029 16:22:26.726608 18277 convert_imageset.cpp:177] Processed 12000 files.
E1029 16:22:28.709786 18277 convert_imageset.cpp:177] Processed 13000 files.
E1029 16:22:30.679446 18277 convert_imageset.cpp:177] Processed 14000 files.
E1029 16:22:32.749186 18277 convert_imageset.cpp:177] Processed 15000 files.
E1029 16:22:34.738240 18277 convert_imageset.cpp:177] Processed 16000 files.
E1029 16:22:36.747352 18277 convert_imageset.cpp:177] Processed 17000 files.
E1029 16:22:38.641937 18277 convert_imageset.cpp:177] Processed 18000 files.
E1029 16:22:40.486120 18277 convert_imageset.cpp:177] Processed 19000 files.
E1029 16:22:42.361822 18277 convert_imageset.cpp:177] Processed 20000 files.
E1029 16:22:44.393012 18277 convert_imageset.cpp:177] Processed 21000 files.
E1029 16:22:46.261831 18277 convert_imageset.cpp:177] Processed 22000 files.
E1029 16:22:48.194182 18277 convert_imageset.cpp:177] Processed 23000 files.
E1029 16:22:50.067462 18277 convert_imageset.cpp:177] Processed 24000 files.
E1029 16:22:51.861429 18277 convert_imageset.cpp:177] Processed 25000 files.
E1029 16:22:53.682601 18277 convert_imageset.cpp:177] Processed 26000 files.
E1029 16:22:55.487434 18277 convert_imageset.cpp:177] Processed 27000 files.
E1029 16:22:57.353525 18277 convert_imageset.cpp:177] Processed 28000 files.
E1029 16:22:59.224647 18277 convert_imageset.cpp:177] Processed 29000 files.
E1029 16:23:00.954020 18277 convert_imageset.cpp:177] Processed 30000 files.
E1029 16:23:02.765365 18277 convert_imageset.cpp:177] Processed 31000 files.
E1029 16:23:04.465656 18277 convert_imageset.cpp:177] Processed 32000 files.
E1029 16:23:06.324586 18277 convert_imageset.cpp:177] Processed 33000 files.
E1029 16:23:08.233001 18277 convert_imageset.cpp:177] Processed 34000 files.
E1029 16:23:10.095021 18277 convert_imageset.cpp:177] Processed 35000 files.
E1029 16:23:11.772003 18277 convert_imageset.cpp:177] Processed 36000 files.
E1029 16:23:13.438344 18277 convert_imageset.cpp:177] Processed 37000 files.
E1029 16:23:15.113284 18277 convert_imageset.cpp:177] Processed 38000 files.
E1029 16:23:16.785182 18277 convert_imageset.cpp:177] Processed 39000 files.
E1029 16:23:18.542855 18277 convert_imageset.cpp:177] Processed 40000 files.
E1029 16:23:20.222836 18277 convert_imageset.cpp:177] Processed 41000 files.
E1029 16:23:21.846194 18277 convert_imageset.cpp:177] Processed 42000 files.
E1029 16:23:23.512187 18277 convert_imageset.cpp:177] Processed 43000 files.
E1029 16:23:25.233923 18277 convert_imageset.cpp:177] Processed 44000 files.
E1029 16:23:27.021883 18277 convert_imageset.cpp:177] Processed 45000 files.
E1029 16:23:28.744632 18277 convert_imageset.cpp:177] Processed 46000 files.
E1029 16:23:30.406282 18277 convert_imageset.cpp:177] Processed 47000 files.
E1029 16:23:32.080144 18277 convert_imageset.cpp:177] Processed 48000 files.
E1029 16:23:33.732635 18277 convert_imageset.cpp:177] Processed 49000 files.
E1029 16:23:35.441408 18277 convert_imageset.cpp:177] Processed 50000 files.
E1029 16:23:36.967576 18277 convert_imageset.cpp:177] Processed 51000 files.
E1029 16:23:38.521507 18277 convert_imageset.cpp:177] Processed 52000 files.
E1029 16:23:40.111491 18277 convert_imageset.cpp:177] Processed 53000 files.
E1029 16:23:41.612237 18277 convert_imageset.cpp:177] Processed 54000 files.
E1029 16:23:43.367305 18277 convert_imageset.cpp:177] Processed 55000 files.
E1029 16:23:44.984333 18277 convert_imageset.cpp:177] Processed 56000 files.
E1029 16:23:46.446621 18277 convert_imageset.cpp:177] Processed 57000 files.
E1029 16:23:48.078578 18277 convert_imageset.cpp:177] Processed 58000 files.
E1029 16:23:49.638720 18277 convert_imageset.cpp:177] Processed 59000 files.
E1029 16:23:51.114985 18277 convert_imageset.cpp:177] Processed 60000 files.
I1029 16:23:51.287727 18379 caffe.cpp:99] Use GPU with device ID 0
I1029 16:23:51.631810 18379 caffe.cpp:107] Starting Optimization
I1029 16:23:51.631933 18379 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 515000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 16:23:51.631959 18379 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 16:23:51.645355 18379 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 16:23:51.645453 18379 net.cpp:67] Creating Layer mnist
I1029 16:23:51.645464 18379 net.cpp:356] mnist -> data
I1029 16:23:51.645481 18379 net.cpp:356] mnist -> label
I1029 16:23:51.645494 18379 net.cpp:96] Setting up mnist
I1029 16:23:51.661409 18379 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 16:23:51.661542 18379 data_layer.cpp:128] output data size: 64,1,50,180
I1029 16:23:51.663182 18379 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 16:23:51.663219 18379 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 16:23:51.663247 18379 net.cpp:67] Creating Layer conv1
I1029 16:23:51.663264 18379 net.cpp:394] conv1 <- data
I1029 16:23:51.663295 18379 net.cpp:356] conv1 -> conv1
I1029 16:23:51.663326 18379 net.cpp:96] Setting up conv1
I1029 16:23:51.664250 18379 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 16:23:51.664319 18379 net.cpp:67] Creating Layer pool1
I1029 16:23:51.664336 18379 net.cpp:394] pool1 <- conv1
I1029 16:23:51.664353 18379 net.cpp:356] pool1 -> pool1
I1029 16:23:51.664373 18379 net.cpp:96] Setting up pool1
I1029 16:23:51.664405 18379 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 16:23:51.664466 18379 net.cpp:67] Creating Layer relu1
I1029 16:23:51.664484 18379 net.cpp:394] relu1 <- pool1
I1029 16:23:51.664500 18379 net.cpp:345] relu1 -> pool1 (in-place)
I1029 16:23:51.664520 18379 net.cpp:96] Setting up relu1
I1029 16:23:51.664532 18379 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 16:23:51.664551 18379 net.cpp:67] Creating Layer drop1
I1029 16:23:51.664563 18379 net.cpp:394] drop1 <- pool1
I1029 16:23:51.664579 18379 net.cpp:345] drop1 -> pool1 (in-place)
I1029 16:23:51.664597 18379 net.cpp:96] Setting up drop1
I1029 16:23:51.664611 18379 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 16:23:51.664634 18379 net.cpp:67] Creating Layer conv2
I1029 16:23:51.664647 18379 net.cpp:394] conv2 <- pool1
I1029 16:23:51.664669 18379 net.cpp:356] conv2 -> conv2
I1029 16:23:51.664690 18379 net.cpp:96] Setting up conv2
I1029 16:23:51.666227 18379 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 16:23:51.666270 18379 net.cpp:67] Creating Layer pool2
I1029 16:23:51.666285 18379 net.cpp:394] pool2 <- conv2
I1029 16:23:51.666303 18379 net.cpp:356] pool2 -> pool2
I1029 16:23:51.666322 18379 net.cpp:96] Setting up pool2
I1029 16:23:51.666338 18379 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 16:23:51.666354 18379 net.cpp:67] Creating Layer relu2
I1029 16:23:51.666366 18379 net.cpp:394] relu2 <- pool2
I1029 16:23:51.666383 18379 net.cpp:345] relu2 -> pool2 (in-place)
I1029 16:23:51.666399 18379 net.cpp:96] Setting up relu2
I1029 16:23:51.666412 18379 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 16:23:51.666435 18379 net.cpp:67] Creating Layer drop2
I1029 16:23:51.666450 18379 net.cpp:394] drop2 <- pool2
I1029 16:23:51.666465 18379 net.cpp:345] drop2 -> pool2 (in-place)
I1029 16:23:51.666491 18379 net.cpp:96] Setting up drop2
I1029 16:23:51.666506 18379 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 16:23:51.666529 18379 net.cpp:67] Creating Layer conv3
I1029 16:23:51.666543 18379 net.cpp:394] conv3 <- pool2
I1029 16:23:51.666560 18379 net.cpp:356] conv3 -> conv3
I1029 16:23:51.666579 18379 net.cpp:96] Setting up conv3
I1029 16:23:51.670621 18379 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 16:23:51.670667 18379 net.cpp:67] Creating Layer pool3
I1029 16:23:51.670681 18379 net.cpp:394] pool3 <- conv3
I1029 16:23:51.670704 18379 net.cpp:356] pool3 -> pool3
I1029 16:23:51.670724 18379 net.cpp:96] Setting up pool3
I1029 16:23:51.670740 18379 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 16:23:51.670758 18379 net.cpp:67] Creating Layer relu3
I1029 16:23:51.670769 18379 net.cpp:394] relu3 <- pool3
I1029 16:23:51.670785 18379 net.cpp:345] relu3 -> pool3 (in-place)
I1029 16:23:51.670801 18379 net.cpp:96] Setting up relu3
I1029 16:23:51.670814 18379 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 16:23:51.670835 18379 net.cpp:67] Creating Layer drop3
I1029 16:23:51.670847 18379 net.cpp:394] drop3 <- pool3
I1029 16:23:51.670863 18379 net.cpp:345] drop3 -> pool3 (in-place)
I1029 16:23:51.670881 18379 net.cpp:96] Setting up drop3
I1029 16:23:51.670894 18379 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 16:23:51.670912 18379 net.cpp:67] Creating Layer ip1
I1029 16:23:51.670925 18379 net.cpp:394] ip1 <- pool3
I1029 16:23:51.670946 18379 net.cpp:356] ip1 -> ip1
I1029 16:23:51.671010 18379 net.cpp:96] Setting up ip1
I1029 16:23:52.064415 18379 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 16:23:52.064492 18379 net.cpp:67] Creating Layer relu4
I1029 16:23:52.064501 18379 net.cpp:394] relu4 <- ip1
I1029 16:23:52.064509 18379 net.cpp:345] relu4 -> ip1 (in-place)
I1029 16:23:52.064518 18379 net.cpp:96] Setting up relu4
I1029 16:23:52.064524 18379 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 16:23:52.064532 18379 net.cpp:67] Creating Layer drop4
I1029 16:23:52.064535 18379 net.cpp:394] drop4 <- ip1
I1029 16:23:52.064541 18379 net.cpp:345] drop4 -> ip1 (in-place)
I1029 16:23:52.064548 18379 net.cpp:96] Setting up drop4
I1029 16:23:52.064553 18379 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 16:23:52.064565 18379 net.cpp:67] Creating Layer ip2
I1029 16:23:52.064570 18379 net.cpp:394] ip2 <- ip1
I1029 16:23:52.064577 18379 net.cpp:356] ip2 -> ip2
I1029 16:23:52.064585 18379 net.cpp:96] Setting up ip2
I1029 16:23:52.075182 18379 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 16:23:52.075254 18379 net.cpp:67] Creating Layer loss
I1029 16:23:52.075261 18379 net.cpp:394] loss <- ip2
I1029 16:23:52.075270 18379 net.cpp:394] loss <- label
I1029 16:23:52.075276 18379 net.cpp:356] loss -> loss
I1029 16:23:52.075286 18379 net.cpp:96] Setting up loss
I1029 16:23:52.075299 18379 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 16:23:52.075304 18379 net.cpp:109]     with loss weight 1
I1029 16:23:52.075340 18379 net.cpp:170] loss needs backward computation.
I1029 16:23:52.075345 18379 net.cpp:170] ip2 needs backward computation.
I1029 16:23:52.075348 18379 net.cpp:170] drop4 needs backward computation.
I1029 16:23:52.075353 18379 net.cpp:170] relu4 needs backward computation.
I1029 16:23:52.075357 18379 net.cpp:170] ip1 needs backward computation.
I1029 16:23:52.075362 18379 net.cpp:170] drop3 needs backward computation.
I1029 16:23:52.075366 18379 net.cpp:170] relu3 needs backward computation.
I1029 16:23:52.075371 18379 net.cpp:170] pool3 needs backward computation.
I1029 16:23:52.075376 18379 net.cpp:170] conv3 needs backward computation.
I1029 16:23:52.075379 18379 net.cpp:170] drop2 needs backward computation.
I1029 16:23:52.075384 18379 net.cpp:170] relu2 needs backward computation.
I1029 16:23:52.075388 18379 net.cpp:170] pool2 needs backward computation.
I1029 16:23:52.075393 18379 net.cpp:170] conv2 needs backward computation.
I1029 16:23:52.075397 18379 net.cpp:170] drop1 needs backward computation.
I1029 16:23:52.075402 18379 net.cpp:170] relu1 needs backward computation.
I1029 16:23:52.075413 18379 net.cpp:170] pool1 needs backward computation.
I1029 16:23:52.075418 18379 net.cpp:170] conv1 needs backward computation.
I1029 16:23:52.075423 18379 net.cpp:172] mnist does not need backward computation.
I1029 16:23:52.075428 18379 net.cpp:208] This network produces output loss
I1029 16:23:52.075438 18379 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 16:23:52.075444 18379 net.cpp:219] Network initialization done.
I1029 16:23:52.075448 18379 net.cpp:220] Memory required for data: 119788292
I1029 16:23:52.075508 18379 solver.cpp:41] Solver scaffolding done.
I1029 16:23:52.075515 18379 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_510000.solverstate
I1029 16:23:52.075520 18379 solver.cpp:160] Solving Captcha
I1029 16:23:52.075538 18379 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_510000.solverstate
I1029 16:23:57.322407 18379 solver.cpp:502] SGDSolver: restoring history
I1029 16:23:58.061046 18379 solver.cpp:191] Iteration 510000, loss = 2.42705
I1029 16:23:58.061106 18379 solver.cpp:206]     Train net output #0: loss = 2.42705 (* 1 = 2.42705 loss)
I1029 16:23:58.061121 18379 solver.cpp:403] Iteration 510000, lr = 0.000516413
I1029 16:28:00.452445 18379 solver.cpp:191] Iteration 511000, loss = 2.71715
I1029 16:28:00.453052 18379 solver.cpp:206]     Train net output #0: loss = 2.71715 (* 1 = 2.71715 loss)
I1029 16:28:00.453090 18379 solver.cpp:403] Iteration 511000, lr = 0.00051567
I1029 16:32:01.485733 18379 solver.cpp:191] Iteration 512000, loss = 2.35855
I1029 16:32:01.489789 18379 solver.cpp:206]     Train net output #0: loss = 2.35855 (* 1 = 2.35855 loss)
I1029 16:32:01.489822 18379 solver.cpp:403] Iteration 512000, lr = 0.000514929
I1029 16:36:02.567929 18379 solver.cpp:191] Iteration 513000, loss = 2.33357
I1029 16:36:02.568637 18379 solver.cpp:206]     Train net output #0: loss = 2.33357 (* 1 = 2.33357 loss)
I1029 16:36:02.568670 18379 solver.cpp:403] Iteration 513000, lr = 0.00051419
I1029 16:40:03.699707 18379 solver.cpp:191] Iteration 514000, loss = 2.33652
I1029 16:40:03.700304 18379 solver.cpp:206]     Train net output #0: loss = 2.33652 (* 1 = 2.33652 loss)
I1029 16:40:03.700341 18379 solver.cpp:403] Iteration 514000, lr = 0.000513454
I1029 16:44:05.235093 18379 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_515000.caffemodel
I1029 16:44:09.569370 18379 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_515000.solverstate
I1029 16:44:13.189733 18379 solver.cpp:228] Iteration 515000, loss = 2.26918
I1029 16:44:13.190275 18379 solver.cpp:233] Optimization Done.
I1029 16:44:13.190284 18379 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 17:07:09.494061 32363 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 17:07:09.494623 32363 net.cpp:358] Input 0 -> data
I1029 17:07:09.494660 32363 net.cpp:67] Creating Layer conv1
I1029 17:07:09.494670 32363 net.cpp:394] conv1 <- data
I1029 17:07:09.494680 32363 net.cpp:356] conv1 -> conv1
I1029 17:07:09.494691 32363 net.cpp:96] Setting up conv1
I1029 17:07:09.495025 32363 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 17:07:09.495046 32363 net.cpp:67] Creating Layer pool1
I1029 17:07:09.495053 32363 net.cpp:394] pool1 <- conv1
I1029 17:07:09.495059 32363 net.cpp:356] pool1 -> pool1
I1029 17:07:09.495066 32363 net.cpp:96] Setting up pool1
I1029 17:07:09.495079 32363 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 17:07:09.495086 32363 net.cpp:67] Creating Layer relu1
I1029 17:07:09.495090 32363 net.cpp:394] relu1 <- pool1
I1029 17:07:09.495095 32363 net.cpp:345] relu1 -> pool1 (in-place)
I1029 17:07:09.495101 32363 net.cpp:96] Setting up relu1
I1029 17:07:09.495106 32363 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 17:07:09.495115 32363 net.cpp:67] Creating Layer drop1
I1029 17:07:09.495118 32363 net.cpp:394] drop1 <- pool1
I1029 17:07:09.495124 32363 net.cpp:345] drop1 -> pool1 (in-place)
I1029 17:07:09.495131 32363 net.cpp:96] Setting up drop1
I1029 17:07:09.495136 32363 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 17:07:09.495143 32363 net.cpp:67] Creating Layer conv2
I1029 17:07:09.495147 32363 net.cpp:394] conv2 <- pool1
I1029 17:07:09.495153 32363 net.cpp:356] conv2 -> conv2
I1029 17:07:09.495160 32363 net.cpp:96] Setting up conv2
I1029 17:07:09.495736 32363 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 17:07:09.495753 32363 net.cpp:67] Creating Layer pool2
I1029 17:07:09.495757 32363 net.cpp:394] pool2 <- conv2
I1029 17:07:09.495762 32363 net.cpp:356] pool2 -> pool2
I1029 17:07:09.495770 32363 net.cpp:96] Setting up pool2
I1029 17:07:09.495776 32363 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 17:07:09.495784 32363 net.cpp:67] Creating Layer relu2
I1029 17:07:09.495790 32363 net.cpp:394] relu2 <- pool2
I1029 17:07:09.495795 32363 net.cpp:345] relu2 -> pool2 (in-place)
I1029 17:07:09.495800 32363 net.cpp:96] Setting up relu2
I1029 17:07:09.495805 32363 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 17:07:09.495811 32363 net.cpp:67] Creating Layer drop2
I1029 17:07:09.495816 32363 net.cpp:394] drop2 <- pool2
I1029 17:07:09.495822 32363 net.cpp:345] drop2 -> pool2 (in-place)
I1029 17:07:09.495831 32363 net.cpp:96] Setting up drop2
I1029 17:07:09.495836 32363 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 17:07:09.495842 32363 net.cpp:67] Creating Layer conv3
I1029 17:07:09.495847 32363 net.cpp:394] conv3 <- pool2
I1029 17:07:09.495856 32363 net.cpp:356] conv3 -> conv3
I1029 17:07:09.495862 32363 net.cpp:96] Setting up conv3
I1029 17:07:09.497416 32363 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 17:07:09.497438 32363 net.cpp:67] Creating Layer pool3
I1029 17:07:09.497444 32363 net.cpp:394] pool3 <- conv3
I1029 17:07:09.497450 32363 net.cpp:356] pool3 -> pool3
I1029 17:07:09.497458 32363 net.cpp:96] Setting up pool3
I1029 17:07:09.497463 32363 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 17:07:09.497472 32363 net.cpp:67] Creating Layer relu3
I1029 17:07:09.497476 32363 net.cpp:394] relu3 <- pool3
I1029 17:07:09.497481 32363 net.cpp:345] relu3 -> pool3 (in-place)
I1029 17:07:09.497488 32363 net.cpp:96] Setting up relu3
I1029 17:07:09.497493 32363 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 17:07:09.497498 32363 net.cpp:67] Creating Layer drop3
I1029 17:07:09.497503 32363 net.cpp:394] drop3 <- pool3
I1029 17:07:09.497508 32363 net.cpp:345] drop3 -> pool3 (in-place)
I1029 17:07:09.497514 32363 net.cpp:96] Setting up drop3
I1029 17:07:09.497519 32363 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 17:07:09.497527 32363 net.cpp:67] Creating Layer ip1
I1029 17:07:09.497532 32363 net.cpp:394] ip1 <- pool3
I1029 17:07:09.497539 32363 net.cpp:356] ip1 -> ip1
I1029 17:07:09.497545 32363 net.cpp:96] Setting up ip1
I1029 17:07:10.003762 32363 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 17:07:10.003821 32363 net.cpp:67] Creating Layer relu4
I1029 17:07:10.003829 32363 net.cpp:394] relu4 <- ip1
I1029 17:07:10.003839 32363 net.cpp:345] relu4 -> ip1 (in-place)
I1029 17:07:10.003847 32363 net.cpp:96] Setting up relu4
I1029 17:07:10.003852 32363 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 17:07:10.003859 32363 net.cpp:67] Creating Layer drop4
I1029 17:07:10.003864 32363 net.cpp:394] drop4 <- ip1
I1029 17:07:10.003871 32363 net.cpp:345] drop4 -> ip1 (in-place)
I1029 17:07:10.003878 32363 net.cpp:96] Setting up drop4
I1029 17:07:10.003885 32363 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 17:07:10.003892 32363 net.cpp:67] Creating Layer ip2
I1029 17:07:10.003896 32363 net.cpp:394] ip2 <- ip1
I1029 17:07:10.003904 32363 net.cpp:356] ip2 -> ip2
I1029 17:07:10.003916 32363 net.cpp:96] Setting up ip2
I1029 17:07:10.012204 32363 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 17:07:10.012276 32363 net.cpp:67] Creating Layer prob
I1029 17:07:10.012284 32363 net.cpp:394] prob <- ip2
I1029 17:07:10.012292 32363 net.cpp:356] prob -> prob
I1029 17:07:10.012303 32363 net.cpp:96] Setting up prob
I1029 17:07:10.012310 32363 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 17:07:10.012313 32363 net.cpp:172] prob does not need backward computation.
I1029 17:07:10.012317 32363 net.cpp:172] ip2 does not need backward computation.
I1029 17:07:10.012321 32363 net.cpp:172] drop4 does not need backward computation.
I1029 17:07:10.012326 32363 net.cpp:172] relu4 does not need backward computation.
I1029 17:07:10.012328 32363 net.cpp:172] ip1 does not need backward computation.
I1029 17:07:10.012332 32363 net.cpp:172] drop3 does not need backward computation.
I1029 17:07:10.012336 32363 net.cpp:172] relu3 does not need backward computation.
I1029 17:07:10.012339 32363 net.cpp:172] pool3 does not need backward computation.
I1029 17:07:10.012343 32363 net.cpp:172] conv3 does not need backward computation.
I1029 17:07:10.012347 32363 net.cpp:172] drop2 does not need backward computation.
I1029 17:07:10.012351 32363 net.cpp:172] relu2 does not need backward computation.
I1029 17:07:10.012354 32363 net.cpp:172] pool2 does not need backward computation.
I1029 17:07:10.012358 32363 net.cpp:172] conv2 does not need backward computation.
I1029 17:07:10.012362 32363 net.cpp:172] drop1 does not need backward computation.
I1029 17:07:10.012365 32363 net.cpp:172] relu1 does not need backward computation.
I1029 17:07:10.012369 32363 net.cpp:172] pool1 does not need backward computation.
I1029 17:07:10.012380 32363 net.cpp:172] conv1 does not need backward computation.
I1029 17:07:10.012384 32363 net.cpp:208] This network produces output prob
I1029 17:07:10.012398 32363 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 17:07:10.012406 32363 net.cpp:219] Network initialization done.
I1029 17:07:10.012410 32363 net.cpp:220] Memory required for data: 1837200
I1029 17:41:42.948117  8676 convert_imageset.cpp:70] Shuffling data
I1029 17:41:43.605233  8676 convert_imageset.cpp:73] A total of 60000 images.
I1029 17:41:43.605312  8676 convert_imageset.cpp:104] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
E1029 17:41:45.687038  8676 convert_imageset.cpp:177] Processed 1000 files.
E1029 17:41:47.965144  8676 convert_imageset.cpp:177] Processed 2000 files.
E1029 17:41:49.901037  8676 convert_imageset.cpp:177] Processed 3000 files.
E1029 17:41:51.816537  8676 convert_imageset.cpp:177] Processed 4000 files.
E1029 17:41:54.376176  8676 convert_imageset.cpp:177] Processed 5000 files.
E1029 17:41:56.078569  8676 convert_imageset.cpp:177] Processed 6000 files.
E1029 17:41:58.030444  8676 convert_imageset.cpp:177] Processed 7000 files.
E1029 17:41:59.911634  8676 convert_imageset.cpp:177] Processed 8000 files.
E1029 17:42:01.710106  8676 convert_imageset.cpp:177] Processed 9000 files.
E1029 17:42:03.594584  8676 convert_imageset.cpp:177] Processed 10000 files.
E1029 17:42:05.660034  8676 convert_imageset.cpp:177] Processed 11000 files.
E1029 17:42:07.523526  8676 convert_imageset.cpp:177] Processed 12000 files.
E1029 17:42:09.275568  8676 convert_imageset.cpp:177] Processed 13000 files.
E1029 17:42:11.518026  8676 convert_imageset.cpp:177] Processed 14000 files.
E1029 17:42:13.317380  8676 convert_imageset.cpp:177] Processed 15000 files.
E1029 17:42:15.026433  8676 convert_imageset.cpp:177] Processed 16000 files.
E1029 17:42:16.813148  8676 convert_imageset.cpp:177] Processed 17000 files.
E1029 17:42:18.621975  8676 convert_imageset.cpp:177] Processed 18000 files.
E1029 17:42:20.398248  8676 convert_imageset.cpp:177] Processed 19000 files.
E1029 17:42:22.134115  8676 convert_imageset.cpp:177] Processed 20000 files.
E1029 17:42:23.841022  8676 convert_imageset.cpp:177] Processed 21000 files.
E1029 17:42:25.647083  8676 convert_imageset.cpp:177] Processed 22000 files.
E1029 17:42:27.458761  8676 convert_imageset.cpp:177] Processed 23000 files.
E1029 17:42:29.127619  8676 convert_imageset.cpp:177] Processed 24000 files.
E1029 17:42:30.783543  8676 convert_imageset.cpp:177] Processed 25000 files.
E1029 17:42:32.419579  8676 convert_imageset.cpp:177] Processed 26000 files.
E1029 17:42:34.071432  8676 convert_imageset.cpp:177] Processed 27000 files.
E1029 17:42:35.790968  8676 convert_imageset.cpp:177] Processed 28000 files.
E1029 17:42:37.411972  8676 convert_imageset.cpp:177] Processed 29000 files.
E1029 17:42:39.117655  8676 convert_imageset.cpp:177] Processed 30000 files.
E1029 17:42:40.893409  8676 convert_imageset.cpp:177] Processed 31000 files.
E1029 17:42:42.546380  8676 convert_imageset.cpp:177] Processed 32000 files.
E1029 17:42:44.208983  8676 convert_imageset.cpp:177] Processed 33000 files.
E1029 17:42:45.818100  8676 convert_imageset.cpp:177] Processed 34000 files.
E1029 17:42:47.501107  8676 convert_imageset.cpp:177] Processed 35000 files.
E1029 17:42:49.164628  8676 convert_imageset.cpp:177] Processed 36000 files.
E1029 17:42:50.821791  8676 convert_imageset.cpp:177] Processed 37000 files.
E1029 17:42:52.510218  8676 convert_imageset.cpp:177] Processed 38000 files.
E1029 17:42:54.218330  8676 convert_imageset.cpp:177] Processed 39000 files.
E1029 17:42:55.908568  8676 convert_imageset.cpp:177] Processed 40000 files.
E1029 17:42:57.475834  8676 convert_imageset.cpp:177] Processed 41000 files.
E1029 17:42:59.035609  8676 convert_imageset.cpp:177] Processed 42000 files.
E1029 17:43:00.599989  8676 convert_imageset.cpp:177] Processed 43000 files.
E1029 17:43:02.437849  8676 convert_imageset.cpp:177] Processed 44000 files.
E1029 17:43:04.325697  8676 convert_imageset.cpp:177] Processed 45000 files.
E1029 17:43:05.915489  8676 convert_imageset.cpp:177] Processed 46000 files.
E1029 17:43:07.417491  8676 convert_imageset.cpp:177] Processed 47000 files.
E1029 17:43:08.982719  8676 convert_imageset.cpp:177] Processed 48000 files.
E1029 17:43:10.639562  8676 convert_imageset.cpp:177] Processed 49000 files.
E1029 17:43:12.188213  8676 convert_imageset.cpp:177] Processed 50000 files.
E1029 17:43:13.698302  8676 convert_imageset.cpp:177] Processed 51000 files.
E1029 17:43:15.185602  8676 convert_imageset.cpp:177] Processed 52000 files.
E1029 17:43:16.825440  8676 convert_imageset.cpp:177] Processed 53000 files.
E1029 17:43:18.444926  8676 convert_imageset.cpp:177] Processed 54000 files.
E1029 17:43:19.995735  8676 convert_imageset.cpp:177] Processed 55000 files.
E1029 17:43:21.462887  8676 convert_imageset.cpp:177] Processed 56000 files.
E1029 17:43:24.252846  8676 convert_imageset.cpp:177] Processed 57000 files.
E1029 17:43:26.407428  8676 convert_imageset.cpp:177] Processed 58000 files.
E1029 17:43:28.435443  8676 convert_imageset.cpp:177] Processed 59000 files.
E1029 17:43:30.056696  8676 convert_imageset.cpp:177] Processed 60000 files.
I1029 17:43:30.276464  8776 caffe.cpp:99] Use GPU with device ID 0
I1029 17:43:30.646436  8776 caffe.cpp:107] Starting Optimization
I1029 17:43:30.646549  8776 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 1000
max_iter: 520000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data"
solver_mode: GPU
net: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt"
I1029 17:43:30.646574  8776 solver.cpp:67] Creating training net from net file: temp/05-3_layers-10000_initial_images-correct_mostuncertain/network_captchas_with_3_convolutional_layers_train.prototxt
I1029 17:43:30.660150  8776 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db"
    batch_size: 64
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  top: "loss"
  name: "loss"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I1029 17:43:30.660377  8776 net.cpp:67] Creating Layer mnist
I1029 17:43:30.660403  8776 net.cpp:356] mnist -> data
I1029 17:43:30.660483  8776 net.cpp:356] mnist -> label
I1029 17:43:30.660521  8776 net.cpp:96] Setting up mnist
I1029 17:43:30.668128  8776 data_layer.cpp:68] Opening lmdb temp/05-3_layers-10000_initial_images-correct_mostuncertain/train_db
I1029 17:43:30.668262  8776 data_layer.cpp:128] output data size: 64,1,50,180
I1029 17:43:30.669987  8776 net.cpp:103] Top shape: 64 1 50 180 (576000)
I1029 17:43:30.670027  8776 net.cpp:103] Top shape: 64 1 1 1 (64)
I1029 17:43:30.670055  8776 net.cpp:67] Creating Layer conv1
I1029 17:43:30.670070  8776 net.cpp:394] conv1 <- data
I1029 17:43:30.670104  8776 net.cpp:356] conv1 -> conv1
I1029 17:43:30.670130  8776 net.cpp:96] Setting up conv1
I1029 17:43:30.670497  8776 net.cpp:103] Top shape: 64 48 25 90 (6912000)
I1029 17:43:30.670532  8776 net.cpp:67] Creating Layer pool1
I1029 17:43:30.670539  8776 net.cpp:394] pool1 <- conv1
I1029 17:43:30.670547  8776 net.cpp:356] pool1 -> pool1
I1029 17:43:30.670555  8776 net.cpp:96] Setting up pool1
I1029 17:43:30.670572  8776 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 17:43:30.670579  8776 net.cpp:67] Creating Layer relu1
I1029 17:43:30.670584  8776 net.cpp:394] relu1 <- pool1
I1029 17:43:30.670590  8776 net.cpp:345] relu1 -> pool1 (in-place)
I1029 17:43:30.670598  8776 net.cpp:96] Setting up relu1
I1029 17:43:30.670603  8776 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 17:43:30.670609  8776 net.cpp:67] Creating Layer drop1
I1029 17:43:30.670614  8776 net.cpp:394] drop1 <- pool1
I1029 17:43:30.670622  8776 net.cpp:345] drop1 -> pool1 (in-place)
I1029 17:43:30.670629  8776 net.cpp:96] Setting up drop1
I1029 17:43:30.670635  8776 net.cpp:103] Top shape: 64 48 13 45 (1797120)
I1029 17:43:30.670642  8776 net.cpp:67] Creating Layer conv2
I1029 17:43:30.670647  8776 net.cpp:394] conv2 <- pool1
I1029 17:43:30.670655  8776 net.cpp:356] conv2 -> conv2
I1029 17:43:30.670663  8776 net.cpp:96] Setting up conv2
I1029 17:43:30.671246  8776 net.cpp:103] Top shape: 64 64 13 45 (2396160)
I1029 17:43:30.671264  8776 net.cpp:67] Creating Layer pool2
I1029 17:43:30.671270  8776 net.cpp:394] pool2 <- conv2
I1029 17:43:30.671277  8776 net.cpp:356] pool2 -> pool2
I1029 17:43:30.671284  8776 net.cpp:96] Setting up pool2
I1029 17:43:30.671290  8776 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 17:43:30.671298  8776 net.cpp:67] Creating Layer relu2
I1029 17:43:30.671301  8776 net.cpp:394] relu2 <- pool2
I1029 17:43:30.671310  8776 net.cpp:345] relu2 -> pool2 (in-place)
I1029 17:43:30.671317  8776 net.cpp:96] Setting up relu2
I1029 17:43:30.671321  8776 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 17:43:30.671329  8776 net.cpp:67] Creating Layer drop2
I1029 17:43:30.671334  8776 net.cpp:394] drop2 <- pool2
I1029 17:43:30.671341  8776 net.cpp:345] drop2 -> pool2 (in-place)
I1029 17:43:30.671349  8776 net.cpp:96] Setting up drop2
I1029 17:43:30.671353  8776 net.cpp:103] Top shape: 64 64 12 44 (2162688)
I1029 17:43:30.671361  8776 net.cpp:67] Creating Layer conv3
I1029 17:43:30.671372  8776 net.cpp:394] conv3 <- pool2
I1029 17:43:30.671380  8776 net.cpp:356] conv3 -> conv3
I1029 17:43:30.671387  8776 net.cpp:96] Setting up conv3
I1029 17:43:30.674269  8776 net.cpp:103] Top shape: 64 128 12 44 (4325376)
I1029 17:43:30.674321  8776 net.cpp:67] Creating Layer pool3
I1029 17:43:30.674336  8776 net.cpp:394] pool3 <- conv3
I1029 17:43:30.674360  8776 net.cpp:356] pool3 -> pool3
I1029 17:43:30.674381  8776 net.cpp:96] Setting up pool3
I1029 17:43:30.674396  8776 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 17:43:30.674413  8776 net.cpp:67] Creating Layer relu3
I1029 17:43:30.674427  8776 net.cpp:394] relu3 <- pool3
I1029 17:43:30.674446  8776 net.cpp:345] relu3 -> pool3 (in-place)
I1029 17:43:30.674464  8776 net.cpp:96] Setting up relu3
I1029 17:43:30.674477  8776 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 17:43:30.674494  8776 net.cpp:67] Creating Layer drop3
I1029 17:43:30.674507  8776 net.cpp:394] drop3 <- pool3
I1029 17:43:30.674525  8776 net.cpp:345] drop3 -> pool3 (in-place)
I1029 17:43:30.674541  8776 net.cpp:96] Setting up drop3
I1029 17:43:30.674556  8776 net.cpp:103] Top shape: 64 128 6 22 (1081344)
I1029 17:43:30.674574  8776 net.cpp:67] Creating Layer ip1
I1029 17:43:30.674587  8776 net.cpp:394] ip1 <- pool3
I1029 17:43:30.674608  8776 net.cpp:356] ip1 -> ip1
I1029 17:43:30.674672  8776 net.cpp:96] Setting up ip1
I1029 17:43:31.142657  8776 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 17:43:31.142719  8776 net.cpp:67] Creating Layer relu4
I1029 17:43:31.142727  8776 net.cpp:394] relu4 <- ip1
I1029 17:43:31.142736  8776 net.cpp:345] relu4 -> ip1 (in-place)
I1029 17:43:31.142746  8776 net.cpp:96] Setting up relu4
I1029 17:43:31.142752  8776 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 17:43:31.142760  8776 net.cpp:67] Creating Layer drop4
I1029 17:43:31.142765  8776 net.cpp:394] drop4 <- ip1
I1029 17:43:31.142772  8776 net.cpp:345] drop4 -> ip1 (in-place)
I1029 17:43:31.142781  8776 net.cpp:96] Setting up drop4
I1029 17:43:31.142786  8776 net.cpp:103] Top shape: 64 3072 1 1 (196608)
I1029 17:43:31.142796  8776 net.cpp:67] Creating Layer ip2
I1029 17:43:31.142802  8776 net.cpp:394] ip2 <- ip1
I1029 17:43:31.142809  8776 net.cpp:356] ip2 -> ip2
I1029 17:43:31.142818  8776 net.cpp:96] Setting up ip2
I1029 17:43:31.151875  8776 net.cpp:103] Top shape: 64 378 1 1 (24192)
I1029 17:43:31.151934  8776 net.cpp:67] Creating Layer loss
I1029 17:43:31.151942  8776 net.cpp:394] loss <- ip2
I1029 17:43:31.151948  8776 net.cpp:394] loss <- label
I1029 17:43:31.151955  8776 net.cpp:356] loss -> loss
I1029 17:43:31.151965  8776 net.cpp:96] Setting up loss
I1029 17:43:31.151978  8776 net.cpp:103] Top shape: 1 1 1 1 (1)
I1029 17:43:31.151983  8776 net.cpp:109]     with loss weight 1
I1029 17:43:31.152019  8776 net.cpp:170] loss needs backward computation.
I1029 17:43:31.152024  8776 net.cpp:170] ip2 needs backward computation.
I1029 17:43:31.152029  8776 net.cpp:170] drop4 needs backward computation.
I1029 17:43:31.152034  8776 net.cpp:170] relu4 needs backward computation.
I1029 17:43:31.152037  8776 net.cpp:170] ip1 needs backward computation.
I1029 17:43:31.152042  8776 net.cpp:170] drop3 needs backward computation.
I1029 17:43:31.152047  8776 net.cpp:170] relu3 needs backward computation.
I1029 17:43:31.152051  8776 net.cpp:170] pool3 needs backward computation.
I1029 17:43:31.152056  8776 net.cpp:170] conv3 needs backward computation.
I1029 17:43:31.152061  8776 net.cpp:170] drop2 needs backward computation.
I1029 17:43:31.152065  8776 net.cpp:170] relu2 needs backward computation.
I1029 17:43:31.152070  8776 net.cpp:170] pool2 needs backward computation.
I1029 17:43:31.152075  8776 net.cpp:170] conv2 needs backward computation.
I1029 17:43:31.152079  8776 net.cpp:170] drop1 needs backward computation.
I1029 17:43:31.152084  8776 net.cpp:170] relu1 needs backward computation.
I1029 17:43:31.152088  8776 net.cpp:170] pool1 needs backward computation.
I1029 17:43:31.152093  8776 net.cpp:170] conv1 needs backward computation.
I1029 17:43:31.152098  8776 net.cpp:172] mnist does not need backward computation.
I1029 17:43:31.152111  8776 net.cpp:208] This network produces output loss
I1029 17:43:31.152122  8776 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 17:43:31.152129  8776 net.cpp:219] Network initialization done.
I1029 17:43:31.152133  8776 net.cpp:220] Memory required for data: 119788292
I1029 17:43:31.152194  8776 solver.cpp:41] Solver scaffolding done.
I1029 17:43:31.152201  8776 caffe.cpp:112] Resuming from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_515000.solverstate
I1029 17:43:31.152206  8776 solver.cpp:160] Solving Captcha
I1029 17:43:31.152225  8776 solver.cpp:165] Restoring previous solver status from temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_515000.solverstate
I1029 17:43:34.665096  8776 solver.cpp:502] SGDSolver: restoring history
I1029 17:43:35.430817  8776 solver.cpp:191] Iteration 515000, loss = 2.54832
I1029 17:43:35.430876  8776 solver.cpp:206]     Train net output #0: loss = 2.54832 (* 1 = 2.54832 loss)
I1029 17:43:35.430893  8776 solver.cpp:403] Iteration 515000, lr = 0.00051272
I1029 17:47:37.186530  8776 solver.cpp:191] Iteration 516000, loss = 2.38598
I1029 17:47:37.187175  8776 solver.cpp:206]     Train net output #0: loss = 2.38598 (* 1 = 2.38598 loss)
I1029 17:47:37.187208  8776 solver.cpp:403] Iteration 516000, lr = 0.000511989
I1029 17:51:38.406613  8776 solver.cpp:191] Iteration 517000, loss = 2.482
I1029 17:51:38.407243  8776 solver.cpp:206]     Train net output #0: loss = 2.482 (* 1 = 2.482 loss)
I1029 17:51:38.407279  8776 solver.cpp:403] Iteration 517000, lr = 0.00051126
I1029 17:55:39.597507  8776 solver.cpp:191] Iteration 518000, loss = 2.37904
I1029 17:55:39.598268  8776 solver.cpp:206]     Train net output #0: loss = 2.37904 (* 1 = 2.37904 loss)
I1029 17:55:39.598300  8776 solver.cpp:403] Iteration 518000, lr = 0.000510534
I1029 17:59:40.688014  8776 solver.cpp:191] Iteration 519000, loss = 2.53892
I1029 17:59:40.688657  8776 solver.cpp:206]     Train net output #0: loss = 2.53892 (* 1 = 2.53892 loss)
I1029 17:59:40.688689  8776 solver.cpp:403] Iteration 519000, lr = 0.00050981
I1029 18:03:42.483404  8776 solver.cpp:317] Snapshotting to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_520000.caffemodel
I1029 18:03:47.607375  8776 solver.cpp:324] Snapshotting solver state to temp/05-3_layers-10000_initial_images-correct_mostuncertain/results/data_iter_520000.solverstate
I1029 18:03:51.655753  8776 solver.cpp:228] Iteration 520000, loss = 2.45683
I1029 18:03:51.656260  8776 solver.cpp:233] Optimization Done.
I1029 18:03:51.656283  8776 caffe.cpp:121] Optimization Done.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1029 18:26:26.962857 22611 net.cpp:39] Initializing net from parameters: 
name: "Captcha"
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "pool1"
  top: "pool1"
  name: "drop1"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 1
  }
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "pool2"
  top: "pool2"
  name: "drop2"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv3"
  top: "pool3"
  name: "pool3"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "pool3"
  top: "pool3"
  name: "drop3"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "pool3"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 3072
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "drop4"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 378
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
input: "data"
input_dim: 1
input_dim: 1
input_dim: 50
input_dim: 180
I1029 18:26:26.962960 22611 net.cpp:358] Input 0 -> data
I1029 18:26:26.962985 22611 net.cpp:67] Creating Layer conv1
I1029 18:26:26.962990 22611 net.cpp:394] conv1 <- data
I1029 18:26:26.962996 22611 net.cpp:356] conv1 -> conv1
I1029 18:26:26.963006 22611 net.cpp:96] Setting up conv1
I1029 18:26:26.963320 22611 net.cpp:103] Top shape: 1 48 25 90 (108000)
I1029 18:26:26.963340 22611 net.cpp:67] Creating Layer pool1
I1029 18:26:26.963345 22611 net.cpp:394] pool1 <- conv1
I1029 18:26:26.963351 22611 net.cpp:356] pool1 -> pool1
I1029 18:26:26.963357 22611 net.cpp:96] Setting up pool1
I1029 18:26:26.963371 22611 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 18:26:26.963379 22611 net.cpp:67] Creating Layer relu1
I1029 18:26:26.963383 22611 net.cpp:394] relu1 <- pool1
I1029 18:26:26.963388 22611 net.cpp:345] relu1 -> pool1 (in-place)
I1029 18:26:26.963394 22611 net.cpp:96] Setting up relu1
I1029 18:26:26.963398 22611 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 18:26:26.963407 22611 net.cpp:67] Creating Layer drop1
I1029 18:26:26.963412 22611 net.cpp:394] drop1 <- pool1
I1029 18:26:26.963417 22611 net.cpp:345] drop1 -> pool1 (in-place)
I1029 18:26:26.963423 22611 net.cpp:96] Setting up drop1
I1029 18:26:26.963428 22611 net.cpp:103] Top shape: 1 48 13 45 (28080)
I1029 18:26:26.963434 22611 net.cpp:67] Creating Layer conv2
I1029 18:26:26.963438 22611 net.cpp:394] conv2 <- pool1
I1029 18:26:26.963443 22611 net.cpp:356] conv2 -> conv2
I1029 18:26:26.963450 22611 net.cpp:96] Setting up conv2
I1029 18:26:26.964000 22611 net.cpp:103] Top shape: 1 64 13 45 (37440)
I1029 18:26:26.964015 22611 net.cpp:67] Creating Layer pool2
I1029 18:26:26.964020 22611 net.cpp:394] pool2 <- conv2
I1029 18:26:26.964025 22611 net.cpp:356] pool2 -> pool2
I1029 18:26:26.964032 22611 net.cpp:96] Setting up pool2
I1029 18:26:26.964037 22611 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 18:26:26.964046 22611 net.cpp:67] Creating Layer relu2
I1029 18:26:26.964049 22611 net.cpp:394] relu2 <- pool2
I1029 18:26:26.964056 22611 net.cpp:345] relu2 -> pool2 (in-place)
I1029 18:26:26.964061 22611 net.cpp:96] Setting up relu2
I1029 18:26:26.964064 22611 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 18:26:26.964071 22611 net.cpp:67] Creating Layer drop2
I1029 18:26:26.964076 22611 net.cpp:394] drop2 <- pool2
I1029 18:26:26.964081 22611 net.cpp:345] drop2 -> pool2 (in-place)
I1029 18:26:26.964087 22611 net.cpp:96] Setting up drop2
I1029 18:26:26.964090 22611 net.cpp:103] Top shape: 1 64 12 44 (33792)
I1029 18:26:26.964097 22611 net.cpp:67] Creating Layer conv3
I1029 18:26:26.964102 22611 net.cpp:394] conv3 <- pool2
I1029 18:26:26.964112 22611 net.cpp:356] conv3 -> conv3
I1029 18:26:26.964118 22611 net.cpp:96] Setting up conv3
I1029 18:26:26.965603 22611 net.cpp:103] Top shape: 1 128 12 44 (67584)
I1029 18:26:26.965622 22611 net.cpp:67] Creating Layer pool3
I1029 18:26:26.965627 22611 net.cpp:394] pool3 <- conv3
I1029 18:26:26.965633 22611 net.cpp:356] pool3 -> pool3
I1029 18:26:26.965639 22611 net.cpp:96] Setting up pool3
I1029 18:26:26.965644 22611 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 18:26:26.965652 22611 net.cpp:67] Creating Layer relu3
I1029 18:26:26.965656 22611 net.cpp:394] relu3 <- pool3
I1029 18:26:26.965662 22611 net.cpp:345] relu3 -> pool3 (in-place)
I1029 18:26:26.965667 22611 net.cpp:96] Setting up relu3
I1029 18:26:26.965672 22611 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 18:26:26.965677 22611 net.cpp:67] Creating Layer drop3
I1029 18:26:26.965680 22611 net.cpp:394] drop3 <- pool3
I1029 18:26:26.965685 22611 net.cpp:345] drop3 -> pool3 (in-place)
I1029 18:26:26.965692 22611 net.cpp:96] Setting up drop3
I1029 18:26:26.965695 22611 net.cpp:103] Top shape: 1 128 6 22 (16896)
I1029 18:26:26.965703 22611 net.cpp:67] Creating Layer ip1
I1029 18:26:26.965708 22611 net.cpp:394] ip1 <- pool3
I1029 18:26:26.965713 22611 net.cpp:356] ip1 -> ip1
I1029 18:26:26.965720 22611 net.cpp:96] Setting up ip1
I1029 18:26:27.430712 22611 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 18:26:27.430775 22611 net.cpp:67] Creating Layer relu4
I1029 18:26:27.430783 22611 net.cpp:394] relu4 <- ip1
I1029 18:26:27.430790 22611 net.cpp:345] relu4 -> ip1 (in-place)
I1029 18:26:27.430800 22611 net.cpp:96] Setting up relu4
I1029 18:26:27.430805 22611 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 18:26:27.430812 22611 net.cpp:67] Creating Layer drop4
I1029 18:26:27.430816 22611 net.cpp:394] drop4 <- ip1
I1029 18:26:27.430824 22611 net.cpp:345] drop4 -> ip1 (in-place)
I1029 18:26:27.430830 22611 net.cpp:96] Setting up drop4
I1029 18:26:27.430836 22611 net.cpp:103] Top shape: 1 3072 1 1 (3072)
I1029 18:26:27.430843 22611 net.cpp:67] Creating Layer ip2
I1029 18:26:27.430847 22611 net.cpp:394] ip2 <- ip1
I1029 18:26:27.430855 22611 net.cpp:356] ip2 -> ip2
I1029 18:26:27.430867 22611 net.cpp:96] Setting up ip2
I1029 18:26:27.441869 22611 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 18:26:27.441944 22611 net.cpp:67] Creating Layer prob
I1029 18:26:27.441952 22611 net.cpp:394] prob <- ip2
I1029 18:26:27.441961 22611 net.cpp:356] prob -> prob
I1029 18:26:27.441970 22611 net.cpp:96] Setting up prob
I1029 18:26:27.441977 22611 net.cpp:103] Top shape: 1 378 1 1 (378)
I1029 18:26:27.441982 22611 net.cpp:172] prob does not need backward computation.
I1029 18:26:27.441985 22611 net.cpp:172] ip2 does not need backward computation.
I1029 18:26:27.441988 22611 net.cpp:172] drop4 does not need backward computation.
I1029 18:26:27.441992 22611 net.cpp:172] relu4 does not need backward computation.
I1029 18:26:27.441997 22611 net.cpp:172] ip1 does not need backward computation.
I1029 18:26:27.441999 22611 net.cpp:172] drop3 does not need backward computation.
I1029 18:26:27.442003 22611 net.cpp:172] relu3 does not need backward computation.
I1029 18:26:27.442006 22611 net.cpp:172] pool3 does not need backward computation.
I1029 18:26:27.442010 22611 net.cpp:172] conv3 does not need backward computation.
I1029 18:26:27.442013 22611 net.cpp:172] drop2 does not need backward computation.
I1029 18:26:27.442018 22611 net.cpp:172] relu2 does not need backward computation.
I1029 18:26:27.442020 22611 net.cpp:172] pool2 does not need backward computation.
I1029 18:26:27.442024 22611 net.cpp:172] conv2 does not need backward computation.
I1029 18:26:27.442028 22611 net.cpp:172] drop1 does not need backward computation.
I1029 18:26:27.442030 22611 net.cpp:172] relu1 does not need backward computation.
I1029 18:26:27.442034 22611 net.cpp:172] pool1 does not need backward computation.
I1029 18:26:27.442037 22611 net.cpp:172] conv1 does not need backward computation.
I1029 18:26:27.442041 22611 net.cpp:208] This network produces output prob
I1029 18:26:27.442061 22611 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1029 18:26:27.442070 22611 net.cpp:219] Network initialization done.
I1029 18:26:27.442075 22611 net.cpp:220] Memory required for data: 1837200
